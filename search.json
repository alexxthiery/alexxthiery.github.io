[
  {
    "objectID": "notes_students/probability_basics/probability_basics.html",
    "href": "notes_students/probability_basics/probability_basics.html",
    "title": "Probability Basics",
    "section": "",
    "text": "The mean or expectation of a random variable \\(X\\) is denoted as \\(\\mathbb{E}[X]\\)l. For a discrete random variable,\n\\[\n\\mathbb{E}[X] = \\sum_{x} x \\cdot \\mathbb{P}(X = x)\n\\]\nand for a continuous random variable,\n\\[\n\\mu_X = \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\n\\]\nwhere \\(f(x)\\) is the probability density function of \\(X\\). It’s important to note that in some cases, particularly for distributions with “heavy” tails, the mean might not be well-defined. This situation arises in distributions like the Cauchy distribution, where the tails of the distribution do not decay rapidly enough to yield a finite expectation. In such cases, the integrals or sums used to define the mean do not converge.\nIndependence is a fundamental concept in probability theory, referring to the relationship between two random variables. Two random variables, \\(X\\) and \\(Y\\), are said to be independent if the occurrence of an event related to \\(X\\) does not influence the probability of an event related to \\(Y\\), and vice versa. Mathematically, \\(X\\) and \\(Y\\) are independent if and only if for every pair of events \\(A\\) and \\(B\\), the probability that both \\(X\\) belongs to \\(A\\) and \\(Y\\) belongs to \\(B\\) is the product of their individual probabilities. This can be expressed as:\n\\[\n\\mathbb{P}(X \\in A \\; \\text{ and } \\; Y \\in B) = \\mathbb{P}(X \\in A) \\cdot \\mathbb{P}(Y \\in B).\n\\]\nThis can equivalently be expressed as the fact that, for any two functions \\(F(\\cdot)\\) and \\(G(\\cdot)\\), the following identity holds\n\\[\n\\mathbb{E}[F(X) \\cdot G(Y)] \\; = \\; \\mathbb{E}[F(X)] \\cdot \\mathbb{E}[G(Y)].\n\\]\nThis definition implies that knowing the outcome of \\(X\\) provides no information about the outcome of \\(Y\\), and this lack of influence is a key characteristic of independent random variables. One extremely important remark is that, for two random variables \\(X\\) and \\(Y\\), the expectation of the sum equals the sum of the expectation,\n\\[\n\\mathbb{E}[X+Y] = \\mathbb{E}[X] + \\mathbb{E}[Y],\n\\]\nas soon as all these quantities exists. This holds even if the two random variables are not independent."
  },
  {
    "objectID": "notes_students/probability_basics/probability_basics.html#mean-expectation",
    "href": "notes_students/probability_basics/probability_basics.html#mean-expectation",
    "title": "Probability Basics",
    "section": "",
    "text": "The mean or expectation of a random variable \\(X\\) is denoted as \\(\\mathbb{E}[X]\\)l. For a discrete random variable,\n\\[\n\\mathbb{E}[X] = \\sum_{x} x \\cdot \\mathbb{P}(X = x)\n\\]\nand for a continuous random variable,\n\\[\n\\mu_X = \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\n\\]\nwhere \\(f(x)\\) is the probability density function of \\(X\\). It’s important to note that in some cases, particularly for distributions with “heavy” tails, the mean might not be well-defined. This situation arises in distributions like the Cauchy distribution, where the tails of the distribution do not decay rapidly enough to yield a finite expectation. In such cases, the integrals or sums used to define the mean do not converge.\nIndependence is a fundamental concept in probability theory, referring to the relationship between two random variables. Two random variables, \\(X\\) and \\(Y\\), are said to be independent if the occurrence of an event related to \\(X\\) does not influence the probability of an event related to \\(Y\\), and vice versa. Mathematically, \\(X\\) and \\(Y\\) are independent if and only if for every pair of events \\(A\\) and \\(B\\), the probability that both \\(X\\) belongs to \\(A\\) and \\(Y\\) belongs to \\(B\\) is the product of their individual probabilities. This can be expressed as:\n\\[\n\\mathbb{P}(X \\in A \\; \\text{ and } \\; Y \\in B) = \\mathbb{P}(X \\in A) \\cdot \\mathbb{P}(Y \\in B).\n\\]\nThis can equivalently be expressed as the fact that, for any two functions \\(F(\\cdot)\\) and \\(G(\\cdot)\\), the following identity holds\n\\[\n\\mathbb{E}[F(X) \\cdot G(Y)] \\; = \\; \\mathbb{E}[F(X)] \\cdot \\mathbb{E}[G(Y)].\n\\]\nThis definition implies that knowing the outcome of \\(X\\) provides no information about the outcome of \\(Y\\), and this lack of influence is a key characteristic of independent random variables. One extremely important remark is that, for two random variables \\(X\\) and \\(Y\\), the expectation of the sum equals the sum of the expectation,\n\\[\n\\mathbb{E}[X+Y] = \\mathbb{E}[X] + \\mathbb{E}[Y],\n\\]\nas soon as all these quantities exists. This holds even if the two random variables are not independent."
  },
  {
    "objectID": "notes_students/probability_basics/probability_basics.html#standard-deviation-variances-and-covariances",
    "href": "notes_students/probability_basics/probability_basics.html#standard-deviation-variances-and-covariances",
    "title": "Probability Basics",
    "section": "Standard Deviation, Variances and Covariances",
    "text": "Standard Deviation, Variances and Covariances\nThe variance of a random variable \\(X\\), denoted as \\(\\text{Var}(X)\\), measures the spread of its values. It is defined as\n\\[\n\\begin{align}\n\\text{Var}(X) &= \\mathbb{E}[(X - \\mu_X )^2] = \\mathbb{E}[X^2] - \\mu_X ^2\\\\\n&= \\int_{-\\infty}^{\\infty} (x - \\mu_X)^2 \\cdot f(x) \\, dx\n\\end{align}\n\\]\nThe standard deviation is the square root of the variance, denoted as \\(\\sigma_X = \\sqrt{\\text{Var}(X)}\\). The notion of covariance measures the linear relationship between two random variables \\(X\\) and \\(Y\\). It is defined as\n\\[\n\\begin{align}\n\\text{Cov}(X, Y)\n&= \\mathbb{E}[(X - \\mu_X )(Y - \\mu_Y )] = \\mathbb{E}[X \\, Y] -  \\mu_X  \\, \\mu_Y\\\\\n&= \\int_{-\\infty}^{\\infty} (x - \\mu_X)(y-\\mu_Y) \\cdot f(x, y) \\, dx\n\\end{align}\n\\]\nwhere \\(f(x,y)\\) is the joined density of the pair of random variables \\((X,Y)\\). The correlation is defined as a normalized version of the covariance,\n\\[\n\\text{Corr}(X,Y) = \\textrm{Cov}\\left\\{ \\frac{X - \\mu_X}{\\sigma_X}, \\frac{Y - \\mu_Y}{\\sigma_Y}\\right\\}\n\\]\nand always satisfies \\(-1 \\leq \\text{Corr}(X,Y) \\leq 1\\), as is easily proved (exercise). Note that if \\(X\\) and \\(Y\\) are independent, then \\(\\text{Cov}(X, Y) = 0\\). However, zero covariance does not imply independence and it is a good exercise to construct such a counter-example. Standard manipulations reveal that for two random variables \\(X\\) and \\(Y\\) we have\n\\[\n\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y) + \\text{Cov}(X, Y),\n\\]\nwhich is indeed the equivalent of the identity \\((x+y)^2 = x^2 + y^2 + 2xy\\). Importantly, if the two random variables \\(X\\) and \\(Y\\) are independent, the variance of the sum equals the sum of the variances, \\(\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y)\\). This also shows that for \\(N\\) independent and identically distributed random variables \\(X_1, \\ldots, X_N\\), we have that\n\\[\n\\text{Var}\\left\\{ \\frac{X_1 + \\ldots + X_N}{N} \\right\\} \\; = \\; \\frac{\\text{Var}(X)}{N}.\n\\]"
  },
  {
    "objectID": "notes_students/probability_basics/probability_basics.html#random-vectors-mean-vector-and-covariance-matrix",
    "href": "notes_students/probability_basics/probability_basics.html#random-vectors-mean-vector-and-covariance-matrix",
    "title": "Probability Basics",
    "section": "Random Vectors: Mean Vector and Covariance Matrix",
    "text": "Random Vectors: Mean Vector and Covariance Matrix\nA random vector is a vector of random variables. For a random vector \\(\\mathbf{X}\\) in \\(\\mathbb{R}^d\\), the mean of \\(\\mu = \\mathbf{X} \\ in \\mathbb{R}^d\\) is a vector in \\(\\mathbb{R}^d\\), each component of which is the mean of one of its \\(d\\) components. The covariance matrix, \\(\\Sigma \\in \\mathbb{R}^{d,d}\\), of \\(\\mathbf{X}\\) is a \\(d \\times d\\) matrix defined by\n\\[\n\\Sigma_{ij} = \\text{Cov}(X_i, X_j)\n=\n\\mathbb{E}[(X_i - \\mu_{X_i}) \\, (X_j - \\mu_{X_j})]\n\\]\nwhere \\(X_i\\) and \\(X_j\\) are the \\(i\\)-th and \\(j\\)-th components of \\(\\mathbf{X}\\), respectively. Each element \\(\\Sigma_{ij}\\) represents the covariance between the \\(i\\)-th and \\(j\\)-th components of the vector \\(\\mathbf{X}\\). If the components are independent, the covariance matrix is diagonal. Furthermore, the covariance matrix, when it exists, is always a symmetric and positive semi-definite matrix."
  },
  {
    "objectID": "notes_students/probability_basics/probability_basics.html#gaussian-distributions",
    "href": "notes_students/probability_basics/probability_basics.html#gaussian-distributions",
    "title": "Probability Basics",
    "section": "Gaussian distributions",
    "text": "Gaussian distributions\nThe Gaussian distribution, also known as the normal distribution, holds a central place in statistics, probability, and applied mathematics due to several key reasons. Firstly, its mathematical properties are well-understood and conducive to analytical work. Secondly, the Central Limit Theorem states that the sum of a large number of independent, identically distributed variables will approximately follow a Gaussian distribution, regardless of the original distribution. This makes it a fundamental tool for inferential statistics. Furthermore, Gaussian distributions arise naturally in numerous contexts due to random noise and errors often tending to distribute normally. Lastly, since Gaussian distributions are extremely tractable, its properties allow for convenient modeling in various fields.\n\nUnivariate case:\nA univariate Gaussian (or normal) distribution for a random variable \\(X\\) is characterized by its mean \\(\\mu\\) and variance \\(\\sigma^2\\). Its probability density function is given by\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{ -\\frac{(x - \\mu)^2}{2\\sigma^2}\\right\\}\n\\]\nThe constant \\(1/\\sqrt{2\\pi\\sigma^2}\\) is often written as \\(1/\\mathcal{Z}\\) where \\(\\mathcal{Z} = \\sqrt{2\\pi\\sigma^2}\\) is referred to as the “normalization factor”. It ensures that the density \\(f(x)\\) integrates to one.\n\n\nMultivariate case:\nA multivariate Gaussian distribution for a random vector \\(\\mathbf{X}\\) in \\(\\mathbb{R}^n\\) is characterized by a mean vector \\(\\boldsymbol{\\mu}\\) and a covariance matrix \\(\\Sigma\\). Its probability density function is\n\\[\nf(\\mathbf{x}) = \\frac{1}{\\sqrt{(2\\pi)^n |\\Sigma|}} \\exp\\left\\{ -\\frac{1}{2} \\langle (\\mathbf{x} - \\boldsymbol{\\mu}), \\Sigma^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\rangle \\right\\}\n\\]\nand \\(|\\Sigma|\\) is the determinant of the covariance matrix \\(\\Sigma\\). Crucially, the inverse of the covariance matrix is inside the dot product \\(\\langle (\\mathbf{x} - \\boldsymbol{\\mu}), \\Sigma^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\rangle\\). In general, zero correlation between variables does not imply their independence. However, this principle has a notable exception in the case of multivariate Gaussian distributions. For a multivariate Gaussian distribution, if the covariance matrix is diagonal, indicating zero correlation between different components, this does imply their statistical independence."
  },
  {
    "objectID": "posts/reverse_and_tweedie/reverse_and_tweedie.html",
    "href": "posts/reverse_and_tweedie/reverse_and_tweedie.html",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "",
    "text": "Video\n\n\nImagine a scalar diffusion process \\(\\{X_t\\}_{t=0}^T\\) defined on the interval \\([0,T]\\),\n\\[dX = \\mu(X) \\, dt + \\sigma \\, dW\\]\nwhere \\(\\mu(X)\\) is the drift term, \\(\\sigma\\) is the diffusion coefficient, and \\(dW\\) is a Wiener process. Denote the distribution of this process at time \\(t\\) (\\(0 \\leq t \\leq T\\)) as \\(p_t(dx)\\) with an initial distribution of \\(p_0(dx)\\). Now, what happens if we reverse time and examine the process backward? In other words, consider the time-reversed process \\(\\overleftarrow{X}\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIntuitively, the process \\(\\overleftarrow{X}\\) is also a diffusion on the interval \\([0, T]\\), but with an initial distribution \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). To gain intuition, consider an Euler discretization of the forward process:\n\\[X_{t+\\delta} = X_{t} + \\mu(X_t)\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, \\mathbf{n} \\tag{1}\\]\nwhere \\(\\mathbf{n}\\sim \\mathcal{N}(0,1)\\) represents a noise term independent from \\(X_{t}\\), and \\(\\delta \\ll 1\\) is a time increment. Re-arranging terms and making the approximation \\(\\mu(X_t) \\approx \\mu(X_{t+\\delta})\\) gives that\n\\[X_{t} \\approx X_{t+\\delta} - \\mu(X_{t+\\delta})\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, (-\\mathbf{n}). \\tag{2}\\]\nThis seems to suggest that the time-reversed process follows the dynamics \\(d\\overleftarrow{X} = -\\mu(\\overleftarrow{X}) \\, dt + \\sigma \\, dW\\) started from \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). However, this conclusion is incorrect because this would suggest that the time-reversed of a standard Brownian motion (where \\(\\mu(x) \\equiv 0\\)) starting at zero is also a Brownian motion starting at \\(p_T(dx) = \\mathcal{N}(0,T)\\), which is clearly not the case. The flaw in this argument lies in assuming that the noise term \\(Z\\) is independent of \\(X_{t+\\delta}\\), which is not true, rendering the Euler discretization argument invalid.\nDeriving the dynamics of the backward process in a rigorous manner is not straightforward (Anderson 1982) (Haussmann and Pardoux 1986). What follows is a heuristic derivation that proceeds by estimating the mean and variance of \\(X_{t}\\) given \\(X_{t+\\delta} = x_{t+\\delta}\\), assuming \\(\\delta \\ll 1\\). Here, \\(x_{t+\\delta}\\) is treated as a fixed and constant value, and we are only interested in the conditional distribution of \\(X_t\\) given \\(x_{t+\\delta}\\). Bayes’ law gives\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta}) \\propto p_t(x) \\, \\exp\\left\\{ -\\frac{\\left( x_{t+\\delta} - [x + \\mu(x) \\, \\delta] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\},\\]\nwhere the exponential term corresponds to the transition of the forward diffusion for \\(\\delta \\ll 1\\). Using the 1st order approximation\n\\[p_t(x) \\approx p_t(x_{t+\\delta}) \\,\\exp\\left( \\langle \\nabla \\log p_t(x_{t+\\delta}), (x-x_{t+\\delta})\\rangle\\right),\\]\neliminating multiplicative constants and higher-order error terms, we obtain:\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta})\n\\propto\n\\exp\\left\\{ -\\frac{\\left( x - [x_{t+\\delta} - \\mu(x_{t+\\delta}) \\delta\n+ {\\color{red} \\sigma^2 \\, \\nabla \\log p_t(x_{t+\\delta}) \\, \\delta} ] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\}.\\]\nFor \\(\\delta \\ll 1\\), this is transition of the reverse diffusion\n\\[\nd\\overleftarrow{X}_t = -\\mu(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\]\nThe notation \\(B\\) is used to emphasize that this Brownian motion is distinct from the one used in the forward diffusion. The additional drift term, denoted as \\({\\color{red} \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X})}\\), is intuitive: it pushes the reverse diffusion toward regions where the forward diffusion spent a significant amount of time, i.e., where \\(p_t(dx)\\) is large. The popular “denoising diffusion models” (Ho, Jain, and Abbeel 2020) can be seen as discretizations of this backward process, employing various techniques to estimate the additional drift term from data."
  },
  {
    "objectID": "posts/reverse_and_tweedie/reverse_and_tweedie.html#reversing-a-diffusion",
    "href": "posts/reverse_and_tweedie/reverse_and_tweedie.html#reversing-a-diffusion",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "",
    "text": "Video\n\n\nImagine a scalar diffusion process \\(\\{X_t\\}_{t=0}^T\\) defined on the interval \\([0,T]\\),\n\\[dX = \\mu(X) \\, dt + \\sigma \\, dW\\]\nwhere \\(\\mu(X)\\) is the drift term, \\(\\sigma\\) is the diffusion coefficient, and \\(dW\\) is a Wiener process. Denote the distribution of this process at time \\(t\\) (\\(0 \\leq t \\leq T\\)) as \\(p_t(dx)\\) with an initial distribution of \\(p_0(dx)\\). Now, what happens if we reverse time and examine the process backward? In other words, consider the time-reversed process \\(\\overleftarrow{X}\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIntuitively, the process \\(\\overleftarrow{X}\\) is also a diffusion on the interval \\([0, T]\\), but with an initial distribution \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). To gain intuition, consider an Euler discretization of the forward process:\n\\[X_{t+\\delta} = X_{t} + \\mu(X_t)\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, \\mathbf{n} \\tag{1}\\]\nwhere \\(\\mathbf{n}\\sim \\mathcal{N}(0,1)\\) represents a noise term independent from \\(X_{t}\\), and \\(\\delta \\ll 1\\) is a time increment. Re-arranging terms and making the approximation \\(\\mu(X_t) \\approx \\mu(X_{t+\\delta})\\) gives that\n\\[X_{t} \\approx X_{t+\\delta} - \\mu(X_{t+\\delta})\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, (-\\mathbf{n}). \\tag{2}\\]\nThis seems to suggest that the time-reversed process follows the dynamics \\(d\\overleftarrow{X} = -\\mu(\\overleftarrow{X}) \\, dt + \\sigma \\, dW\\) started from \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). However, this conclusion is incorrect because this would suggest that the time-reversed of a standard Brownian motion (where \\(\\mu(x) \\equiv 0\\)) starting at zero is also a Brownian motion starting at \\(p_T(dx) = \\mathcal{N}(0,T)\\), which is clearly not the case. The flaw in this argument lies in assuming that the noise term \\(Z\\) is independent of \\(X_{t+\\delta}\\), which is not true, rendering the Euler discretization argument invalid.\nDeriving the dynamics of the backward process in a rigorous manner is not straightforward (Anderson 1982) (Haussmann and Pardoux 1986). What follows is a heuristic derivation that proceeds by estimating the mean and variance of \\(X_{t}\\) given \\(X_{t+\\delta} = x_{t+\\delta}\\), assuming \\(\\delta \\ll 1\\). Here, \\(x_{t+\\delta}\\) is treated as a fixed and constant value, and we are only interested in the conditional distribution of \\(X_t\\) given \\(x_{t+\\delta}\\). Bayes’ law gives\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta}) \\propto p_t(x) \\, \\exp\\left\\{ -\\frac{\\left( x_{t+\\delta} - [x + \\mu(x) \\, \\delta] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\},\\]\nwhere the exponential term corresponds to the transition of the forward diffusion for \\(\\delta \\ll 1\\). Using the 1st order approximation\n\\[p_t(x) \\approx p_t(x_{t+\\delta}) \\,\\exp\\left( \\langle \\nabla \\log p_t(x_{t+\\delta}), (x-x_{t+\\delta})\\rangle\\right),\\]\neliminating multiplicative constants and higher-order error terms, we obtain:\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta})\n\\propto\n\\exp\\left\\{ -\\frac{\\left( x - [x_{t+\\delta} - \\mu(x_{t+\\delta}) \\delta\n+ {\\color{red} \\sigma^2 \\, \\nabla \\log p_t(x_{t+\\delta}) \\, \\delta} ] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\}.\\]\nFor \\(\\delta \\ll 1\\), this is transition of the reverse diffusion\n\\[\nd\\overleftarrow{X}_t = -\\mu(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\]\nThe notation \\(B\\) is used to emphasize that this Brownian motion is distinct from the one used in the forward diffusion. The additional drift term, denoted as \\({\\color{red} \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X})}\\), is intuitive: it pushes the reverse diffusion toward regions where the forward diffusion spent a significant amount of time, i.e., where \\(p_t(dx)\\) is large. The popular “denoising diffusion models” (Ho, Jain, and Abbeel 2020) can be seen as discretizations of this backward process, employing various techniques to estimate the additional drift term from data."
  },
  {
    "objectID": "posts/reverse_and_tweedie/reverse_and_tweedie.html#denoising-score-matching-tweedie-formula",
    "href": "posts/reverse_and_tweedie/reverse_and_tweedie.html#denoising-score-matching-tweedie-formula",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "Denoising Score Matching & Tweedie formula",
    "text": "Denoising Score Matching & Tweedie formula\n\n\n\n\nMaurice Tweedie (1919–1996)\n\n\n\nThe previous section shows that a quantity often referred to as the “score” in machine learning (ML) literature, i.e. \\(\\mathcal{S}_t(x) \\equiv \\nabla_x \\log p_t(x)\\), naturally arises when a diffusion process runs backward in time. Interestingly, it is worth noting that since the beginning of times statisticians have been using the term “score” to refer to the other derivative, i.e. the derivative with respect to a model’s parameter, which is a completely different object!\nConsider a Brownian diffusion process \\(dX = \\sigma \\, dW\\) initiated from a distribution \\(\\mu(dx) = p_0(dx)\\). If this process is ran forward for a duration \\(\\delta&gt;0\\), we have:\n\\[X_{\\delta} = X_0 + \\mathcal{N}(0, \\sigma^2 \\, \\delta)\\]\nwhere \\(X_0 \\sim \\mu(dx)\\). Now, focusing on a specific sample \\(y = X_{\\delta}\\), the backward dynamics \\(d\\overleftarrow{X} = \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X}) , dt + \\sigma \\, dB\\) suggests that for sufficiently small \\(\\delta\\), the following approximation holds:\n\\[\n\\mathbb{E}[X_0 \\, | X_{\\delta} = y] \\; { \\color{\\red} \\approx} \\; y + \\nabla \\log p_{\\delta}(y) \\, \\sigma^2 \\delta.\n\\tag{3}\\]\nMaybe surprisingly, in the case of Brownian dynamics (no drift), this relationship holds exactly, even for arbitrarily large time increments \\(\\delta&gt;0\\). This observation attributed in (Efron 2011) to Maurice Tweedie (1919–1996) has a straightforward proof. Specifically, if \\(X \\sim \\mu(dx)\\), then \\(Y = X + \\mathcal{N}(0, \\Gamma^2)\\) has a density given by:\n\\[\np_Y(dy) = \\int \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx\n\\]\nwhere \\(\\rho_{\\Gamma}(z) \\propto \\exp[-z^2/(2 \\Gamma^2)]\\) is a centred Gaussian with variance \\(\\Gamma^2\\). It follows that\n\\[\n\\frac{ \\mathbb{E}[X \\, | Y = y]-y}{\\Gamma^2} = \\frac{ \\int \\left( \\frac {x-y }{\\Gamma^2} \\right)\\, \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx }{\\int \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx}\n=\n\\nabla_y \\log \\left\\{ \\int \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx \\right\\}\n\\]\nsince \\(\\nabla_y \\rho_{\\Gamma}(y-x) = (x-y)/\\Gamma^2\\). This leads to Tweedie’s formula:\n\\[\n\\mathbb{E}[X \\, | Y = y] \\; {\\color{red} =} \\; y + \\Gamma^2 \\, \\nabla \\log p_Y(y),\n\\]\nwhich is exactly the same as Equation 3 but with an equality sign: no approximation needed! Interestingly, this is what Machine-Learners often refer to as “denoising score matching” (Vincent 2011): if we have access to a large number of samples \\((X_i,Y_i)\\), where \\(X_i \\sim \\mu(dx)\\) and \\(Y_i = X_i + \\mathcal{N}(0, \\Gamma^2)\\), and fit a regression model \\(F_{\\theta}\\) by minimizing the mean-squared error \\(\\theta \\mapsto \\mathbb{E} \\|X - F_{\\theta}(Y)\\|^2\\), then Tweedie formula shows that in the limit of infinite samples and with sufficient flexibility in the regression model \\(F_{\\theta_\\star}(y) = y + \\Gamma^2 \\nabla \\log p_Y(y)\\). This allows one to estimate \\(\\nabla \\log p_Y\\) from data, which is often a good approximation of \\(\\nabla \\log \\mu\\) if the variance \\(\\Gamma^2\\) of the added noise is not too large. Indeed, things can go bad if \\(\\Gamma^2\\) is very small and the number of training data is not large, no free lunch!"
  },
  {
    "objectID": "posts/hello_world/index.html",
    "href": "posts/hello_world/index.html",
    "title": "Hello World",
    "section": "",
    "text": "Just testing whether Latex is working correctly: \\[\\frac{1}{2}\\left( \\int \\exp\\left\\{ -\\frac{x^2}{2}\\right\\} \\, dx \\right)^2 \\approx 22/7\\]\nHere is a reference (MacKay 2003) and below is an animation:\n\n\nVideo\n\n\n\n\n\n\nReferences\n\nMacKay, David JC. 2003. Information Theory, Inference and Learning Algorithms. Cambridge university press."
  },
  {
    "objectID": "posts/index_blog.html",
    "href": "posts/index_blog.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Blogposts (vaguely) related to research, notes for students, announcements, things I would like to write down to understand better and/or not forget, etc… Comments, corrections, suggestions are welcome!\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n12-06-2023\n\n\nReverse diffusions, Score & Tweedie\n\n\nSDE,Generative-Model\n\n\n\n\n06-06-2023\n\n\nHello World\n\n\ntesting\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes_DRAFT/SSM/SSM.html#state-space-models",
    "href": "notes_DRAFT/SSM/SSM.html#state-space-models",
    "title": "State Space Models",
    "section": "State Space Models:",
    "text": "State Space Models:\nConsider a Markov chain \\((X_t, Y_t)\\) with \\(X_t \\in \\mathcal{X}\\) and \\(Y_t \\in \\mathcal{Y}\\) with initial distribution \\(p(X_0 \\in dx_0) = \\mu_0(x_0) \\, dx_0\\) and described by the dynamics\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nx_{t+1} &\\sim f(x_{t+1} | x_t) \\\\\ny_{t+1} &\\sim g(y_{t+1} | x_{t+1})\n\\end{aligned}\n\\right.\n\\end{align}\n\\tag{1}\\]\nfor some transition and observation densities \\(f(\\cdot | \\cdot)\\) and \\(g(\\cdot | \\cdot)\\). These functions could be time-dependent but we will assume not for lightening notations."
  },
  {
    "objectID": "notes_DRAFT/SSM/SSM.html#filtering",
    "href": "notes_DRAFT/SSM/SSM.html#filtering",
    "title": "State Space Models",
    "section": "Filtering:",
    "text": "Filtering:\nConsider an empirical approximation of the filtering distribution \\(p(x_t | y_{0:t})\\) using \\(N \\geq 1\\) particles \\((x^1_{t}, \\ldots, x^N_{t})\\),\n\\[\np(x_t | y_{0:t})\n\\approx\n\\sum_{i=1}^N w_{i,t} \\, \\delta_{x^i_{t}}(x_t).\n\\]\nThe filtering distribution at time \\((t+1)\\) follows the recursion:\n\\[\np(x_{t+1} | y_{0:t+1}) \\propto \\int p(x_{t} | y_{0:t}) \\, f(x_{t+1} | x_t) \\, g(y_{t+1} | x_t) \\, dx_t.\n\\]\nThis is the \\(x_{t+1}\\)-marginal of the joint density proportional to \\(p(x_{t} | y_{0:t}) \\, f(x_{t+1} | x_t) \\, g(y_{t+1} | x_t)\\). One can approximate this joint density using importance sampling with a proposal \\(p(x_{t} | y_{0:t}) \\, q_t(x_{t+1} | x_t)\\), where \\(q_t(x_{t+1} | x_t)\\) is any reasonable Markov kernel. The choice of \\(q_t(x_{t+1} | x_t) \\equiv f(x_{t+1} | x_t)\\) leads to the bootstrap particle filter. Alternatively, \\(q_t(x_{t+1} | x_t) = p(x_{t+1} | x_t, y_{t+1})\\) can yield better estimates, but is often hard to implement, though approximating it can be a viable strategy.\nUsing these methods, the filtering distribution at time \\((t+1)\\) can be approximated by a new set of particles \\(\\sum_{i=1}^N w_{i,t+1} \\, \\delta_{x^i_{t+1}}(x_{t+1})\\), where \\(x^{i}_{t+1} \\sim q_t(x^{i}_{t+1} | x^{i}_{t})\\) and weights are calculated as:\n\\[\nw^i_{t+1}  \\propto \\frac{f(x^{i}_{t+1} | x^{i}_{t}) \\, g(y_{t+1} | x^i_{t+1})}{q_t(x^{i}_{t+1} | x^{i}_{t})}.\n\\]"
  },
  {
    "objectID": "notes_DRAFT/SSM/SSM.html#smoothing",
    "href": "notes_DRAFT/SSM/SSM.html#smoothing",
    "title": "State Space Models",
    "section": "Smoothing:",
    "text": "Smoothing:\nWhen estimating parameters in time-series models, it is often crucial to evaluate sums like\n\\[\n\\sum_{t=0}^{t-1} \\mathop{\\mathrm{\\mathbb{E}}}[ s_t(x_t, x_{t+1}) | y_{0:T}]\n\\]\nwhere \\(s_t(\\cdot, \\cdot)\\) is a specific function. For instance, in the Expectation-Maximization (EM) algorithm, we have \\(s_t(x_t, x_{t+1}) = \\log[ f(x_{t+1}|x_t) , g(y_{t+1}|x_{t+1})]\\). Essentially, this means it is important to accurately approximate the \\((x_t, x_{t+1})\\)-marginals of the smoothing distribution.\n\nForward only and Fixed-Lag approximation:\nA standard filtering approach on path-space, often referred to as the “poor man’s smoother”, can be used to approximate the full smoothing distribution. However, this method often leads to path degeneracy, especially for smoothing marginals at time \\(t \\ll T\\), which are typically very poor. In cases where the State-Space Model (SSM) has a “forgetting property”, the fixed-lag approximation can be useful. This is expressed as:\n\\[\np(x_t | y_{0:T}) \\; \\approx \\; p(x_t | y_{0:{t+L}}).\n\\]\nHere, \\(L\\) is the lag parameter, usually chosen to be quite small. To implement this fixed-lag approximation, one simply needs to run a filtering method and track the last \\(L\\) ancestors; that it straightforward since the SSM in Equation 1 can readily be extended to a SSM describing the dynamics of \\((x_{t-L}, \\ldots, x_{t-1}, x_t)\\).\n\n\nForward-Filtering Backward Smoothing:\nThe smoothing distribution satisfy the following backward recursion\n\\[\n\\begin{align}\np(x_t| y_{0:T})\n&=\n\\int p(x_t, x_{t+1} | y_{0:T}) \\, dx_{t+1}\\\\\n&=\n\\underbrace{p(x_t | y_{1:t})}_{\\text{(filtering)}} \\, \\int \\, \\underbrace{p(x_{t+1} | y_{0:T})}_{\\text{(smoothing)}} \\, \\frac{f(x_{t+1}|x_t)}{p(x_{t+1} | y_{1:t})} \\, dx_{t+1}.\n\\end{align}\n\\tag{2}\\]\nTo exploit this backward recursion, one can first a standard filtering methods to obtain particle approximations of all the filtering distributions. It is then straightforward to discretize Equation 2 to either:\n\ngenerate a single trajectory from the smoothing distribution starting from the final time \\(T\\). Generating this single smoothing trajectory can be implementing in \\(\\mathcal{O}(N \\, T)\\).\nreweight the particle approximations of the filtering distributions to obtain particle approximation of the marginal smoothing distributions of \\(x_t\\) or \\((x_t, x_{t+1})\\). This procedure can be implemented in \\(\\mathcal{O}(N^2 \\, T)\\)"
  },
  {
    "objectID": "notes_DRAFT/SSM/SSM_references.html",
    "href": "notes_DRAFT/SSM/SSM_references.html",
    "title": "State Space Models: References",
    "section": "",
    "text": "Monte-Carlo methods for SSM:\n\n(Cappé, Moulines, and Rydén 2009)\n(Douc, Moulines, and Stoffer 2014)\n(Chopin, Papaspiliopoulos, et al. 2020)\n\n(Särkkä and Svensson 2023)"
  },
  {
    "objectID": "notes_DRAFT/SSM/SSM_references.html#books",
    "href": "notes_DRAFT/SSM/SSM_references.html#books",
    "title": "State Space Models: References",
    "section": "",
    "text": "Monte-Carlo methods for SSM:\n\n(Cappé, Moulines, and Rydén 2009)\n(Douc, Moulines, and Stoffer 2014)\n(Chopin, Papaspiliopoulos, et al. 2020)\n\n(Särkkä and Svensson 2023)"
  },
  {
    "objectID": "notes_DRAFT/SSM/SSM_references.html#web",
    "href": "notes_DRAFT/SSM/SSM_references.html#web",
    "title": "State Space Models: References",
    "section": "Web:",
    "text": "Web:\n\nReference webpage maintained by Arnaud Doucet.\nReference webpage maintained by Pierre Del Moral."
  },
  {
    "objectID": "notes_DRAFT/SSM/SSM_references.html#smoothing",
    "href": "notes_DRAFT/SSM/SSM_references.html#smoothing",
    "title": "State Space Models: References",
    "section": "Smoothing:",
    "text": "Smoothing:\n\nFixed Lag Smoothing:\n\n(Kitagawa and Sato 2001): proposes the fixed-lag smoothing approach\n\n\nForward Filtering-Backward Sampling:\n\n(Godsill, Doucet, and West 2004) generates a smoothing trajectory in \\(\\mathcal{O}(N \\, T)\\)"
  },
  {
    "objectID": "publications/index_pubs.html#theory-methods",
    "href": "publications/index_pubs.html#theory-methods",
    "title": "Alexandre Thiéry",
    "section": "THEORY & METHODS:",
    "text": "THEORY & METHODS:\n\n\n\n Doubly Adaptive Importance Sampling   van den Boom, W., Cremaschi, A., Thiery, A.H, (2024)   Submitted    (Arxiv) \n\n\n Ensemble Kalman Filtering-Aided Variational Inference for Gaussian Process State-Space Models   Lin, Z., Sun, Y.,Yin, F., Thiery, A.H, (2023)   Submitted    (Arxiv) \n\n\n GIT-Net: Generalized Integral Transform for Operator Learning   Chao Wang and Alexandre H. Thiery, (2023)   Transactions on Machine Learning Research, 2023    (Arxiv)   (Journal)  \n\n\n Computational Doob’s H-transforms for Online Filtering of Discretely Observed Diffusions   Nicolas Chopin, Andras Fulop, Jeremy Heng, Alexandre H. Thiery, (2023)   International Conference on Machine Learning (ICML) 2023    (Arxiv) \n\n\n Conditional sequential Monte Carlo in high dimensions   Axel Finke and Alexandre H. Thiery, (2023)   Annals of Statistics 2023, In Press    (Arxiv)   (Journal)  \n\n\n Pretrained equivariant features improve unsupervised landmark discovery   Rahul Rahaman, Atin Ghosh and Alexandre H. Thiery, (2022)   International Conference of Pattern Recognition (ICPR) 2022    (Arxiv) \n\n\n Manifold lifting: scaling MCMC to the vanishing noise regime   Khai Xiang Au, Matthew Graham, Alexandre H. Thiery, (2022)   JRSSB 2022    (Arxiv)   (Journal)  \n\n\n A discrete Bouncy Particle Sampler   Chris Sherlock and Alexandre H. Thiery, (2022)   Biometrika, Volume 109, Issue 2 (2022)    (Arxiv)   (Journal)  \n\n\n A Generalized & Robust Framework For Timestamp Supervision in Temporal Action Segmentation   Rahul Rahaman, Dipika Singhania, Alexandre H. Thiery, Angela Yao, (2022)   European Conference on Computer Vision (ECCV) 2022    (Journal)  \n\n\n Manifold Markov chain Monte Carlo methods for Bayesian inference in a wide class of diffusion models   Matthew Graham, Alexandre H. Thiery, Alex Beskos, (2022)   JRSSB, Volume 84 (4), 2022    (Arxiv)   (Journal)  \n\n\n Sequential Ensemble Transform for Bayesian Inverse Problems   Aaron Myers, Alexandre H. Thiery, Kainan Wang and Tan Bui-Tanh, (2021)   Journal of Computational Physics, Volume 427 (2021)    (Arxiv)   (Journal)  \n\n\n On Data-Augmentation and Consistency-Based Semi-Supervised Learning   Atin Ghosh and Alexandre H. Thiery, (2021)   ICLR 2021    (Arxiv) \n\n\n Uncertainty Quantification and Deep Ensembles   Rahul Rahaman and Alexandre H. Thiery, (2021)   NeuRIPS 2021    (Arxiv) \n\n\n On importance-weighted autoencoders   Axel Finke and Alexandre H. Thiery, (2019)   Tech Report    (Arxiv) \n\n\n A scalable optimal-transport based local particle filter   Matthew Graham and Alexandre H. Thiery, (2019)   Tech Report    (Arxiv) \n\n\n Particle Filter efficiency under limited communication   Deborshee Sen and Alexandre H. Thiery, (2019)   Tech Report    (Arxiv) \n\n\n Error Bounds for Sequential Monte Carlo Samplers for Multimodal Distributions   Daniel Paulin, Ajay Jasra and Alexandre H. Thiery, (2019)   Bernoulli, Volume 25, Number 1 (2019)    (Arxiv)   (Journal)  \n\n\n Asymptotic Analysis of the Random-Walk Metropolis Algorithm on Ridged Densities   Alex Beskos, Gareth Roberts, Alexandre H. Thiery and Natesh Pillai, (2018)   Annals of Applied Probability, Volume 28, Number 5 (2018)    (Arxiv)   (Journal)  \n\n\n On Coupling Particle Filter Trajectories   Deborshee Sen, Alexandre H. Thiery, Ajay Jasra, (2018)   Statistics and Computing, Volume 28, Number 2 (2018)    (Arxiv)   (Journal)  \n\n\n Levy statistics of interacting Rydberg gases   Thibault Vogt, Jingshan Han, Alexandre H. Thiery, Wenhui Li, (2017)   Physical Review A, Volume 95, Number 5 (2017)    (Arxiv)   (Journal)  \n\n\n Pseudo-marginal Metropolis–Hastings using averages of unbiased estimators   Chris Sherlock, Alexandre H. Thiery and Anthony Lee, (2017)   Biometrika, Volume 104, Number 3 (2017)    (Arxiv)   (Journal)  \n\n\n On the Convergence of Adaptive Sequential Monte Carlo Methods   Alex Beskos, Ajay Jasra, Nikolas Kantas, Alexandre H. Thiery, (2016)   Annals of Applied Probability, Volume 26, Number 2 (2016)    (Arxiv)   (Journal)  \n\n\n Consistency and fluctuations for stochastic gradient Langevin dynamics   Yee Whye Teh, Alexandre H. Thiery, Sebastian Vollmer, (2016)   Journal of Machine Learning Research, Volume 17 (2016)    (Arxiv)   (Journal)  \n\n\n On the efficiency of pseudo-marginal random walk Metropolis algorithms   Chris Sherlock, Alexandre H. Thiery, Gareth Roberts, Jeff Rosenthal, (2015)   Annals of Statistics, Volume 43, Number 1 (2015)    (Arxiv)   (Journal)  \n\n\n On non-negative unbiased estimators   Pierre Jacob, Alexandre H. Thiery, (2015)   Annals of Statistics, Volume 43, Number 2 (2015)    (Arxiv)   (Journal)  \n\n\n Efficiency of delayed-acceptance random walk Metropolis algorithms   Chris Sherlock, Alexandre H. Thiery and Andrew Golightly, (2015)   Annals of Statistics (In Press)    (Arxiv)   (Journal)  \n\n\n Noisy gradient flow from a random walk in Hilbert space   Natesh Pillai, Andrew Stuart, Alexandre H. Thiery, (2014)   Stochastic Partial Differential Equations: Analysis and Computations, Volume 2, Number 2 (2014)    (Arxiv)   (Journal)  \n\n\n Optimal Scaling and Diffusion Limits for the Langevin Algorithm in High Dimensions   Natesh Pillai, Andrew Stuart, Alexandre H. Thiery, (2012)   Annals of Applied Probability, Volume 22, Number 6 (2012)    (Arxiv)   (Journal)  \n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/index_pubs.html#applications",
    "href": "publications/index_pubs.html#applications",
    "title": "Alexandre Thiéry",
    "section": "APPLICATIONS",
    "text": "APPLICATIONS\n\n\n\n Three-Dimensional Structural Phenotype of the Optic Nerve Head as a Function of Glaucoma Severity   Braeu, F.A., Chuangsuwanich, T., Tun, T.A., Perera, S.A., Husain, R., Kadziauskienė, A., Schmetterer, L., Thiéry, A.H., Barbastathis, G., Aung, T. and Girard, M.J.A., (2023)   JAMA Ophthalmology, 2023    (Arxiv)   (Journal)  \n\n\n Towards Label-Free 3D Segmentation of Optical Coherence Tomography Images of the Optic Nerve Head Using Deep Learning   Devalla,S.K., Pham, T.H., Panda, S.K, Zhang,L., Subramanian,G., Swaminathan,A., Chin,Z.Y, Rajan,M., Mohan,S., Krishnadas,R., Senthil,V., Leon,J.M, Tun,T.A., Cheng,C.Y., Schmetterer, L., Perera,S., Aung,T., Thiery,A.H., Girard,M.J.A., (2023)   Biomedical Optics Express, Vol. 11, Issue 11    (Arxiv)   (Journal)  \n\n\n Geometric Deep Learning to Identify the Critical 3D Structural Features of the Optic Nerve Head for Glaucoma Diagnosis   Braeu, F., Thiery, A.H, Tun, T.A., Kadziauskiene, A., Barbastathis, G., Aung, T., and Girard. M.J.A., (2023)   American Journal of Ophthalmology, 2023    (Arxiv)   (Journal)  \n\n\n Medical Application of Geometric Deep Learning for the Diagnosis of Glaucoma   Thiery, A.H, Braeu, F., Tun, T.A., Aung, T., Girard. M.J.A., (2023)   Translational Vision Science and Technology, 2023    (Arxiv)   (Journal)  \n\n\n Detection of m6A from direct RNA sequencing using a Multiple Instance Learning framework   Hendra C, Pratanwanich PN, Wan YK, Goh WS, Thiery A, Göke J+., (2022)   Nature Methods (2022)    (Arxiv)   (Journal)  \n\n\n Describing the Structural Phenotype of the Glaucomatous Optic Nerve Head Using Artificial Intelligence   Panda, S.K., Cheong, H., Tun,T.A., Devella, S.K., Krishnadas, R., Buist, M.L., Perera, S., Cheng, C-Y., Aung, T., Thiery A.H., Girard, M.J.A., (2022)   American Journal of Ophthalmology, 2022    (Arxiv)   (Journal)  \n\n\n The Three-Dimensional Structural Configuration of the Central Retinal Vessel Trunk and Branches as a Glaucoma Biomarker   Satish K Panda, Haris Cheong, Tin A Tun, Thanadet Chuangsuwanich, Aiste Kadziauskiene, Vijayalakshmi Senthil, Ramaswami Krishnadas, Martin L Buist, Shamira Perera, Ching-Yu Cheng, Tin Aung, Alexandre H Thiery, Michaël JA Girard, (2022)   American Journal of Ophthalmology, 2022    (Journal)  \n\n\n Deep Learning Algorithms to Isolate and Quantify the Structures of the Anterior Segment in Optical Coherence Tomography Images   Pham, T.H, Devalla,S.K., Ang, A., Da,S.Z., Thiery,A.H., Boote,C., Cheng,C.Y., Koh,V., Girard, M.J.A., (2021)   British Journal of Ophthalmology    (Arxiv)   (Journal)  \n\n\n Identification of differential RNA modifications from nanopore direct RNA sequencing with xPore   Ploy N. Pratanwanich, Fei Yao, Ying Chen, Casslynn W.Q. Koh, Christopher Hendra, Polly Poon, Yeek Teck Goh, Phoebe M. L. Yap, Choi Jing Yuan, Wee Joo Chng, Sarah Ng, Alexandre Thiery, W.S. Sho Goh, Jonathan Goeke, (2021)   Nature Biotechnology, 2021    (Arxiv)   (Journal)  \n\n\n Beyond quadratic error: Case-study of a multiple criteria approach to the performance assessment of numerical forecasts of solar irradiance in the tropics   Verbois, H., Blanc, P., Huva, R., Saint-Drenan, Y-M, Rusydi, A.. Thiery, A., (2020)   Renewable and Sustainable Energy Reviews, Volume 117, (2020)    (Journal)  \n\n\n NanoVar: Accurate Characterization of Patients Genomic Structural Variants Using Low-Depth Nanopore Sequencing   Tham, C.Y, Tirado-Magallanes, R., Goh, Y., Fullwood, M. J., Koh, B.T.H. , Wang, W., Ng, C.H, Chng, W.J., Thiery, A.H., Tenen, D.G, Benoukraf, (2020)   Genome Biology (2020)    (Arxiv)   (Journal)  \n\n\n DeshadowGAN: A Deep Learning Approach to Remove Shadows from Optical Coherence Tomography Images   Cheong, H., Devalla, S.K., Pham, T.H., Liang, Z., Tun, T.A., Wang, X., Perera, S., Schmetterer, L., Tin, A., Boote, C., Thiery, A.H., Girard, M.J.A., (2020)   Translational Vision Science & Technology    (Arxiv)   (Journal)  \n\n\n Deep Learning Algorithms to Isolate and Quantify the Structures of the Anterior Segment in Optical Coherence Tomography Images   Pham, T.H, Devalla,S.K., Ang, A., Da, S.Z., Thiery, A.H., Boote,C., Cheng,C.Y., Koh,V., Girard, M.J.A, (2020)   British Journal of Ophthalmology, 2020    (Arxiv)   (Journal)  \n\n\n A Deep Learning Approach to Denoise Optical Coherence Tomography Images of the Optic Nerve Head   Devalla SK, Subramanian G, Pham TH, Wang X, Perera S, Tun TA, Aung T, Schmetterer L, Thiery A.H., Girard MJA, (2019)   Scientific Reports (2019)    (Arxiv)   (Journal)  \n\n\n Glaucoma management in the era of artificial intelligence   Devalla S.K., Liang Z., Pham T.H., Boote, C., Strouthidis, N.G., Thiery A.H., Girard M.J.A., (2019)   British Journal of Ophthalmology (2019)    (Journal)  \n\n\n DRUNET: A Dilated-Residual U-Net Deep Learning Network to Digitally Stain Optic Nerve Head Tissues in Optical Coherence Tomography Images   Devalla SK, Renukanand PK, Sreedhar BK, Perera SA, Mari JM, Chin KS, Tun TA, Strouthidis N, Aung T, Thiery A.H., Girard MJA, (2018)   Biomedical Optics Express, Vol. 9, Issue 7 (2018)    (Arxiv)   (Journal)  \n\n\n Probabilistic forecasting of day-ahead solar irradiance using quantile gradient boosting   Verbois, H., Rusydi, A., Thiery, A.H., (2018)   Solar Energy 173, 313-327 (2018)    (Journal)  \n\n\n\nNo matching items"
  },
  {
    "objectID": "about/about.html",
    "href": "about/about.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Email: a.h.thiery [at] nus.edu.sg\nOffice: Building S16, Office #06-113"
  },
  {
    "objectID": "about/about.html#address",
    "href": "about/about.html#address",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Email: a.h.thiery [at] nus.edu.sg\nOffice: Building S16, Office #06-113"
  },
  {
    "objectID": "about/about.html#qualifications",
    "href": "about/about.html#qualifications",
    "title": "Alexandre Thiéry",
    "section": "Qualifications",
    "text": "Qualifications\n\nPh.D., Probability & Statistics, Warwick University, 2009-2013.\nEcole Normale Superieure of Paris, Mathematics, 2005-2009\n\nResearch Assistant, Statslab (Cambridge, UK)\nMSc (Probability & Finance), University of Paris VI\nMSc (Partial Differential Equations & Modeling), University of Paris VI"
  },
  {
    "objectID": "about/about.html#employment-history",
    "href": "about/about.html#employment-history",
    "title": "Alexandre Thiéry",
    "section": "Employment history",
    "text": "Employment history\n\nAssociate Professor, Department of Statistics & Data Sciences, NUS, 2020–present.\nAffiliate, NUS Centre for Data Science and Machine Learning, 2021–present\nAffiliate, NUS Institute of Data Science, 2028–present\nAffiliate, NUS Graduate School for Integrative Sciences and Engineering, 2017–present\nAssistant Professor, Department of Statistics & Probability, NUS, 2014–2019.\nResearch Fellow, Department of Statistics & Probability, NUS, 2013"
  },
  {
    "objectID": "about/about.html#leadership",
    "href": "about/about.html#leadership",
    "title": "Alexandre Thiéry",
    "section": "Leadership",
    "text": "Leadership\n\nDeputy Director of Institute for Mathematical Sciences, 7/2020 – 12/2023"
  },
  {
    "objectID": "about/about.html#service",
    "href": "about/about.html#service",
    "title": "Alexandre Thiéry",
    "section": "Service",
    "text": "Service\n\nArea Chair for AISTAT (2023), ACML (2023), NeurIPS (2023), ICLR (2024)\nAssociate Editor for Statistics & Computing (2020–2022)"
  },
  {
    "objectID": "about/about.html#awards-and-honours",
    "href": "about/about.html#awards-and-honours",
    "title": "Alexandre Thiéry",
    "section": "Awards and honours",
    "text": "Awards and honours\n\nNUS Faculty of Sciences Dean’s Chair Associate Professor, 2022–2025\nNUS Faculty Teaching Excellence Award, 2022\nNUS Faculty Teaching Excellence Award, 2019\nNUS Faculty Teaching Excellence Award, 2018\nNUS Young Scientist Award. (Faculty of Science: 1 awardee per year), 2017\nNUS Young Investigator Award. (Faculty of Science: 3 awardees per year), 2016\nJohn Copas prize for the best PhD dissertation, 2013"
  },
  {
    "objectID": "about/about.html#industrial-activities",
    "href": "about/about.html#industrial-activities",
    "title": "Alexandre Thiéry",
    "section": "Industrial Activities",
    "text": "Industrial Activities\n\nCo-founder of Abyss Processing, start-up developing AI solutions for ophthalmology\nConsulting in the finance and health-care industries"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Google-Scholar\n  \n  \n    \n     Arxiv\n  \n  \n    \n     twitter\n  \n\n      \nAssociate Professor\nDepartment of Statistics & Data Science\nNational University of Singapore\n\n\n\nComputational Statistics & Machine Learning\nMonte-Carlo methods\nUncertainty Quantification\nInverse problems\n\n\n\n\n\nTwo Research Fellow positions: Data Assimilation\n\n\n\n\nOffice: Building S16, Office #06-113\nEmail: a.h.thiery [at] nus.edu.sg"
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Computational Statistics & Machine Learning\nMonte-Carlo methods\nUncertainty Quantification\nInverse problems"
  },
  {
    "objectID": "index.html#available-positions",
    "href": "index.html#available-positions",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Two Research Fellow positions: Data Assimilation"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Office: Building S16, Office #06-113\nEmail: a.h.thiery [at] nus.edu.sg"
  },
  {
    "objectID": "notes/MCMC_on_manifold/mcmc_manifold.html",
    "href": "notes/MCMC_on_manifold/mcmc_manifold.html",
    "title": "RWM & HMC on manifolds",
    "section": "",
    "text": "Consider a smooth manifold \\(\\mathcal{M}\\subset \\mathbb{R}^n\\) of dimension \\(d_{\\mathcal{M}} = (n-d)\\) defined as the zero set of a well-behaved “constraint” function \\(C: \\mathbb{R}^n \\to \\mathbb{R}^d\\),\n\\[\n\\mathcal{M}= \\{ x \\in \\mathbb{R}^n \\; \\text{such that} \\; C(x) = 0 \\}.\n\\]\nWe would like to use MCMC to sample from a probability distribution supported on \\(\\mathcal{M}\\) with density \\(\\pi(x)\\) with respect to the uniform Hausdorff measure on \\(\\mathcal{M}\\). It is relatively straightforward to adapt standard MCMC methods when dealing with simple manifolds such as a sphere or a torus since their geodesics and several other geometric quantities are analytically tractable. Maybe surprisingly, it is in fact relatively straightforward to design MCMC samplers on general implicitly defined manifold such as \\(\\mathcal{M}\\). The article (Zappa, Holmes-Cerfon, and Goodman 2018) explains these ideas beautifully.\n\nManifold Random Walk Metropolis-Hastings\nAssume that \\(x_n \\in \\mathcal{M}\\) is the current position of the MCMC chain. To generate a proposal \\(y_n \\in \\mathcal{M}\\) that will eventually be accepted or rejected, one can proceed very similarly to the standard RWM algorithm with Gaussian perturbations with variance \\(\\sigma^2\\). First, generate a vector \\(v \\in T_{x_n}\\) from a centred Gaussian distribution with covariance \\(\\sigma^2 \\, I\\) on the tangent space \\(T_{x_n}\\) to \\(\\mathcal{M}\\) at \\(x_n\\). To do so, it suffices for example to generate a standard Gaussian vector \\(z \\sim \\mathcal{N}(0, \\sigma^2 I_n)\\) in \\(\\mathbb{R}^n\\) and orthogonal-project it onto \\(T_{x_n}\\). Indeed, one cannot simply define the proposal as \\(x_n + v\\) since it would not necessarily lie on \\(\\mathcal{M}\\). Instead, one projects \\(x_n + v\\) back to \\(\\mathcal{M}\\). To do so, one needs to define the direction used for the projection and the manifold RWM algorithm uses \\(T_{x_n}^\\perp\\), for reasons that will become clear later. In other words, the proposal \\(y_n\\) is obtained by seeking a vector \\(w \\in T_{x_n}^{\\perp}\\) such that \\(x_n + v + w \\in \\mathcal{M}\\).\n\n\n\n\nProjection onto \\(\\mathcal{M}\\) from (Zappa, Holmes-Cerfon, and Goodman 2018)\n\n\n\nIf one calls \\(J_{x_n}\\) the Jacobian matrix of \\(C\\) at \\(x_n\\), i.e. the matrix whose rows are the gradients of the components of \\(C\\), this projection operation boils down to finding a vector \\(\\lambda \\in \\mathbb{R}^d\\) such that\n\\[\nC( \\, x_n + v + J_{x_n}^\\top \\lambda) = 0 \\in \\mathbb{R}^d.\n\\tag{1}\\]\nNote that Equation 1 is a non-linear equation in \\(\\lambda\\) that can have no solution, one solution or many solutions – this can seem like a fundamental roadblock to the design of a valid MCMC algorithm, but we will see that it is not! Before discussing in slightly more details the resolution of Equation 1, assume that a standard root-finding algorithm takes the pair \\((x_n+v, J_{x_n})\\) as input and attempts to produces the projection \\(y_n\\),\n\\[\n\\text{Proj}: \\quad (x_n+v, J_{x_n}) \\; \\underbrace{\\mapsto}_{\\text{root-finding}} \\; y_n \\in \\mathcal{M}.\n\\]\nThe algorithm will either converge to one of the possible solutions or fail. If the algorithm fails to converge, one can simply reject the proposal \\(y_n\\) and set \\(y_n = \\text{(Failed)}\\) and set \\(x_{n+1} = x_n\\). If the algorithm converges, this defines a valid proposal \\(y_n \\in \\mathcal{M}\\). To ensure reversibility, and it is one of the main novelty of the article (Zappa, Holmes-Cerfon, and Goodman 2018), one needs to verify that the reverse proposal \\(y_n \\mapsto x_n\\) is possible.\n\n\n\n\nReversibility check (Zappa, Holmes-Cerfon, and Goodman 2018)\n\n\n\nTo do so, note that the only possibility for the reverse move \\(y_n \\to x_n\\) to happen is if \\(x_n = \\text{Proj}(y_n + v', J_{y_n})\\) where\n\\[\nx_n-y_n \\;=\\; \\underbrace{v'}_{\\in T_{y_n}}  \\, + \\, \\underbrace{w'}_{\\in T_{y_n}^{\\perp}}.\n\\]\nThe uniqueness follows from the decomposition \\(\\mathbb{R}^n \\equiv T_{y_n} \\otimes T_{y_n}^{\\perp}\\). The reverse move is consequently possible if and only if the following reversibility check condition is satisfied,\n\\[\nx_n = \\text{Proj}(y_n + v', J_{y_n}).\n\\tag{2}\\]\nThis reversibility check is necessary as it is not guaranteed that the root-finding algorithm started from \\(y_n + v'\\) converges at all, or converges to \\(x_n\\) in the case when there are several solutions. If Equation 2 is not satisfied, the proposal \\(y_n\\) is rejected and one sets \\(x_{n+1} = x_n\\). On the other hand, if Equation 2 is satisfied, the proposal \\(y_n\\) is accepted with the usual Metropolis-Hastings probability\n\\[\n\\min \\left\\{1, \\frac{\\pi(y_n) \\, p(v'|x_n)}{\\pi(x_n) \\, p(v|x_n)} \\right\\}\n\\]\nwhere \\(p(v|x) = Z^{-1} \\, \\exp(-\\|v\\|^2 / 2 \\sigma^2)\\) denotes the Gaussian density on the tangent space \\(T_{x_n}\\) The above description defines a valid MCMC algorithm on \\(\\mathcal{M}\\) that is reversible with respect to the target distribution \\(\\pi(x)\\).\n\n\nProjection onto the manifold\nAs described above, the main difficulty is to solve the non-linear equation Equation 1 describing the projection of the proposal \\((x_n + v)\\) back to the manifold \\(\\mathcal{M}\\). The projection is along the space spanned by the columns of \\(J_{x_n}^\\top \\in \\mathbb{R}^{n,d}\\), i.e. find a vector \\(\\lambda \\in \\mathbb{R}^d\\) such that\n\\[\n\\Phi(\\lambda) = C( \\, x_n + v + J_{x_n}^\\top \\lambda) = 0 \\in \\mathbb{R}^d.\n\\]\nOne can use a standard Newton’s method to solve this equation started from \\(\\lambda_0=0\\). Setting for notational convenience \\(q(\\lambda) = x_n + v + J_{x_n}^T \\, \\lambda\\), this boils down to iterating\n\\[\n\\lambda_{k+1} - \\lambda_{k}\n=\n- \\left( J_{q(\\lambda_k)} \\, J_{x_n}^\\top \\right)^{-1} \\, \\Phi(\\lambda_k).\n\\]\nAs described in (Barth et al. 1995), it can sometimes be computationally advantageous to use a quasi-Newton method and use instead\n\\[\n\\lambda_{k+1} - \\lambda_{k}\n=\n- G^{-1} \\, \\Phi(\\lambda_k)\n\\]\nwith fixed positive definite matrix \\(G = J_{x_n} \\, J_{x_n}^\\top\\) since one can then pre-compute a decomposition of \\(G\\) and use it to solve the linear systems at each iterations. In some recent and related work (Au, Graham, and Thiery 2022), we observed that the standard Newton method performed well in the settings we considered and there was most of the time no computational advantage to using a quasi-Newton method. In practice, the main computational bottleneck is to compute the Jacobian matrix \\(J_{x_n}\\), although it is problem-dependent and some structure can typically be exploited. In practice, only a relatively small number of iterations are performed and the root-finding algorithm is stopped as soon as \\(\\|\\Phi(\\lambda_k)\\|\\) is below a certain threshold. If the stepsize is small, i.e. \\(\\|v\\| \\ll 1\\), it is typically the case that the Newton’s method will converge to a solution in only a very small number of iterations – indeed, Newton’s method is quadratically convergent when close to a solution.\n\n\n\n\n30k RWM chains ran in parallel to explore a double torus.\n\n\n\nIn the figure above, I have implemented the RWM algorithm above described to sample from the uniform distribution supported on a double torus described by the constraint function \\(C: \\mathbb{R}^3 \\to \\mathbb{R}\\) given as\n\\[\nC(x,y,z) = (x^2 \\, (x^2 - 1) + y^2)^2+z^2-0.03.\n\\]\nThe figure shows \\(30,000\\) chains ran in parallel, which is straightforward to implement in practice with JAX (Bradbury et al. 2018). All the chains are initialized from the same position so that one can visualize the evolution of the density of particles.\n\n\n\n\nTuning of manifold-RWM\n\n\n\nOne can for example monitor the usual expected squared jump distance\n\\[\n\\textrm{(ESJD)} \\equiv \\mathbb{E}[\\|X_{n+1} - X_n\\|^2]\n\\]\nand maximize it to tune the RWM step-size; it would probably make slightly more sense to monitor the squared geodesic distances instead the naive squared norm \\(\\|X_{n+1} - X_n\\|^2\\), but that’s way to much hassle and would probably make only a negligible difference. In the figure above, I have plotted the expected squared jump distance as a function of the acceptance rate for different step-sizes. It is interesting to see a pattern extremely similar to the one observed in the standard RWM algorithm (Roberts and Rosenthal 2001): in this double torus example, the optimal acceptance rate is around \\(25\\%\\). Note that since the target distribution is uniform, the rate of acceptance is only very slightly lower than the proportion of successful reversibility checks.\n\n\nHamiltonian Monte Carlo (HMC) on manifolds\nWhile the Random Walk Metropolis-Hastings algorithm is interesting, exploiting gradient information is often necessary to design efficient MCMC samplers. Consider a single iteration of a standard Hamiltonian Monte Carlo (HMC) sampler targeting a density \\(\\pi(q)\\) on \\(q \\in \\mathbb{R}^n\\). The method proceeds by simulating from a dynamics that is reversible with respect to an extended target density \\(\\bar{\\pi}(q,p)\\) on \\(\\mathbb{R}^n \\otimes \\mathbb{R}^n\\) defined as\n\\[\n\\begin{aligned}\n\\bar{\\pi}(q,p)\n&\\propto \\pi(q) \\, \\exp \\left\\{ -\\frac{1}{2m} \\|p\\|^2 \\right\\}\\\\\n&= \\exp\\left\\{ \\log \\pi(q) - K(p) \\right\\}\n\\end{aligned}\n\\]\nfor a user-defined mass parameter \\(m &gt; 0\\). In general, the mass parameter is a positive definite matrix but generalizing this to manifolds is slightly less useful in practice. For a time-discretization step \\(\\varepsilon&gt; 0\\) and a current position \\((q_n,p_n)\\), the method proceeds by generating a proposal \\((q_{*},p_{*})\\) defined as\n\\[\n\\left\\{\n\\begin{aligned}\np_{n+1/2} &= p_n + \\frac{\\varepsilon}{2} \\nabla \\log \\pi(q_n)\\\\\nq_{*} &= q_n + \\varepsilon\\, m^{-1} \\, p_{n+1/2}\\\\\np_{*} &= p_{n+1/2} + \\frac{\\varepsilon}{2} \\nabla \\log \\pi(q_{*}).\n\\end{aligned}\n\\right.\n\\]\nThis proposal is accepted with probability \\(\\min\\left( 1, \\bar{\\pi}(q_*, p_*)/\\bar{\\pi}(q_n, p_n) \\right)\\). Indeed, in standard implementation, several leapfrog steps are performed instead of a single one. One can also choose to perform a single leapfrog step as above and only do a partial refreshment of the momentum after each leapfrog step – this may be more efficient or easier to implement when running a large number of HMC chains in parallel on a GPU for example. To adapt the HMC algorithm to sample from a density \\(\\pi(q)\\) supported on a manifold \\(\\mathcal{M}\\), one can proceed similarly to the RWM algorithm by interleaving additional projection steps. These projections are needed to ensure that the momentum vectors \\(p_n\\) remain in the right tangent spaces and the position vectors \\(q_n\\) remain on the manifold \\(\\mathcal{M}\\),\n\\[\n(q_n, p_n) \\; \\in \\; \\mathcal{M}\\times T_{q_n}.\n\\]\nAs in the RWM algorithm, reversibility checks need to be performed to ensure that the overall algorithm is reversible with respect to the target distribution \\(\\overline{\\pi}(q,p)\\). The resulting algorithm for generating a proposal \\((q_n, p_n) \\mapsto (q_*, p_*)\\) reads as follows:\n\\[\n\\left\\{\n\\begin{aligned}\n\\widetilde{p}_{n+1/2} &= p_n + \\frac{\\varepsilon}{2} \\nabla \\log \\pi(q_n)\\\\\np_{n+1/2} &=  \\textcolor{red}{\\text{orthogonal project $\\widetilde{p}_{n+1/2}$ onto $T_{q_n}$}} \\\\\n\\widetilde{q}_{*} &= q_n + \\varepsilon\\, m^{-1} \\, p_{n+1/2}\\\\\nq_{*} &=  \\textcolor{red}{\\text{Proj$(\\widetilde{q}_{*}, J_{q_n})$}}\\\\\n\\overline{p}_{n+1/2} &=  \\textcolor{red}{\\text{orthogonal project $(q_{*}-q_n) \\, m / \\varepsilon$ onto $T_{q_{*}}$}} \\\\\n\\widetilde{p}_{*} &= \\overline{p}_{n+1/2} + \\frac{\\varepsilon}{2} \\nabla \\log \\pi(q_{*})\\\\\np_{*} &=  \\textcolor{red}{\\text{orthogonal project $\\widetilde{p}_{*}$ onto $T_{q_{*}}$}}.\n\\end{aligned}\n\\right.\n\\]\nIf any of the projection operations fail, the proposal is rejected. If no failure occurs, a reversibility check is performed by running the algorithm backward starting from \\((q_*, -p_*)\\). If the reversibility check is successful, the proposal is accepted with the usual Metropolis-Hastings probability \\(\\min\\left( 1, \\bar{\\pi}(q_*, p_*)/\\bar{\\pi}(q_n, p_n) \\right)\\).\n\n\n\n\n5k HMC chains ran in parallel: the momentum is not refreshed\n\n\n\nThe article (Lelièvre, Rousset, and Stoltz 2019) provides a detailed description of several of these ideas along with detailed analysis and extensions.\n\n\n\n\n\nReferences\n\nAu, Khai Xiang, Matthew M Graham, and Alexandre H Thiery. 2022. “Manifold Lifting: Scaling MCMC to the Vanishing Noise Regime.” Journal of the Royal Statistical Society: Series B. https://arxiv.org/abs/2003.03950.\n\n\nBarth, Eric, Krzysztof Kuczera, Benedict Leimkuhler, and Robert D Skeel. 1995. “Algorithms for Constrained Molecular Dynamics.” Journal of Computational Chemistry 16 (10). Wiley Online Library: 1192–1209. https://doi.org/10.1002/jcc.540161003.\n\n\nBradbury, James, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, et al. 2018. “JAX: Composable Transformations of Python+NumPy Programs.” http://github.com/google/jax.\n\n\nLelièvre, Tony, Mathias Rousset, and Gabriel Stoltz. 2019. “Hybrid Monte Carlo Methods for Sampling Probability Measures on Submanifolds.” Numerische Mathematik 143 (2). Springer: 379–421. https://arxiv.org/abs/1807.02356.\n\n\nRoberts, Gareth O, and Jeffrey S Rosenthal. 2001. “Optimal Scaling for Various Metropolis-Hastings Algorithms.” Statistical Science 16 (4). Institute of Mathematical Statistics: 351–67. https://doi.org/10.1214/ss/1015346320.\n\n\nZappa, Emilio, Miranda Holmes-Cerfon, and Jonathan Goodman. 2018. “Monte Carlo on Manifolds: Sampling Densities and Integrating Functions.” Communications on Pure and Applied Mathematics 71 (12). Wiley Online Library: 2609–47. https://arxiv.org/abs/1702.08446."
  },
  {
    "objectID": "notes/DDPM_deterministic/DDPM_deterministic.html",
    "href": "notes/DDPM_deterministic/DDPM_deterministic.html",
    "title": "From Denoising Diffusion to ODEs",
    "section": "",
    "text": "Consider an empirical data distribution \\(\\pi_{\\mathrm{data}}\\). In order to simulate approximate samples from \\(\\pi_{\\mathrm{data}}\\), Denoising Diffusion Probabilistic Models (DDPM) simulate a forward diffusion process \\(\\{X_t\\}_{[0,T]}\\) on an interval \\([0,T]\\). The diffusion is initialized at the data distribution, i.e. \\(X_0 \\sim \\pi_{\\mathrm{data}}\\), and is chosen so that that the distribution of \\(X_T\\) is very close to a known and tractable reference distribution \\(\\pi_{\\mathrm{ref}}\\), e.g. a Gaussian distribution. Denote by \\(p_t(dx)\\) the marginal distribution at time \\(0 \\leq t \\leq T\\), i.e. \\(\\mathop{\\mathrm{\\mathbb{P}}}(X_t \\in dx) = p_t(dx)\\). By choosing the forward distribution with simple and tractable transition probabilities, e.g. an Ornstein-Uhlenbeck, it is relatively easy to estimate \\(\\nabla \\log p_t(x)\\) from simulated data: this can be formulated as a simple regression problem. This allows one to simulate the diffusion backward in time and generate approximate samples from \\(\\pi_{\\mathrm{data}}\\). Why this is useful is another question…\nThe fact that the mapping from data-samples at time \\(t=0\\) to (approximate) Gaussian samples at time \\(t=T\\) is stochastic and described by diffusion processes is cumbersome. This would be much more convenient to build a deterministic mapping between the data-distribution \\(\\pi_{\\mathrm{data}}\\) and the Gaussian reference distribution \\(\\pi_{\\mathrm{ref}}\\): this would allows one to associate a likelihood to data samples and to easily “encode”/“decode” data-samples. To so this, one can try to replace diffusion by Ordinary Differential Equations."
  },
  {
    "objectID": "notes/DDPM_deterministic/DDPM_deterministic.html#likelihood-computation",
    "href": "notes/DDPM_deterministic/DDPM_deterministic.html#likelihood-computation",
    "title": "From Denoising Diffusion to ODEs",
    "section": "Likelihood computation",
    "text": "Likelihood computation\nWith the diffusion-ODE trick, we have just seen that it is possible to build a vector fields \\(F[0,T] \\times \\mathbb{R}^d \\to \\mathbb{R}^d\\) such that the forward ODE\n\\[\n\\frac{d}{dt} \\overrightarrow{Y_t} =\nF(t,\\overrightarrow{Y_t})\n\\qquad \\textrm{initialized at} \\qquad\n\\overrightarrow{Y}_0 \\sim \\pi_{\\mathrm{data}}\n\\tag{3}\\]\nand the backward ODE defined as\n\\[\n\\frac{d}{ds} \\overleftarrow{Y_s} =\n-F(T-s,\\overleftarrow{Y_s})\n\\qquad \\textrm{initialized at} \\qquad\n\\overleftarrow{Y}_0 \\sim \\pi_{\\mathrm{ref}}\n\\]\nare such that \\(\\overrightarrow{Y}_T \\approx \\pi_{\\mathrm{ref}}\\) and \\(\\overleftarrow{Y}_T \\approx \\pi_{\\mathrm{data}}\\).\nIn general, consider a vector field \\(F(t,x)\\) and a bunch of particles distributed according to a distribution \\(p_t\\) at time \\(t\\). If each particle follows the vector field for an amount of time \\(\\delta \\ll 1\\), the particles that were in the vicinity of some \\(x \\in \\mathbb{R}^d\\) at time \\(t\\) end up in the vicinity of \\(x + F(x,t) \\, \\delta\\) at time \\(t+\\delta\\). At the same time, a volume element \\(dx\\) around \\(x \\in \\mathbb{R}^d\\) gets stretch by a factor \\(1+\\delta \\, \\mathop{\\mathrm{Tr}}[\\mathop{\\mathrm{\\mathrm{Jac}}}F(x,t)] = 1 + \\delta \\mathop{\\mathrm{div}}F(x,t)\\) while following the vector field \\(F\\), which means that the density of particles at time \\(t+\\delta\\) and around \\(x + F(x,t) \\, \\delta\\) equals \\(p_t(x) / [1 + \\delta \\mathop{\\mathrm{div}}F(x,t)]\\). In other words \\(\\log p_{t+\\delta}(x + F(x,t) \\, \\delta) \\approx \\log p_t(x) - \\delta \\, \\mathop{\\mathrm{div}}F(x,t)\\). This means that if we follows a trajectory of \\(\\tfrac{d}{dt} X_t = F(t,X_t)\\) one gets\n\\[\n\\log p_T(X_T) = \\log p_0(X_0) - \\int_{t=0}^{T} \\mathop{\\mathrm{div}}F(X_t,t) \\, dt.\n\\]\nThat is the Lagrangian description of the density \\(p_t\\) of particles. Indeed, one could directly get this identity by differentiating \\(p_t(X_t)\\) with respect to time while using the continuity Equation 1. When applied to the DDPM, this gives a way to assign likelihood the data samples, namely\n\\[\n\\log \\pi_{\\mathrm{data}}(x) = \\log \\pi_{\\mathrm{ref}}(\\overrightarrow{Y_T}) + \\int_{t=0}^{T} \\mathop{\\mathrm{div}}F(t, \\overrightarrow{Y_t})\\, dt\n\\]\nwhere \\(\\overrightarrow{Y_t}\\) is trajectory of the forward ODE Equation 3 initialized as \\(\\overrightarrow{Y_0} = x\\). Note that in high-dimensional setting, it may be computationally expensive to compute the divergence term \\(\\mathop{\\mathrm{div}}F(t, \\overrightarrow{Y_t})\\) since it typically is \\(d\\) times slower that a gradient computation; for this reason, it is often advocated to use the Hutchinson trace estimator to get an unbiased estimate of it at a much lower computational cost."
  },
  {
    "objectID": "notes/reverse_and_tweedie/reverse_and_tweedie.html",
    "href": "notes/reverse_and_tweedie/reverse_and_tweedie.html",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "",
    "text": "Imagine a scalar diffusion process \\(\\{X_t\\}_{t=0}^T\\) defined on the interval \\([0,T]\\),\n\\[dX = \\mu(X) \\, dt + \\sigma \\, dW\\]\nwhere \\(\\mu(X)\\) is the drift term, \\(\\sigma\\) is the diffusion coefficient, and \\(dW\\) is a Wiener process. Denote the distribution of this process at time \\(t\\) (\\(0 \\leq t \\leq T\\)) as \\(p_t(dx)\\) with an initial distribution of \\(p_0(dx)\\). Now, what happens if we reverse time and examine the process backward? In other words, consider the time-reversed process \\(\\overleftarrow{X}\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIntuitively, the process \\(\\overleftarrow{X}\\) is also a diffusion on the interval \\([0, T]\\), but with an initial distribution \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). To gain intuition, consider an Euler discretization of the forward process:\n\\[X_{t+\\delta} = X_{t} + \\mu(X_t)\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, \\mathbf{n} \\tag{1}\\]\nwhere \\(\\mathbf{n}\\sim \\mathcal{N}(0,1)\\) represents a noise term independent from \\(X_{t}\\), and \\(\\delta \\ll 1\\) is a time increment. Re-arranging terms and making the approximation \\(\\mu(X_t) \\approx \\mu(X_{t+\\delta})\\) gives that\n\\[X_{t} \\approx X_{t+\\delta} - \\mu(X_{t+\\delta})\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, (-\\mathbf{n}). \\tag{2}\\]\nThis seems to suggest that the time-reversed process follows the dynamics \\(d\\overleftarrow{X} = -\\mu(\\overleftarrow{X}) \\, dt + \\sigma \\, dW\\) started from \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). However, this conclusion is incorrect because this would suggest that the time-reversed of a standard Brownian motion (where \\(\\mu(x) \\equiv 0\\)) starting at zero is also a Brownian motion starting at \\(p_T(dx) = \\mathcal{N}(0,T)\\), which is clearly not the case. The flaw in this argument lies in assuming that the noise term \\(Z\\) is independent of \\(X_{t+\\delta}\\), which is not true, rendering the Euler discretization argument invalid.\nDeriving the dynamics of the backward process in a rigorous manner is not straightforward (Anderson 1982) (Haussmann and Pardoux 1986). What follows is a heuristic derivation that proceeds by estimating the mean and variance of \\(X_{t}\\) given \\(X_{t+\\delta} = x_{t+\\delta}\\), assuming \\(\\delta \\ll 1\\). Here, \\(x_{t+\\delta}\\) is treated as a fixed and constant value, and we are only interested in the conditional distribution of \\(X_t\\) given \\(x_{t+\\delta}\\). Bayes’ law gives\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta}) \\propto p_t(x) \\, \\exp\\left\\{ -\\frac{\\left( x_{t+\\delta} - [x + \\mu(x) \\, \\delta] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\},\\]\nwhere the exponential term corresponds to the transition of the forward diffusion for \\(\\delta \\ll 1\\). Using the 1st order approximation\n\\[p_t(x) \\approx p_t(x_{t+\\delta}) \\,\\exp\\left( \\langle \\nabla \\log p_t(x_{t+\\delta}), (x-x_{t+\\delta})\\rangle\\right),\\]\neliminating multiplicative constants and higher-order error terms, we obtain:\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta})\n\\propto\n\\exp\\left\\{ -\\frac{\\left( x - [x_{t+\\delta} - \\mu(x_{t+\\delta}) \\delta\n+ {\\color{red} \\sigma^2 \\, \\nabla \\log p_t(x_{t+\\delta}) \\, \\delta} ] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\}.\\]\nFor \\(\\delta \\ll 1\\), this is transition of the reverse diffusion\n\\[\nd\\overleftarrow{X}_t =  \\textcolor{red}{-}\\mu(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\tag{3}\\]\nThe notation \\(B\\) is used to emphasize that this Brownian motion is distinct from the one used in the forward diffusion. The additional drift term, denoted as \\({\\color{red} \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X})}\\), is intuitive: it pushes the reverse diffusion toward regions where the forward diffusion spent a significant amount of time, i.e., where \\(p_t(dx)\\) is large. Note the minus sign in front of the original drift term \\(\\mu(X) \\, dt\\) It interesting to note that if \\(X\\) is a Langevin diffusion reversible with respect to a probability density \\(\\pi(x)\\),\n\\[\ndX = \\tfrac{\\sigma^2}{2} \\nabla \\log \\pi(X) \\, dt + \\sigma \\, dB,\n\\]\nthen the reverse diffusion diffusion Equation 3 reads simply as\n\\[\nd\\overleftarrow{X}_t =  \\tfrac{\\sigma^2}{2} \\nabla \\log \\pi(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log [p_{T-t} / \\pi](\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\]\nNote that there is no minus sign in front of the original drift term. This highlight that, when at stationarity, the reverse diffusion indeed follows the same dynamics as the forward diffusion. The popular “denoising diffusion models” (Ho, Jain, and Abbeel 2020) can be seen as discretizations of the backward process Equation 3, employing various techniques to estimate the additional drift term from data."
  },
  {
    "objectID": "notes/reverse_and_tweedie/reverse_and_tweedie.html#reversing-a-diffusion",
    "href": "notes/reverse_and_tweedie/reverse_and_tweedie.html#reversing-a-diffusion",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "",
    "text": "Imagine a scalar diffusion process \\(\\{X_t\\}_{t=0}^T\\) defined on the interval \\([0,T]\\),\n\\[dX = \\mu(X) \\, dt + \\sigma \\, dW\\]\nwhere \\(\\mu(X)\\) is the drift term, \\(\\sigma\\) is the diffusion coefficient, and \\(dW\\) is a Wiener process. Denote the distribution of this process at time \\(t\\) (\\(0 \\leq t \\leq T\\)) as \\(p_t(dx)\\) with an initial distribution of \\(p_0(dx)\\). Now, what happens if we reverse time and examine the process backward? In other words, consider the time-reversed process \\(\\overleftarrow{X}\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIntuitively, the process \\(\\overleftarrow{X}\\) is also a diffusion on the interval \\([0, T]\\), but with an initial distribution \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). To gain intuition, consider an Euler discretization of the forward process:\n\\[X_{t+\\delta} = X_{t} + \\mu(X_t)\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, \\mathbf{n} \\tag{1}\\]\nwhere \\(\\mathbf{n}\\sim \\mathcal{N}(0,1)\\) represents a noise term independent from \\(X_{t}\\), and \\(\\delta \\ll 1\\) is a time increment. Re-arranging terms and making the approximation \\(\\mu(X_t) \\approx \\mu(X_{t+\\delta})\\) gives that\n\\[X_{t} \\approx X_{t+\\delta} - \\mu(X_{t+\\delta})\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, (-\\mathbf{n}). \\tag{2}\\]\nThis seems to suggest that the time-reversed process follows the dynamics \\(d\\overleftarrow{X} = -\\mu(\\overleftarrow{X}) \\, dt + \\sigma \\, dW\\) started from \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). However, this conclusion is incorrect because this would suggest that the time-reversed of a standard Brownian motion (where \\(\\mu(x) \\equiv 0\\)) starting at zero is also a Brownian motion starting at \\(p_T(dx) = \\mathcal{N}(0,T)\\), which is clearly not the case. The flaw in this argument lies in assuming that the noise term \\(Z\\) is independent of \\(X_{t+\\delta}\\), which is not true, rendering the Euler discretization argument invalid.\nDeriving the dynamics of the backward process in a rigorous manner is not straightforward (Anderson 1982) (Haussmann and Pardoux 1986). What follows is a heuristic derivation that proceeds by estimating the mean and variance of \\(X_{t}\\) given \\(X_{t+\\delta} = x_{t+\\delta}\\), assuming \\(\\delta \\ll 1\\). Here, \\(x_{t+\\delta}\\) is treated as a fixed and constant value, and we are only interested in the conditional distribution of \\(X_t\\) given \\(x_{t+\\delta}\\). Bayes’ law gives\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta}) \\propto p_t(x) \\, \\exp\\left\\{ -\\frac{\\left( x_{t+\\delta} - [x + \\mu(x) \\, \\delta] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\},\\]\nwhere the exponential term corresponds to the transition of the forward diffusion for \\(\\delta \\ll 1\\). Using the 1st order approximation\n\\[p_t(x) \\approx p_t(x_{t+\\delta}) \\,\\exp\\left( \\langle \\nabla \\log p_t(x_{t+\\delta}), (x-x_{t+\\delta})\\rangle\\right),\\]\neliminating multiplicative constants and higher-order error terms, we obtain:\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta})\n\\propto\n\\exp\\left\\{ -\\frac{\\left( x - [x_{t+\\delta} - \\mu(x_{t+\\delta}) \\delta\n+ {\\color{red} \\sigma^2 \\, \\nabla \\log p_t(x_{t+\\delta}) \\, \\delta} ] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\}.\\]\nFor \\(\\delta \\ll 1\\), this is transition of the reverse diffusion\n\\[\nd\\overleftarrow{X}_t =  \\textcolor{red}{-}\\mu(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\tag{3}\\]\nThe notation \\(B\\) is used to emphasize that this Brownian motion is distinct from the one used in the forward diffusion. The additional drift term, denoted as \\({\\color{red} \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X})}\\), is intuitive: it pushes the reverse diffusion toward regions where the forward diffusion spent a significant amount of time, i.e., where \\(p_t(dx)\\) is large. Note the minus sign in front of the original drift term \\(\\mu(X) \\, dt\\) It interesting to note that if \\(X\\) is a Langevin diffusion reversible with respect to a probability density \\(\\pi(x)\\),\n\\[\ndX = \\tfrac{\\sigma^2}{2} \\nabla \\log \\pi(X) \\, dt + \\sigma \\, dB,\n\\]\nthen the reverse diffusion diffusion Equation 3 reads simply as\n\\[\nd\\overleftarrow{X}_t =  \\tfrac{\\sigma^2}{2} \\nabla \\log \\pi(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log [p_{T-t} / \\pi](\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\]\nNote that there is no minus sign in front of the original drift term. This highlight that, when at stationarity, the reverse diffusion indeed follows the same dynamics as the forward diffusion. The popular “denoising diffusion models” (Ho, Jain, and Abbeel 2020) can be seen as discretizations of the backward process Equation 3, employing various techniques to estimate the additional drift term from data."
  },
  {
    "objectID": "notes/reverse_and_tweedie/reverse_and_tweedie.html#denoising-score-matching-tweedie-formula",
    "href": "notes/reverse_and_tweedie/reverse_and_tweedie.html#denoising-score-matching-tweedie-formula",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "Denoising Score Matching & Tweedie formula",
    "text": "Denoising Score Matching & Tweedie formula\n\n\n\n\nMaurice Tweedie (1919–1996)\n\n\n\nThe previous section shows that a quantity often referred to as the “score” in machine learning (ML) literature, i.e. \\(\\mathcal{S}_t(x) \\equiv \\nabla_x \\log p_t(x)\\), naturally arises when a diffusion process runs backward in time. Interestingly, it is worth noting that since the beginning of times statisticians have been using the term “score” to refer to the other derivative, i.e. the derivative with respect to a model’s parameter, which is a completely different object!\nConsider a Brownian diffusion process \\(dX = \\sigma \\, dW\\) initiated from a distribution \\(p_0(dx)\\). If this process is ran forward for a duration \\(\\delta&gt;0\\), we have:\n\\[X_{\\delta} = X_0 + \\mathcal{N}(0, \\sigma^2 \\, \\delta)\\]\nwhere \\(X_0 \\sim p_0(dx)\\). Now, focusing on a specific sample \\(y = X_{\\delta}\\), the backward dynamics \\(d\\overleftarrow{X} = \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X}) \\, dt + \\sigma \\, dB\\) suggests that for sufficiently small \\(\\delta\\), the following approximation holds:\n\\[\n\\mathbb{E}[X_0 \\, | X_{\\delta} = y] \\; { \\color{\\red} \\approx} \\; y + \\nabla \\log p_{\\delta}(y) \\, \\sigma^2 \\delta.\n\\tag{4}\\]\nMaybe surprisingly, in the case of Brownian dynamics (no drift), this relationship holds exactly, even for arbitrarily large time increments \\(\\delta&gt;0\\). This observation attributed in (Efron 2011) to Maurice Tweedie (1919–1996) has a straightforward proof. Specifically, if \\(X \\sim p_0(dx)\\), then \\(Y = X + \\mathcal{N}(0, \\Gamma^2)\\) has a density given by:\n\\[\np_Y(dy) = \\int p_0(x) \\, \\rho_{\\Gamma}(y-x) \\, dx\n\\]\nwhere \\(\\rho_{\\Gamma}(z) \\propto \\exp[-z^2/(2 \\Gamma^2)]\\) is a centred Gaussian with variance \\(\\Gamma^2\\). It follows that\n\\[\n\\frac{ \\mathbb{E}[X \\, | Y = y]-y}{\\Gamma^2}\n=\n\\frac{ \\int \\left( \\frac {x-y }{\\Gamma^2} \\right)\\, p_0(x) \\, \\rho_{\\Gamma}(y-x) \\, dx }{\\int p_0(x) \\, \\rho_{\\Gamma}(y-x) \\, dx}.\n\\]\nSince \\(\\nabla_y \\log \\rho_{\\Gamma}(y-x) = (x-y)/\\Gamma^2\\), this also read:\n\\[\n\\frac{ \\mathbb{E}[X \\, | Y = y]-y}{\\Gamma^2}\n=\n\\nabla_y \\log \\left\\{ \\int p_0(x) \\, \\rho_{\\Gamma}(y-x) \\, dx \\right\\}.\n\\]\nThis leads to Tweedie’s formula:\n\\[\n\\mathbb{E}[X \\, | Y = y] \\; {\\color{red} =} \\; y + \\Gamma^2 \\, \\nabla \\log p_Y(y),\n\\]\nwhich is exactly the same as Equation 4 but with an equality sign: no approximation needed! Interestingly, this is what Machine-Learners often refer to as “denoising score matching” (Vincent 2011): if we have access to a large number of samples \\((X_i,Y_i)\\), where \\(X_i \\sim p_0(dx)\\) and \\(Y_i = X_i + \\mathcal{N}(0, \\Gamma^2)\\), and fit a regression model \\(F_{\\theta}\\) by minimizing the mean-squared error \\(\\theta \\mapsto \\mathbb{E} \\|X - F_{\\theta}(Y)\\|^2\\), then Tweedie formula shows that in the limit of infinite samples and with sufficient flexibility in the regression model \\(F_{\\theta_\\star}(y) = y + \\Gamma^2 \\nabla \\log p_Y(y)\\). This allows one to estimate \\(\\nabla \\log p_Y\\) from data, which is often a good approximation of \\(\\nabla \\log p_0\\) if the variance \\(\\Gamma^2\\) of the added noise is not too large. Indeed, things can go bad if \\(\\Gamma^2 \\ll 1\\) and the number of training data is not large enough, no free lunch!"
  },
  {
    "objectID": "notes/sanov/sanov.html",
    "href": "notes/sanov/sanov.html",
    "title": "Sanov’s Theorem",
    "section": "",
    "text": "Sanov’s Theorem\nConsider a random variable \\(X\\) on the finite alphabet \\(\\{a_1, \\ldots, a_K\\}\\) with \\(\\mathop{\\mathrm{\\mathbb{P}}}(X=a_k) = p_k\\). For \\(N \\gg 1\\), consider a sequence \\((x_1, \\ldots, x_N)\\) obtained by sampling \\(N\\) times independently from \\(X\\) and set\n\\[\n\\widehat{p}_k = \\frac{1}{N} \\, \\sum_{i=1}^N \\, \\mathbf{1} {\\left( x_i = a_k \\right)}\n\\]\nthe proportion of \\(a_k\\) within this sequence. In other words, the empirical distribution obtained from the samples \\((x_1, \\ldots, x_N)\\) reads\n\\[\n\\widehat{p} = \\sum_{k=1}^K \\, \\widehat{p}_k \\, \\delta_{a_k}.\n\\]\nIndeed, the LLN indicates that \\(\\widehat{p}_k \\to p_k\\) as \\(N \\to \\infty\\), and it is important to estimate the probability that \\(\\widehat{p}_k\\) significantly deviates from \\(p_k\\). To this end, note that for another probability vector \\(q=(q_1, \\ldots, q_K)\\) the probability that\n\\[\n(\\widehat{p}_1, \\ldots, \\widehat{p}_K) \\; = \\; (q_1, \\ldots, q_K)\n\\]\nis straightforward to compute and reads\n\\[\n\\mathop{\\mathrm{\\mathbb{P}}}(\\widehat{p} = q) \\; = \\; \\binom{N}{N q_1, \\ldots, N q_K} \\, p_1^{N q_1} \\ldots p_R^{N q_K}.\n\\]\nStirling’s approximation \\(m! \\asymp m \\, \\ln(m)\\) then gives that\n\\[\n\\mathop{\\mathrm{\\mathbb{P}}}(\\widehat{p} = q) \\; \\asymp \\;\n\\exp {\\left( -N \\cdot \\mathop{\\mathrm{D_{\\text{KL}}}}(q,p) \\right)}\n\\]\nwhere \\(\\mathop{\\mathrm{D_{\\text{KL}}}}(q,p) = \\sum_{k=1}^K q_k \\, \\log[q_k / p_k]\\) is the Kullback–Leibler divergence of \\(q\\) from \\(p\\). In other words, as soon as \\(q \\neq p\\), the probability of observing \\(\\widehat{p} \\approx q\\) falls exponentially quickly to zero. With the language of Large Deviations, one can make this statement slightly more precise, rigorous and general, but it is essentially the content of Sanov’s Theorem.\n\n\nRare events happen in the least unlikely manner\nGiven a list of mutually exclusive events \\(E_1, \\ldots, E_R\\) and the knowledge that at least one of these events has taken place, the probability that the event \\(E_k\\) was the one that happened is \\(\\mathop{\\mathrm{\\mathbb{P}}}(E_k) / [\\mathop{\\mathrm{\\mathbb{P}}}(E_1) + \\ldots + \\mathop{\\mathrm{\\mathbb{P}}}(E_R)]\\). The implication is that if all the events are rare, that is \\(p_k \\approx e^{-N \\, I_k} \\ll 1\\), and it is known that one event has indeed occurred, there is a high probability that the event with the smallest \\(I_k\\) value was the one that happened: the rare event took place in the least unlikely manner.\nConsider an iid sequence \\((X_1, \\ldots, X_N)\\) of \\(N \\gg 1\\) discrete real-valued random variables with \\(\\mathop{\\mathrm{\\mathbb{P}}}(X = a_k) = p_k\\) and mean \\(\\mathop{\\mathrm{\\mathbb{E}}}(X) \\in \\mathbb{R}\\). Suppose one observes the rare event\n\\[\n\\frac{1}{N} \\sum_{i=1}^N x_i  \\geq \\mu\n\\tag{1}\\]\nfor some level \\(\\mu\\) significantly above \\(\\mathop{\\mathrm{\\mathbb{E}}}(X)\\). Naturally, the least unlikely way for this to happen is if \\((x_1 + \\ldots + x_N) / N \\, \\approx \\, \\mu\\). Furthermore, one may be interested in the empirical distribution \\(\\widehat{p}\\) associated to the sequence \\((x_1, \\ldots, x_N)\\) when the rare event Equation 1 does happen. The least unlikely empirical distribution is the one that minimizes \\(\\mathop{\\mathrm{D_{\\text{KL}}}}(\\widehat{p}, p)\\) under the constraint that\n\\[\n\\sum_{i=1}^K a_k \\, \\widehat{p}_k = \\mu.\n\\tag{2}\\]\nThe function \\(\\widehat{p} \\mapsto \\mathop{\\mathrm{D_{\\text{KL}}}}(\\widehat{p}, p)\\) is convex and the introduction of Lagrange multipliers shows that the solution to this constraint minimization problem is given by the Boltzmann distribution \\(p_{\\beta_\\mu}\\) defined as\n\\[\np_{\\beta_\\mu}(a_k) = \\frac{ p_k \\, e^{-\\beta_{\\mu} \\, a_k} }{Z(\\beta_{\\mu})}.\n\\tag{3}\\]\nThe parameter \\(\\beta_{\\mu} \\in \\mathbb{R}\\) is chosen so that the constraint Equation 2 be satisfied and the minus sign is to follow the “physics” convention. Note in passing that, in fact, the joint function \\((\\widehat{p},p) \\mapsto \\mathop{\\mathrm{D_{\\text{KL}}}}(\\widehat{p}, p)\\) is convex! As usual, if one defines the log-partition function as \\(\\Phi(\\beta) = -\\log Z(\\beta)\\), with\n\\[\nZ(\\beta) \\; = \\; \\sum_{k=1}^K \\, p_k \\, e^{-\\beta \\, a_k},\n\\]\none obtains that the constraint is equivalent to requiring \\(\\mu = \\frac{d}{d \\beta} \\Phi(\\beta) \\mid_{\\beta = \\beta_\\mu}\\). Furthermore, since \\(\\Phi\\) is smooth and strictly concave, the function \\(\\beta \\mapsto \\left&lt; \\mu, \\beta \\right&gt; - \\Phi(\\beta)\\) is convex and the condition \\(\\mu = \\frac{d}{d \\beta} \\Phi(\\beta) \\mid_{\\beta = \\beta_\\mu}\\) is equivalent to setting\n\\[\n\\beta_{\\mu} \\; = \\; \\mathop{\\mathrm{argmin}}_{\\beta \\in \\mathbb{R}} \\; \\left&lt; \\mu, \\beta \\right&gt; - \\Phi(\\beta).\n\\]\nNaturally, one can now also estimate the probability of the event \\(\\mathop{\\mathrm{\\mathbb{P}}}[(X_1 + \\ldots + X_N)/N \\approx \\alpha]\\) happening since one now knows that it is equivalent (on a log scale) to \\(\\exp[-N \\, \\mathop{\\mathrm{D_{\\text{KL}}}}(p_{\\beta_\\mu}, p)]\\). Algebra gives\n\\[\n\\begin{align}\n\\mathop{\\mathrm{D_{\\text{KL}}}}(p_{\\beta_\\mu}, p)\n&= \\Phi(\\beta_\\mu) - \\left&lt; \\mu, \\beta_\\mu \\right&gt;\\\\\n&= \\max_{\\beta \\in \\mathbb{R}} \\; \\Phi(\\beta) - \\left&lt; \\mu, \\beta \\right&gt;.\n\\end{align}\n\\]\nAs a sanity check, note that since \\(\\Phi(0)=0\\), we have that \\(\\mathop{\\mathrm{D_{\\text{KL}}}}(p_{\\beta_\\mu}, p) \\geq 0\\), as required. The statement that\n\\[\n\\frac{1}{N} \\, \\log \\mathop{\\mathrm{\\mathbb{P}}} {\\left\\{ \\frac{X_1 + \\ldots + X_N}{N} \\; \\approx \\; \\mu \\right\\}}  \\; = \\; - I(\\mu)\n\\]\nwith a (Large Deviation) rate function given by\n\\[\nI(\\mu) \\; = \\; \\max_{\\beta \\in \\mathbb{R}} \\; \\Phi(\\beta) - \\left&lt; \\mu, \\beta \\right&gt;\n\\]\nis more or less the content of Cramer’s Theorem. The rate function \\(I(\\mu)\\) and the function \\(\\log Z( \\textcolor{red}{-}\\beta)\\) are related by a Legendre transform.\n\n\nExample: averaging uniforms…\nNow, to illustrate the above discussion, consider \\(N=10\\) iid uniform random variables on the interval \\([0,1]\\). It is straightforward to simulate these \\(N=10\\) uniforms conditioned on the event that their mean exceeds the level \\(\\mu = 0.7\\), which is a relatively rare event. When this is done repetitively and the empirical distribution is plotted, the resulting distribution is as follows:\n\n\n\n\nMean of \\(10\\) uniforms conditioned on being larger than \\(\\mu = 0.7\\)\n\n\n\nIndeed, the distribution in blue is (very close to) the Boltzmann distribution with density \\(\\mathcal{D}_{\\beta}(x) = e^{-\\beta \\, x} / Z(\\beta)\\) with \\(\\beta \\in \\mathbb{R}\\) chosen so that \\(\\int_{0}^{1} x \\, \\mathcal{D}_{\\beta}(dx) = \\mu\\)."
  },
  {
    "objectID": "notes/Gaussian_Assimilation/gaussian_assimilation_smoothing.html",
    "href": "notes/Gaussian_Assimilation/gaussian_assimilation_smoothing.html",
    "title": "Ensemble Kalman Smoother (EnKS)",
    "section": "",
    "text": "Consider a linear-Gaussian state space model with \\(\\mathbb{R}^{D_x}\\)-valued dynamics \\(X_{t+1} \\sim F \\, X_t + \\mathcal{N}(0,Q)\\) and \\(\\mathbb{R}^{D_y}\\)-valued observations \\(Y_t \\sim H X_t + \\mathcal{N}(0,R)\\). Assuming a Gaussian initial distribution, the filtering distributions \\(\\mathop{\\mathrm{\\mathbb{P}}}(X_t \\in dx \\, | Y_{1:t}) \\equiv \\mathcal{N}(\\mu_{t|t}, P_{t|t})\\) are Gaussian and can be sequentially computed with the Kalman Filter. Similarly, the predictive distributions \\(\\mathop{\\mathrm{\\mathbb{P}}}(X_{t+1} \\in dx \\, | Y_{1:t}) \\equiv \\mathcal{N}(\\mu_{t+1|t}, P_{t+1|t})\\) are straightforward to obtain from the filtering distributions: \\(\\mu_{t+1|t} = F \\, \\mu_{t|t}\\) and \\(P_{t+1|t} = F \\, P_{t|t} \\, F^\\top + Q\\). Given observations \\(y_{1:T} \\equiv (y_1, \\ldots, y_T)\\) and \\(1 \\leq t \\leq T\\), the smoothing distributions \\(\\mathop{\\mathrm{\\mathbb{P}}}(X_t \\in dx \\, | Y_{1:T}) \\equiv \\mathcal{N}(\\mu_{t|T}, P_{t|T})\\) can computed by performing a “backward pass”. Since everything is linear and Gaussian, it is just an exercise in Linear Algebra & Gaussian-conditioning, as described by the Rauch-Tung-Striebel (Rauch, Tung, and Striebel 1965) smoothing recursions. The backward recursion reads\n\\[\n\\left\\{\n\\begin{aligned}\n\\mu_{t|T}\n&= \\mu_{t|t} + B_t \\,  {\\left( \\mu_{t+1|T} - \\mu_{t+1|t} \\right)} \\\\\nP_{t|T}\n&=\nP_{t|t} + B_t  {\\left(  P_{t+1|T} - P_{t+1|t}  \\right)}  B^\\top_{t}\n\\end{aligned}\n\\right.\n\\tag{1}\\]\nand allows one to compute the smoothing means and covariances matrices \\((\\mu_{t|T}, P_{t|T})\\) for \\(1 \\leq t \\leq T\\) starting from the knowledge of \\((\\mu_{T|T}, P_{T|T})\\). In Equation 1, the smoothing gain matrix \\(B_t\\) is given by\n\\[\n\\begin{align}\nB_t &=\n\\mathop{\\mathrm{Cov}}(X_t, X_{t+1} \\, | y_{1:t}) \\, \\mathop{\\mathrm{Var}}(X_{t+1} \\, | y_{1:t})^{-1} \\\\\n&=\nP_{t|t} F^\\top \\,  {\\left( F \\, P_{t|t} \\, F^\\top + Q \\right)} ^{-1}.\n\\end{align}\n\\tag{2}\\]\nThe Ensemble Kalman Filter (EnKF) is a non-linear equivalent of the Kalman filter, and the purpose of these notes is to derive the equivalent “ensemble version” of the backward recursion Equation 1. For this purpose, it is important to understand slightly better the role of the smoothing gain matrix \\(B_t\\). Consider the pair of random variable \\((X^f_t, X^p_{t+1})\\) distributed according to the joint distribution between the filtering distribution at time \\(t\\) and the predictive distribution at time \\(t+1\\) in the sense that\n\\[\n(X^f_t, X^p_t) \\; \\underbrace{=}_{\\text{(law)}}\\; (X_t, X_{t+1} \\, \\mid \\, y_{1:t}).\n\\]\nThis means that \\(X^f_t \\sim \\mathcal{N}(\\mu_{t|t}, P_{t|t})\\) and \\(X^p_{t+1} \\sim \\mathcal{N}(\\mu_{t+1|t}, P_{t+1|t})\\) and \\(X^p_t = F \\, X^f_t + \\mathcal{N}(0, Q)\\). Furthermore, Equation 2 and the standard gaussian conditional probabilities formulas give that the conditional means and covariances are given by\n\\[\n\\left\\{\n\\begin{align}\n\\textrm{Mean} {\\left( X^f_t | (X^p_{t+1}=x_{t+1}) \\right)}  \n\\; &= \\;\n\\mu_{t|t} + B_t (x_{t+1} - \\mu_{t+1|t}) \\\\\n\\textrm{Cov} {\\left( X^f_t | (X^p_{t+1}=x_{t+1}) \\right)}  \n\\; &= \\;\nP_{t|t} - B_t \\, P_{t+1|t} \\, B_t^\\top.\n\\end{align}\n\\right.\n\\tag{3}\\]\nThe above expression for the conditional mean also shows that the matrix \\(B_t\\) is a minimizer of the loss\n\\[\nM \\; \\mapsto \\;\n\\mathop{\\mathrm{\\mathbb{E}}} {\\left(  \\left\\| (X^f_t - \\mu_{t|t}) - B (X^p_{t+1} - \\mu_{t+1|t}) \\right\\|^2  \\right)}\n\\]\nover all matrices \\(M \\in \\mathbb{R}^{D_x, D_x}\\). Heuristically, this shows that the smoothing gain matrix \\(B_t\\) can easily be computed by regressing \\(X^f_t\\) against \\(X^p_{t+1}\\). We can use this remark to build an ensemble version of the backward recursion Equation 1. Recall that when running a EnKF for filtering the observations \\(y_{1:T}\\), the final stage proceeds in two steps:\n\nObtain an ensemble of particles \\(X^{i,p}_{T} = F \\, X^{i,f}_{T-1} + \\mathcal{N}(0,Q)\\) that approximate the predictive distribution \\(\\mathop{\\mathrm{\\mathbb{P}}}(X_T | y_{1:T-1})\\).\n\nAssimilate the last observation \\(y_T\\) using the Kalman gain matrix \\(K_T\\) and the correction \\(\\Delta_T^i = K_T \\, (\\tilde{y}_{i,\\star} - H \\, X^{i,p}_T)\\) by setting \\[\nX^{i,s}_T = X^{i,p}_T + \\Delta_T^i.\n\\tag{4}\\] The particles \\(X^{i,s}_T\\) approximate the smoothing distribution \\(\\mathop{\\mathrm{\\mathbb{P}}}(X_T | y_{1:T})\\).\n\nFollowing our discussion of the smoothing gain matrix \\(B_{t}\\) and Equation 4, it seems sensible to set\n\\[\n\\begin{align}\nX^{i,s}_{T-1}\n&= X^{i,f}_{T-1} + B_{T-1} \\, \\Delta^i_T\\\\\n&= X^{i,f}_{T-1} + B_{T-1} \\, (X^{i,s}_{T} - X^{i,p}_{T})\n\\end{align}\n\\tag{5}\\]\nand hope that the ensemble of updated particles \\(X^{i,s}_{T-1}\\) approximate the smoothing distribution \\(\\mathop{\\mathrm{\\mathbb{P}}}(X_{T-1} | y_{1:T})\\). In words, the particle \\(X^{i,s}_{T-1}\\) is obtained by “pulling” the correction term \\(\\Delta^i_{T} = X^{i,s}_{T} - X^{i,p}_{T}\\) back to \\(X^{i,f}_{T-1}\\) through the “regression” smoothing gain matrix \\(B_{T-1}\\). To check that the particles \\(X^{i,s}_{T-1}\\) indeed approximate the smoothing distribution \\(\\mathop{\\mathrm{\\mathbb{P}}}(X_{T-1} \\,|y_{1:T})\\), it suffices to compute the mean/variance and verify that they are matching the one given by Equation 1. Recall that Equation 3 gives that the filtering/predictive distributions satisfy\n\\[\nX^f_{T-1} = \\mu_{T-1|T-1} + B_{T-1} \\, (X^p_{T} - \\mu_{T|T-1}) + \\varepsilon_t\n\\]\nwhere \\(\\varepsilon_t \\sim \\mathcal{N}(0, P_{T-1|T-1} - B_{T-1} \\, P_{T|T-1} \\, B_{T-1}^\\top)\\) is independent from all other sources of randomness. Plugging this into Equation 5 gives that\n\\[\nX^{i,s}_{T-1}\n=\n\\mu_{T-1|T-1} + B_{T-1} \\, (X^{i,s}_{T} - \\mu_{T|T-1}) + \\varepsilon_t.\n\\]\nSince the \\(X^{i,s}_{T}\\) are distributed according to the smoothing distribution, i.e. \\(X^{i,s}_{T} \\sim \\mathcal{N}(\\mu_{T|T}, P_{T|T})\\), this immediately shows that \\(X^{i,s}_{T-1}\\) is Gaussian with\n\\[\n\\left\\{\n\\begin{align}\n\\textrm{Mean} &= \\mu_{T-1|T} = \\mu_{T-1|T-1} + B_{T-1} \\,  {\\left( \\mu_{T|T} - \\mu_{T|T-1} \\right)} \\\\\n\\textrm{Covariance} &= P_{T-1|T} = P_{T-1|T-1} + B_{T-1}  {\\left(  P_{T|T} - P_{T|T-1}  \\right)}  B^\\top_{T-1},\n\\end{align}\n\\right.\n\\]\nas it should. One can then iterate this construction to obtain particle approximations of the smoothing distributions \\(\\mathop{\\mathrm{\\mathbb{P}}}(X_t | y_{1:T})\\) for \\(1 \\leq t \\leq T\\) by running a backward pass and recursively setting\n\\[\nX^{i,s}_t \\; = \\; X^{i,f}_t + B_t \\,  {\\left( X^{i,s}_{t+1} - X^{i,p}_{t+1} \\right)} .\n\\]\nThe ensemble of particles \\(X^{i,s}_t\\) approximates the smoothing distribution \\(\\mathop{\\mathrm{\\mathbb{P}}}(X_t | y_{1:T})\\). In a nonlinear setting, it suffices to approximate the smoothing gain matrices with\n\\[\n\\widehat{B}_t = \\mathop{\\mathrm{Cov}} {\\left(  x^f_{t,i}, x^p_{t+1,i}  \\right)}  \\, \\mathop{\\mathrm{Var}} {\\left(  x^p_{t+1,i}  \\right)} ^{-1}.\n\\]\n[Experiments: TODO]\n\n\n\n\nReferences\n\nRauch, Herbert E, F Tung, and Charlotte T Striebel. 1965. “Maximum Likelihood Estimates of Linear Dynamic Systems.” AIAA Journal 3 (8): 1445–50."
  },
  {
    "objectID": "notes/Gaussian_Assimilation/gaussian_assimilation.html",
    "href": "notes/Gaussian_Assimilation/gaussian_assimilation.html",
    "title": "Gaussian Assimilation & the EnKF",
    "section": "",
    "text": "Assume a prior Gaussian prior distribution \\(\\pi_0 \\equiv \\mathcal{N}(m_0,P_0)\\) and a noisy observation \\(y_\\star \\in \\mathbb{R}^{D_y}\\) with\n\\[\ny_\\star = H x + \\xi\n\\qquad \\text{with} \\qquad\n\\xi \\sim \\mathcal{N}(0,R)\n\\]\nwhere \\(x \\in \\mathbb{R}^{D_x}\\) is an unknown quantity of interest. The posterior distribution \\(\\pi \\equiv \\mathcal{N}(m,P)\\) is Gaussian and is given by\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &= m_0 + P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}(y_\\star - H m)\\\\\nP &= P_0 - P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1} \\, H P_0,\n\\end{aligned}\n\\right.\n\\end{align}\n\\]\nas standard Gaussian conditioning shows it. This can also be written as\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &= m_0 + K \\, (y_\\star - H m_0)\\\\\nP &= (I - K \\, H) P_0,\n\\end{aligned}\n\\right.\n\\end{align}\n\\]\nfor Kalman Gain Matrix \\(K \\in \\mathbb{R}^{D_x,D_y}\\) defined as\n\\[\nK \\; = \\; P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}.\n\\tag{1}\\]\nwhich can also be expressed as\n\\[\nK \\; = \\; \\mathop{\\mathrm{Cov}}(X, HX) \\, \\mathop{\\mathrm{Var}}(Y)^{-1}.\n\\]\nfor \\(X \\sim \\pi_0\\) and \\(Y = HX + \\mathcal{N}(0,R)\\); this point of view can be a useful for establishing generalization to non-linear scenarios. The important remark is that the posterior covariance matrix \\(P \\in \\mathbb{R}^{D_x,D_x}\\) and the posterior mean \\(m \\in \\mathbb{R}^{D_x}\\) can also be expressed as\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &=  \\textcolor{red}{(I - K \\, H)} \\, m_0 +  \\textcolor{blue}{K} \\, y_\\star\\\\\nP &= \\;  \\textcolor{red}{(I - K \\, H)} \\, P_0 \\,  \\textcolor{red}{(I - K \\, H)^\\top} +  \\textcolor{blue}{K} R  \\textcolor{blue}{K^\\top}.\n\\end{aligned}\n\\right.\n\\end{align}\n\\tag{2}\\]\nThis shows that \\(P\\) is indeed positive semi-definite. More importantly, this gives a mechanism for transforming samples from the prior distributions into samples from the posterior distributions. Indeed, consider \\(N\\) iid samples from the prior distribution, \\(x_1, \\ldots, x_N \\sim \\pi_0(dx)\\), and set\n\\[\n\\widetilde{x}_i\n=\n\\textcolor{red}{(I - K \\, H)} \\, x_i +  \\textcolor{blue}{K}(y_\\star + \\xi_i)\n\\]\nfor iid noise terms \\(\\xi_i \\sim \\mathcal{N}(0,R)\\). From Equation 2 it is clear that \\(\\widetilde{x}_1, \\ldots, \\widetilde{x}_N\\) are iid samples from the Gaussian posterior distribution \\(\\pi = \\mathcal{N}(m,P)\\). It is more intuitive to write this as\n\\[\n\\widetilde{x}_i\n=\nx_i + K \\, (  \\textcolor{green}{ \\widetilde{y}_{i,\\star} } - H \\, x_i)\n\\]\nwhere \\(\\widetilde{y}_{i,\\star} \\in \\mathbb{R}^{D_y}\\) are fake observations that are obtained by perturbing the actual observation \\(y_\\star\\) with additive Gaussian noise terms with covariance \\(R\\),\n\\[\n\\textcolor{green}{\\widetilde{y}_{i,\\star} \\, = \\, y_\\star + \\xi_i}.\n\\]\n\n\nSuppose that we would like to estimate \\(x \\in \\mathbb{R}^{D_x}\\) from the noisy observation\n\\[\ny_\\star = \\mathcal{H}(x) + \\xi\n\\qquad \\text{with} \\qquad\n\\xi \\sim \\mathcal{N}(0,R)\n\\]\nand possibly-nonlinear observation operator \\(\\mathcal{H}: \\mathbb{R}^{D_x} \\to \\mathbb{R}^{D_y}\\). Assume that we also have \\(N\\) samples \\(x_1, \\ldots, x_N\\) generated from some (unkown) prior distribution. For example, these samples could come from another numerical procedure. In order to obtain \\(N\\) approximate samples from the posterior distribution, one can set\n\\[\n\\widetilde{x}_i\n=\nx_i + \\widehat{K} \\, [  \\textcolor{green}{ \\widetilde{y}_{i,\\star} } - \\mathcal{H}(x_i)]\n\\]\nfor fake observations \\( \\textcolor{green}{\\widetilde{y}_{i,\\star} } = y_\\star + \\xi_i\\). The approximate Kalman gain matrix \\(\\widehat{K}\\) is obtained by noting that in Equation 1 giving\n\\[\nK \\; = \\; P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}\n\\]\nwe have \\(P_0 H^\\top = \\mathop{\\mathrm{Cov}}(X,HX)\\) and \\(H P_0 H^\\top = \\mathop{\\mathrm{Var}}(HX)\\) for \\(X \\sim \\pi_0\\). This means that an approximate Kalman matrix can be obtained using empirical estimates of these covariance matrices:\n\\[\n\\widehat{K} \\; = \\; \\widehat{\\mathop{\\mathrm{Cov}}}([x_i]_i, [\\mathcal{H}(x_i)]_i) \\,  {\\left( \\widehat{\\mathop{\\mathrm{Var}}}([\\mathcal{H}(x_i)]_i) + R \\right)} ^{-1}.\n\\]\nThese updates form the basis of the Ensemble Kalman filter (EnKF), and very successful and scalable approach to data-assimilation of high-dimensional dynamical systems. This method is operationally employed across various weather forecasting centers across the globe.\n\n\n\n\nEnKF Bible by Geir Evensen\n\n\n\n\n\n\nInterestingly enough, the remarks above can be design in a relatively principled manner a derivative free optimizer (Huang et al. 2022). For example, assume one would like to minimize a functional of the type\n\\[\n\\Psi(x) \\; = \\; \\|y_\\star - \\mathcal{H}(x)\\|^2_{R^{-1}}.\n\\]\nOne can start with a cloud of particles \\(x_1, \\ldots, x_N\\) and keep updating them by assuming that one assimilates the noisy observation \\(y_\\star\\) generated from a postulated observation process\n\\[\ny_\\star = \\mathcal{H}(x) + \\varepsilon^{-1} \\, \\xi\n\\]\nfor \\(\\xi \\sim \\mathcal{N}(0,R)\\) and \\(\\varepsilon\\ll 1\\) a “step-size”. Each assimilation step steers the cloud of points towards the rights direction. Careful choice of the step-size \\(\\varepsilon\\) is often crucial, as in any optimization procedure. It is indeed related to Information-Geometric Optimization Algorithms (IGO): the article (Ollivier et al. 2017) is beautiful!"
  },
  {
    "objectID": "notes/Gaussian_Assimilation/gaussian_assimilation.html#ensemble-kalman-updates",
    "href": "notes/Gaussian_Assimilation/gaussian_assimilation.html#ensemble-kalman-updates",
    "title": "Gaussian Assimilation & the EnKF",
    "section": "",
    "text": "Assume a prior Gaussian prior distribution \\(\\pi_0 \\equiv \\mathcal{N}(m_0,P_0)\\) and a noisy observation \\(y_\\star \\in \\mathbb{R}^{D_y}\\) with\n\\[\ny_\\star = H x + \\xi\n\\qquad \\text{with} \\qquad\n\\xi \\sim \\mathcal{N}(0,R)\n\\]\nwhere \\(x \\in \\mathbb{R}^{D_x}\\) is an unknown quantity of interest. The posterior distribution \\(\\pi \\equiv \\mathcal{N}(m,P)\\) is Gaussian and is given by\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &= m_0 + P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}(y_\\star - H m)\\\\\nP &= P_0 - P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1} \\, H P_0,\n\\end{aligned}\n\\right.\n\\end{align}\n\\]\nas standard Gaussian conditioning shows it. This can also be written as\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &= m_0 + K \\, (y_\\star - H m_0)\\\\\nP &= (I - K \\, H) P_0,\n\\end{aligned}\n\\right.\n\\end{align}\n\\]\nfor Kalman Gain Matrix \\(K \\in \\mathbb{R}^{D_x,D_y}\\) defined as\n\\[\nK \\; = \\; P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}.\n\\tag{1}\\]\nwhich can also be expressed as\n\\[\nK \\; = \\; \\mathop{\\mathrm{Cov}}(X, HX) \\, \\mathop{\\mathrm{Var}}(Y)^{-1}.\n\\]\nfor \\(X \\sim \\pi_0\\) and \\(Y = HX + \\mathcal{N}(0,R)\\); this point of view can be a useful for establishing generalization to non-linear scenarios. The important remark is that the posterior covariance matrix \\(P \\in \\mathbb{R}^{D_x,D_x}\\) and the posterior mean \\(m \\in \\mathbb{R}^{D_x}\\) can also be expressed as\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &=  \\textcolor{red}{(I - K \\, H)} \\, m_0 +  \\textcolor{blue}{K} \\, y_\\star\\\\\nP &= \\;  \\textcolor{red}{(I - K \\, H)} \\, P_0 \\,  \\textcolor{red}{(I - K \\, H)^\\top} +  \\textcolor{blue}{K} R  \\textcolor{blue}{K^\\top}.\n\\end{aligned}\n\\right.\n\\end{align}\n\\tag{2}\\]\nThis shows that \\(P\\) is indeed positive semi-definite. More importantly, this gives a mechanism for transforming samples from the prior distributions into samples from the posterior distributions. Indeed, consider \\(N\\) iid samples from the prior distribution, \\(x_1, \\ldots, x_N \\sim \\pi_0(dx)\\), and set\n\\[\n\\widetilde{x}_i\n=\n\\textcolor{red}{(I - K \\, H)} \\, x_i +  \\textcolor{blue}{K}(y_\\star + \\xi_i)\n\\]\nfor iid noise terms \\(\\xi_i \\sim \\mathcal{N}(0,R)\\). From Equation 2 it is clear that \\(\\widetilde{x}_1, \\ldots, \\widetilde{x}_N\\) are iid samples from the Gaussian posterior distribution \\(\\pi = \\mathcal{N}(m,P)\\). It is more intuitive to write this as\n\\[\n\\widetilde{x}_i\n=\nx_i + K \\, (  \\textcolor{green}{ \\widetilde{y}_{i,\\star} } - H \\, x_i)\n\\]\nwhere \\(\\widetilde{y}_{i,\\star} \\in \\mathbb{R}^{D_y}\\) are fake observations that are obtained by perturbing the actual observation \\(y_\\star\\) with additive Gaussian noise terms with covariance \\(R\\),\n\\[\n\\textcolor{green}{\\widetilde{y}_{i,\\star} \\, = \\, y_\\star + \\xi_i}.\n\\]\n\n\nSuppose that we would like to estimate \\(x \\in \\mathbb{R}^{D_x}\\) from the noisy observation\n\\[\ny_\\star = \\mathcal{H}(x) + \\xi\n\\qquad \\text{with} \\qquad\n\\xi \\sim \\mathcal{N}(0,R)\n\\]\nand possibly-nonlinear observation operator \\(\\mathcal{H}: \\mathbb{R}^{D_x} \\to \\mathbb{R}^{D_y}\\). Assume that we also have \\(N\\) samples \\(x_1, \\ldots, x_N\\) generated from some (unkown) prior distribution. For example, these samples could come from another numerical procedure. In order to obtain \\(N\\) approximate samples from the posterior distribution, one can set\n\\[\n\\widetilde{x}_i\n=\nx_i + \\widehat{K} \\, [  \\textcolor{green}{ \\widetilde{y}_{i,\\star} } - \\mathcal{H}(x_i)]\n\\]\nfor fake observations \\( \\textcolor{green}{\\widetilde{y}_{i,\\star} } = y_\\star + \\xi_i\\). The approximate Kalman gain matrix \\(\\widehat{K}\\) is obtained by noting that in Equation 1 giving\n\\[\nK \\; = \\; P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}\n\\]\nwe have \\(P_0 H^\\top = \\mathop{\\mathrm{Cov}}(X,HX)\\) and \\(H P_0 H^\\top = \\mathop{\\mathrm{Var}}(HX)\\) for \\(X \\sim \\pi_0\\). This means that an approximate Kalman matrix can be obtained using empirical estimates of these covariance matrices:\n\\[\n\\widehat{K} \\; = \\; \\widehat{\\mathop{\\mathrm{Cov}}}([x_i]_i, [\\mathcal{H}(x_i)]_i) \\,  {\\left( \\widehat{\\mathop{\\mathrm{Var}}}([\\mathcal{H}(x_i)]_i) + R \\right)} ^{-1}.\n\\]\nThese updates form the basis of the Ensemble Kalman filter (EnKF), and very successful and scalable approach to data-assimilation of high-dimensional dynamical systems. This method is operationally employed across various weather forecasting centers across the globe.\n\n\n\n\nEnKF Bible by Geir Evensen\n\n\n\n\n\n\nInterestingly enough, the remarks above can be design in a relatively principled manner a derivative free optimizer (Huang et al. 2022). For example, assume one would like to minimize a functional of the type\n\\[\n\\Psi(x) \\; = \\; \\|y_\\star - \\mathcal{H}(x)\\|^2_{R^{-1}}.\n\\]\nOne can start with a cloud of particles \\(x_1, \\ldots, x_N\\) and keep updating them by assuming that one assimilates the noisy observation \\(y_\\star\\) generated from a postulated observation process\n\\[\ny_\\star = \\mathcal{H}(x) + \\varepsilon^{-1} \\, \\xi\n\\]\nfor \\(\\xi \\sim \\mathcal{N}(0,R)\\) and \\(\\varepsilon\\ll 1\\) a “step-size”. Each assimilation step steers the cloud of points towards the rights direction. Careful choice of the step-size \\(\\varepsilon\\) is often crucial, as in any optimization procedure. It is indeed related to Information-Geometric Optimization Algorithms (IGO): the article (Ollivier et al. 2017) is beautiful!"
  },
  {
    "objectID": "notes/wasserstein_langevin/wasserstein_langevin.html",
    "href": "notes/wasserstein_langevin/wasserstein_langevin.html",
    "title": "Wasserstein Gradients & Langevin Diffusions",
    "section": "",
    "text": "Overdamped Langevin Diffusion as a gradient flow on the space of probability distributions endowed with the \\(2\\)-Wasserstein metric\n\n\n\nConsider a target probability density \\(\\pi(x) = \\frac{\\overline{\\pi}(x)}{\\mathcal{Z}}\\) on \\(\\mathbb{R}^D\\) that is known up to a normalizing constant \\(\\mathcal{Z}&gt; 0\\). We also have a different probability density \\(p_0(x)\\). The goal is to gradually tweak \\(p_0(x)\\) so that it eventually matches \\(\\pi(x)\\). More concretely, we aim to perform a gradient descent on the space of probability distributions to reduce the functional\n\\[\n\\mathcal{F}(p) \\; = \\; \\mathop{\\mathrm{D_{\\text{KL}}}} {\\left( p, \\pi \\right)}  \\; = \\; \\int p(x) \\, \\log  {\\left\\{ \\frac{p(x)}{\\overline{\\pi}(x)} \\right\\}}  \\, dx \\, + \\, \\textrm{(constant)}.\n\\]\nThis approach can be discretized: assume \\(N \\gg 1\\) particles \\(X_0^1, \\ldots, X_0^N \\in \\mathbb{R}^D\\) forming an empirical distribution that approximates \\(p_0(dx)\\),\n\\[\np_0(dx) \\; \\approx \\; \\frac{1}{N} \\sum_{i=1}^N \\, \\delta_{X_0^i}(dx).\n\\]\nDefine \\(X_{\\delta}^i = X_0^i + \\delta_t \\, \\mu(X_0^i)\\) where \\(\\delta_t \\ll 1\\) denotes a time discretization parameter and \\(\\mu:\\mathbb{R}^D \\to \\mathbb{R}^D\\) is a “drift” function. Finding a suitable ‘drift function’ is the main problem. According to the Fokker-Planck equation, the computed empirical distribution\n\\[\np_{\\delta_t}(dx) \\; \\approx \\; \\frac{1}{N} \\sum_{i=1}^N \\, \\delta_{X_{\\delta_t}^i}(dx)\n\\]\napproximates \\(p_{\\delta_t}(x)\\) given by\n\\[\n\\frac{p_{\\delta_t}(x)- p_0(x)}{\\delta_t} \\; = \\; -\\nabla \\cdot  {\\left[ \\mu(x) \\, p_0(x) \\right]} .\n\\tag{1}\\]\nWhat is the optimal drift function \\(\\mu: \\mathbb{R}^D \\to \\mathbb{R}^D\\) that ensures that \\(p_{\\delta_t}\\) comes as close as possible to \\(\\pi\\)? Typically, we select \\(\\mu:\\mathbb{R}^D \\to \\mathbb{R}^D\\) such that the quantity \\(\\mathcal{F}(p_{\\delta_t})\\) is minimized, provided that \\(p_{\\delta_t}\\) is not drastically different from \\(p_0\\). One method is to use the \\(L^2\\) Wasserstein distance and assume the constraint\n\\[\n\\mathop{\\mathrm{D_{\\text{Wass}}}}(p_{0}, p_{\\delta_t}) \\approx \\int p_0(x) \\, \\| \\delta_t \\, \\mu(x) \\|^2 \\, dx \\leq \\varepsilon\n\\tag{2}\\]\nfor a parameter \\(\\varepsilon\\ll 1\\). More pragmatically, it is generally easier (eg. proximal methods) to minimize the joint objective\n\\[\n\\mathcal{F}(p_{\\delta_t}) + \\frac{1}{2 \\varepsilon} \\, \\mathop{\\mathrm{D_{\\text{Wass}}}}(p_{0}, p_{\\delta_t}).\n\\tag{3}\\]\nBased on equations Equation 1 and Equation 2, a first-order expansion shows that the joint objective Equation 3 can be approximated by\n\\[\n\\begin{align}\n-\\int &\\nabla \\cdot \\Big\\{  \\textcolor{red}{[\\delta_t \\mu]}(x) \\, p_0(x) \\Big\\} \\, \\log  {\\left\\{  \\frac{p_0(x)}{\\overline{\\pi}(x)}  \\right\\}} \\, dx \\, \\\\ &\\qquad + \\qquad \\, \\frac{1}{2 \\varepsilon} \\, \\int p_0(x) \\, \\|  \\textcolor{red}{[\\delta_t \\, \\mu]}(x) \\|^2 \\, dx,\n\\end{align}\n\\tag{4}\\]\na relatively straightforward quadratic function of the drift function \\(\\mu: \\mathbb{R}^D \\to \\mathbb{R}^D\\). The optimal drift function, ie. the minimizer of Equation 4, is given by \\[\n\\mu(x) \\; = \\; - {\\left(  \\frac{\\varepsilon}{\\delta_t}  \\right)}  \\, \\nabla \\log  {\\left\\{  \\frac{p_0(x)}{\\overline{\\pi}(x) }  \\right\\}} .\n\\]\nPut simply, this suggests that we should select the drift function proportional to \\(-\\nabla \\log[p_0(x) / \\overline{\\pi}(x)]\\). To implement this scheme, we begin by sampling \\(N \\gg 1\\) particles \\(X_0^i \\sim p_0(dx)\\) and let evolve each particle according to the following differential equation\n\\[\n\\frac{d}{dt} X_t^i \\; = \\; - \\nabla \\log  {\\left\\{  \\frac{p_t(X_t^i) }{ \\overline{\\pi}(X_t^i) }  \\right\\}}\n\\]\nwhere \\(p_t\\) is the density of the set of particles at time \\(t\\). It is the usual diffusion-ODE trick for describing the evolution of the density of an overdamped Langevin diffusion,\n\\[\ndX_t \\; = \\; -\\nabla \\log \\overline{\\pi}(X_t) \\, dt \\; + \\; \\sqrt{2} \\, dW_t.\n\\]\nThis can be shown by writing down the associated Fokker-Planck equation. This heuristic discussion shows that minimizing \\(\\mathop{\\mathrm{D_{\\text{KL}}}}(p, \\pi)\\) by introducing a gradient flow in the space of probability distributions with the Wasserstein metric essentially produces a standard overdamped Langevin diffusion. Indeed, transforming this heuristic discussion into a formal statement is not trivial: the constructive proof in (Jordan, Kinderlehrer, and Otto 1998) is now usually referred to as the JKO scheme.\nThe above derivation shows that the Wasserstein distance plays particularly well with minimizing functionals of the space of probability distributions. The same heuristic discussion shows that minimizing a functional of the type\n\\[\n\\mathcal{F}(p) \\; = \\; \\int \\Phi[p(x)] \\, \\nu(dx)\n\\]\nfor some cost function \\(\\Phi: (0, \\infty) \\to \\mathbb{R}\\) and distribution \\(\\nu(dx)\\) leads to choosing a drift function \\(\\mu:\\mathbb{R}\\to \\mathbb{R}\\) minimizing\n\\[\n\\int -\\nabla \\cdot \\Big\\{  \\textcolor{red}{[\\delta_t \\, \\mu]}(x) \\, p(x) \\Big\\} \\Phi'[p(x)] \\, \\nu(dx)\n\\, + \\, \\frac{1}{2 \\varepsilon} \\, \\int p_0(x) \\, \\|  \\textcolor{red}{[\\delta_t \\, \\mu]}(x) \\|^2 \\, dx.\n\\]\nThis can be approached identically to what as been done in the case of minimizing \\(\\mathop{\\mathrm{D_{\\text{KL}}}}(p, \\pi)\\).\n\n\n\n\nReferences\n\nJordan, Richard, David Kinderlehrer, and Felix Otto. 1998. “The Variational Formulation of the Fokker–Planck Equation.” SIAM Journal on Mathematical Analysis 29 (1). SIAM: 1–17."
  },
  {
    "objectID": "notes/index_notes.html",
    "href": "notes/index_notes.html",
    "title": "Notes",
    "section": "",
    "text": "Notes for students, half-baked write-ups for myself, and probably other random maths/stats/ML texts. Certainly full of typos, comments welcome!\n\n\nNotes indexed by categories.\n\n\nDenoising Diffusion Probabilistic Models\n\nNoising and Reverse Ornstein-Uhlenbeck\nFrom Denoising Diffusion to ODEs\nReverse diffusions, Score & Tweedie\n\n\n\nInformation Theory\n\nReferences & Readings\nEntropy and Basic Definitions\nShannon Source Coding Theorem\nFano’s inequality\nShearer’s Lemma\n\n\n\nMonte-Carlo methods\n\nDeriving the Langevin MCMC algorithm\nGaussian Assimilation & the EnKF\nEnsemble Kalman Smoothers\nMCMC with deterministic proposals\nRWM & HMC on Manifolds\n\n\n\nProbability Misc\n\nAuxiliary variable trick\nSanov’s Theorem\nWasserstein Gradients & Langevin Diffusions\nPoisson Equation & Asymptotic Variance\nAveraging and Homogenization\nJoe Doob & Change of measures on path space\nGirsanov and importance sampling\nDoob, Girsanov and Bellman"
  },
  {
    "objectID": "notes/girsanov/girsanov.html",
    "href": "notes/girsanov/girsanov.html",
    "title": "Girsanov and Importance Sampling",
    "section": "",
    "text": "Igor Girsanov (1934 – 1967)\n\n\n\nLet \\(q(dx) \\equiv \\mathcal{N}(\\mu,\\Gamma)\\) be the Gaussian distribution with mean \\(\\mu \\in \\mathbb{R}^D\\) and covariances \\(\\Gamma \\in \\mathbb{R}^{D \\times D}\\). For a direction \\(u \\in \\mathbb{R}^D\\), consider the distribution \\(q^{u}(dx) \\equiv \\mathcal{N}(\\mu + \\Gamma^{1/2} \\, u, \\Gamma)\\), i.e. the same Gaussian distribution but shifted by an amount \\(\\Gamma^{1/2} \\, u\\). Algebra directly gives that\n\\[\n\\frac{q^{u}(x)}{q(x)}\n=\n\\exp {\\left\\{ - \\frac{1}{2} \\| u\\|^2 + \\left&lt; u, \\, \\Gamma^{-1/2}(x-\\mu) \\right&gt; \\right\\}} .\n\\tag{1}\\]\nWe will see that, not very surprisingly, a similar change-of-probability result holds in continuous time. On the time interval \\([0,T]\\), let \\(W_t\\) be a standard Brownian motion in \\(\\mathbb{R}^D\\) and \\(X_t\\) be the solution to the SDE\n\\[\ndX_t \\; = \\; b(X_t) \\, dt + \\sigma(X_t) \\, dW_t\n\\tag{2}\\]\nfor some drift \\(b: \\mathbb{R}^D \\to \\mathbb{R}^D\\) and diffusion \\(\\sigma: \\mathbb{R}^D \\to \\mathbb{R}^{D \\times D}\\) and initial distribution \\(\\mu_0(dx_0)\\). This SDE defines a probability measure \\(\\mathbb{P}\\) on the path-space \\(C([0,T]; \\mathbb{R}^D)\\), the space of continuous functions from \\([0,T]\\) to \\(\\mathbb{R}^D\\). Consider a perturbation drift function \\(u: \\mathbb{R}^D \\to \\mathbb{R}^D\\) and associated perturbed SDE given by\n\\[\ndX_t^u \\; = \\; b(X_t^u) \\, dt + \\sigma(X_t^u) \\,  {\\left\\{  dW_t +  \\textcolor{blue}{u(X_t^u) \\, dt}  \\right\\}} .\n\\tag{3}\\]\nThis perturbed SDE, started from the same initial distribution \\(\\mu_0(dx_0)\\), defines a probability measure \\(\\mathbb{P}^u\\) on the path-space \\(C([0,T]; \\mathbb{R}^D)\\) and it is often useful to understand the Radon-Nikodym derivative of \\(\\mathbb{P}^u\\) with respect to \\(\\mathbb{P}\\). I have never really liked the way this is usually derived, and also never really remember the result. It takes only a few lines of algebra to re-derive these results, at least informally. For this purpose, consider a simpler Euler discretization of the SDE with time-discretization \\(\\delta = T/N\\) for \\(N \\gg 1\\). Consider a discretized paths \\((x_0, x_{\\delta}, \\ldots, x_{T})\\) of Equation 2 obtained by iterating the update\n\\[\nx_{t_{k+1}} \\; = \\; x_{t_k} + b(x_{t_k})\\,\\delta + \\sigma(x_{t_k}) \\, (\\Delta W_{t_k})\n\\]\nwith \\(t_k = k\\delta\\) and \\(\\Delta W_{t_k} = W_{t_{k+1}} - W_{t_k}\\). The probability of observing such a path reads \\[\n\\frac{1}{\\mathcal{Z}} \\, \\mu_0(x_0) \\, \\exp {\\left\\{ -\\frac{1}{2 \\delta} \\sum_{k=0}^{N-1}\n\\|x_{t_{k+1}} - [x_{t_k} + b(x_{t_k})\\,\\delta]\\|^2_{\\Gamma^{-1}(x_{t_k}) } \\right\\}}\n\\]\nwith \\(\\Gamma(x) \\equiv \\sigma(x) \\sigma^\\top(x)\\) the volatility matrix and an irrelevant multiplicative constant \\(\\mathcal{Z}\\). One obtains a similar expression for a discretized path of the perturbed SDE Equation 3 and the ratio of these two quantities equals\n\\[\n\\frac{d \\widetilde{\\mathbb{P}}^{u}}{d \\widetilde{\\mathbb{P}}}(x) = \\exp {\\left\\{ \\sum_{k=0}^{N-1} -\\frac{\\delta}{2} \\|u(x_{t_k})\\|^2  +\n\\left&lt; x_{t_{k+1}}-x_{t_k}-b(x_{t_k})\\delta, \\sigma(x_{t_k}) \\, u(x_{t_k}) \\right&gt;_{\\Gamma^{-1}(x_{t_k})}  \\right\\}} .\n\\]\nwhere the tilde notation denotes the discretized version of the measures. Since\n\\[\nx_{t_{k+1}}-x_{t_k}-b(x_{t_k})\\delta = \\sigma(x_{t_k}) \\, \\Delta W_{t_k},\n\\]\nunder \\(\\widetilde{\\mathbb{P}}\\) and as \\(N \\to \\infty\\), for a path \\(dx_t \\; = \\; b(x_t) \\, dt + \\sigma(x_t) \\, dW_t\\), we have\n\\[\n\\frac{d \\widetilde{\\mathbb{P}}^{u}}{d \\widetilde{\\mathbb{P}}}(x) \\to \\exp {\\left\\{\n-\\frac 12 \\, \\int_0^T \\|u(x_t)\\|^2 \\, dt + \\int_{0}^T u^\\top(x_t) \\, dW_t\n\\right\\}} .\n\\]\nSimilarly, under \\(\\widetilde{\\mathbb{P}}^u\\) and as \\(N \\to \\infty\\), for a path \\(dx^{u}_t \\; = \\; b(x^u_t) \\, dt + \\sigma(x^u_t) \\,  {\\left(  dW_t + u(x^u_t) \\right)} \\), we have\n\\[\n\\frac{d \\widetilde{\\mathbb{P}}}{d \\widetilde{\\mathbb{P}}^u}(x^u) \\to \\exp {\\left\\{\n-\\frac 12 \\, \\int_0^T \\|u(x^u_t)\\|^2 \\, dt - \\int_{0}^T u^\\top(x^u_t) \\, dW_t\n\\right\\}} .\n\\]\nThese results remain identical for time-dependent drift and volatility functions, as is clear from this non-rigorous argument. The above two formulas for \\(d\\mathbb{P}^u/d\\mathbb{P}(x)\\) and \\(d\\mathbb{P}/d\\mathbb{P}^u(x)\\) may be slightly confusing since they are not immediately recognizable as inverse of each other. Another way to write these results that is very similar to Equation 1 and that is often used in the physics literature is as follows,\n\\[\n\\frac{d \\mathbb{P}^{u}}{d \\mathbb{P}}(x) = \\exp {\\left\\{\n-\\frac 12 \\, \\int_0^T \\|u(x_t)\\|^2 \\, dt + \\int_{0}^T u^\\top(x_t) \\, \\frac{dx_t - b(x_t) dt}{\\sigma(x_t)} \\right\\}} ,\n\\]\nFrom this expression, it is slightly easier to see the relationship between \\(d\\mathbb{P}^u/d\\mathbb{P}(x)\\) and \\(d\\mathbb{P}/d\\mathbb{P}^u(x)\\). As described below, these change of variables formulae are often useful when performing importance sampling on path-space. As a sanity check, one can see that in the case of a scalar Brownian motion \\(dX = \\sigma \\, dW\\) and drifted version of it \\(dX^u = \\sigma \\, dW + u \\, dt\\), we indeed have that \\(d\\mathbb{P}^u/d\\mathbb{P}(x)\\) has unit expectation under \\(\\mathbb{P}\\) since it is equivalent to the fact \\(\\mathop{\\mathrm{\\mathbb{E}}}[\\exp(\\sigma \\, \\xi)] = \\exp(\\sigma^2/2)\\) for a standard Gaussian random variable \\(\\xi\\). Finally, note that the Kullback-Leibler divergence between \\(\\mathbb{P}\\) and \\(\\mathbb{P}^u\\) has a particularly simple form. Since \\(\\mathop{\\mathrm{D_{\\text{KL}}}}(\\mathbb{P}, \\mathbb{P}^u) = \\mathop{\\mathrm{\\mathbb{E}}}_{\\mathbb{P}} {\\left[ -\\log {\\left\\{ \\frac{d \\mathbb{P}^{u}}{d \\mathbb{P}}(X) \\right\\}}  \\right]} \\) one obtains\n\\[\n\\mathop{\\mathrm{D_{\\text{KL}}}}(\\mathbb{P}, \\mathbb{P}^u) = \\frac12 \\mathop{\\mathrm{\\mathbb{E}}} {\\left[  \\int_0^T \\|u(X_t)\\|^2 \\, dt  \\right]} .\n\\]\n\nImportance Sampling on path-space\nConsider a functional \\(\\Phi: C([0,T]; \\mathbb{R}^D) \\to \\mathbb{R}\\) on path-space; a typical example is\n\\[\n\\Phi(x) = \\exp {\\left\\{ \\int_0^T f(X_t) \\, dt \\, + \\, g(X_T) \\right\\}} .\n\\]\nSuppose that we would like to evaluate the expectation of \\(\\Phi\\) under the measure \\(\\mathbb{P}\\). Naive Monte-Carlo (MC) would require sampling \\(M\\) trajectories from Equation 2 and computing the average of \\(\\Phi\\) on these trajectories. To reduce the variance of this naive MC estimator, one can also use importance sampling by sampling \\(M\\) trajectories \\(x^{1,u}, \\ldots, x^{M,u}\\) from the measure \\(\\mathbb{P}^u\\) and compute the average\n\\[\n\\frac{1}{M} \\, \\sum_{i=1}^M \\Phi(x^{i,u}) \\, W(x^{i,u})\n\\]\nwith weights given by the Radon-Nikodym derivative\n\\[\nW(x^{i,u}) \\; = \\; \\exp {\\left\\{\n-\\frac 12 \\, \\int_0^T \\|u(x^{i,u}_t)\\|^2 \\, dt - \\int_{0}^T u^\\top(x^{i,u}_t) \\, dW_t\n\\right\\}} .\n\\]\nChoosing the optimal “control” function \\(u\\) that minimizes the variance of the estimator is not entirely straightforward, although this previous note already gives the answer. More on this in another note."
  },
  {
    "objectID": "notes/doob_transforms/doob.html",
    "href": "notes/doob_transforms/doob.html",
    "title": "Joe Doob & Change of measures on path-space",
    "section": "",
    "text": "Joseph Doob (1910 – 2004)\n\n\n\nConsider a continuous time Markov process \\(X_t\\) on the time interval \\([0,T]\\) and with value in the state space \\(\\mathcal{X}\\). This defines a probability \\(\\mathbb{P}\\) on the set of \\(\\mathcal{X}\\)-valued paths. Now, it is often the case that one has to consider a perturbed probability distribution \\(\\mathbb{Q}\\) defined as\n\\[\n\\frac{d\\mathbb{Q}}{d\\mathbb{P}}(x_{[0,T]}) = \\frac{1}{\\mathcal{Z}} \\, \\exp[g(X_T)]\n\\tag{1}\\]\nfor a (typically unknown) normalization constant \\(\\mathcal{Z}\\) and some function \\(g: \\mathcal{X}\\to \\mathbb{R}\\). For example, collecting a noisy observation \\(y_T \\sim \\mathcal{F}(X_T) + \\textrm{(noise)}\\) at time \\(T\\), the distribution \\(\\mathbb{Q}\\) defined with the log-likelihood function \\(g(x) = \\log \\mathop{\\mathrm{\\mathbb{P}}}(y_T \\mid X_T=x)\\) describes the dynamics of the Markov process \\(X_t\\) conditioned on the observation \\(y_T\\); we will use this interpretation in the following since this is the most common use case and gives the most intuitive interpretation. Doob h-transforms are a powerful tool to describe the dynamics of the conditioned process.\nFor convenience, let us use the notation \\(\\mathop{\\mathrm{\\mathbb{E}}}_x[\\ldots] \\equiv \\mathop{\\mathrm{\\mathbb{E}}}[\\ldots \\mid x_t=x]\\). For a test function \\(\\varphi: \\mathcal{X}\\to \\mathbb{R}\\) and a time increment \\(\\delta &gt; 0\\), we have\n\\[\n\\begin{align*}\n\\mathop{\\mathrm{\\mathbb{E}}}[\\varphi(x_{t+\\delta}) | x_t, y_T] &= \\mathop{\\mathrm{\\mathbb{E}}}_{x_t}[\\varphi(x_{t+\\delta}) \\, \\exp(g(x_T)) ] \\, / \\, \\mathop{\\mathrm{\\mathbb{E}}}_{x_t}[\\exp(g(x_T))]\\\\\n&= \\frac{ \\mathop{\\mathrm{\\mathbb{E}}}_{x_t}[\\varphi(x_{t+\\delta}) \\, h(t+\\delta, x_{t+\\delta})] }{h(t, x)}.\n\\end{align*}\n\\tag{2}\\]\nWe have introduced the important function \\(h:[0,T] \\times \\mathcal{X}\\to \\mathbb{R}\\) defined as\n\\[\nh(t, x) \\; = \\; \\mathop{\\mathrm{\\mathbb{E}}} {\\left[  \\exp[g(x_T)] \\mid x_t = x \\right]}  \\; = \\; \\mathbb{P}(y_T \\mid x_t = x).\n\\]\nOne can readily check that the function \\(h\\) satisfies the Kolmogorov equation\n\\[\n(\\partial_t + \\mathcal{L}) \\, h = 0\n\\]\nwith boundary condition \\(h(T,x) = \\exp[g(x)]\\). Furthermore, denoting by \\(\\mathcal{L}\\) the infinitesimal generator of the Markov process \\(X_t\\), we have:\n\\[\n\\begin{align*}\n\\mathop{\\mathrm{\\mathbb{E}}}_{x_t}[\\varphi(x_{t+\\delta}) & h(t+\\delta, x_{t+\\delta}) ]\n\\; \\approx \\;\n\\varphi(x_t) h(t, x_t) \\\\\n&+ \\; \\delta \\, (\\partial_t + \\mathcal{L})[h \\, \\varphi] \\, (t, x_t)\n\\; + \\; o(\\delta).\n\\end{align*}\n\\tag{3}\\]\nThe infinitesimal generator \\(\\mathcal{L}^{\\star}\\) of the conditioned process is\n\\[\n\\mathcal{L}^{\\star} \\varphi(t, x_t) = \\lim_{\\delta \\to 0^+} \\; \\frac{\\mathop{\\mathrm{\\mathbb{E}}}[\\varphi(x_{t+\\delta}) | x_t, y_T] - \\varphi(x_t)}{\\delta}.\n\\]\nPlugging Equation 3 within Equation 2 directly gives that\n\\[\n\\mathcal{L}^{\\star} \\varphi\\; = \\; \\mathcal{L}\\varphi+ \\frac{\\mathcal{L}[\\varphi\\, h]}{h} + \\varphi\\frac{\\partial_t h}{h}.\n\\]\nThe generator \\(\\mathcal{L}^{\\star}\\) describes the dynamics of the conditioned process. In fact, the same computation holds with a more general change of measure of the type \\[\n\\textcolor{green}{\\frac{d\\mathbb{Q}}{d\\mathbb{P}}(x_{[0,T]}) = \\frac{1}{\\mathcal{Z}} \\, \\exp {\\left\\{ \\int_{0}^T f(s, X_s) \\, ds + g(x_T) \\right\\}}  }\n\\tag{4}\\]\nfor some function \\(f:[0,T] \\times \\mathcal{X}\\to \\mathbb{R}\\). One can define the function \\(h\\) similarly as\n\\[\n\\textcolor{green}{ h(t, x_t) \\; = \\; \\mathop{\\mathrm{\\mathbb{E}}} {\\left[  \\exp {\\left\\{ \\int_{t}^T f(X_s) \\, ds + g(x_T) \\right\\}}  \\mid x_t \\right]} }.\n\\tag{5}\\]\nThis function satisfies the Feynman-Kac formula \\((\\partial_t + \\mathcal{L}+ f) \\, h = 0\\) and one obtains entirely similarly that the probability distribution \\(\\mathbb{Q}\\) describes a Markov process with infinitesimal generator\n\\[\n\\textcolor{green}{\\mathcal{L}^{\\star} \\varphi\\; = \\; \\mathcal{L}\\varphi+ \\frac{\\mathcal{L}[h \\, \\varphi]}{h} +  {\\left(  \\frac{\\partial_t h} {h} + f \\right)}  \\, \\varphi.}\n\\tag{6}\\]\nTo see how this works, let us see a few examples:\n\nGeneral diffusions\nConsider a diffusion process\n\\[\ndX = b(X) \\, dt + \\sigma(X) \\, dW\n\\]\nwith generator \\(\\mathcal{L}\\varphi= b \\nabla \\varphi+ \\tfrac12 \\, \\sigma \\sigma^\\top : \\nabla^2 \\varphi\\) and initial distribution \\(\\mu_0(dx)\\). We are interested in describing the dynamics of the “conditioned” process given by the probability distribution \\(\\mathbb{Q}\\) defined in Equation 4. Algebra applied to Equation 5 then shows that\n\\[\n\\mathcal{L}^\\star \\varphi\\; = \\; \\mathcal{L}\\varphi+ \\underbrace{\\frac{\\varphi\\, (\\partial_t + \\mathcal{L}+ f)[h]}{h}}_{=0}\n+ \\sigma \\, \\sigma^\\top \\, (\\nabla \\log h) \\, \\nabla \\varphi\n\\]\nwhere the function \\(h\\) is described in Equation 5. Since \\((\\partial_t + \\mathcal{L}+ f) \\, h = 0\\), this reveals that the probability distribution \\(\\mathbb{Q}\\) describes a diffusion process \\(X^\\star\\) with dynamics\n\\[\ndX^\\star = b(X^\\star) \\, dt + \\sigma(X^\\star) \\,  {\\left\\{  dW +  \\textcolor{blue}{u^\\star(t, X^\\star)} \\, dt \\right\\}} .\n\\]\nThe additional drift term \\(\\sigma(X^\\star) \\,  \\textcolor{blue}{u^\\star(t, X^\\star)} \\, dt\\) is involves a “control” \\( \\textcolor{blue}{u^\\star(t, X^\\star)}\\) with \\[\n\\textcolor{blue}{u^\\star(t, x) = \\sigma^\\top(x) \\, \\nabla \\log h(t, x)}.\n\\tag{7}\\]\nNote that the initial distribution of the conditioned process is\n\\[\n\\mu_0^\\star(dx) = \\frac{1}{\\mathcal{Z}} \\, \\mu_0(dx) \\, h(0,x).\n\\]\nUnfortunately, apart from a few straightforward cases such as a Brownian motion or an Ornstein-Uhlenbeck process, the function \\(h\\) is generally intractable. However, there are indeed several numerical methods available to approximate it effectively.\n\n\nBrownian bridge\nWhat about a Brownian motion in \\(\\mathbb{R}^D\\) conditioned to hit the state \\(x_\\star \\in \\mathbb{R}^D\\) at time \\(t=T\\), i.e. a Brownian bridge? In that case, the function \\(h\\) is given by\n\\[\nh(t,x) = \\mathop{\\mathrm{\\mathbb{P}}}(B_T = x_\\star \\mid B_t = x)\n=\n\\exp {\\left\\{ -\\frac{\\|x-x_\\star\\|^2}{2 \\, (T-t)} \\right\\}}  / \\mathcal{Z}_{T-t}\n\\]\nfor some irrelevant normalization constant \\(\\mathcal{Z}_{T-t}\\) that only depends on \\(T-t\\). Plugging this into Equation 7 gives that the conditioned Brownian \\(X^{\\star}\\) motion has dynamics\n\\[\ndX^\\star \\;=\\;  \\textcolor{blue}{- \\frac{X^\\star - x_\\star}{T-t} \\, dt} + dB.\n\\]\nThe additional drift term \\(-(X^\\star - x_\\star)/(T-t)\\) is intuitive: it points in the direction of \\(x^\\star\\) and gets increasingly large as \\(t \\to T\\).\n\n\nPositive Brownian motion\nWhat about a scalar Brownian conditioned to stay positive at all times? Let us consider \\(T\\) and let us condition first on the event that the Brownian motion stays positive within \\([0,T]\\) and later consider the limit \\(T \\to \\infty\\). The function \\(h\\) reads\n\\[\nh(t,x) = \\mathop{\\mathrm{\\mathbb{P}}} {\\left( \\text{$B_t$ stays $&gt;0$ on $[t,T]$} \\mid B_t=x \\right)} .\n\\]\nThis can easily be calculated with the reflection principle. It equals\n\\[\nh(t,x) = 1 - 2 \\, \\mathop{\\mathrm{\\mathbb{P}}}(B_T &lt; 0 \\mid B_T = x)\n=\n\\mathop{\\mathrm{\\mathbb{P}}}(\\sqrt{T-t} \\, \\| \\xi \\| &lt; x)\n\\]\nfor a standard Gaussian \\(\\xi \\sim \\mathcal{N}(0,1)\\). Plugging this into Equation 7 gives that the additional drift term is\n\\[\n\\nabla \\log h(t,x) = \\frac{\\exp {\\left( -x^2 / (2 \\, (T-t)) \\right)} }{x} \\quad \\to \\quad \\frac{1}{x}\n\\]\nas \\(T \\to \\infty\\). This shows that a Brownian motion conditioned to stay positive at all times has a upward drift of size \\(1/x\\),\n\\[\ndX^\\star \\;=\\; \\frac{1}{X^{\\star}} + dB.\n\\]\nIncidentally, it is the dynamics of a Bessel process of dimension \\(d=3\\), i.e. the law of the modulus of a three-dimensional Brownian motion. More generally, if one conditions a Brownian motion to stay within a closed domain \\(\\mathcal{D}\\), the conditioned dynamics exhibit a repulsive drift term of size about \\(1/\\textrm{dist}(x, \\partial \\mathcal{D})\\) near the boundary \\(\\partial \\mathcal{D}\\) of the domain, as described below.\n\n\nBrownian motion staying in a domain\nWhat about a Brownian motion conditioned to stay within a domain \\(\\mathcal{D}\\) forever? As before, consider an time horizon \\(T\\) and define the function \\(h\\) as\n\\[\nh(t,x) = \\mathop{\\mathrm{\\mathbb{P}}} {\\left( \\text{$B_t$ stays in $\\mathcal{D}$ on $[t,T]$} \\mid B_t=x \\right)} .\n\\]\nOne can see that the function \\(h\\) satisfies the PDE\n\\[\n(\\partial_t + \\Delta) \\, h = 0\n\\]\nand equals zero on the boundary \\(\\partial \\mathcal{D}\\) of the domain. Furthermore \\(h(t,x) \\to 1\\) as \\(t \\to T\\) for all \\(x \\in \\mathcal{D}\\). Consider the eigenfunctions \\(\\psi_k\\) of the negative Laplacian \\(-\\Delta\\) with Dirichlet boundary conditions on \\(\\partial \\mathcal{D}\\). Recall that \\(-\\Delta\\) is a positive operator with a discrete spectrum \\(\\lambda_1 \\leq \\lambda_2 \\leq \\ldots\\) of non-negative eigenvalues. The eigenfunction corresponding to the smallest eigenvalue \\(\\lambda_1\\) is the principal eigenfunction \\(\\psi_1\\) and it is standard that it is a positive function within the domain \\(\\mathcal{D}\\), as a “slight” generalization of the Perron-Frobenius in linear algebra shows it. Expanding \\(h\\) in the basis of eigenfunctions \\(\\psi_k\\) gives that\n\\[\nh(t,x) = \\underbrace{c_1 \\, e^{-\\lambda_1 \\, (T-t)} \\, \\psi_1(x)}_{\\textrm{dominant contribution}} + \\sum_{k \\geq 2} c_k \\, e^{-\\lambda_k \\, (T-t)} \\, \\psi_k(x).\n\\]\n\n\n\n\nEigenfunctions of the Laplacian\n\n\n\nSince we are interested in the regime \\(T \\to \\infty\\), it holds that\n\\[ \\nabla_x \\log h(t,x) \\; \\to \\; \\nabla \\log \\psi_1(x).\\]\nThis shows that the conditioned Brownian motion has a drift term expressed in terms of the principal eigenfunction \\(\\psi_1\\) of the Laplacian:\n\\[\ndX^\\star \\;=\\;  \\textcolor{blue}{ \\nabla \\log \\psi_1(X^\\star) \\, dt} + dB.\n\\]\nFor example, if \\(\\mathcal{D}\\equiv [0,L]\\) for a 1D Brownian motion, the principal eigenfunction is \\(\\psi_1(x) = \\sin(\\pi \\, x /L)\\). This shows that there is a upward drift of size \\(\\sim 1/x\\) near \\(x \\approx 0\\) and a downward drift of size \\(\\sim 1/(L-x)\\) near \\(x \\approx L\\)."
  },
  {
    "objectID": "notes/information_theory_fano/information_theory_fano.html",
    "href": "notes/information_theory_fano/information_theory_fano.html",
    "title": "Information Theory: Fano’s inequality",
    "section": "",
    "text": "Robert Fano (1917 – 2016)\n\n\n\n\nFano’s inequality\nConsider a three random variables forming a Markov chain,\n\\[\nX \\mapsto Y \\mapsto \\widehat{X}\n\\tag{1}\\]\nin the sense that \\(Y = \\textrm{function}(X, \\text{noise})\\) and \\(Z = \\textrm{function}(Y, \\text{noise})\\). Typical situations include:\n\nWe select a parameter \\(\\theta\\) for a probabilistic model \\(\\mathop{\\mathrm{\\mathbb{P}}}_{\\theta}\\). Afterward, we collect data \\(X\\) from this model, and our goal is to estimate the parameter \\(\\theta\\) solely from the data \\(X\\).\nWe generate data \\(X\\), compress this data into \\(X_{\\text{zip}}\\), and then attempt to recover the original data \\(X\\) as closely as we can.\n\nSince each step in Equation 1 destroys some information (eg. data processing), it is important to measure how accurately \\(\\widehat{X}\\) estimates the initial input, \\(X\\). In other words, we want to know how much more information (expressed as ‘bits’) we need to reconstruct \\(X\\) using knowledge of \\(\\widehat{X}\\) alone, i.e. we would like to upper-bound \\(H(X \\, | \\widehat{X})\\). For this purpose, imagine an “error” variable \\(E\\) that indicates whether \\(\\widehat{X}\\) perfectly matches \\(X\\),\n\\[\nE = \\mathbf{1} {\\left( \\widehat{X} \\neq X \\right)} .\n\\]\nThe probability of error is \\(p_E = \\mathop{\\mathrm{\\mathbb{P}}}(\\widehat{X} \\neq X)\\) and \\(E = \\text{Bern}(p_E)\\). To estimate \\(X\\) from \\(\\widehat{X}\\), we can start by learning if \\(\\widehat{X}\\) equals \\(X\\), which costs us \\(H(E | \\widehat{X}) \\leq H(E) = h_2(p_E)\\) ‘bits’ of information. If it turns out that \\(E=0\\), we are done asking. If we find that \\(E=1\\), however, we need to ask additional \\(H(X | \\widehat{X}, E)\\) questions. Crucially, \\(H(X | \\widehat{X}, E) \\leq H(X)\\), but also \\(H(X | \\widehat{X}, E) \\leq \\log_2(|\\mathcal{X}|-1)\\) since \\(X\\) can take any value in \\(\\mathcal{X}\\) except \\(\\widehat{X}\\) when \\(E=1\\). Writing this reasoning quantitatively gives Fano’s inequality:\n\\[\n\\begin{align}\nH(X | \\widehat{X})\n&\\leq h_2(p_E) + p_E \\, \\log_2(|\\mathcal{X}|-1).\n\\end{align}\n\\tag{2}\\]\nApparently, this inequality was first derived by Robert Fano in the 50s while teaching a Ph.D. seminar at MIT. In words: a large \\(H(X | \\widehat{X})\\) means that \\(\\widehat{X}\\) offers insufficient information about \\(X\\), and as a result, the probability of error \\(p_E\\) must be high.\n\n\nApplications:\n\nConverse of Shannon’s coding theorem"
  },
  {
    "objectID": "notes/on_Langevin_MCMC/on_Langevin_MCMC.html",
    "href": "notes/on_Langevin_MCMC/on_Langevin_MCMC.html",
    "title": "Deriving Langevin MCMC",
    "section": "",
    "text": "Julian Besag (1945 – 2010)\n\n\n\nConsider a target density \\(\\pi(x)\\) in \\(\\mathbb{R}^D\\). Since the Langevin diffusion\n\\[\ndX_t = \\nabla \\log \\pi(X_t) \\, dt + \\sqrt{2} \\, dW\n\\tag{1}\\]\nis reversible with respect to \\(\\pi\\), it is natural to use a Euler-Maruyama discretization of Equation 1 to build MCMC proposals: in a MCMC simulation and for a time discretization parameter \\(\\varepsilon&gt; 0\\), if the current position is \\(x \\in \\mathbb{R}^D\\), a proposal \\(y \\in \\mathbb{R}^D\\) can be generated as\n\\[\ny = x + \\varepsilon\\, \\nabla \\log \\pi(X_t) + \\sqrt{2 \\varepsilon} \\, \\xi\n\\]\nwith \\(\\xi \\sim \\mathcal{N}(0,I)\\) before being accepted-or-reject according to the usual Metropolis-Hastings ratio. This MCMC method, first proposed by Julian Besag in 1994, is commonly referred to as the Metropolis-Adjusted-Langevin-Algorithm (MALA). But how can one come-up with this proposal mechanism without knowing before hand the existence of this reversible Langevin diffusion Equation 1? While it is intuitively clear that following the direction of \\(\\nabla \\log \\pi\\) is not such a bad idea, i.e. one would like to move towards areas of “high probability mass”, where does this \\(\\sqrt{2}\\) comes from? Naturally, one could look at proposals of the type \\(y = x + \\nabla \\log \\pi(X_t) \\, \\varepsilon+ \\lambda \\, \\xi\\) for some free parameter \\(\\lambda &gt; 0\\) and study the behavior of the Metropolis-Hastings ratio in the regime \\(\\varepsilon\\to 0\\): as simple as it sounds, it is not entirely straightforward and requires quite a bit of algebra (do it!). Instead, I very much like the type of approaches described in (Titsias and Papaspiliopoulos 2018). To summarize, we would like to generate a MCMC proposal \\(y \\in \\mathbb{R}^D\\) that stays in the vicinity of the current position \\(x \\in \\mathbb{R}^D\\) while exploiting the knowledge of \\(\\nabla \\log \\pi(x)\\). One cannot simply approximate the target distribution as \\(\\pi(x) \\approx \\pi(x_k) e^{\\left&lt; \\nabla \\log \\pi(x_k), x-x_k \\right&gt;}\\) and sample from this approximation since it is typically does not define a probability distribution. Instead, consider the following extended target distribution\n\\[\n\\overline{\\pi}(x,z) \\, \\propto \\pi(x) \\, \\exp {\\left\\{ -\\frac{1}{2\\varepsilon}\\|z-x\\|^2 \\right\\}} .\n\\]\nIn other words, the Gaussian auxiliary variable \\(z \\in \\mathbb{R}^D\\) is centred at \\(x\\) and at distance about \\(\\sqrt{\\varepsilon}\\) of it. Now, given the current position \\(x_k\\), to generate a proposal \\(y_\\star\\) that stays in the vicinity of \\(x_k\\), one can proceed in two steps, in the spirit of a Gibbs-sampling approach:\n\nFirst, generate \\(z_\\star \\sim \\overline{\\pi}(dz | x_k) \\sim \\mathcal{N}(x_k, \\sqrt{\\varepsilon}I)\\)\nSecond, sample from \\(y_\\star \\sim \\overline{\\pi}(dx | z_\\star)\\).\n\nUnfortunately, the second step is typically not tractable. Nevertheless, the conditional density \\(\\overline{\\pi}(dx | z_\\star)\\) is concentrated in a \\(\\sqrt{\\varepsilon}\\)-neighborhood of \\(z_\\star\\) and a simple Gaussian approximation around \\((x_k, z_\\star)\\) should be enough for our purpose. We have:\n\\[\n\\begin{align}\n\\log \\overline{\\pi}(dx | z_\\star)\n&=\n\\log \\pi(x) - \\frac{1}{2 \\varepsilon} \\|x - z_\\star\\|^2 + \\textrm{(Cst)}\\\\\n&\\approx\n\\left&lt;  \\nabla \\log \\pi(x_k), x-x_k  \\right&gt; - \\frac{1}{2 \\varepsilon} \\|x - z_\\star\\|^2 + \\textrm{(Cst)}\\\\\n&=\n- \\frac{1}{2 \\varepsilon} \\|x - [z_\\star + \\varepsilon\\, \\nabla \\log \\pi(x_k)]\\|^2 + \\textrm{(Cst)}.\n\\end{align}\n\\]\nThis shows that the conditional \\(\\overline{\\pi}(dx | z_\\star)\\) can be approximated by a Gaussian distribution centred at \\([z_\\star + \\nabla \\log \\pi(x_k)]\\) and variance \\(\\varepsilon\\, I\\). This means that the final proposal \\(y \\in \\mathbb{R}^D\\) can be generated as \\(y \\sim z_\\star + \\nabla \\log \\pi(x_k) + \\xi\\) where \\(\\xi \\sim \\mathcal{N}(0,\\varepsilon)\\). But that is equivalent to setting\n\\[\ny \\sim x + \\varepsilon\\, \\nabla \\log \\pi(x_k) + \\sqrt{2 \\varepsilon} \\, \\xi\n\\]\nwith \\(\\xi \\sim \\mathcal{N}(0,I)\\) since \\(z_\\star \\sim \\mathcal{N}(x, \\sqrt{\\varepsilon} I)\\). It is exactly the MALA proposal. Naturally, one can also try to be slightly more clever and use an extended distribution\n\\[\n\\overline{\\pi}(x,z) \\, \\propto \\, \\pi(x) \\, \\exp {\\left\\{  -\\frac{1}{2\\varepsilon} \\left&lt; (z-x), M^{-1} \\, (z-x) \\right&gt;  \\right\\}}\n\\]\nfor some appropriate positive-definite “mass” matrix \\(M \\in \\mathbb{R}^{D,D}\\). Indeed, this immediately leads to preconditioned MALA methods. I really like this approach since it can be adapted and generalized to quite a few other situations!\n\n\n\n\nReferences\n\nTitsias, Michalis K, and Omiros Papaspiliopoulos. 2018. “Auxiliary Gradient-Based Sampling Algorithms.” Journal of the Royal Statistical Society Series B: Statistical Methodology 80 (4). Oxford University Press: 749–67."
  },
  {
    "objectID": "notes/averaging_homogenization/averaging_homogenization.html",
    "href": "notes/averaging_homogenization/averaging_homogenization.html",
    "title": "Averaging and homogenization",
    "section": "",
    "text": "Averaging\nConsider a pair of (coupled) Markov processes \\(X_t \\in \\mathcal{X}\\) and \\(Y_t \\in \\mathcal{Y}\\) with dynamics that can informally be described as\n\\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon}/dt &= F(X^{\\varepsilon},Y^{\\varepsilon}, W^X)\\\\\ndY^{\\varepsilon}/dt &=  \\textcolor{red}{\\varepsilon^{-1}} \\, G(X^{\\varepsilon},Y^{\\varepsilon}, W^Y)\\\\\n\\end{align}\n\\right.\n\\]\nfor two independent “noise” terms \\(W^X\\) and \\(W^Y\\) and a time-scale parameter \\(\\varepsilon\\ll 1\\). We assume that \\(X\\) is a slow component that moves by \\(\\mathcal{O}(\\delta)\\) in on the time interval \\([t, t+\\delta]\\). The scaling \\(\\varepsilon^{-1}\\) in the dynamics of fast process \\(Y^{\\varepsilon}\\) indicates that we expect the process \\(Y\\) to evolve on a time scale of order \\(\\mathcal{O}(\\varepsilon)\\). We are interested in the limit \\(\\varepsilon\\to 0\\) and hope to “average out” the fast process \\(Y^{\\varepsilon}\\) and be able to describe the slow (and interesting) process \\(X^{\\varepsilon}\\) without referring to the fast process. Informally, we would like to describe the process \\(X^{\\varepsilon}\\), in the limit \\(\\varepsilon\\to 0\\), as following an effective Markovian dynamics\n\\[\ndX/dt = \\overline{F}(X, W^X).\n\\]\nFor describing the averaging phenomenon, we typically assume some ergodicity conditions on the fast process \\(Y\\). Here, we assume that for each fixed \\(x \\in \\mathcal{X}\\), the fast process process \\(Y^{[x]}\\) with fixed slow-component \\(x \\in \\mathcal{X}\\), i.e.\n\\[\ndY^{[x]}/dt = G(x,Y^{[x]}, W^Y)\n\\]\nis ergodic with respect to some probability distribution \\(\\rho_x(dy)\\). Although the averaging phenomenon is quite general, it is somewhat easier to illustrate it for diffusion processes. In this case, let us assume that the slow process is given by\n\\[\ndX^{\\varepsilon} = \\mu(X^{\\varepsilon}, Y^{\\varepsilon}) \\, dt + \\sigma(X^{\\varepsilon}, Y^{\\varepsilon}) \\, dW^x.\n\\]\nFor \\(X^{\\varepsilon}_{t} = x\\) and for a time increment \\(\\delta \\ll 1\\), since the process \\(X^{\\varepsilon}\\) can be considered constant we have\n\\[\n\\begin{align}\nX^{\\varepsilon}_{t+\\delta} - x\n&\\approx \\;\n{\\left(  \\frac{ \\int_{t}^{t+\\delta} \\mu(x, Y^{\\varepsilon}) \\, dt}{\\delta}  \\right)}  \\, \\delta + \\\n{\\left(  \\frac{ \\int_{t}^{t+\\delta} \\sigma^2(x, Y^{\\varepsilon}) }{\\delta} \\right)} ^{1/2} \\, \\mathcal{N}(0, \\delta).\n\\end{align}\n\\]\nThis can be regarded as a time-discretization of the averaged process\n\\[\ndX \\, = \\; \\overline{\\mu}(X) \\, dt + \\overline{\\sigma}(X) \\, dW\n\\]\nfor averaged drift and volatility functions give by\n\\[\n\\left\\{\n\\begin{align}\n\\overline{\\mu}(x)\n&=\n\\int \\mu(x,y) \\, \\rho_x(dy) \\\\\n\\overline{\\sigma}^2(x)\n&= \\int \\sigma^2(x,y) \\, \\rho_x(dy).\n\\end{align}\n\\right.\n\\tag{1}\\]\nOne standard approach for proving this type of results is to write the Kolmogorov equations\n\\[\\frac{d}{dt}\\varphi^{\\varepsilon}(x,y,t) = \\mathcal{L}^{\\varepsilon} \\varphi^{\\varepsilon}(x,y,t)\\] for \\(\\varphi^{\\varepsilon}(x,y,t) = \\mathop{\\mathrm{\\mathbb{E}}}[\\varphi(X^{\\varepsilon}_{t}, Y^{\\varepsilon}_{t}, t) | X^{\\varepsilon}_{0}=x, Y^{\\varepsilon}_{0}=y]\\) and perform a multiscale expansion (Hinch 1991) (Pavliotis and Stuart 2008) (Weinan 2011)\n\\[\n\\varphi^{\\varepsilon}(x,y,t)\n=\nA(x,t) + \\varepsilon B(x,y,t) + \\mathcal{O}(\\varepsilon^2).\n\\tag{2}\\]\nIndeed, the first order term \\(A(x,t)\\) is expected to not depend on the initial condition \\(y\\) since the process \\((X^{\\varepsilon}_t, Y^{\\varepsilon}_t)\\) forgets \\(Y^{\\varepsilon}_0 = y\\) on time scales of order \\(\\varepsilon\\) and we are interested in the regime \\(\\varepsilon\\to 0\\). From Equation 2 one can obtain the dynamics of the averaged process described by the function \\(A(x,t)\\). One finds that \\(A\\) is described by the averaged generator of the slow component, i.e. averaging \\(\\mathcal{L}^{X^{\\varepsilon}}\\) under \\(\\rho_x(dy)\\); this exactly gives Equation 1 in the case of diffusions. A typical example could be as follows:\n\\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon} &= \\mu(X^{\\varepsilon}, Y^{\\varepsilon}) dt + \\sigma(X^{\\varepsilon}, Y^{\\varepsilon}) \\, dW^X\\\\\ndY^{\\varepsilon} &= - \\textcolor{red}{\\varepsilon^{-1}} \\frac{ (Y^{\\varepsilon} - X^{\\varepsilon}) }{\\sigma^2} \\, dt + \\sqrt{2  \\textcolor{red}{\\varepsilon^{-1}} } \\, dW^Y.\\\\\n\\end{align}\n\\right.\n\\]\nThe fast process \\(Y^{\\varepsilon}_t\\) is a Ornstein-Uhlenbeck process sped-up by a factor \\(1/\\varepsilon\\) that will very rapidly oscillate around \\(X^{\\varepsilon}_t\\), with Gaussian fluctuations with variance \\(\\sigma^2&gt;0\\), ie:\n\\[\n\\rho_x(dy) \\; = \\; \\frac{ e^{-(y-x)^2/2} }{\\sqrt{2\\pi \\sigma^2}}\\, dy.\n\\]\nThis averaging phenomenon is relatively straightforward and not extremely surprising. More interesting is the homogenization phenomenon described in the next Section.\n\n\nHomogenization\nConsider the presence of an additional intermediate time scale \\(\\varepsilon^{-1/2}\\), \\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon}/dt &=  \\textcolor{blue}{\\varepsilon^{-1/2} H(X^{\\varepsilon},Y^{\\varepsilon})} \\, + \\,F(X^{\\varepsilon},Y^{\\varepsilon}, W^X)\\\\\ndY^{\\varepsilon}/dt &=  \\textcolor{red}{\\varepsilon^{-1}} \\, G(X^{\\varepsilon},Y^{\\varepsilon}, W^Y)\\\\\n\\end{align}\n\\right.\n\\] with the same assumption that for any fixed \\(x \\in \\mathcal{X}\\) the process \\(dY^{[x]}/dt = G(x,Y^{[x]}, W^Y)\\) is ergodic with respect to the probability distribution \\(\\rho_x(dy)\\). The same reasoning as in the averaging case shows that averaging the term \\(F(X^{\\varepsilon},Y^{\\varepsilon}, W^X)\\) is relatively straightforward and has the exact same expression: it suffices to average under \\(\\rho_x(dy)\\). This means that one can study instead\n\\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon}/dt &=  \\textcolor{blue}{\\varepsilon^{-1/2} H(X^{\\varepsilon},Y^{\\varepsilon})} \\, + \\, \\overline{F}(X^{\\varepsilon}, W^X)\\\\\ndY^{\\varepsilon}/dt &=  \\textcolor{red}{\\varepsilon^{-1}} \\, G(X^{\\varepsilon},Y^{\\varepsilon}, W^Y)\\\\\n\\end{align}\n\\right.\n\\]\nwith, informally, \\(\\overline{F}(x,w) = \\int F(x,y,w) \\, \\rho_x(dy)\\). The new interesting phenomenon is coming from the intermediate time scale \\(\\varepsilon^{-1/2}\\). Contrarily to the averaging phenomenon of the previous section that was only relying on a Law of Large Numbers, dealing with the intermediate time-scale requires exploiting a CLT and quantifying the rate of mixing of the fast process \\(Y^{[x]}\\) Note that since \\(\\varepsilon^{-1/2} \\gg 1\\), for the dynamics to not explode one needs the centering condition:\n\\[\n\\int_{\\mathcal{Y}} H(x,y) \\, \\rho_x(dy) = 0\n\\qquad \\textrm{for all } x \\in \\mathcal{X}.\n\\tag{3}\\]\nBecause of the centering condition*, the term \\( \\textcolor{blue}{\\varepsilon^{-1/2} H(X^{\\varepsilon},Y^{\\varepsilon})}\\) will contribute an additional noise term in the effective dynamics of the slow process. To describe this additional noise term, assume an ergodic central limit theorem (CLT) for the fast process \\(dY^{[x]}/dt = G(x,Y^{[x]}, W^Y)\\): for a test function \\(\\varphi: \\mathcal{Y}\\to \\mathbb{R}\\) with zero expectation under \\(\\rho_x(dy)\\) we have:\n\\[\n\\lim_{t \\to \\infty} \\; T^{-1/2}\n\\int_{t=0}^T \\, \\varphi(Y^{[x]}_t) \\, dt\n\\; = \\; \\mathcal{N}(0, V_x[\\varphi])\n\\tag{4}\\]\nfor asymptotic variance \\(V_x[\\varphi] \\geq 0\\). For a time increment \\(\\delta &gt; 0\\) and assuming \\(X^{\\varepsilon}_{t}=x\\) we have\n\\[\n\\begin{align}\nX^{\\varepsilon}_{t+\\delta} - x\n&\\approx  \\textcolor{blue}{ \\varepsilon^{-1/2} \\, \\int_{u=t}^{t+\\delta} H(X^{\\varepsilon}_u,Y^{\\varepsilon}_u)} \\, du \\, + \\, \\int_{u=t}^{t+\\delta} \\overline{F}(x, W^X_u) \\, du.\n\\end{align}\n\\tag{5}\\]\nThe second integral term is an averaging term that can be treated easily. Approximating the process \\(t \\mapsto Y^{\\varepsilon}_t\\) by \\(t \\mapsto Y^{[x]}_{t \\varepsilon^{-1}}\\), the first integral on the RHS of Equation 5 can be approximated as\n\\[\n\\begin{align}\n\\underbrace{\\varepsilon^{-1/2} \\int_{u=t}^{t+\\delta}  H(x,Y^{[x]}_{u \\, \\varepsilon^{-1}}) \\, du}_{\\textrm{CLT}} \\,\n+\n\\underbrace{\\int_{u=t}^{t+\\delta} \\varepsilon^{-1/2} \\partial_x H(x,Y^{[x]}_{u \\, \\varepsilon^{-1}}) \\, (X^{\\varepsilon}_u - x) \\, \\, du}_{\\textrm{(drift)}}.\n\\end{align}\n\\]\nAfter a time-rescaling, one can readily see that the first term is described by the CLT of Equation 4,\n\\[\n\\varepsilon^{-1/2} \\int_{u=t}^{t+\\delta}  H(x,Y^{[x]}_{u \\, \\varepsilon^{-1}}) \\, du\n\\approx V_x[H(x, \\cdot)]^{1/2} \\mathcal{N}(0, \\delta).\n\\]\nThe second term is further approximated as\n\\[\n\\begin{align}\n\\varepsilon^{-1} \\, &\\int_{u=t}^{t+\\delta}\\int_{v=t}^{t+\\delta}\n\\partial_x H(x,Y^{[x]}_{u \\, \\varepsilon^{-1}}) \\, H(x,Y^{[x]}_{v \\, \\varepsilon^{-1}}) \\, 1_{v&lt;u} \\, du \\, dv\\\\\n&=  {\\left(  \\frac{1}{\\delta \\varepsilon^{-1}} \\int_{u=t}^{t+\\delta \\varepsilon^{-1}}\\int_{v=t}^{t+\\delta \\varepsilon^{-1}} \\partial_x H(x,Y^{[x]}_{u}) \\, H(x,Y^{[x]}_{v}) \\, 1_{v&lt;u}  \\right)}  \\, \\delta,\n\\end{align}\n\\]\nthe second equality coming from the time-rescaling \\(t \\mapsto t \\varepsilon\\). The process \\(Y^{[x]}\\) mixes on scale \\(\\mathcal{O}(1)\\) so that the term inside bracket \\( {\\left( \\ldots \\right)} \\) converges to its expectation. Setting \\(T = \\delta \\, \\varepsilon^{-1} \\to \\infty\\), one obtains\n\\[\n\\begin{align}\nI(x) &=\n\\frac{1}{T} \\iint_{[0,T]^2}\n\\partial_x H(x,Y^{[x]}_{u}) \\, H(x,Y^{[x]}_{v}) \\, 1_{v&lt;u}  \\, du \\, dv \\\\\n&\\to\n\\int \\rho_x(dy) \\, H(x,y) \\,   {\\left\\{  \\int_{s=0}^{\\infty} \\mathop{\\mathrm{\\mathbb{E}}}[\\partial_x H(\\hat{x}, Y^{[x]}_s) \\, |  Y^{[x]}_0=y] \\, ds  \\right\\}} .\n\\end{align}\n\\]\nIn conclusion, the fast-slow system\n\\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon} &=  \\textcolor{blue}{\\varepsilon^{-1/2} \\, H(X^{\\varepsilon}, Y^{\\varepsilon})} \\, dt + \\mu(X^{\\varepsilon}, Y^{\\varepsilon}) \\, dt + \\sigma(X^{\\varepsilon}, Y^{\\varepsilon}) \\, dW^x\\\\\ndY^{\\varepsilon} &=  \\textcolor{red}{\\varepsilon^{-1}} \\, G(X^{\\varepsilon},Y^{\\varepsilon}, W^Y) \\, dt\n\\end{align}\n\\right.\n\\]\ncan be described in the regime \\(\\varepsilon\\to 0\\) by the effective dynamics\n\\[\ndX =  \\textcolor{blue}{I(X) \\, dt + \\Gamma^{1/2}(X) \\, dW^{H}}\n+\n\\overline{\\mu}(X) \\, dt + \\overline{\\sigma}(X) \\, dW^X.\n\\]\nfor two independent Brownian motions \\(W^X\\) and \\(W^H\\). The volatility terms \\( \\textcolor{blue}{\\Gamma(x)}\\) comes from the CLT and the drift term \\( \\textcolor{blue}{I(x)}\\) comes from the self-interaction term:\n\\[\n\\left\\{\n\\begin{align}\n\\Gamma(x)\n&= \\lim_{T \\to \\infty} \\; \\mathop{\\mathrm{Var}} {\\left\\{ T^{-1/2} \\int_{0}^T H(x, Y^{[x]}_t) \\, dt \\right\\}} \\\\\n%\nI(x)\n&= \\lim_{T \\to \\infty} \\; \\frac{1}{T} \\iint_{0&lt;u&lt;v&lt;T} H(x, Y^{[x]}_u) \\, \\partial_x H(x, Y^{[x]}_v) \\, du \\, dv.\n\\end{align}\n\\right.\n\\tag{6}\\]\nFor the drift function, the scaling \\(T^{-1} \\int_{0&lt;u&lt;v&lt;T} (\\ldots)\\) may look a bit surprising at first sight as one may expect \\(T^{-2} \\int_{0&lt;u&lt;v&lt;T} (\\ldots)\\) instead. Note that since the process \\(Y^{[x]}\\) mixes on a time scale \\(\\mathcal{O}(1)\\) and the centering condition \\(\\int H(x, y) \\rho_x(dy)=0\\) holds, the expectation \\(\\mathop{\\mathrm{\\mathbb{E}}}[H(x, Y^{[x]}_u) \\, \\partial_x H(x, Y^{[x]}_v)]\\) goes to zero as soon as \\(|u-v| \\gg 1\\). This means that only the subset \\(|u-v| = \\mathcal{O}(1)\\) of \\([0,T]^2\\) really matters in that double integral, hence the \\((1/T)\\) normalization factor.\n\n\nClosed form solution & Poisson equation:\nThe drift and volatility terms \\(\\Gamma(x)\\) and \\(I(x)\\) quantify the mixing properties of the fast process \\(Y^{[x]}\\). While formulas Equation 6 are intuitive, they can be difficult to deal with if one needs the exact expressions of the drift and volatility functions. Instead, they can also be expressed in terms of the solution to an appropriate Poisson equations.\n\\[\n\\begin{align}\nI(x) &=\n\\frac{1}{T} \\iint_{[0,T]^2}\nH(x,Y^{[x]}_{v}) \\, \\partial_x H(x,Y^{[x]}_{u}) \\, 1_{v&lt;u}  \\, du \\, dv \\\\\n&\\to\n\\int \\rho_x(dy) \\, H(x,y) \\, \\partial_{\\hat{x}}  {\\left\\{  \\int_{s=0}^{\\infty} \\mathop{\\mathrm{\\mathbb{E}}}[H(\\hat{x}, Y^{[x]}_s) \\, |Y^{[x]}_0=y] \\, ds  \\right\\}} \\\\\n&=\n-\\int \\rho_x(dy) \\, H(x,y) \\, \\partial_{x} \\Phi(x,y)\\\\\n&= -\\left&lt; H(x, \\cdot), \\partial_x \\Phi(x, \\cdot) \\right&gt;_{\\rho_x}\n\\end{align}\n\\tag{7}\\]\nwhere the function \\(\\Phi(x,y)\\) is solution to the Poisson equation\n\\[\n\\mathcal{L}^{Y^{[x]}} \\Phi(x, \\cdot) = H(x, \\cdot)\n\\]\nfor all \\(x \\in \\mathcal{X}\\) and \\(\\mathcal{L}^{Y^{[x]}}\\) is the generator of the fast process \\(dY^{[x]}/dt = G(x,Y^{[x]}, W^Y)\\). The last equality in Equation 7 follows from the integral representation of the Poisson equation. Similarly, and also as explained here, the asymptotic variance term can also be expressed in terms of the function \\(\\Phi\\),\n\\[\n\\begin{align}\n\\Gamma(x)\n&= \\lim_{T \\to \\infty} \\; \\mathop{\\mathrm{Var}} {\\left\\{ T^{-1/2} \\int_{0}^T H(x, Y^{[x]}_t) \\, dt \\right\\}} \\\\\n&= -2 \\int_{\\mathcal{Y}} \\Phi(x, y) \\, H(x, y) \\, \\rho_x(dy)\\\\\n&= -2 \\left&lt; \\Phi, \\mathcal{L}^{Y^{[x]}} \\Phi \\right&gt;_{\\rho_x}.\n\\end{align}\n\\]\n\n\nExample: integrated OU process\nConsider a slow process obtained by integrating an OU process,\n\\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon} &= \\varepsilon^{-1/2} \\, Y^{\\varepsilon} \\, dt\\\\\ndY^{\\varepsilon} &= -\\lambda \\varepsilon^{-1}\\, Y^{\\varepsilon} \\, dt + \\sqrt{2 \\lambda/\\varepsilon} \\, dW^Y,\n\\end{align}\n\\right.\n\\]\nwhere \\(\\lambda &gt; 0\\) is just a fixed time-scaling parameter. The fast OU process mixes on time scales of order \\(\\mathcal{O}(\\varepsilon)\\) and has a standard Gaussian distribution as invariant distribution. Homogenization gives that in the regime \\(\\varepsilon\\to 0\\), the slow process can be approximated as\n\\[\ndX = \\sqrt{2/\\lambda} \\, dW\n\\tag{8}\\]\nsince the asymptotic variance is\n\\[\n\\mathop{\\mathrm{Var}} {\\left\\{  T^{-1/2} \\int_{t=0}^{T} Y_t \\, dt \\right\\}}\n\\to\n2 \\, \\int_{0}^{\\infty} C(r) \\, dr = 2/\\lambda\n\\]\nwhere \\(C(r) = \\mathop{\\mathrm{\\mathbb{E}}}[Y_t Y_{t+r}] = \\exp[-\\lambda r]\\) is the autocorrelation function of the fast OU process, as explained here. The fact that the effective diffusion is (twice) the integrated autocorrelation of the fast process is an example of Green-Kubo relations.\n\n\nExample: Overdamped Langevin Dynamics\nThis example does not exactly fall within the homogenization result described in the previous section, but almost. Consider a potential \\(U\\) and the slow-fast dynamics:\n\\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon} &= \\varepsilon^{-1/2} \\, Y^{\\varepsilon} \\, dt\\\\\ndY^{\\varepsilon} &= - \\varepsilon^{-1}\\, [Y^{\\varepsilon}+\\varepsilon^{1/2}\\nabla U(X^{\\varepsilon})] \\, dt + \\sqrt{2 /\\varepsilon} \\, dW^Y.\n\\end{align}\n\\right.\n\\]\nFor any fixed value of \\(x \\in \\mathcal{X}\\), the fast OU-dynamics\n\\[\ndY = -[Y^{\\varepsilon}+\\varepsilon^{1/2}\\nabla U(x)] \\, dt + \\sqrt{2} \\, dW^Y\n\\]\nconverges to a Gaussian distribution with mean \\(-\\nabla U(x)\\) and unit variance. The same arguments as the previous section immediately give that, starting from \\(X^{\\varepsilon}_0=x\\), we have\n\\[\n\\varepsilon^{-1/2} \\, \\int_{0}^{\\delta} Y^{\\varepsilon} \\, dt\n\\; \\to \\;\n-\\nabla U(x) \\, \\delta + \\sqrt{2 \\delta} \\, \\mathcal{N}(0,1).\n\\]\nThe \\(\\sqrt{2}\\) terms comes from the OU asymptotic variance. this shows that the slow process converges as \\(\\varepsilon\\to 0\\) to the overdamped Langevin dynamics\n\\[\ndX = -\\nabla U(X) \\; + \\; \\sqrt{2} \\, dW.\n\\]\n\n\nExample: Stratonovich Corrections\nConsider a function \\(f: \\mathbb{R}\\to \\mathbb{R}\\) and the slow-fast system\n\\[\ndX^{\\varepsilon} = \\varepsilon^{-1/2} \\, f(X^{\\varepsilon}) \\, Y^{\\varepsilon} \\, dt\n\\]\nwhere \\(dY^{\\varepsilon} = -(\\lambda/\\varepsilon) Y^{\\varepsilon} + \\sqrt{2 \\lambda / \\varepsilon}\\) is a fast OU process mixing on scales of order \\(\\mathcal{O}(\\varepsilon)\\) and with standard centred Gaussian invariant distribution \\(\\rho(dy)\\).The discussion leading to Equation 8 suggest that the term \\(\\varepsilon^{-1/2} \\, Y^{\\varepsilon} \\, dt\\) can be heuristically be thought of as \\((2/\\lambda)^{1/2} \\, dW\\), which would imply that the effective dynamics for the slow-process is\n\\[\ndX = \\sqrt{2/\\lambda} \\, f(X) \\, dW.\n\\]\nWe will see that this heuristic is wrong! In order to obtain the effective dynamics of the slow process as \\(\\varepsilon\\to 0\\), since the generator of the fast-OU reads \\(\\mathcal{L}\\varphi= \\lambda [ -y\\,\\varphi_y + \\varphi_{yy}]\\), one can solve the Poisson equation \\(\\mathcal{L}\\Psi(x,y) = f(x)y\\) to obtain that \\(\\Phi(x,y) = -f(x)y/\\lambda\\). One already knows that \\(\\mathop{\\mathrm{Var}}[T^{-1}\\int_{[0,T]} Y_t \\, dt] = 2/\\lambda\\). The drift term is given by\n\\[\n\\begin{align}\nI(x) &= \\int f(x) \\partial_x \\Phi(x,y) \\, \\rho(dy)\\\\\n&= \\lambda^{-1} \\int f(x) f'(x) y^2 \\, \\rho(dy)\\\\\n&= \\lambda^{-1} f(x) f'(x).\n\\end{align}\n\\]\nPutting everything together gives that the effective slow dynamics reads\n\\[\n\\begin{align}\ndX &=  \\textcolor{blue}{ \\lambda^{-1} f'(X) f(X) \\, dt } + \\sqrt{2/\\lambda} \\, f(X) \\, dW\\\\\n&= \\sqrt{2/\\lambda} \\, f(X)  \\textcolor{red}{\\circ} dW\n\\end{align}\n\\]\nwhere \\( \\textcolor{red}{\\circ}\\) denotes Stratonovich integration.\n\n\nReadings\nThe book (Pavliotis and Stuart 2008) is beautiful, and I quite like the section on multiscale expansion in (Weinan 2011). For proving this type of results with the “martingale problem” approach (Stroock and Varadhan 1997), the lectures (Papanicolaou 1977) are nicely done.\n\n\n\n\n\nReferences\n\nHinch, E. J. 1991. Perturbation Methods. Cambridge University Press.\n\n\nPapanicolaou, George. 1977. “Martingale Approach to Some Limit Theorems.” In Papers from the Duke Turbulence Conference, Duke Univ., Durham, NC, 1977.\n\n\nPavliotis, Grigoris, and Andrew Stuart. 2008. Multiscale Methods: Averaging and Homogenization. Springer Science & Business Media.\n\n\nStroock, Daniel W, and SR Srinivasa Varadhan. 1997. Multidimensional Diffusion Processes. Vol. 233. Springer Science & Business Media.\n\n\nWeinan, E. 2011. Principles of Multiscale Modeling. Cambridge University Press."
  },
  {
    "objectID": "notes/information_theory_shearer_lemma/shearer_lemma.html",
    "href": "notes/information_theory_shearer_lemma/shearer_lemma.html",
    "title": "Shearer’s lemma",
    "section": "",
    "text": "The Shearer’s lemma (Chung et al. 1986) is concerned with a generalization of the sub-additivity of the Shannon Entropy,\n\\[\nH(X_1, \\ldots, X_N) \\; \\leq \\; H(X_1) + \\ldots + H(X_N).\n\\]\nInstead, consider an integer \\(t \\geq 1\\) and a family \\(S_1, \\ldots, S_K\\) of subsets of \\(\\{1, \\ldots, N\\}\\) such that any index \\(1 \\leq n \\leq N\\) appears in at least \\(t\\) of these subsets. Note that for a subset \\(S_i = \\{ \\alpha_1, \\ldots, \\alpha_{r_i}\\}\\) with \\(\\alpha_1 &lt; \\ldots &lt; \\alpha_{r_i}\\) we have\n\\[\n\\begin{align}\nH(X_{S_i}) &\\equiv H(X_{\\alpha_1}, \\ldots, X_{\\alpha_{r_i}})\\\\\n&= H(X_{\\alpha_1}) + H(X_{\\alpha_2} | X_{\\alpha_1}) + \\ldots + H(X_{\\alpha_{r_i}} | X_{\\alpha_{r_i-1}}, \\ldots, X_{\\alpha_1} ) \\\\\n&\\geq H(X_{\\alpha_1}) + H(X_{\\alpha_2} | X_{1:(\\alpha_2-1)}) + \\ldots + H(X_{\\alpha_{r_i}} | X_{1:\\alpha_{r_i}-1}).\n\\end{align}\n\\tag{1}\\]\nSince each index appears in at least \\(t\\) of the subsets, summing Equation 1 over all the subset \\(S_i\\) yields\n\\[\n\\sum_{i=1}^K H(X_{S_k}) \\geq t \\, \\sum_{i=1}^N H(X_i \\, | X_{1:(i-1)}) = t \\, H(X).\n\\]\nThis means that the following inequality holds,\n\\[\nH(X) \\leq \\frac{1}{t} \\, \\sum_{k=1}^K H(X_{S_k})\n\\]\nIndeed, the standard sub-additivity property of the entropy corresponds to the set \\(S_k = [k]\\) for \\(1 \\leq k \\leq N\\) and \\(t=1\\).\n\nApplication: projection on hyperplanes\nConsider a measurable set \\(A \\subset \\mathbb{R}^n\\) and call \\(A_k\\) the projection of \\(A\\) on the hyperplane \\(\\{x=(x_1, \\ldots, x_n) \\in \\mathbb{R}^n \\, : \\, x_k=0\\}\\). A Theorem of Loomis and Whitney (Loomis and Whitney 1949) states that the lebesgue measure \\(|A|\\) of the set \\(A\\) satisfies\n\\[\n|A| \\; \\leq \\; \\prod_{k=1}^n |A_k|^{1/(n-1)}.\n\\]\nIn other words, if all the projections \\(A_k\\) of the set \\(A\\) are small then, necessarily, the set \\(A\\) itself is small. To proceed, one can approximate this set \\(A\\) with the union \\(A_{\\varepsilon}\\) of small cubes of side \\(\\varepsilon\\) centred on \\(\\varepsilon\\, \\mathbb{Z}^n\\). If one can prove the statement for \\(A^{[\\varepsilon]}\\), the results follows from a standard approximation argument (ie. outer measure). Now, each cube can be indexed with a \\(n\\)-uple of integers \\((x_1, \\ldots, x_n) \\in \\mathbb{Z}^n\\), and one can consider the random variable \\(X=(X_1, \\ldots, X_n)\\) that is uniformly distributed on the set of cubes coordinates. Because \\(2^{H(X)} = |A^{[\\varepsilon]}| / \\varepsilon^n\\) and \\(2^{H(X_2, \\ldots, X_n)} = |A_1^{[\\varepsilon]}| / \\varepsilon^n\\) etc…, choosing the subsets \\(S_i=[1:n] \\setminus \\{i\\}\\) and \\(t = (n-1)\\) in Shearer’s Lemma immediately gives the conclusion.\n\n\n\n\n\nReferences\n\nChung, Fan RK, Ronald L Graham, Peter Frankl, and James B Shearer. 1986. “Some Intersection Theorems for Ordered Sets and Graphs.” Journal of Combinatorial Theory, Series A 43 (1). Academic Press: 23–37.\n\n\nLoomis, Lynn H, and Hassler Whitney. 1949. “An Inequality Related to the Isoperimetric Inequality.”"
  },
  {
    "objectID": "notes/HJB/HJB.html",
    "href": "notes/HJB/HJB.html",
    "title": "Doob, Girsanov and Bellman",
    "section": "",
    "text": "Richard Bellman (1920 – 1984)\n\n\n\nConsider a diffusion in \\(\\mathbb{R}^D\\) with deterministic starting position \\(x_0 \\in \\mathbb{R}^D\\) and dynamics\n\\[\ndX_t = b(X_t)dt + \\sigma(X_t) \\, dW_t\n\\]\nfor a drift and volatility functions \\(b: \\mathbb{R}^D \\to \\mathbb{R}^D\\) and \\(\\sigma: \\mathbb{R}^D \\to \\mathbb{R}^{D \\times D}\\). On the time interval \\([0,T]\\), this defines a probability \\(\\mathbb{P}\\) on the path-space \\(C([0,T];\\mathbb{R}^D)\\). For two functions \\(f: [0,T] \\times \\mathbb{R}^D \\to \\mathbb{R}\\) and \\(g: \\mathbb{R}^D \\to \\mathbb{R}\\), consider the probability distribution \\(\\mathbb{Q}\\) defined as\n\\[\n\\frac{d \\mathbb{Q}}{d \\mathbb{P}} = \\frac{1}{\\mathcal{Z}} \\exp  {\\left\\{  \\int_0^T f(X_s) \\, ds + g(X_T)  \\right\\}}\n\\]\nwhere \\(\\mathcal{Z}\\) denotes the normalizing constant \\[\n\\mathcal{Z}\\; = \\; \\mathop{\\mathrm{\\mathbb{E}}} {\\left[  \\exp  {\\left\\{  \\int_0^T f(X_s) \\, ds + g(X_T)  \\right\\}}   \\right]} .\n\\tag{1}\\]\nThe distribution \\(\\mathbb{Q}\\) places more probability mass on trajectories such that \\(\\int_0^T f(X_s) \\, ds + g(X_T)\\) is large. As described in these notes on Doob h-transforms, the tilted probability distribution \\(\\mathbb{Q}\\) can be described by a diffusion process \\(X^\\star\\) with dynamics\n\\[\ndX^\\star = b(X^\\star)dt + \\sigma(X^\\star) \\,  {\\left\\{  dW_t +  \\textcolor{blue}{u^\\star(t, X^\\star)} \\, dt  \\right\\}} .\n\\]\nThe control function \\( \\textcolor{blue}{u^\\star: [0,T] \\times \\mathbb{R}^D \\to \\mathbb{R}^D}\\) is of the gradient form\n\\[\n\\textcolor{blue}{u^\\star(t, x)} \\; = \\; \\sigma^\\top(x) \\, \\nabla \\log[  \\textcolor{green}{h(t,x)} ]\n\\tag{2}\\]\nand the function \\( \\textcolor{green}{h(t,x)}\\) is described by the conditional expectation,\n\\[\n\\textcolor{green}{h(t,x) = \\mathbb{E} {\\left[  \\exp  {\\left\\{  \\int_t^T f(X_s) \\, ds + g(X_T)  \\right\\}}   \\mid X_t = x  \\right]} }.\n\\]\nThe expression \\( \\textcolor{blue}{u^\\star(t, x)} \\; = \\; \\sigma^\\top(x) \\, \\nabla \\log[  \\textcolor{green}{h(t,x)} ]\\) is quite intuitive; in order to describe the tilted measure \\(\\mathbb{Q}\\) that places more probability mass on trajectories such that \\(\\int_0^T f(X_s) \\, ds + g(X_T)\\) is large, the optimal control \\(u^\\star(t,x)\\) should be in the direction of states such that the “reward-to-go” quantity \\(\\int_t^T f(X_s) \\, ds + g(X_T)\\) is large.\nTo obtain a variational description of the optimal control function \\( \\textcolor{blue}{u^\\star}\\), it suffices to express it as the solution of an optimization problem. It turns out that KL-divergences between diffusion processes are the right tool for this: we will write \\(\\mathbb{Q}\\) as the minimizer of \\(\\mathop{\\mathrm{D_{\\text{KL}}}}(\\mathbb{P}^u \\| \\mathbb{Q})\\) for a class of tractable probability distributions \\(\\mathbb{P}^u\\) described by controlled diffusions. As described in these notes on the Girsanov Theorem, for any control function \\(u(t,x)\\), the controlled diffusion \\(X^u\\) with dynamics\n\\[\ndX^u = b(X^u)dt + \\sigma(X^u) \\,  {\\left\\{  dW_t +  \\textcolor{blue}{u(t, X^u)} \\, dt  \\right\\}}\n\\]\nand started at \\(x_0\\) induces a probability distribution \\(\\mathbb{P}^u\\) on path-space given by\n\\[\n\\frac{d\\mathbb{P}}{d\\mathbb{P}^u}(x)\n\\; = \\;\n\\exp {\\left\\{ -\\frac 12 \\int_{0}^{T} \\|u(s,X^u_S)\\|^2 \\, ds - \\int_{0}^{T} u(s,X^u_s)^\\top \\, dW_s \\right\\}} .\n\\tag{3}\\]\nThis allows one to write down explicitly the expression for the negative KL divergence\n\\[\n-\\mathop{\\mathrm{D_{\\text{KL}}}}(\\mathbb{P}^u \\| \\mathbb{Q}) =\n\\mathbb{E}_u {\\left[   \\log {\\left\\{ \\frac{d\\mathbb{Q}}{d\\mathbb{P}^u}(X^u) \\right\\}}  \\right]}\n\\]\nbetween \\(\\mathbb{P}^u\\) and the tilted distribution \\(\\mathbb{Q}\\). The notation \\(\\mathbb{E}_u\\) denotes the expectation with respect to the controlled diffusion \\(X^u\\). The negative KL is, up to a constant, the usual Evidence Lower Bound (ELBO) used in variational inference. Since the quantity \\(\\log {\\left\\{ \\frac{d\\mathbb{Q}}{d\\mathbb{P}^u}(X^u) \\right\\}} \\) can be expressed as\n\\[\n\\log {\\left\\{ \\frac{d\\mathbb{P}}{d\\mathbb{P}^u}(X^u) \\right\\}}  - \\log(\\mathcal{Z})\n+ \\int_0^T f(X^u_s) \\, ds + g(X^u_T)\n\\]\nit follows from Equation 3 that \\(-\\mathop{\\mathrm{D_{\\text{KL}}}}(\\mathbb{P}^u \\| \\mathbb{Q})\\) equals\n\\[\n-\\log(\\mathcal{Z}) +\n\\mathop{\\mathrm{\\mathbb{E}}} {\\left[  \\int_{0}^{T}  {\\left\\{  -\\tfrac 12 \\|u(s,X^u_s)\\|^2 + f(X^u_s)  \\right\\}}  \\, ds + g(X^u_T) \\right]} .\n\\]\nSince the KL divergence is positive and the optimal control \\(u^\\star\\) in Equation 2 drives the KL divergence to zero, we have that\n\\[\n\\max_u \\; \\text{ELBO}(u) = \\log \\mathcal{Z}\n\\]\nwhere the minimization is over all (reasonably well-behaved) control functions \\(u: [0,T] \\times \\mathbb{R}^D \\to \\mathbb{R}^D\\) and\n\\[\n\\text{ELBO}(u) \\; = \\; \\mathop{\\mathrm{\\mathbb{E}}} {\\left[  \\int_{0}^{T}  {\\left\\{  -\\tfrac 12 \\|u(s,X^u_s)\\|^2 + f(X^u_s)  \\right\\}}  \\, ds + g(X^u_T) \\right]} .\n\\]\nFor maximizing the ELBO, the control needs to drive the trajectories to regions where \\(\\int_{0}^{T} f(X^u_s) \\, ds + g(X^u_T)\\) is large while at the same time keep the control effort \\(\\int_{0}^{T} \\|u(s,X^u_s)\\|^2 \\, ds\\) small. The optimal control \\(u^\\star\\) is given by Equation 2 and Equation 1 gives that\n\\[\n\\begin{align}\n\\log \\mathcal{Z}\n&= \\log \\mathop{\\mathrm{\\mathbb{E}}} {\\left[  \\exp  {\\left\\{  \\int_0^T f(X_s) \\, ds + g(X_T)  \\right\\}}   \\right]} \\\\\n&= \\log[  \\textcolor{green}{ h(0,x_0) } ].\n\\end{align}\n\\]\nSince there was nothing really special about the starting point \\(x_0\\) and the time horizon \\(T&gt;0\\), the above derivation gives the solution to the following stochastic optimal control problem. It is written as a maximization problem although a large part of the control and physics literature writes it as an equivalent minimization problem. Consider the reward-to-go function (a.k.a. value function) defined as\n\\[\nV(t,x) = \\sup_u \\; \\mathop{\\mathrm{\\mathbb{E}}} {\\left[  \\int_{t}^{T}  {\\left\\{  -\\tfrac 12 \\|u(s,X^u_s)\\|^2 + f(X^u_s)  \\right\\}}  \\, ds + g(X^u_T) \\mid X_t = x \\right]} .\n\\]\nWe have that\n\\[\n\\begin{align}\nV(t,x)\n&= \\log \\mathbb{E} {\\left[  \\exp  {\\left\\{  \\int_t^T f(X_s) \\, ds + g(X_T)  \\right\\}}  \\mid X_t = x \\right]} \\\\\n&= \\log[  \\textcolor{green}{h(t, x)} ].\n\\end{align}\n\\]\nThis shows that optimal control \\(u^\\star\\) can also be expressed as\n\\[\nu^\\star(t,x) = \\sigma^\\top(x) \\nabla \\log[  \\textcolor{green}{ h(t,x) }]\n= \\sigma^\\top(x) \\, \\nabla V(t,x) .\n\\tag{4}\\]\nThe expression \\(\\sigma^\\top(x) \\, \\nabla V(t,x)\\) is intuitive: since we are trying to maximize the reward-to-go function, the optimal control should be in the direction of the gradient of the reward-to-go function.\nFinally, let us mention that one can easily derive the Hamilton-Jacobi-Bellman equation for the reward-to-go function \\(V(t,x)\\). We have\n\\[\nV(t,x) = \\sup_u \\; \\mathop{\\mathrm{\\mathbb{E}}} {\\left[  \\int_{t}^T C_s \\, ds + g(X^u_T) \\right]}\n\\]\nwith \\(C_s = -\\tfrac12 \\|u(s,X^u_s)\\|^2 + f(X^u_s)\\). For \\(\\delta \\ll 1\\), we have\n\\[\n\\begin{align}\nV(t,x)\n&\\; = \\;\n\\sup_u \\;  {\\left\\{  C_t \\, \\delta + \\mathop{\\mathrm{\\mathbb{E}}} {\\left[  V(t+\\delta, X^u_{t+\\delta}) \\mid X^u_t=x \\right]}   \\right\\}}  + o(\\delta)\\\\\n&\\; = \\;\nV(t,x) + \\delta \\, \\sup_{u(t,x)} \\;  {\\left\\{  C_t + (\\partial_t + \\mathcal{L}+ \\sigma(x) \\, u(t,x) \\, \\nabla) \\, V(t,x) \\right\\}}  + o(\\delta)\n\\end{align}\n\\]\nwhere \\(\\mathcal{L}= b \\nabla + \\sigma \\sigma^\\top : \\nabla^2\\) is the generator of the uncontrolled diffusion. Since \\(C_t = -\\tfrac12 \\|u(t,x)\\|^2 + f(x)\\) is a simple quadratic function, the supremum over the control \\(u(t,x)\\) can be computed in closed form,\n\\[\n\\begin{align}\nu^\\star(t,x)\n&= \\mathop{\\mathrm{argmax}}_{z \\in \\mathbb{R}^D} \\; -\\tfrac12 \\|z\\|^2 + \\left&lt; z, \\sigma^\\top(x) \\nabla V(t,x)  \\right&gt;\\\\\n&= \\sigma^\\top(x) \\, \\nabla V(t,x),\n\\end{align}\n\\]\nas we already knew from Equation 4. This implies that the reward-to-go function \\(V(t,x)\\) satisfies the HJB equation\n\\[\n{\\left( \\partial_t + \\mathcal{L} \\right)} V + \\frac12 \\| \\sigma^\\top \\nabla V \\|^2 + f = 0\n\\tag{5}\\]\nwith terminal condition \\(V(T,x) = g(x)\\). Another route to derive Equation 5 is to simply use the fact that \\(V(t,x) = \\log h(t,x)\\); since the Feynman-Kac gives that the function \\(h(t,x)\\) satisfies \\((\\partial_t + \\mathcal{L}+ f) h = 0\\), the conclusion follows from a few lines of algebra by starting writing \\(\\partial_t V = h^{-1} \\, \\partial_t h = -h^{-1}(\\mathcal{L}+ f)[h]\\), expanding \\(\\mathcal{L}h\\) and expressing everything back in terms of \\(V\\). The term \\(\\|\\sigma^\\top \\nabla V\\|^2\\) naturally arises when expressing the diffusion term \\(\\sigma \\sigma^\\top : \\nabla^2 h\\) as a function of the second derivative of \\(V\\); it is the idea of the standard Cole-Hopf transformation."
  },
  {
    "objectID": "notes/MCMC_deterministic_proposals/MCMC_deterministic.html",
    "href": "notes/MCMC_deterministic_proposals/MCMC_deterministic.html",
    "title": "Metropolis-Hastings ratio with deterministic proposals",
    "section": "",
    "text": "Consider a probability density \\(\\pi(x)\\) on \\(\\mathbb{R}^d\\) and a (deterministic) function \\(F: \\mathbb{R}^d \\to \\mathbb{R}^d\\). Assume further that \\(F\\) is an involution in the sense that\n\\[\nF(F(x)) = x\n\\]\nfor all \\(x \\in \\mathbb{R}^d\\). To keep things simple since it is not really the point of this short note, suppose that \\(\\pi(x)&gt;0\\) everywhere and that \\(F\\) is smooth. This type of transformations can be used to define Markov Chain Monte Carlo algorithms, eg. the standard Hamiltonian Monte Carlo (HMC) algorithm. To design a MCMC scheme with this involution \\(F\\), one needs to answer the following basic question: suppose that \\(X \\sim \\pi(dx)\\) and the proposal \\(Y = F(X)\\) is constructed and accepted with probability \\(\\alpha(X)\\), how should the acceptance probability function \\(\\alpha: \\mathbb{R}^d \\to [0,1]\\) be chosen so that the resulting random variable \\[Z \\; = \\; Y \\, B + (1-B) \\, X\\] is also distributed according to \\(\\pi(dx)\\)? The Bernoulli random variable \\(B\\) is such that \\(\\mathop{\\mathrm{\\mathbb{P}}}(B=1|X=x)=\\alpha(x)\\). In other words, for any test function \\(\\varphi: \\mathbb{R}^d \\to \\mathbb{R}\\), we would like \\(\\mathop{\\mathrm{\\mathbb{E}}}[\\varphi(Z)] = \\mathop{\\mathrm{\\mathbb{E}}}[\\varphi(X)]\\), which means that\n\\[\n\\int  {\\left\\{  \\varphi(F(x)) \\, \\alpha(x) + \\varphi(x) \\, (1-\\alpha(x))  \\right\\}}  \\, \\pi(dx) = \\int \\varphi(x) \\, \\pi(dx).\n\\tag{1}\\]\nRequiring for Equation 1 to hold for any test function \\(\\varphi\\) is easily seen to be equivalent to asking for the equation\n\\[\n\\alpha(x) \\, \\pi(x) \\; = \\; \\alpha(y) \\, \\pi(y) \\, |J_F(x)|\n\\]\nto hold for any \\(x \\in \\mathbb{R}^d\\) where \\(y=F(x)\\) and \\(J_F(x)\\) is the Jacobian of \\(F\\) at \\(x\\). Since \\(|J_F(y)| \\times |J_F(x)| = 1\\) because the function \\(F\\) is an involution, this also reads\n\\[\n\\alpha(x) \\, \\frac{\\pi(x) }{|J_F(x)|^{1/2}} \\; = \\;\n\\alpha(y) \\, \\frac{\\pi(y) }{|J_F(y)|^{1/2}}.\n\\]\nAt this point, it becomes clear to anyone familiar with the the correctness-proof of the usual Metropolis-Hastings algorithm that a possible solution is\n\\[\n\\alpha(x) \\; = \\; \\min {\\left\\{ 1, \\frac{\\pi(y) / |J_F(y)|^{1/2}}{\\pi(x) / |J_F(x)|^{1/2}} \\right\\}}\n\\]\nalthough there are indeed many other possible solutions. Since \\(|J_F(y)| \\times |J_F(x)| = 1\\), this also reads\n\\[\n\\alpha(x) \\; = \\; \\min {\\left\\{ 1, \\frac{\\pi(y)}{\\pi(x)} \\, |J_F(x)| \\right\\}} .\n\\tag{2}\\]\nOne can reach a similar conclusion by looking at the Radon-Nikodym ratio \\([\\pi(dx) \\otimes q(x,dy)] / [\\pi(dy) \\otimes q(y,dx)]\\) where \\(q(x,dy)\\) is the markov kernel described the deterministic transformation (Green 1995), but I do not find this approach significantly simpler. The very neat article (Andrieu, Lee, and Livingstone 2020) describes much more sophisticated and interesting generalizations. Indeed, Equation 2 is often used in the simpler case when \\(F\\) is volume preserving, i.e. \\(|J_F(x)|=1\\), as is the case for the Hamiltonian Monte Carlo (HMC). The discussion above was prompted by a student implementing a variant of this but with the wrong acceptance ratio \\(\\alpha(x) = \\min {\\left\\{ 1, \\frac{\\pi(y)}{\\pi(x)} \\, \\frac{|J_F(x)|}{|J_F(y)|} \\right\\}} \\) and us taking quite a bit of time to find the bug…\nNote that there are interesting and practical situations when the function \\(F\\) satisfies the involution property \\(F(F(x))=x\\) only when \\(x\\) belongs to a subset of the state-space. For instance, this can happen when implementing MCMC on a manifold \\(\\mathcal{M} \\subset \\mathbb{R}^d\\) and the function \\(F\\) involves a “projection” on the manifold \\(\\mathcal{M}\\), as for example described in the really interesting article (Zappa, Holmes-Cerfon, and Goodman 2018). In that case, it suffices to add a “reversibility check”, i.e. make sure that when applying \\(F\\) to the proposal \\(y=F(x)\\), one goes back to \\(x\\) in the sense that \\(F(y)=x\\). The acceptance probability in that case should be amended and expressed as\n\\[\n\\alpha(x) \\; = \\; \\min {\\left\\{ 1, \\frac{\\pi(y)}{\\pi(x)} \\, |J_F(x)| \\right\\}}  \\, \\mathbf{1} {\\left( F(y)=x \\right)} .\n\\]\nIn other words, if applying \\(F\\) to the proposal \\(y=F(x)\\) does not lead back to \\(x\\), the proposal is always rejected.\n\nSame, but without involution\nin some situations, the requirement for \\(F\\) to be an involution can seem cumbersome. What if we consider the more general situation of a smooth bijection \\(T: \\mathbb{R}^d \\to \\mathbb{R}^d\\) and its inverse \\(T^{-1}\\)? In that case, one can directly apply what has been described in the previous section: it suffices to consider an extended state-space \\((x,\\varepsilon)\\) obtained by including an index \\(\\varepsilon\\in \\{-1,1\\}\\) and the involution \\(F\\) defined as\n\\[\nF:\n\\left\\{\n\\begin{align}\n(x,\\varepsilon=+1) &\\mapsto (T(x), \\varepsilon=-1)\\\\\n(x,\\varepsilon=-1) &\\mapsto (T^{-1}(x), \\varepsilon=+1).\n\\end{align}\n\\right.\n\\tag{3}\\]\nThis allows one to define a Markov kernel that lets the distribution \\(\\overline{\\pi}(x, \\varepsilon) = \\pi(dx)/2\\) invariant. Things can even start to get a bit more interesting if a deterministic “flip” \\((x, \\varepsilon) \\mapsto (x, -\\varepsilon)\\) is applied after each application of the Markov kernel above describe: doing so avoids immediately coming back to \\(x\\) in the event the move \\((x,\\varepsilon) \\mapsto (T^{\\varepsilon}(x), -\\varepsilon)\\) is accepted. There are indeed quite a few papers exploiting this type of ideas.\n\n\nA mixture of deterministic transformations?\nTo conclude these notes, here is a small riddle whose answer I do not have. One can check that for any \\(c \\in \\mathbb{R}\\), the function \\(F_{c}(x) = c + 1/(x-c)\\) is an involution of the real line. This means that for any target density \\(\\pi(x)\\) on the real line, one can build the associated Markov kernel \\(M_c\\) defined as\n\\[\nM_c(x, dy) = \\alpha_c(x) \\, \\delta_{F_c(x)}(dy) + (1-\\alpha_c(x)) \\, \\delta_x(dy)\n\\]\nfor an acceptance probability \\(\\alpha_c(x)\\) described as above,\n\\[\n\\alpha_c(x) = \\min {\\left\\{ 1, \\frac{\\pi[F_c(x)]}{\\pi(x)} |F'_c(x)| \\right\\}} .\n\\]\nFinally, choose a \\(N \\geq 2\\) values \\(c_1, \\ldots, c_N \\in \\mathbb{R}\\) and consider the mixture of Markov kernels\n\\[\nM(x,dy) \\; = \\; \\frac{1}{N} \\sum_{i=1}^N M_{c_i}(x, dy).\n\\]\nThe Markov kernel \\(M(x, dy)\\) lets the distribution \\(\\pi\\) invariant since each Markov kernel \\(M_{c_i}(x, dy)\\) does, but it is not clear at all (to me) under what conditions the associated MCMC algorithm does converge to \\(\\pi\\). One can empirically check that if \\(N\\) is very small, things can break down quite easily. On the other, for \\(N\\) large, the mixuture of Markov kernels \\(M(x,dy)\\) empirically seems to behave as if it were ergodic with respect to \\(\\pi\\).\n\n\n\n\n\n\n\nFor \\(N=5\\) values \\(c_1, \\ldots, c_5 \\in \\mathbb{R}\\) chosen at random, the illustration aboves shows the empirical distribution of the associated Markov chain ran for \\(T=10^6\\) iterations and targeting the standard Gaussian distribution \\(\\pi(dx) \\equiv \\mathcal{N}(0,1)\\): the fit seems almost perfect.\n\n\n\n\n\nReferences\n\nAndrieu, Christophe, Anthony Lee, and Sam Livingstone. 2020. “A General Perspective on the Metropolis-Hastings Kernel.” arXiv Preprint arXiv:2012.14881.\n\n\nGreen, Peter J. 1995. “Reversible Jump Markov Chain Monte Carlo Computation and Bayesian Model Determination.” Biometrika 82 (4). Oxford University Press: 711–32.\n\n\nZappa, Emilio, Miranda Holmes-Cerfon, and Jonathan Goodman. 2018. “Monte Carlo on Manifolds: Sampling Densities and Integrating Functions.” Communications on Pure and Applied Mathematics 71 (12). Wiley Online Library: 2609–47."
  },
  {
    "objectID": "notes/information_theory_shannon_coding/information_theory_shannon_coding.html",
    "href": "notes/information_theory_shannon_coding/information_theory_shannon_coding.html",
    "title": "Shannon Source Coding Theorem",
    "section": "",
    "text": "Claude Shannon (1916 – 2001)\n\n\n\n\nTransmission through a noisy channel\nConsider a scenario involving a “noisy channel,” where a message \\((x_1,x_2, \\ldots)\\) expressed in an alphabet \\(\\mathcal{X}\\) is transmitted before being received as a potentially different and corrupted message \\((y_1, y_2,\\ldots)\\) expressed using a potentially different alphabet \\(\\mathcal{Y}\\). One can assume that letter \\(x \\in \\mathcal{X}\\) is transformed into \\(y \\in \\mathcal{Y}\\) with probability \\(p(x \\to y)\\) so that the matrix \\(M_{x,y} = [p(x \\to y)]_{(x,y) \\in \\mathcal{X}\\times \\mathcal{Y}}\\) has rows summing-up to one, and that the “letters” of the message \\((x_1 x_2 \\ldots)\\) are transmitted one by one and independently from each other (ie. there is no memory effect in the channel).\nNow, imagine I have a text that needs to be transmitted through this channel. Assume that the text is represented using \\(N\\) bits. My goal is to encode it in a way that introduces redundancy, utilizing the alphabet \\(\\mathcal{X}\\), so that it can be transmitted through the noisy channel. Eventually, I want it to be successfully decoded, allowing the original message to be recovered with minimal errors.\n\n\n\n\nA Mathematical Theory of Communication\n\n\n\nIf transmitting each letter from the alphabet \\(\\mathcal{X}\\) takes \\(1\\) unit of time, I need to estimate the overall time it will take to transmit the entire text of \\(N\\) bits. In cases where the channel does not completely destroy the information, one can use about any non-idiotic encoding and transmit the same text multiple times to increase the chances of accurately recovering the original message. This can be achieved by employing techniques such as majority voting to decode the received messages, which are all distorted versions of the original text.\nThe transmission rate represents the inverse of the time required to transfer a single bit of information:\n\\[\n\\textrm{R = (Transmission Rate)} = \\frac{1}{\\textrm{(average time it takes to transfer one bit)}}.\n\\]\nIn other words, it takes about \\(N \\times R\\) unit of times to transfer a text of \\(N\\) bits. Moreover, the error rate refers to the percentage of errors in the decoded message, indicating the fraction of erroneous bits among the \\(N\\) decoded bits. Naturally, there exists a tradeoff, and it is evident that one can reach a vanishing error rate if one is willing to allow an arbitrarily slow transmission rate (eg. majority voting after transmitting a very large number of times the same text). For example, if \\(\\mathcal{X}= \\mathcal{Y}= \\{0,1\\}\\) and bits are flipped with probability \\(p_{\\text{flip}} \\ll 1\\), transmitting the text \\((2K+1)\\) times would lead to a transmission rate of \\(R = 1/(2K+1)\\) and an error rate approximately equal to \\(p_{\\text{flip}}^{K+1}\\).\nThe groundbreaking discovery made by Shannon is that it is possible to achieve a vanishing error rate even when transmitting at a finite transmission rate. He also managed to identify this optimal transmission rate. Shannon’s paper (Shannon 1948) is beautifully written and surprisingly readable for a text written more than 50 years ago.\n\n\nVanishing error rate: Shannon Codebooks\nLet’s imagine that we have a piece of information encoded in a variable, \\(X\\). We send \\(X\\) through a noisy channel, and at the other end we receive a somewhat distorted message, \\(Y\\). So, how much of our original information actually was transmitted? To reconstruct our original message, \\(X\\), using our received message, \\(Y\\), we require an average of \\(H(X|Y)\\) additional bits of information. On average, \\(X\\) contains \\(H(X)\\) bits of information. So, if we encode \\(H(X)\\) bits of useful information in \\(X\\), the variable \\(Y\\) that is correlated with \\(X\\) still holds \\(I(X;Y) = H(X) - H(X \\, | Y)\\) bits of that original information. The quantity \\(I(X;Y)\\) is the mutual information between the random variables \\(X\\) and \\(Y\\). In a noisy channel that transmits one “letter” at a time, the conditional probabilities \\(p(x \\rightarrow y)\\) are fixed. However, we can optimize the distribution of incoming messages. For instance, we can choose to transmit letters that are less likely to be corrupted. This discussion suggests that on average, transmitting \\(N\\) symbols through the channel can provide up to \\(N \\times C\\) bits of information, where \\(C = \\max I(X;Y)\\), the maximization being over the distribution of \\(X\\) while keeping the conditional probabilities \\(p(x \\rightarrow y)\\) fixed. It may seem that this implies a noisy channel cannot transmit information at a rate higher than \\(C\\). This hypothesis was precisely proven by Claude Shannon, who further established that this transmission rate can indeed be reached.\nTo prove that this transmission rate is achievable, Shannon’s idea was to simultaneously encode blocks of letters. To put it simply, consider the \\(2^N\\) feasible blocks \\(\\{ t^{[1]}, \\ldots, t^{[2^N]} \\}\\) of \\(N\\) binary letters. Each block \\(t^{[i]} \\in \\{0,1\\}^N\\) has \\(N\\) binary letters, \\(t^{[i]} = (t_1^{[i]}, \\ldots, t_N^{[i]})\\). Associate to each of block \\(t^{[i]} \\in \\{0,1\\}^N\\) a codeword \\(x^{[i]} \\in \\mathcal{X}^K\\) of size \\(K\\) in the alphabet \\(\\mathcal{X}\\). The set of these \\(2^N\\) codewords is usually called the codebook,\n\\[\n\\mathcal{C}= \\left\\{ x^{[1]}, x^{[2]}, \\ldots, x^{[2^N]} \\right\\} \\; \\subset \\mathcal{X}^{K}\n\\tag{1}\\]\nTo transmit a block of \\(N\\) letters from the original text, this block is first transformed into its associated codeword \\(x=(x_1, \\ldots, x_K) \\in \\mathcal{X}^K\\). This codeword is then sent through the noisy channel, resulting in a received message \\((y_1, \\ldots, y_K) \\in \\mathcal{Y}^K\\). The objective is to design a codebook with enough redundancy so that one can reconstruct the original codeword from the received message \\((y_1, \\ldots, y_K)\\): the higher the ratio \\(K/N\\), the larger the redundancy and the easier it should be to achieve this goal. The transmission rate is defined as \\(R = \\frac{N}{K}\\) since transmitting a binary text of length \\(N\\) with vanishing errors takes \\(K\\) units of time.\nFor generating the codebook in Equation 1, Shannon adopted a simple approach consisting in generating each \\(x^{[i]}_k\\) for \\(1 \\leq i \\leq 2^N\\) and \\(1 \\leq k \\leq K\\) independently at random from some (encoding) distribution \\(p_{\\text{code}}(dx)\\). The choice of this encoding distribution can be optimized at a later stage.\nConsider the codeword \\(x^{[0]} = (x^{[0]}_1, \\ldots, x^{[0]}_K)\\). After being transmitted through the noisy channel, this gives rise to a message \\(y_{\\star}\\). The codeword \\(x^{[0]}\\) can be easily recovered if \\((x^{[0]}, y_\\star)\\) is typical while all the other pairs \\((x^{[i]}, y_\\star)\\) for \\(2 \\leq i \\leq 2^N\\) are atypical. Since there are about \\(2^{K \\, H(X | Y)}\\) elements \\(x \\in \\mathcal{X}^K\\) such that \\((x, y_\\star)\\) is typical, and each codeword was chosen approximately uniformly at random within its typical set of size \\(2^{K \\, H(X)}\\), the probability for a random codeword to be atypical is about\n\\[1-2^{-K \\, [H(X) - H(X|Y)]} = 1 - 2^{-K \\, I(X;Y)}\\]\nConsequently, the probability that all the other pairs \\((x^{[i]}, y_\\star)\\) for \\(2 \\leq i \\leq 2^N\\) are atypical is\n\\[\np_{\\text{success}} = (1 - 2^{-K \\, I(X;Y)})^{2^N-1} \\approx (1 - 2^{-K \\, I(X;Y)})^{2^{KR}}.\n\\]\nThe probability \\(p_{\\text{success}} \\to 1\\) as soon as \\(R &lt; I(X;Y)\\) as \\(N \\to \\infty\\). Furthermore, remembering that one were free to optimize the encoding distribution \\(p_{\\text{code}}(dx)\\), a vanishing error rate is possible as soon as the transmission \\(R\\) rate is lower than\n\\[\n\\text{(Channel Capacity)} = C \\equiv \\max_{p_{\\text{code}}} \\; I(X;Y).\n\\]\nTo sum-up, consider \\(p_{\\mathcal{C}, \\text{success}}\\) the success rate of the codebook \\(\\mathcal{C}\\), ie. the probability that a random codeword of \\(\\mathcal{C}\\) is successfully decoded when passing through the noisy channel. The reasoning above shows that the averaged success rate \\(p_{\\text{success}} = \\left&lt; p_{\\mathcal{C}, \\text{success}} \\right&gt;\\), i.e. averaging \\(p_{\\mathcal{C}, \\text{success}}\\) over all possible codebooks \\(\\mathcal{C}\\), converges to one as long as the transmission rate is below the channel capacity \\(C\\). This means that one can find at least one codebook that works well! This reasoning is an example of the “probabilistic method”… Indeed, one also expect most random codebook to work well!\n\n\nNo vanishing error below the channel capacity\nTo demonstrate that transmission at vanishing error-rate is impossible when the transmission rate exceeds the channel capacity, \\(C\\), we can utilize Fano’s inequality.\nImagine selecting a message \\(M\\) uniformly at random within \\(\\{0,1\\}^N\\) and encode this message into the sequence \\(X=(X_1, ..., X_K) \\in \\mathcal{X}^K\\). We send \\(X\\) through a channel with capacity \\(C\\) and receive a corresponding, though somewhat distorted, signal \\(Y=(Y_1, ..., Y_K)\\). Finally, we decode this received message into \\(\\widehat{M}\\), an estimate of our original message:\n\\[\nM \\rightarrow X \\rightarrow Y \\rightarrow \\widehat{M}.\n\\]\nFano’s inequality points out that the error probability, \\(p_E = \\mathop{\\mathrm{\\mathbb{P}}}(\\widehat{M} \\neq M)\\) is such that\n\\[\n\\begin{align}\nH(M | \\widehat{M})\n&\\leq 1 + p_E \\, \\log_2(\\# \\textrm{possible values of } M)\\\\\n&= 1 + p_E \\, N\n\\end{align}\n\\]\nApplying the data-processing inequality to \\(M \\rightarrow X \\rightarrow Y \\rightarrow \\widehat{M}\\) proves:\n\\[\n\\begin{align}\nN &= H(M) = H(M | \\widehat{M}) + I(M; \\widehat{M}) \\\\\n& \\leq H(M | \\widehat{M}) + I(X; Y)\\\\\n& \\leq 1 + N \\, p_E + I(X; Y).\n\\end{align}\n\\]\nTo wrap up, recall that each received letter \\(Y_i\\) in the message (Y_1, , Y_K)$ depends solely on the corresponding letter \\(X_i\\) in the message sent through the channel. This implies that \\(I(X; Y) \\leq \\sum_{i=1}^K I(X_i; Y_i) \\leq K \\, C\\).This yields:\n\\[\nN \\leq 1 + N \\, p_E + K \\, C.\n\\]\nThis reveals that for the probability of error to go to zero, i.e. \\(p_E \\rightarrow 0\\) as \\(N \\rightarrow \\infty\\), the transmission rate \\(N/K\\) must be lower than \\(C\\).\n\n\nExperiment\nConsider the Binary Symmetric Channel (BSC) that randomly flips \\(0 \\mapsto 1\\) and \\(1 \\mapsto 0\\) with equal probability \\(0&lt;q&lt;1\\). The capacity of this channel is easily computed and equals \\(C = 1 - h_2(q)\\) where \\(h_2(q) = -[q \\, \\log_2(q) + (1-q) \\, \\log_2(1-q)]\\) is the binary entropy function: the optimal encoding distribution is\n\\[p_{\\text{code}}(0) = p_{\\text{code}}(2) = 1/2.\\]\nFor a flipping rate of \\(q=0.1\\) the channel capacity equals \\(C=0.53\\). To estimate the performance of the random Shannon codebook strategy, I chose \\(N=13\\) and several values of \\(K \\geq N\\). This means generating a random codebook \\(\\mathcal{C}= \\{x^{[1]}, \\ldots, x^{[2^N]}\\}\\) of size \\(2^{13} = 8192\\) consisting of random binary vectors of size \\(K\\). For a randomly chosen codeword \\(x^{[i]}\\), a received message \\(y_\\star\\) is generated by flipping each of the \\(K\\) coordinates of \\(x^{[i]}\\) independently with probability \\(q\\). In the BSC setting, it is easily seen that the codeword of \\(\\mathcal{C}\\) that was the most likely to have originated \\(y_{\\star}\\) is\n\\[\nx_\\star \\; = \\; \\mathop{\\mathrm{argmin}}_{x \\in \\mathcal{C}} \\; \\|x - y_\\star\\|_{L^2}.\n\\]\nThe nearest neighbor \\(x_\\star\\) can be relatively efficiently computed with a nearest-neighbor routine (eg. FAISS). The figure below reports the probability of error (i.e. “Block Error Rate”),\n\\[\n\\text{(Block Error Rate)} \\; = \\; \\mathop{\\mathrm{\\mathbb{P}}}(x_\\star \\neq x^{[i]})\n\\]\nwhen the codeword \\(x^{[i]}\\) is chosen uniformly at random within the codebook.\n\n\n\n\n\n\n\nIt can be seen that, although the error rate does go to zero for low transmission rate, the choice of \\(K = N / C\\) where \\(C\\) is the channel capacity still yields a relatively large block error rate. This indicates that the block size \\(N=13\\) is still far too low for the “law of large number” arguments presented in the previous section to kick-in. I did try for \\(N=20\\) and a codebook of \\(2^{20} \\approx 10^6\\) and the performace was still not impressive. This shows that even though the Shannon codebook approaches is an elegant construction, it is far from being practically useful. It requires a very large codebook of size \\(2^N\\) and decoding requires doing a nearest-neighbors search that can become slow as \\(N\\) increases.\n\n\n\n\n\nReferences\n\nShannon, Claude Elwood. 1948. “A Mathematical Theory of Communication.” The Bell System Technical Journal 27 (3). Nokia Bell Labs: 379–423."
  },
  {
    "objectID": "notes/information_theory_basics/information_theory_entropy.html",
    "href": "notes/information_theory_basics/information_theory_entropy.html",
    "title": "Information Theory: Entropy and Basic Definitions",
    "section": "",
    "text": "Claude Shannon (1916 – 2001)\n\n\n\n\nShannon Entropy & Compression\nIf Alice chooses a number \\(X\\) uniformly at random from the set \\(\\{1,2, \\ldots, N\\}\\), Bob can use a simple “dichotomy” strategy to ask Alice \\(\\log_2(N)\\) binary Yes/No questions and correctly identify the number. This result can be generalized to non-uniform distributions (cf. Huffman codes, and also Kraft-McMillan inequality). If Alice chooses a number \\(X\\) from \\(\\{1,2, \\ldots, N\\}\\) with probabilities \\(\\mathop{\\mathrm{\\mathbb{P}}}(X=k) = p_k\\), Bob can design a deterministic strategy to find the answer using, on average, about\n\\[\nH(X) = - \\sum_{k=1}^N p_k \\, \\log_2(p_k)\n\\tag{1}\\]\nbinary questions, ie. bits. To be more precise, there are strategies that require at most \\(H(X) + 1\\) questions on average, and none that can require less than \\(H(X)\\). Note that applying this remark to an iid sequence \\(X_{1:T} = (X_1, \\ldots, X_T)\\) and using the the fact that \\(H(X_1, \\ldots, X_T) = T \\, H(X)\\), this shows that one can exactly determining the sequence \\(X_{1:T}\\) with at most \\(T \\, H(X) + 1\\) binary questions on average. The quantity \\(H(X)\\) defined in Equation 1, known as the Shannon Entropy of the distribution \\((p_1, \\ldots, p_N)\\), also implies that there are strategies that can encode each integer \\(1 \\leq x \\leq N\\) as a binary string of length \\(L(x)\\) (i.e. with \\(L(x)\\) bits), with the expected length \\(\\mathop{\\mathrm{\\mathbb{E}}}[L(X)]\\) approximately equal to \\(H(X)\\). It is because a sequence of binary questions can be thought of as a binary tree, etc…\nThis remark can be used for compression. Imagine a very long sequence \\((X_1, \\ldots, X_T)\\) of iid samples from \\(X\\). Encoding each \\(X_i\\) with \\(L(X_i)\\) bits, one should be able to encode the resulting sequence with\n\\[\nL(X_1) + \\ldots + L(X_T) \\approx T \\, \\mathop{\\mathrm{\\mathbb{E}}}[L(X)] \\approx T \\cdot H(X)\n\\]\nbits. Can the usual zip compression algorithm do this? To test this, choose a probability distribution on \\(\\{1, \\ldots, N\\}\\), generate an iid sequence of length \\(T \\gg 1\\), compress this using the \\(\\texttt{gzip}(\\ldots)\\) command, and finally look at the size of the resulting file (in bits). I have done that a few times, with a few different values of \\(16 \\leq N \\leq 256\\) and a few random distributions on \\(\\{1, \\ldots, N\\}\\), and with \\(T = 10^6\\). The plot of size of the compressed files versus the Shannon entropy \\(H\\) looks as below:\n\n\n\n\n\n\n\nSeems like the zip-algorithm works almost optimally for compressing iid sequences.\n\n\nSequence of random variables\nNow consider a pair of discrete random variables \\((X,Y)\\). If Alice draws samples from this pair of rvs, one can ask \\(H(X,Y)\\) binary questions on average to exactly find out these values. To do that, one can ask \\(H(X)\\) questions to estimate \\(X\\), and once \\(X=x\\) is estimated, one can then ask about \\(H(Y|X=x) = -\\sum_y \\mathop{\\mathrm{\\mathbb{P}}}(Y=y|X=x) \\, \\log_2(\\mathop{\\mathrm{\\mathbb{P}}}(Y=y|X=x))\\) to estimate \\(Y\\). This strategy requires on average \\(H(X) + \\sum_x \\mathop{\\mathrm{\\mathbb{P}}}(X=x) \\, H(Y|X=x)\\) binary questions and is actually optimal, showing that\n\\[\nH(X,Y) = H(X) + H(Y | X)\n\\tag{2}\\]\nwhere we have defined \\(H(Y | X) = \\sum_x \\mathop{\\mathrm{\\mathbb{P}}}(X=x) \\, H(Y|X=x)\\).\nIndeed, one can generalize these concepts to more than two random variables. Iterating Equation 2 shows that the trajectory \\(X_{1:T} \\equiv (X_1, \\ldots, X_N)\\) of a stationary ergodic Markov chain can be estimated on average with \\(H(X_{1:T})\\) binary questions where\n\\[\n\\begin{align}\nH(X_{1:T})\n&= H(X_1) + H(X_2|X_1) + \\ldots + H(X_{T} | X_{t-1})\\\\\n&\\approx T \\, H(X_{k+1} | X_k)\\\\\n&= - T \\, \\sum_x \\pi(x) \\, \\sum_{y} p(x \\to y) \\, \\log_2[p(x \\to y)].\n\\end{align}\n\\]\nHere, \\(\\pi(dx)\\) is the equilibrium distribution of the Markov chain and \\(p(x \\to y)\\) are the transition probabilities.\nCan \\(\\texttt{gzip}(\\ldots)\\) compress Markovian trajectories and roughly achieve that level of compression? Indeed, one can test this by generating random Markov transition matrices (that are, with very high probability, ergodic with respect to an equilibrium distribution \\(\\pi\\)). Doing this with trajectories of length \\(10^4\\) (ie. quite short because it is quite slow to) on \\(\\{1, \\ldots, N\\}\\) with \\(2 \\leq N \\leq 64\\), one get the following results:\n\n\n\n\n\n\n\nIn red is the entropy estimated without using the Markovian structure and assuming that the \\(X_i\\) are iid samples. One can clearly see that the dependencies between successive samples are exploited for better compression. Nevertheless, the optimal compression rate given by the Shannon entropy is not reached. Indeed, \\(\\texttt{gzip}(\\ldots)\\) is not an optimal algorithm – it cannot even compress well enough the sequence \\((1,2,3,1,2,3,1,2,3,\\ldots)\\)!\n\n\nAsymptotic Equipartition Property (AEP)\nThe AEP is simple remark that gives a convenient way of reasoning about long sequences random variables \\(X_{1:T} = (X_1, \\ldots, X_T)\\) with \\(T \\gg 1\\). For example, assuming that the random variables \\(X_i\\) are independent and identically distributed as the random variable \\(X\\), the law of large numbers (LLN) gives that\n\\[\n-\\frac{1}{T} \\, \\log_2 p(X_{1:T}) = -\\frac{1}{T} \\, \\sum \\log_2 p(X_i) \\approx H(X).\n\\]\nThis means that any “typical” sequence has a probability about \\(2^{-T \\, H(X)}\\) of occurring, which also means that there are about \\(2^{T \\, H(X)}\\) such “typical” sequences. Indeed, one could use the LLN for Markov chains and obtain a similar statement for Markovian sequences. All this can be made rigorous with large deviation theory, for example. This also establishes the link with the “statistical physics” definition of entropy as the logarithm of the number of configurations… Set of the type\n\\[\nA_{\\varepsilon} = \\left\\{ x_{1:T} \\; : \\; \\left| -\\frac{1}{T} \\, \\log_2 p(x_{1:T}) - H(X)\\right| &lt; \\varepsilon\\right\\}\n\\]\nare usually called typical set: for any \\(\\varepsilon&gt; 0\\), the probability of \\(X_{1:T}\\) to belongs to \\(A_{\\varepsilon}\\) goes to one as \\(T \\to \\infty\\). For these reasons, it is often a good heuristic to think of a draw of \\((X_1, \\ldots, X_T)\\) as a uniformly distributed on the associated typical set. For example, if \\((X_1, \\ldots, X_N)\\) are \\(N\\) iid draws from a Bernoulli distribution with \\(\\mathop{\\mathrm{\\mathbb{P}}}(X=1) = 1-\\mathop{\\mathrm{\\mathbb{P}}}(X=0) =p\\), the set \\(A \\subset \\{0,1\\}^N\\) of sequences such that \\(x_1 + \\ldots + x_N = Np\\) has \\(\\binom{N}{Np} \\approx 2^{N \\, h_2(q)}\\) elements where \\(h_2(q) = -[q \\, \\log_2(q) + (1-q) \\, \\log_2(q)]\\) is the entropy of a \\(\\text{Bern}(q)\\) random variable.\n\n\nMutual information\nConsider a pair of random variables \\((X,Y)\\). Assuming that \\(X\\) stores (on average) \\(H(X)\\) bits of useful information, how much of this information can be extracted from \\(Y\\)? Let us call this quantity \\(I(X;Y)\\) since we will see in a second that this quantity is symmetric. If \\(Y\\) is independent from \\(X\\), no useful information about \\(X\\) is contained in \\(Y\\) and \\(I(X;Y) = 0\\). On the contrary, if \\(X=Y\\), the knowledge of \\(Y\\) already contains all the information about \\(Y\\) and \\(I(X;Y) = H(X) = H(Y)\\). If one knows \\(Y\\), one needs on average \\(H(X|Y)\\) binary questions (ie. bits of additional information) in order to determine \\(X\\) certainly and recover all the information contained in \\(X\\). This means that the knowledge of \\(Y\\) already contains \\(I(X;Y) = H(X) - H(X|Y)\\) useful bits of information about \\(X\\)! This quantity is called the mutual information of the two random variable \\(X\\) and \\(Y\\), and it has the good taste of being symmetric:\n\\[\n\\begin{align}\nI(X;Y) &= H(X) - H(X|Y) \\\\\n&= H(X) + H(Y) - H(X,Y).\n\\end{align}\n\\]\nNaturally, one can define conditional version of it by setting \\(I(X;Y \\, |Z) = \\sum_{z} \\mathop{\\mathrm{\\mathbb{P}}}(Z=z) \\, I(X_z | Y_z)\\) where \\((X_z, Y_z)\\) has the law of \\((X,Z)\\) conditioned on \\(Z=z\\). Since \\(I(X;Y \\,|Z)\\) is the reduction in uncertainty of \\(X\\) due to \\(Y\\) when \\(Z\\) is given, there are indeed situations when \\(I(X;Y \\, | Z)\\) is larger than \\(I(X;Y)\\) – it is to be contrasted to the intuitive inequality \\(H(X|Z) \\leq H(X)\\), which is indeed true. A standard such examples is when \\(X\\) and \\(Y\\) are independent \\(\\text{Bern}(1/2)\\) random variables and \\(Z = X+Y\\): a short computation gives that \\(I(X;Y \\, | Z) = 1/2\\) while, indeed, \\(I(X;Y) = 0\\). This definition of conditional mutual information leads to a chain-rule property,\n\\[\nI(X; (Y_1,Y_2)) = I(X;Y_1) + I(X;Y_2 | Y_1),\n\\]\nwhich can indeed be generalized to any number of variables. Furthermore, if the \\(Y_i\\) are conditionally independent given \\(X\\) (eg. if \\(X=(X_1, \\ldots, X_T)\\) and \\(Y_i\\) only depend on \\(X_i\\)), then the sub-additivity of the entropy readily gives that\n\\[\nI(X; (Y_1, \\ldots, Y_N)) \\leq \\sum_{i=1}^N I(X; Y_i).\n\\]\nImportantly, algebra shows that \\(I(X;Y)\\) can also be expressed as the Kullback-Leibler divergence between the joint distribution \\(\\mathop{\\mathrm{\\mathbb{P}}}_{(X,Y)}\\) and the product of the marginals \\(\\mathop{\\mathrm{\\mathbb{P}}}_X \\otimes \\mathop{\\mathrm{\\mathbb{P}}}_Y\\),\n\\[\nI(X;Y) \\; = \\;\n\\mathop{\\mathrm{D_{\\text{KL}}}} {\\left(  (X,Y) \\, \\| \\, X \\otimes Y \\right)} .\n\\]\nThis diagram from (MacKay 2003) nicely illustrate the different fundamental quantities \\(H(X)\\) and \\(H(X,Y)\\) and \\(H(Y|X)\\) and \\(I(X;Y)\\) and \\(H(X,Y)\\):\n\n\n\n\nFrom: Information Theory, Inference, and Learning Algorithms\n\n\n\nNaturally, if one considers three random variables \\(X \\mapsto Y \\mapsto Z\\) forming a “Markov chain”, we have the so-called data-processing inequality,\n\\[\nI(X;Z) \\leq I(X;Y)\n\\qquad \\text{and} \\qquad\nI(X;Z) \\leq I(Y;Z).\n\\]\nThe first inequality is clear since all the useful information contained in \\(Z\\) must be coming from \\(Y\\), and \\(Y\\) only contains \\(I(X;Y)\\) bits about \\(X\\). For the second inequality, note that if \\(Z\\) contains \\(I(Y;Z)\\) bits about \\(Y\\), and \\(Y\\) contains \\(H(Y;X)\\) bits about \\(X\\), then \\(Z\\) cannot contain more than \\(I(Y;Z)\\) bits of \\(X\\):\n\n\n\n\nData Processing Inequality for Markov \\(X \\to Y \\to Z\\)\n\n\n\n\n\n\n\n\nReferences\n\nMacKay, David JC. 2003. Information Theory, Inference and Learning Algorithms. Cambridge university press."
  },
  {
    "objectID": "notes/index_notes_as_list.html",
    "href": "notes/index_notes_as_list.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n11-06-2024\n\n\nDoob, Girsanov and Bellman\n\n\nSDE,markov\n\n\n\n\n03-06-2024\n\n\nGirsanov and Importance Sampling\n\n\nSDE,markov\n\n\n\n\n14-05-2024\n\n\nJoe Doob & Change of measures on path-space\n\n\nSDE,markov\n\n\n\n\n09-03-2024\n\n\nRWM & HMC on manifolds\n\n\nMCMC,manifold\n\n\n\n\n18-12-2023\n\n\nMetropolis-Hastings ratio with deterministic proposals\n\n\nauxiliary-variable\n\n\n\n\n28-11-2023\n\n\nAveraging and homogenization\n\n\ndiffusion\n\n\n\n\n18-11-2023\n\n\nEnsemble Kalman Smoother (EnKS)\n\n\nenkf,data-assimilation\n\n\n\n\n11-11-2023\n\n\nAsymptotic variance & Poisson Equation\n\n\nmarkov\n\n\n\n\n23-10-2023\n\n\nGaussian Assimilation & the EnKF\n\n\nenkf,data-assimilation\n\n\n\n\n19-10-2023\n\n\nDeriving Langevin MCMC\n\n\nMCMC\n\n\n\n\n16-10-2023\n\n\nWasserstein Gradients & Langevin Diffusions\n\n\ndiffusion\n\n\n\n\n09-10-2023\n\n\nSanov’s Theorem\n\n\nLargeDeviation\n\n\n\n\n03-10-2023\n\n\nAuxiliary variable trick\n\n\nauxiliary-variable\n\n\n\n\n02-10-2023\n\n\nShearer’s lemma\n\n\ninfoTheory\n\n\n\n\n30-09-2023\n\n\nInformation Theory: References and Readings\n\n\ninfoTheory\n\n\n\n\n26-09-2023\n\n\nShannon Source Coding Theorem\n\n\ninfoTheory\n\n\n\n\n23-09-2023\n\n\nInformation Theory: Fano’s inequality\n\n\ninfoTheory\n\n\n\n\n23-09-2023\n\n\nInformation Theory: Entropy and Basic Definitions\n\n\ninfoTheory\n\n\n\n\n02-07-2023\n\n\nFrom Denoising Diffusion to ODEs\n\n\nDDPM,score\n\n\n\n\n02-07-2023\n\n\nDenoising Diffusion Probabilistic Models (DDPM)\n\n\nDDPM,score\n\n\n\n\n12-06-2023\n\n\nReverse diffusions, Score & Tweedie\n\n\nSDE,score\n\n\n\n\n01-01-2023\n\n\nNotes\n\n\nindex\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/DDPM/DDPM.html",
    "href": "notes/DDPM/DDPM.html",
    "title": "Denoising Diffusion Probabilistic Models (DDPM)",
    "section": "",
    "text": "Setting & Goals\nConsider \\(N\\) samples \\(\\mathcal{D}\\equiv \\{x_i\\}_{i=1}^N\\) in \\(\\mathbb{R}^D\\) from an unknown data distribution \\(\\pi_{\\mathrm{data}}(dx)\\). We would like to build an mechanism that can generate other samples from this data distribution. Implicitly, this means that we will be fitting a statistical model to this finite set of samples and have an algorithmic procedure to generate samples from the fitted probabilistic model. This type of models that directly define a stochastic procedure that generates data are called implicit probabilistic models in the ML literature.\n\n\nOrnstein–Uhlenbeck Noising process\nDenoising Diffusion Probabilistic Models (DDPMs) work as follows. Consider a diffusion process \\(\\{ X_t \\}_{t=0}^T\\) that starts from the data distribution \\(p_0(dx) \\equiv \\pi_{\\mathrm{data}}(dx)\\) at time \\(t=0\\). The notation \\(p_t(dx)\\) refers to the marginal distribution of the diffusion at time \\(0 \\leq t \\leq T\\). Assume furthermore that at time \\(t=T\\), the marginal distribution is (very close to) a reference distribution \\(p_T(dx) = \\pi_{\\mathrm{ref}}(dx)\\) that is straightforward to sample from. Typically, \\(\\pi_{\\mathrm{ref}}(dx)\\) is an isotropic Gaussian distribution. This diffusion process is often called the noising process since it transform the data distribution into a reference measure that can be thought of as “pure noise”. It is often chosen as an Ornstein–Uhlenbeck (OU) diffusion,\n\\[\ndX = - \\frac12 X \\, dt + dW.\n\\tag{1}\\]\nThis diffusion is reversible with respect to, and quickly converges to, the reference distribution \\(\\pi_{\\mathrm{ref}} = \\mathcal{N}(0, I)\\) and has the good taste of having simple transition densities: the law of \\(X_{t+s}\\) given that \\(X_t = x_t\\) is the same as \\(e^{-s/2} x_t + \\sqrt{1-e^{-s}} \\, \\mathbf{n}\\), which we write as\n\\[\n\\alpha_s \\, x + \\sigma_s \\, \\mathbf{n}\n\\]\nwith \\(\\alpha_s^2 + \\sigma_s^2 = 1\\) and \\(\\alpha_s = e^{-s/2}\\) and \\(\\mathbf{n}\\sim \\pi_{\\mathrm{ref}} = \\mathcal{N}(0,I)\\). The forward probability transitions are tractable and given by \\[\n\\mathop{\\mathrm{\\mathbb{P}}}(X_{t+s} \\in dy \\, | \\, X_t = x )\n\\; \\propto \\;\n\\exp {\\left\\{ -\\frac{(y - \\alpha_s \\, x)^2}{2 \\, \\sigma^2_s} \\right\\}}  \\, dy.\n\\tag{2}\\]\nThis also means that one can directly generate samples from \\(p_t(dx)\\) by first choosing a data samples \\(x_i\\) from the data distribution \\(p_{\\mathrm{data}} \\equiv p_0\\) and blend it with noise by setting \\(x_i^{(t)} = \\alpha_t \\, x_i + \\sigma_t \\, \\mathbf{n}\\). In other words, one does not need to simulate the diffusion to generate samples from the data distribution, i.e. the method is “simulation free”.\n\n\nThe reverse diffusion\nIn order to generate samples from the data distribution, the DDPM strategy consists in sampling from the Gaussian reference measure \\(\\pi_{\\mathrm{ref}}\\) at time \\(t=T\\) and simulate the OU process backward in time. In other words, one would like to simulate from the reverse process \\(\\overleftarrow{X}_t\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIn other words, the reverse process is distributed as \\(\\overleftarrow{X}_0 \\sim \\pi_{\\mathrm{ref}}\\) at time \\(t=0\\) and, crucially, we have that \\(\\overleftarrow{X}_T \\sim \\pi_{\\mathrm{data}}\\). Furthermore, and as explained in this note, the reverse diffusion follows the dynamics\n\\[\nd\\overleftarrow{X}_t = {\\color{red} + }\\frac12 \\overleftarrow{X}_t \\, dt\n\\; {\\color{red} + \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\;\n+ dB\n\\tag{3}\\]\nwhere \\(B\\) is another Wiener process. I have used the notation \\(B\\) to emphasize that there is no link between this Wiener process and the one used to simulate the forward process. Note that if the initial data distribution \\(p_0 \\equiv \\pi_{\\mathrm{data}}\\) were equal to the reference measure \\(\\pi_{\\mathrm{ref}}\\), i.e. \\(p_0 = p_T = \\pi_{\\mathrm{ref}}\\) then it is easy to see that the reverse diffusion would have exactly teh same dynamics as the forward diffusion. In order to simulate the reverse diffusion, one needs to be able to estimate the new term \\({\\color{red}\\nabla \\log p_{T-t}(x)}\\) called the score. If one can estimate the score, it is straightforward to simulate the reverse process \\(\\overleftarrow{X}_t\\) and obtain samples from the data distribution. Estimating the score is the crux of the game.\n\n\nDenoising to estimating the score\nIn practice, the score is unknown and one has to build an approximation of it\n\\[\\mathcal{S}^\\theta(t,x) \\; \\approx \\; \\nabla_x \\log p_t(x).\\]\nThe approximate score \\(\\mathcal{S}^\\theta(t,x)\\) is often parametrized by a neural network with parameters \\(\\theta \\in \\Theta\\). Since\n\\[\n\\log p_t(x) \\; = \\; \\log \\int \\; p(X_t=x \\mid X_0 = x_0)\\; \\pi_{\\mathrm{data}}(d x_0)\n\\]\nand \\(p(X_t=x \\mid X_0 = x_0)\\) is given in Equation 2, algebra gives that\n\\[\n\\nabla_x \\log p_t(x) \\; = \\; -\\frac{x - \\alpha_t \\,  \\textcolor{green}{\\widehat{x}_0(x,t)}}{\\sigma_t^2}\n\\tag{4}\\]\nwhere \\(\\widehat{x}_0(x,t)\\) is a “denoising” estimate of the initial position \\(x_0\\) given a noisy estimate \\(X_t=x\\) at time \\(t\\), i.e.\n\\[\n\\textcolor{green}{ \\widehat{x}_0(x_t,t) \\; = \\; \\mathop{\\mathrm{\\mathbb{E}}}[X_0  \\; \\mid \\; X_t = x_t] }.\n\\]\nNote that this derivation fundamentally relies on the tractability of the transition density of the OU process, although any other linear Gaussian diffusion could have been used. For simplifying notation, I will often write \\(\\widehat{x}_0(x_t, t)\\) as \\(\\widehat{x}_0(x_t)\\) when it is clear that \\(x_t\\) is a sample obtained at time \\(0 \\leq t \\leq T\\). Equation 4 means that to estimate the score, one only needs to train a denoising function \\(\\widehat{x}^{\\theta}_0: [0,T] \\times \\mathbb{R}^d \\to \\mathbb{R}^d\\) that approximate the true conditional expectation,\n\\[\n\\widehat{x}^{\\theta}_0(t,x_t)\n\\; \\approx \\;\n\\widehat{x}_0(t,x_t) = \\mathop{\\mathrm{\\mathbb{E}}}[X_0 \\mid X_t=x_t].\n\\]\nIt is a standard regression problem: take a bunch of pairs \\((X_0, X_t)\\) that can be generated as\n\\[\nX_0 \\sim \\pi_{\\mathrm{data}}\n\\qquad \\textrm{and} \\qquad\nX_t = \\alpha_t X_0 + \\sigma_t \\, \\mathbf{n}\n\\]\nand minimize the Mean Squared Error (MSE) loss, i.e.\n\\[\n\\theta \\mapsto \\mathop{\\mathrm{\\mathbb{E}}}\\|X_0 - \\widehat{x}^{\\theta}_0(t, X_t)\\|^2.\n\\]\nThis can be implemented with a stochastic gradient descent method or any other stochastic optimization procedure. Indeed, this implicitly relies on the standard fact that the expectation of a squared integrable random variable \\(Z\\) is the minimizer of the function \\(\\gamma \\mapsto \\mathop{\\mathrm{\\mathbb{E}}}[(Z-\\gamma)^2]\\). The score is then defined as\n\\[\n\\mathcal{S}^{\\theta}(t,x) \\; = \\; -\\frac{x - \\alpha_t \\, \\widehat{x}^{\\theta}_0(t,x)}{\\sigma^2_t}.\n\\]\nOne of the important take-away is that a denoiser, i.e. a function that estimates the conditional expectation of \\(X_0\\) given \\(X_t=x\\), can be upgraded to a sampler! Furthermore, for a given estimate \\(\\mathcal{S}(t,x_t)\\) of the score function, one can easily obtain bounds on the quality of the resulting approximation of the data distribution. For example, if \\(q^{\\mathcal{S}}\\) denotes the probability distribution on path-space induced by the reverse diffusion with approximate score \\(\\mathcal{S}\\) while \\(p\\) denotes the “true” reverse diffusion, the chain rule for the KL divergence gives\n\\[\n\\mathop{\\mathrm{D_{\\text{KL}}}}(\\pi_{\\mathrm{data}}, q^{\\mathcal{S}}_0) = \\mathop{\\mathrm{D_{\\text{KL}}}}(p_0, q^{\\mathcal{S}}_0)\n\\leq \\mathop{\\mathrm{D_{\\text{KL}}}}(p, q^{\\mathcal{S}}).\n\\]\nFurthermore, as described in these notes, Girsanov’s theorem gives that\n\\[\n\\mathop{\\mathrm{D_{\\text{KL}}}}(p, q^{\\mathcal{S}}) = \\frac12 \\, \\mathop{\\mathrm{\\mathbb{E}}} {\\left\\{ \\int_{0}^T \\|\\mathcal{S}(t,X_t) - \\nabla_x \\log p_t(X_t) \\|^2 \\, dt \\right\\}} .\n\\]\nIf there were a time-dependent volatility \\(\\gamma_t\\) in the dynamics, i.e. \\(dX = \\mu(X) \\, dt + \\gamma_t \\, dW\\) instead of the one in Equation 1, the expression would remain essentially the same, with the only difference that the error term \\(\\|\\mathcal{S}(t,X_t) - \\nabla_x \\log p_t(X_t) \\|^2\\) would be scaled by a factor \\(\\gamma^2_t\\). Indeed, this means that when the approximation of the score is perfect, i.e. \\(\\mathcal{S}(t,x_t) = \\nabla_x \\log p_t(x_t)\\), the data distribution is perfectly recovered, as expected. When expressed in terms of the denoiser \\(\\widehat{x}\\), the upper-bound on \\(\\mathop{\\mathrm{D_{\\text{KL}}}}(\\pi_{\\mathrm{data}}, q^{\\mathcal{S}}_0)\\) reads\n\\[\n\\frac12 \\, \\mathop{\\mathrm{\\mathbb{E}}} {\\left\\{ \\int_{0}^T  {\\left( \\frac{\\alpha_t}{\\sigma_t^2} \\right)} ^2 \\, \\|\\widehat{x}^{\\theta}_0(t,X_t) - \\mathop{\\mathrm{\\mathbb{E}}}[X_0 \\mid X_t] \\|^2 \\, dt \\right\\}} .\n\\]\nSince \\((\\alpha_t / \\sigma_t) \\to \\infty\\) as \\(t \\to 0\\), this shows that it is probably a good idea to approximate denoiser \\(\\widehat{x}_0(t,x_t)\\) much better for small times \\(t \\to 0\\) than for large times \\(t \\to T\\).\n\n\nDenoiser: practical parametrization\nIn practice, it may not be efficient, or stable, to try to directly parametrize the denoiser \\(\\widehat{x}^\\theta_0(t, x_t)\\) with a neural network and simply minimize the loss\n\\[\n\\mathop{\\mathrm{\\mathbb{E}}}\\|X_0 - \\widehat{x}^{\\theta}_0(t, X_t)\\|^2.\n\\tag{5}\\]\nFor example, for \\(t \\approx 0\\), we have that \\(\\widehat{x}_0(t,X_t) \\approx X_t \\approx X_0\\) so that it is very easy to reconstruct \\(X_0\\) from \\(X_t\\). On the contrary, for large \\(t\\), there is almost no information contained within \\(X_t\\) to reconstruct \\(X_0\\). This means that the typical value of the naive loss Equation 5 depends widely on \\(t\\), which makes it difficult to optimize: with this parametrization, since large values of the loss will be typically concentrated to large values of \\(t\\), the denoiser will not be accurate for \\(t \\approx 0\\), which is exactly the opposite than what is required, as discussed at the end of the previous section. Since \\(x_t = \\alpha_t \\, x_0 + \\sigma_t \\, \\mathbf{n}\\), one can defined the Signal-to-Noise-Ratio as\n\\[\\mathrm{SNR}(t) = \\frac{\\alpha_t}{\\sigma_t}.\\]\nIn order to normalize by the reconstruction difficulty, it makes more sense to train a denoiser by minimizing the loss:\n\\[\n\\mathop{\\mathrm{\\mathbb{E}}} {\\left[  \\mathrm{SNR}^2(t) \\times \\|X_0 - \\widehat{x}^{\\theta}_0(t, X_t)\\|^2  \\right]} .\n\\]\nIt turns out that it is entirely equivalent to minimizing the loss\n\\[\n\\mathop{\\mathrm{\\mathbb{E}}} {\\left[  \\| \\mathbf{n}- \\widehat{\\mathbf{n}}^{\\theta}(t, X_t)\\|^2  \\right]} .\n\\]\nwhere \\(x_t = \\alpha_t \\, x_0 + \\sigma_t \\, \\mathbf{n}\\) while the denoiser \\(\\widehat{x}^{\\theta}_0\\) and noise estimator \\(\\widehat{\\mathbf{n}}^{\\theta}\\) are parametrized so that\n\\[\nx_t = \\alpha_t \\, \\widehat{x}^{\\theta}_0(t, x_t) + \\sigma_t \\, \\widehat{\\mathbf{n}}^{\\theta}(t,x_t).\n\\]\nThat is one of the reasons why most of the papers on DDPM are parametrizing the denoiser \\(\\widehat{x}_0\\) by building instead a “noise estimator” \\(\\widehat{\\mathbf{n}}^{\\theta}\\) with a neural network and setting\n\\[\n\\widehat{x}^{\\theta}_0(t,x_t) = \\frac{x_t - \\sigma_t \\, \\widehat{\\mathbf{n}}^{\\theta}(t,x_T)}{\\alpha_t}.\n\\tag{6}\\]\nSince \\(\\alpha_t \\to 1\\) and \\(\\sigma_t \\to 0\\) for \\(t \\to 0\\), this also implicitly ensures that \\(\\widehat{x}_0(t,x_t) \\approx x_t\\) for \\(t \\to 0\\), as required. With the parametrization Equation 6, the score is given by\n\\[\n\\mathcal{S}^{\\theta}(t,x_t) \\, = \\, -\\frac{\\widehat{\\mathbf{n}}^{\\theta}(t,x_t)}{\\sigma_t},\n\\]\nwhich can still lead to instability issues for small times \\(t \\to 0\\), although to a lesser extent. It is unlikely that there is an easy way to avoid these issues since, as soon as the data distribution is supported on a (neighbourhood of) lower dimensional manifold, the actual score function indeed does become very large for \\(t \\to 0^+\\) since a strong “drift” is required to bring the diffusion back to the manifold.\n\n\nThe “denoising” diffusion\nOnce the denoiser \\(\\widehat{x}_0(\\ldots)\\) trained, the reverse diffusion defined \\(\\overleftarrow{X}_s = X_{T-s}\\) has to be simulated. Plugging Equation 4 back in the expression of the dynamics of the reverse diffusion shows that\n\\[\nd\\overleftarrow{X}_s =\n-\\frac12 \\, \\frac{1}{\\tanh((T-s)/2)} \\,\n{\\left(  \\overleftarrow{X}_s - \\frac{\\widehat{x}_0(\\overleftarrow{X}_s)}{\\cosh((T-s)/2)}  \\right)}\n\\; + \\;\ndB\n\\tag{7}\\]\nThis dynamics is intuitive: as \\(s \\to T\\) we have \\(\\cosh((T-s)/2) \\to 1\\) and \\(\\varepsilon^2 \\equiv \\tanh((T-s)/2) \\sim (T-s)/2 \\to 0\\) so that the dynamics is similar to\n\\[\nd\\overleftarrow{X}_s =\n-\\frac12 \\, \\frac{1}{\\varepsilon^2} \\,\n{\\left(  \\overleftarrow{X}_s - \\widehat{x}_0 \\right)}  + dB.\n\\]\nThat is an Ornstein-Uhlenbeck process that converges quickly, i.e. on time-scale of order \\(\\mathcal{O}(\\varepsilon^2)\\), towards a Gaussian distribution centred at \\(\\widehat{x}_0\\) and with variance \\(\\varepsilon^2\\).\nTo discretize the reverse dynamics Equation 7 on a small interval \\([\\overline{s}, \\overline{s}+\\delta]\\), one can for example consider the slightly simplified (linear) dynamics\n\\[\nd\\overleftarrow{X}_s =\n-\\frac12 \\, \\frac{1}{\\tanh((T-s)/2)} \\,\n{\\left(  \\overleftarrow{X}_s - \\mu \\right)}  + dB.\n\\]\nHere, \\(\\mu = \\widehat{x}_0(\\overleftarrow{X}_{\\overline{s}}) / \\cosh((T-\\overline{s})/2)\\) with \\(\\overleftarrow{X}_{\\overline{s}} = \\overleftarrow{x}_{\\overline{s}}\\). Algebra gives that, conditioned upon \\(\\overleftarrow{X}_{\\overline{s}} = \\overleftarrow{x}_{\\overline{s}}\\), we have\n\\[\n\\left\\{\n\\begin{aligned}\n\\mathop{\\mathrm{\\mathbb{E}}}[ \\overleftarrow{X}_{\\overline{s} + \\delta} ]\n\\quad &= \\quad \\mu + \\lambda \\, (\\overleftarrow{x}_{\\overline{s}} - \\mu)\\\\\n\\mathop{\\mathrm{Var}}[ \\overleftarrow{X}_{\\overline{s} + \\delta} ]\n\\quad &= \\quad\n\\tanh {\\left( \\frac{T-\\overline{s}-\\delta}{2} \\right)}  \\, (1-\\lambda^2)\n\\end{aligned}\n\\right.\n\\]\nwhere the coefficient \\(0&lt;\\lambda&lt;1\\) is given by\n\\[\n\\lambda = \\frac{\\sinh(T-\\overline{s}-\\delta)}{\\sinh(T-\\overline{s})}.\n\\]\nThis discretization is more stable and efficient than a naive Euler-Maruyama approach since the drift term can get large as \\(s \\to T\\). WIth the above discretization, one can easily simulate the reverse diffusion on \\([0,T]\\) and generate approximate samples from \\(\\pi_{\\mathrm{data}}\\).\nIn the animation below, the method described in these notes was used. This very small example trains in less than a minute on an old laptop without a GPU. The noise estimator was parametrized with an MLP with \\(\\mathrm{elu}(\\ldots)\\) non-linearity and two hidden-layers with size \\(H=128\\). It was very useful to add a few Fourier features as input of the network, I could not make this work properly without.\n\n\nVideo\n\n\nThe literature on DDPM is enormous and still growing!"
  },
  {
    "objectID": "notes/Poisson_Eq_Asymp_Var/Poisson_Eq_Asymp_Var.html",
    "href": "notes/Poisson_Eq_Asymp_Var/Poisson_Eq_Asymp_Var.html",
    "title": "Asymptotic variance & Poisson Equation",
    "section": "",
    "text": "Consider a continuous time Markov process \\(X_t\\) on \\(\\mathbb{R}^D\\) that is ergodic with respect to the probability distribution \\(\\pi(dx)\\). A Langevin diffusion is a typical example. Call \\(\\mathcal{L}\\) the generator of this process so that for a test function \\(\\varphi: \\mathbb{R}^D \\to \\mathbb{R}\\) we have\n\\[\n\\varphi(X_t) = \\varphi(X_0) + \\int_{s=0}^t \\mathcal{L}\\varphi(X_s) \\, ds + \\textrm{($M_t \\equiv$ martingale)}.\n\\tag{1}\\]\nNow, assume further that \\(\\mathop{\\mathrm{\\mathbb{E}}}_{\\pi}[\\varphi(X)] = 0\\) and that a Central Limit Theorem holds,\n\\[\n\\frac{1}{\\sqrt{T}} \\int_{s=0}^T \\varphi(X_s) \\, ds \\; \\to \\; \\mathcal{N}(0, \\sigma^2).\n\\tag{2}\\]\nHow can one estimate the asymptotic variance \\(\\sigma^2\\)?\n\nApproach I: Integrated autocovariance\nOne can directly try to compute the second moment of Equation 2 and obtain that\n\\[\n\\sigma^2 \\; = \\; \\lim_{T \\to \\infty} \\;\n\\frac{1}{T} \\, \\iint_{0 \\leq s,t \\leq T} \\mathop{\\mathrm{\\mathbb{E}}}[\\varphi(X_s) \\varphi(X_t)] \\, ds \\, dt\n\\]\nSince \\(\\mathop{\\mathrm{\\mathbb{E}}}[\\varphi(X_s) \\varphi(X_t)]\\) falls quickly to zero as \\(|s-t| \\to 0\\) and defining the auto-covariance at lag \\(r &gt; 0\\) as\n\\[\nC(r) = \\mathop{\\mathrm{\\mathbb{E}}}[\\varphi(X_t) \\varphi(X_{t+r})],\n\\]\none obtains that an expression of the asymptotic as the integrated autocovariance function,\n\\[\n\\sigma^2 \\; = \\; 2 \\, \\int_{r=0}^\\infty C(r) \\, dr.\n\\tag{3}\\]\nIn the MCMC literature, this relation is often expressed as\n\\[\n\\sigma^2 \\; = \\; \\mathop{\\mathrm{Var}}_{\\pi}[\\varphi] \\, \\times \\, \\textrm{(IACT)}\n\\]\nwhere the integrated autocorrelation function is defined as\n\\[\n\\textrm{(IACT)} = 2 \\, \\int_{r=0}^\\infty \\rho(r) \\, dr.\n\\]\nfor autocorrelation at lag \\(r\\geq 0\\) defined as \\(\\rho(r) \\equiv \\mathop{\\mathrm{Corr}}[\\varphi(X_t), \\varphi(X_{t+r})]\\). The slower the autocorrelation function \\(\\rho(r)\\) falls to zero as \\(r \\to \\infty\\), the larger the asymptotic variance \\(\\sigma^2\\). Although Equation 3 is very intuitive, it can be difficult to estimate the autocorrelation function.\n\n\nApproach II: Poisson Equation\nUnder relatively general and mild conditions, since the expectation of \\(\\varphi\\) under the invariant distribution \\(\\pi\\) is zero and the Markov process is ergodic with respect to \\(\\pi\\), there exists a function \\(\\Phi: \\mathbb{R}^D \\to \\mathbb{R}\\) such that\n\\[\n\\mathcal{L}\\Phi \\; = \\; \\varphi.\n\\tag{4}\\]\nEquation 4 is called a Poisson Equation since \\(\\mathcal{L}\\) is often a Laplacian-like operator (eg. diffusion-type processes). Equation 1 gives that\n\\[\n\\frac{1}{\\sqrt{T}} \\int_{s=0}^T \\varphi(X_s) \\, ds\n\\; = \\;\n\\frac{M_T}{\\sqrt{T}} +  {\\left\\{  \\frac{\\Phi(X_T) - \\Phi(X_0)}{\\sqrt{T}}  \\right\\}}\n\\]\nwhere \\(M_T\\) is the martingale and \\([\\Phi(X_T) - \\Phi(X_0)]/\\sqrt{T}\\) typically vanishes as \\(T \\to \\infty\\) and can be neglected. For computing the asymptotic variance, it suffices to estimate \\(\\mathop{\\mathrm{\\mathbb{E}}}(M_T^2)\\). And using the martingale property, it equals \\(\\int_{s=0}^T \\mathop{\\mathrm{\\mathbb{E}}}(dM_t)^2\\). Also, since \\(M_t = \\varphi(X_t) - \\varphi(X_0) - \\int_{s=0}^t \\mathcal{L}\\varphi(X_s) \\, ds\\), algebra gives that\n\\[\n\\frac{1}{\\varepsilon} \\, \\mathop{\\mathrm{\\mathbb{E}}} {\\left[  (M_{t+\\varepsilon} - M_t)^2  \\right]}  \\approx 2 \\mathop{\\mathrm{\\mathbb{E}}} {\\left[  (\\Gamma \\Phi)(X_t)  \\right]}\n\\]\nwhere the so-called carré du champ \\((\\Gamma \\Phi)\\) is defined as\n\\[\n2 \\, (\\Gamma \\Phi)(X_t)\n\\; = \\;  {\\left(  \\mathcal{L}(\\Phi^2) - 2 \\Phi \\mathcal{L}\\Phi \\right)} (X_t)\n\\; = \\; \\lim_{\\varepsilon\\to 0} \\; \\frac{1}{\\varepsilon} \\mathop{\\mathrm{Var}}(\\Phi(X_{t+\\varepsilon}) \\, | \\, X_t).\n\\]\nThis shows that the asymptotic variance satisfies\n\\[\n\\sigma^2\n\\; = \\;\n\\lim_{T \\to \\infty} \\frac{2}{T} \\int_{s=0}^T \\Gamma \\Phi(X_s) \\, ds\n\\; = \\;\n2 \\, \\int_{x \\in \\mathbb{R}^D} \\Gamma \\Phi(x) \\, \\pi(dx).\n\\]\nFinally, since \\(\\int (\\mathcal{L}\\Phi^2)(x) \\, \\pi(dx) = 0\\), this can equivalently be written as\n\\[\n\\sigma^2\n\\; = \\;\n-2 \\, \\int_{x \\in \\mathbb{R}^D} \\Phi(x) \\, \\mathcal{L}\\Phi(x) \\, \\pi(dx)\n\\; = \\; 2 \\, \\mathcal{D}(\\Phi)\n\\tag{5}\\]\nwhere \\(\\mathcal{D}(\\Phi)\\) is the so-called Dirichlet form. In summary, we have just shown that the asymptotic variance of the additive functional \\(T^{-1/2} \\, \\int_0^T \\varphi(X_s) \\, ds\\) is given by two times the Dirichlet form \\(\\mathcal{D}(\\Psi)\\) where \\(\\Phi\\) is solution to the Poisson equation \\(\\mathcal{L}\\Phi = \\varphi\\). Note that this implies that the generator \\(\\Phi\\) is a negative operator in the sense that for a test function \\(\\Phi\\) we have that\n\\[\n\\left&lt; \\Phi, \\mathcal{L}\\Phi \\right&gt;_{\\pi} \\; \\leq \\; 0\n\\]\nwhere we have used the dot-product notation \\(\\left&lt; f,g \\right&gt;_{\\pi} = \\int f(x) g(x) \\pi(dx)\\).\n\n\nPoisson equation: Integral representation\nIt is often useful to think of the generator \\(\\mathcal{L}\\) as an infinite dimensional equivalent of a standard negative definite symmetric matrix/operator \\(M \\in \\mathbb{R}^{n,n}\\). And since \\(M^{-1} = -\\int_{t=0}^{\\infty} \\exp(tM) \\, dt\\), as can be seen by diagonalizing \\(M\\), one can expect the following equation to hold,\n\\[\n\\mathcal{L}^{-1} \\; = \\; -\\int_{t=0}^{\\infty} e^{t \\, \\mathcal{L}} \\, dt.\n\\]\nThat is just another way of writing that the solution \\(\\Phi\\) to the Poisson equation \\(\\mathcal{L}\\Phi = \\varphi\\), with the centering condition \\(\\mathop{\\mathrm{\\mathbb{E}}}_{\\pi}[\\Phi(X)]=0\\) for picking one solution out of the many possible solutions to the Poisson equation differing from each other by an additive constant, can be expressed as\n\\[\n\\Phi(x) \\, = \\, -\\int_{t=0}^{\\infty} \\mathop{\\mathrm{\\mathbb{E}}}[\\varphi(X_t)|X_0=x] \\, dt.\n\\tag{6}\\]\nEquation 6 is easily proved with Equation 1 by writing\n\\[\n\\Phi(x)-\\Phi(X_T) = -\\int_{t=0}^\\infty \\varphi(X_t) \\, dt + \\textrm{(martingale)}\n\\]\nand by taking expectation from both sides and noticing that \\(\\mathop{\\mathrm{\\mathbb{E}}}[\\Phi(X_T)] \\to 0\\) thanks to the assumed centering condition \\(\\mathop{\\mathrm{\\mathbb{E}}}_{\\pi}[\\Phi(X)]=0\\). Note that this remarks allows to give another derivation of Equation 5 starting from the integrated autocovariance formulation Equation 3. Indeed, note that\n\\[\n\\begin{align}\n\\sigma^2 &= 2 \\, \\int_{r=0}^{\\infty} C(r) \\, dr\\\\\n&=\n2 \\, \\int_{x \\in \\mathbb{R}^D} \\varphi(x)  {\\left\\{  \\int_{r=0}^{\\infty}  \\mathop{\\mathrm{\\mathbb{E}}}[\\varphi(X_t) | X_0=x] \\, dr  \\right\\}}  \\, \\pi(dx)\\\\\n&=\n-2 \\, \\int_{x \\in \\mathbb{R}^D} \\varphi(x) \\Phi(x) \\, \\pi(dx)\n=\n-2 \\left&lt; \\Phi, \\mathcal{L}\\Phi \\right&gt;_{\\pi}.\n\\end{align}\n\\]\n\n\nExample: OU process\nConsider a OU process that is ergodic with respect to the standard Gaussian density \\(\\pi(x) = e^{-x^2/2} / \\sqrt{2\\pi}\\),\n\\[\ndX\n\\; = \\;\n-\\varepsilon^{-1}X \\, dt + \\sqrt{2 \\, \\varepsilon^{-1}} \\, dW.\n\\]\nThat’s a standard OU process accelerated by a factor \\(\\varepsilon^{-1} &gt; 0\\). Its generator reads\n\\[\n\\mathcal{L}\\, = \\, \\varepsilon^{-1} [-x \\, \\partial_x + \\partial_{xx}].\n\\]\nThe function \\(\\varphi(x)=x\\) is such that \\(\\pi(\\varphi)=0\\) and a solution to the Poisson equation \\(\\mathcal{L}\\Phi = \\varphi\\) is \\(\\Phi(x) = -\\varepsilon\\, x\\). This shows that the asymptotic variance is\n\\[\n\\sigma^2\n\\; = \\;\n2 \\, \\int_{x \\in \\mathbb{R}^D} \\varepsilon x^2 \\, \\pi(dx) \\; = \\; 2 \\varepsilon.\n\\]\nAs expected, accelerating the OU process by a factor \\(\\varepsilon^{-1}\\) means reducing the variance by a factor \\(\\varepsilon\\)."
  },
  {
    "objectID": "notes/information_theory_references/information_theory_references.html",
    "href": "notes/information_theory_references/information_theory_references.html",
    "title": "Information Theory: References and Readings",
    "section": "",
    "text": "Books\n\n“Elements of information theory” by T. M. Cover and J. A. Thomas – perfect intro book to the topic.\n“Information Theory, Inference, and Learning Algorithms” by David J.C. MacKay\n“Information Theory From Coding to Learning” by Yury Polyanskiy and Yihong Wu\n\n\n\nLecture Notes & Articles\n\n“A Mathematical Theory of Communication” by C. Shannon (2948) – entertaining and readable, even 70+ years later!\n“Lecture Notes on Statistics and Information Theory” by John Duchi\n“Information-theoretic methods for high-dimensional statistics” by Yihong Wu"
  },
  {
    "objectID": "notes/auxiliary_variable_trick/auxiliary_variable_trick.html",
    "href": "notes/auxiliary_variable_trick/auxiliary_variable_trick.html",
    "title": "Auxiliary variable trick",
    "section": "",
    "text": "Consider a complicated distribution on the state space \\(x \\in \\mathcal{X}\\) given by\n\\[\n\\pi(x) = \\frac{1}{\\mathcal{Z}} \\, e^{C(x) + B(x)}\n\\]\nfor a “complicated” functions \\(C(x)\\) and a simpler one \\(B(x)\\). In some situations, it is possible to introduce an auxiliary random variable \\(a \\in \\mathcal{A}\\) and an extended probability distribution \\(\\pi(x,a)\\) on the extended space \\(\\mathcal{X}\\times \\mathcal{A}\\),\n\\[\n\\pi(x,a) = \\pi(x) \\,  \\textcolor{red}{\\pi(a | x)} = \\frac{1}{\\mathcal{Z}} e^{C(x) + B(x)} \\,  \\textcolor{red}{e^{-C(x) + D(x, a)}},\n\\]\nwith a tractable conditional probability \\(\\pi(a | x)\\). This extended target distribution \\(\\pi(x,a) = (1/\\mathcal{Z}) \\, \\exp[B(x) + D(x,a)]\\) can be often be easier to explore, for example when \\(a\\) is continuous while \\(x\\) is discrete, or to analyze, since the “complicated” term \\(C(x)\\) has disappeared. Furthermore, there are a number of scenarios when the variable \\(x\\) can be averaged out of the extended distribution, i.e. the distribution\n\\[\n\\pi(a) = \\frac{1}{\\mathcal{Z}} \\, \\int_{x \\in \\mathcal{X}} e^{B(x) + D(x,a)}\n\\]\ncan be evaluated exactly.\n\nSwendsen–Wang algorithm\nConsider a set of edges \\(\\mathcal{E}\\) on a graph with vertices \\(\\{1, \\ldots, N\\}\\). The Ising model is defined as \\[\n\\pi(x) \\propto \\exp  {\\left\\{  \\sum_{(i,j) \\in \\mathcal{E}} \\beta x_i x_j  \\right\\}}\n\\]\nfor spin configurations \\(x=(x_1, \\ldots, x_N) \\in \\{-1,1\\}^N\\). The term \\(\\exp[\\beta x_i x_j]\\) couples the two spins \\(x_i\\) and \\(x_j\\) for each edge \\((i,j) \\in \\mathcal{E}\\). The idea of the Swendsen–Wang_algorithm is to introduce an auxiliary variable \\(u_{i,j}\\) for each edge \\((i,j) \\in \\mathcal{E}\\) that is uniformly distributed on the interval \\([0, \\exp(\\beta x_i x_j)]\\), i.e.\n\\[\n\\pi(u_{i,j} | x) \\; = \\; \\frac{ \\mathbf{1}  {\\left\\{  0 &lt; u_{i,j} &lt; \\exp[\\beta x_i x_j]  \\right\\}}  }{\\exp[\\beta x_i x_j] }\n\\]\nIt follows that the extended distribution on \\(\\{-1,1\\}^N \\times (0,\\infty)^{|\\mathcal{E}|}\\) reads\n\\[\n\\pi(x,u) = \\frac{1}{Z} \\prod_{(i,j) \\in \\mathcal{E}} \\; \\mathbf{1}  {\\left\\{  0 &lt; u_{i,j} &lt; \\exp[\\beta x_i x_j]  \\right\\}}\n\\]\nfor \\(x=(x_1, \\ldots, x_N)\\) and \\(u = (u_{i,j})_{(i,j) \\in \\mathcal{E}}\\): the coupling term \\(\\exp[\\beta x_i x_j]\\) has disappeared. Furthermore, it is straightforward to sample from the conditional distribution \\(\\pi(u | x)\\) and, perhaps surprisingly, it is also relatively straightforward to sample from the other conditional distribution \\(\\pi(x | u)\\) – this boils down to finding the connect components of the graph on \\(\\{1, \\ldots, N\\}\\) with an edge \\(i \\sim j\\) present if \\(u_{i,j} &gt; e^{-\\beta}\\) and flipping a fair coin for setting each connected component to \\(\\pm 1\\). This leads to an efficient Gibbs sampling scheme to sample from the extended distribution.\n\n\n\n\nSwendsen-Wang MCMC algorithm at critical temperature\n\n\n\n\n\nGaussian Integral trick: Curie-Weiss model\nFor an inverse temperature \\(\\beta &gt; 0\\), consider the distribution on \\(x \\in \\{-1,1\\}^N\\)\n\\[\n\\pi(x) = \\frac{1}{\\mathcal{Z}(\\beta)} \\, e^{\\beta \\, N \\, m^2}\n\\]\nwhere the magnetization of the system of spins \\(x=(x_1, \\ldots, x_N)\\) is defined as\n\\[\nm = \\frac{x_1 + \\ldots + x_N}{N}.\n\\]\nThe distribution \\(\\pi(x)\\) for \\(\\beta \\gg 1\\) favours configurations with a magnetization close to \\(+1\\) or \\(-1\\). The normalization constant (i.e. partition function) \\(\\mathcal{Z}(\\beta)\\) is a sum of \\(2^N\\) terms,\n\\[\n\\mathcal{Z}(\\beta) = \\sum_{s_1 \\in \\{ \\pm 1\\} } \\ldots \\sum_{s_N \\in \\{ \\pm 1\\} } \\exp  {\\left\\{  \\frac{\\beta}{N}  {\\left(  \\sum_{i=1}^N x_i  \\right)} ^2 \\right\\}} .\n\\]\nIt is not difficult to estimate \\(\\log \\mathcal{Z}(\\beta)\\) as \\(N \\to \\infty\\) with combinatorial arguments. Nevertheless, another way to proceed is as follows. One can introduce a Gaussian auxiliary random variable \\(\\pi(a | x) = \\mathcal{N}(\\alpha \\,  {\\left( \\sum_i x_i \\right)}  , \\sigma^2)\\) with mean \\(\\mu = \\alpha \\,  {\\left( \\sum_i x_i \\right)} \\) and variance \\(\\sigma^2\\): the parameters \\(\\alpha\\) and \\(\\sigma^2 &gt; 0\\) can then be judiciously chosen to cancel the bothering term \\(\\exp[\\frac{\\beta}{N} \\, m^2]\\). This approach is often called the a Hubbard-Stratonovich transformation. The bothering “coupling” term disappears when when choosing \\(\\frac{\\alpha^2}{2 \\sigma^2} = \\frac{\\beta}{N}\\). With such a choice, it follows that\n\\[\n\\pi(x, a) = \\frac{1}{\\mathcal{Z}(\\beta)} \\, \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp {\\left\\{  - \\frac{a^2}{2 \\sigma^2} + \\frac{\\alpha}{\\sigma^2} a \\,  {\\left( \\sum_i x_i \\right)}  \\right\\}} .\n\\]\nAveraging out the \\(x_i \\in \\{-1, +1\\}\\) gives that the partition function reads\n\\[\n\\mathcal{Z}(\\beta) \\; = \\;\n\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\,\n\\int_{a = -\\infty}^{a=\\infty} \\,\n\\exp {\\left\\{ -\\frac{a^2}{2 \\sigma^2} +  \\textcolor{red}{N} \\, \\log[ 2 \\, \\cosh(\\alpha a / \\sigma^2)] \\right\\}} .\n\\]\nIn order to use the method of steepest descent, it would be useful to have an integrand of the type \\(\\exp[N \\times (\\ldots)]\\). One can choose \\(1/(2 \\, \\sigma^2) = \\beta \\, N\\) and \\(\\alpha = 1/N\\). This gives\n\\[\n\\mathcal{Z}(\\beta) \\; = \\;\n\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\,\n\\int_{a = -\\infty}^{a=\\infty} \\,\n\\exp  {\\left\\{   \\textcolor{red}{N}  {\\left[  -\\beta \\, a^2 + \\log[ 2 \\, \\cosh(2 \\beta a )] \\right]}  \\right\\}}  \\, da\n\\]\nfrom which one directly obtains that:\n\\[\n\\lim_{N \\to \\infty} \\; -\\frac{\\log \\mathcal{Z}(\\beta)}{N}\n\\; = \\;\n\\min_{a \\in \\mathbb{R}} \\; \\Big\\{ \\beta a^2 - \\log[2 \\, \\cosh(2 \\beta a)] \\Big\\}.\n\\]\n\n\nSherrington–Kirkpatrick model\nConsider the distribution on \\(x \\in \\{-1,1\\}^N\\)\n\\[\n\\pi(x) = \\frac{1}{\\mathcal{Z}(\\beta)} \\,\n\\exp {\\left\\{ \\frac 12 \\, \\sum_{i,j} W_{ij} x_i x_j \\right\\}}\n=\n\\frac{1}{\\mathcal{Z}(\\beta)} \\,\n\\exp {\\left\\{ \\frac 12 \\, \\left&lt; x, W x \\right&gt; \\right\\}}\n\\]\nwhere the \\(w_{ij}\\) are some fixed weights with \\(w_{ij} = w_{ji}\\). We assume that the matrix \\(W = [W_{ij}]_{ij}\\) is positive definite: this can be achieved by adding \\(\\lambda \\, I_N\\) to it if necessary, which does not change the distribution \\(\\pi\\). As described in (Zhang et al. 2012), although I would not be surprised if similar ideas appeared in the physics literature much earlier, one can introduce a Gaussian auxiliary random variable \\(a = (a_1, \\ldots, a_N)\\) so that \\(\\pi(a | x)\\) has mean \\(Fx\\) and covariance \\(\\Gamma\\). In other words,\n\\[\n\\pi(a | x) = \\frac{1}{(2 \\pi)^{d/2} \\, |\\Gamma|^{1/2}}\n\\, \\exp {\\left\\{ -\\frac 12 \\left&lt; (a - Fx), \\Gamma^{-1} (a - Fx) \\right&gt; \\right\\}} .\n\\]\nIn order to cancel-out the \\(\\left&lt; x, W, x \\right&gt;\\) it suffices to make sure that \\(F^\\top \\, \\Gamma^{-1} \\, F = W\\). There are a number of possibilities, the simplest approaches being perhaps\n\\[\n(F,\\Gamma) = (W^{1/2}, I_N)\n\\quad \\text{or} \\quad\n(F,\\Gamma) = (I, W^{-1})\n\\quad \\text{or} \\quad\n(F,\\Gamma) = (W, W).\n\\]\nIn any case, the joint distribution reads\n\\[\n\\pi(x,a) \\; = \\; \\frac{1}{\\mathcal{Z}} \\, \\frac{1}{(2 \\pi)^{d/2} \\, |\\Gamma|^{1/2}} \\,\n\\exp {\\left\\{ -\\frac{1}{2} \\left&lt; a, \\Gamma^{-1} a \\right&gt; + \\left&lt; x, F^\\top \\Gamma^{-1} \\, a \\right&gt; \\right\\}} .\n\\]\nIndeed, one can try to implement some Gibbs-style update in order to explain this joint distribution since both \\(\\pi(x|a)\\) and \\(\\pi(a|x)\\) are straightforward to sample from: it is indeed related Restricted Boltzmann Machine models. One can also average-out the spins \\(x_i \\in \\{-1,1\\}\\) and obtain that\n\\[\n\\mathcal{Z}= \\int_{\\mathbb{R}^N}\n\\exp {\\left\\{  \\sum_{i=1}^N \\log {\\left( 2 \\, \\cosh([F^\\top \\Gamma^{-1} \\, a]_i) \\right)}  \\right\\}}  \\, \\mathcal{D}_{\\Gamma}(da)\n\\]\nwhere \\(\\mathcal{D}_{\\Gamma}\\) is the density of a centred Gaussian distribution with covariance \\(\\Gamma\\). [TODO: add SMC experiments to estimate \\(\\mathcal{Z}\\)].\n\n\n\n\n\nReferences\n\nZhang, Yichuan, Zoubin Ghahramani, Amos J Storkey, and Charles Sutton. 2012. “Continuous Relaxations for Discrete Hamiltonian Monte Carlo.” Advances in Neural Information Processing Systems 25."
  },
  {
    "objectID": "teaching/teaching.html",
    "href": "teaching/teaching.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "YSC4216: Machine Learning, Yale-NUS (2024)\nDSA4212: Large Scale optimization, NUS (2017 – 2024)\n\nList of possible projects\n\nYSC4236: Optimization for Large Scale Inference, Yale-NUS, (2022)\nYSC3252: Computational Statistics, Yale-NUS, (2022)\nDSA5843: Learning from Data: Neural Networks, NUS (2020–2021)\nST3233: Applied Times Series, NUS, (2016, 2019, 2021)\nST4240: Data Mining, NUS, (2014–2017)\nST4231: Computationally Intensive Statistics, NUS (2014–2015)\nMultivariable Calculus II, Chennai (CMI), (2018)"
  },
  {
    "objectID": "teaching/teaching.html#semester-long-courses",
    "href": "teaching/teaching.html#semester-long-courses",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "YSC4216: Machine Learning, Yale-NUS (2024)\nDSA4212: Large Scale optimization, NUS (2017 – 2024)\n\nList of possible projects\n\nYSC4236: Optimization for Large Scale Inference, Yale-NUS, (2022)\nYSC3252: Computational Statistics, Yale-NUS, (2022)\nDSA5843: Learning from Data: Neural Networks, NUS (2020–2021)\nST3233: Applied Times Series, NUS, (2016, 2019, 2021)\nST4240: Data Mining, NUS, (2014–2017)\nST4231: Computationally Intensive Statistics, NUS (2014–2015)\nMultivariable Calculus II, Chennai (CMI), (2018)"
  },
  {
    "objectID": "teaching/teaching.html#lecture-series",
    "href": "teaching/teaching.html#lecture-series",
    "title": "Alexandre Thiéry",
    "section": "Lecture Series:",
    "text": "Lecture Series:\n\nSummer school on Bayesian statistics (5h Lecture), VIASM July 2023"
  },
  {
    "objectID": "teaching/teaching.html#notes",
    "href": "teaching/teaching.html#notes",
    "title": "Alexandre Thiéry",
    "section": "Notes:",
    "text": "Notes:\n\nProbability Basics"
  },
  {
    "objectID": "teaching/DSA4212/assignments.html",
    "href": "teaching/DSA4212/assignments.html",
    "title": "DSA4212 Assignments",
    "section": "",
    "text": "Traveling Salesman Problem [80%]\nGoal: The Traveling Salesman Problem (TSP) is a classic problem in combinatorial optimization. Given a list of cities and the distances between each pair of cities, the problem is to find the shortest possible route that visits each city exactly once and returns to the origin city. You are tasked with implementing a few methods to solve the TSP and compare their performance. Your report should not include any actual code. Instead, it should describe the problem and provide a discussion of the results. You are encouraged to compare your results with the best known solutions.\nPossible References:\n\nWikipedia\nTSPLIB\n\n\n\n\nThompson problem [80%]\nGoal: The Thompson problem is a classic problem in computational geometry. Consider N = 300 points \\(P_1, P_2, \\ldots, P_N \\in \\mathbb{R}^3\\) on the unit sphere: for \\(1 \\leq i \\leq N\\), we have \\(\\|P_i\\| = 1\\). How should these points be placed so that the quantity\n\\[\nE = \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\frac{1}{\\|P_i - P_j\\|}\n\\]\nis minimized. Note that it is the norm, and not the norm squared, in the denominator. Your report should not include any actual code. Instead, it should describe the problem and provide a discussion of the results. You are encouraged to compare your results with the best known solutions.\nPossible References:\n\nWikipedia\n\n\n\n\nImage classification with convolutional neural networks [80%]\nGoal: Implement a simple convolutional neural network in Python and use it to classify images from the CIFAR-10 dataset. For this purpose, you will implement a small CNN on a GPU (eg. Google Colab) – the actual details of the CNN is not the emphasis of this assignment and you should not spend too much time optimizing the architecture. Instead, you will study:\n\nthe influence of the choice of optimizer\n\nthe effect of the learning rate on the speed of convergence and the final test accuracy\n\nwhether using a learning rate schedule improves performance\n\nthe effect of the batch size on the speed of convergence and the final test accuracy\n\nYour report should not include any actual code. Instead, it should describe your experiments and their results, and provide a discussion of the findings.\nPossible References:\n\nSuper-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates\n\nHow to train your ResNet\nDeep Learning Tuning Playbook\n\n\n\n\nAutomatic differentiation [90%]\nGoal: Implement a simple version of automatic differentiation in Python. Your implementation should be able to easily compute the gradient of the composition of any number of (simple) multivariate functions. While your report should not include any actual code, it should describe the mechanics of your implementation and provide examples of its use.\nPossible References:\n\nAutomatic Differentiation\n\nAndrej Karpathy\n\nAri Seff\n\nChris Olah\n\n\n\n\nCollaborative Filtering [90%]\nGoal: Implement a collaborative filtering algorithm in Python from scratch – you should not use any high-level library such as surprise since this would defeat the purpose of the assignment. For testing your implementation, you will use the song dataset available here. You are tasked to implement variations of the matrix factorization algorithms discussed in class and compare their performance with other approaches of your choice. Your report should not include any actual code. Instead, it should describe the approach you used, the results, and provide a discussion of the findings.\n\n\n\nNatural Evolution Strategies [100%]\nThe class of algorithms known as NaturalEvolutionStrategies (NES) has demonstrated significant potential in various fields of applied mathematics and machine learning. In this project, you will provide an overview of NES and apply these approaches to three optimization problems of your choice. Your report should not include any actual code. Instead, it should describe what NES are, how they work, and provide a discussion of the results.\nPossible References:\n\nInformation-geometric optimization algorithms: A unifying pic- ture via invariance principles\n\nEvolution Strategies as a Scalable Alternative to Reinforcement Learning\n\nEvolution Strategies\nNatural Evolution Strategies\n\n\n\n\nWord Embeddings [100%]\nGoal: Implement a simple word embedding in Python (from scratch) and use it to find the most similar words to a given word. Your report should not include any actual code. Instead, it should describe how word embeddings work, how you implemented it, and provide a discussion of the results.\nPossible References:\n\nAndrew Ng\n\nThe illustrated Word2Vec\n\n\n\n\nLSTM for time series prediction [100%]\nGoal: Implement a simple LSTM in Python and from scratch, ie. not use the LSTM(...) function of a high-level library. You will then use your implementation to predict the next value in a time series dataset of your choice. Your report should not include any actual code. Instead, it should describe how a LSTM works, how you implemented it, and provide a discussion of the results.\nPossible References:\n\nUnderstanding LSTM Networks\nZico Kolter\n\nThe Unreasonable Effectiveness of Recurrent Neural Networks\nCan recurrent neural networks warp time?\n\n\n\n\nTransformer architecture [100%]\nGoal: Implement a simple transformer in Python and from scratch, ie. not use the Transformer(...) function of a high-level library.\nSome examples of simple task that can be used to test the transformer are:\n\nSequence Reversal: reversing a sequences of numbers, [1,3,2,4,5,3] -&gt; [3,5,4,2,3,1].\n\nBasic Arithmetic: learning simple arithmetic operation is a surprisingly challenging task for a transformer. For example, given a sequence of numbers and arithmetic operations, the model should output the result of the operations. [1,3,2,\"+\",9,4,2]-&gt;[1,0,7,4]. [1,3,2,\"+\",9,4,2]-&gt;[1,0,7,4]\n\nCopying Task: A simple task where the model’s objective is to output the same sequence it receives as input. This might seem trivial but is a good starting point: [1,3,2,4,5,3]-&gt;[1,3,2,4,5,3]\n\nSorting Numbers: This can be very challenging for a transformer, especially if the sequence is long. [1,3,2,4,5,3]-&gt;[1,2,3,3,4,5]\n\nCharacter-Level Text Generation: Generating text one character at a time, based on a given prompt or starting character.\n\nYour report should not include any actual code. Instead, it should describe how a transformer works, how you implemented it, and provide a discussion of the results.\nPossible References:\n\nThe Annotated Transformer\n\nJAX: Transformers and Multi-Head Attention\nGPT in 60 lines of code\nThe Transformer Family\nThe Illustrated Transformer\npytorch transformer from scratch\n\nTransformers: Zero to Hero\n\n\n\n\nTopic of your choice\nPropose a topic and contact me to discuss it and for approval."
  },
  {
    "objectID": "people/index_people.html",
    "href": "people/index_people.html",
    "title": "Research team",
    "section": "",
    "text": "Zhidi Lin Research Fellow (2024-present): High-Dimensional models time-series model and data-assimilation.\nHai Dang Dau Research Fellow (2024-present): High-Dimensional models time-series model and data-assimilation, diffusion models"
  },
  {
    "objectID": "people/index_people.html#phdpostdoc-co-supervision",
    "href": "people/index_people.html#phdpostdoc-co-supervision",
    "title": "Research team",
    "section": "",
    "text": "Zhidi Lin Research Fellow (2024-present): High-Dimensional models time-series model and data-assimilation.\nHai Dang Dau Research Fellow (2024-present): High-Dimensional models time-series model and data-assimilation, diffusion models"
  },
  {
    "objectID": "people/index_people.html#alumni",
    "href": "people/index_people.html#alumni",
    "title": "Research team",
    "section": "Alumni",
    "text": "Alumni\n\nWang Chao Research Fellow (2021-2023): Bayesian inverse problems, generative models, data-driven priors.\nChristopher Hendra PhD (2019-2022): Genomics, Nanopore sequencing – co-supervised by Jonathan Göke (GIS). Chris is now a Senior Scientist at MSD.\nKhai Xiang Au PhD (2019-2023): PDE constrained Bayesian inverse problems, uncertainty quanification, variational inference. Khai is now working as a data scientist at American Express.\nRahul Rahaman PhD(2018-2022): Bayesian inference, Uncertainty Quantification, Deep-Learning. Rahul is now an Applied Research Scientist at Amazon.\nAtin Ghosh PhD (2017-2021): Deep Learning for Glaucoma Understanding, representation learning, generative models, semi-supervised learning. Atin is now an Applied Research Scientist at Amazon.\nSe-In Jang Research Fellow (2019-2021): Computer vision and application to ophthalmology. Se-In is now a research fellow in the Center for Advanced Medical Computing and Analysis and the Gordon Center for Medical Imaging, Massachusetts General Hospital (MGH) and Harvard Medical School.\nAxel Finke Research Fellow (2017-2020): Sequential Monte Carlo, MCMC, algorithms for high-dimensional problems; applications in finance, economics, ecology and molecular biology. Axel is now an assistant professor at Loughborough University (UK).\nZuozhu Liu Research Fellow (2019-2020): Bayesian inference, deep generative models, 3D vision and medical applications. Zuozhu is now Assistant Professor at the Zhejiang University-University of Illinois at Urbana-Champaign Institute.\nMatt Graham Research Fellow (2017-2020): Approximate inference methods, MCMC, approximate Bayesian computation, numerical simulation. Matt is now a research data scientist in the Advanced Research Computing Centre at University College London.\nWillem van den Boom Research Fellow (2018-2019): Willem is now a Senior Research Fellow in the Division of Biomedical Data Science at the Yong Loo Lin School of Medicine of the National University of Singapore.\nKhai Sing Chin Research Associate (2017-2018): Khai Sing is now working in the finance industry.\nDeborshee Sen PhD (NUS, 2014-2017). Winner of the 2017 DSAP NUS best researcher award. After a postdoc at Duke University and a position as an assistant Professor at Bath University, Deborshee now works as a Research Scientist at Amazon.\nDaniel Paulin Research Fellow (NUS, 2014-2015). Daniel is now an Assistant Professor at the School of Mathematics at the University of Edinburgh.\nEge Muzaffer PhD (NUS, 2016): Bayesian inverse problems and Sequential Monte Carlo. Ege is now a Machine Learning EngineerMachine Learning Engineer Ubisoft RedLynx"
  },
  {
    "objectID": "people/index_people.html#msc",
    "href": "people/index_people.html#msc",
    "title": "Research team",
    "section": "MSc",
    "text": "MSc\n\nQuang Huy Nguyen MSc (2018-2019): representation learning, robust models for image segmentation.\nAugustin Hoff (NUS, 2016-2017). Deep Neural Networks and Features Extraction. Augustin is now a Senior Data Scientist at MAIF.\nMajdi Rabia (NUS, 2016-2017). Numerical Method for Backward-Stochastic-Differential-Equations. Majdi is Co-founder and CTO @Fairphonic.\nBenjamin Scellier (NUS, 2015). Deep Learning. After his MSc at NUS, Benjamin joined Yoshua Bengio’s Group as a PhD. He is now a principal research scientist at Rain"
  },
  {
    "objectID": "jobs/2024_koopman/2024_koopman.html",
    "href": "jobs/2024_koopman/2024_koopman.html",
    "title": "Research Fellow positions: Data-Assimilation",
    "section": "",
    "text": "Two Postdoctoral Research Fellow positions are currently open in the Department of Statistics & Data Science at the National University of Singapore (NUS). These are two-year contracts, with the possibility of renewal upon review.\nWhile the research themes are flexible, our primary interest lies in the design and analysis of statistical methods for data-assimilation of high-dimensional time-series data. We anticipate applying tools from reinforcement learning, Koopman operators, and unsupervised representation learning, supplementing the more conventional techniques in state-space modeling (particle filters, EnKF, variational approaches, etc…)\n\nDetails:\n\nStart Date: Immediate\nDuration: 2 years (Renewable for up to an additional year)\nSalary Range: S$ 70k-100k / Year, plus benefits (Health Insurance / etc…)\nTravel: Generous budget for attending conferences\n\n\n\nRequirements:\nIdeal candidates should have:\n\na Ph.D. in Statistics, Computer Science, Physics, or a closely related field.\n\nstrong experience in Python programming.\n\nexperience with at least one deep-learning framework\na demonstrated record of academic publications\n\nresearch experience that aligns with the core focus of the research programme.\n\n\n\nContact:\nQueries about this post should be addressed to Alex Thiery at a.h.thiery@nus.edu.sg. We highly encourage prospective applicants to get in touch informally to discuss further details about the position first. This project is in collaboration with Jeremy Heng.\nThe positions will remain open until we find the right candidates. [Positions have been filled]"
  },
  {
    "objectID": "jobs/2024_koopman/2024_control/2024_controlled.html",
    "href": "jobs/2024_koopman/2024_control/2024_controlled.html",
    "title": "Research Fellow positions: Data-Assimilation and Generative Modeling",
    "section": "",
    "text": "Two Postdoctoral Research Fellow positions are available in the Department of Statistics & Data Science at the National University of Singapore (NUS). These positions are offered on a two-year contract basis, with the possibility of renewal subject to review.\nAlthough the research themes are flexible, our primary focus is on the design and analysis of statistical methods for data assimilation of high-dimensional time-series data and generative modeling. With the recent advances in diffusion-based models, concepts initially developed for the inference of time series and in the control theory literature can now be adapted for generative models. Conversely, the latest methodological developments in generative models (e.g., denoising diffusion, stochastic interpolants, flow-methods) can be applied to enhance the performance of traditional statistical tools used in data assimilation for high-dimensional time series models (e.g., ensemble Kalman filters, particle filters). In this project, we propose to explore the intersection of these two areas, aiming to enhance the robustness and accuracy of methods for data-assimilation of high-dimensional dynamical systems, as well as to develop new sampling algorithms.\n\nDetails:\n\nStart Date: Immediate\nDuration: 2 years (Renewable for up to an additional year)\nSalary Range: S$ 70k-90k / Year, plus benefits (Health Insurance / etc…)\nTravel: Generous budget for attending conferences\n\n\n\nRequirements:\nIdeal candidates should have:\n\na Ph.D. in Statistics, Computer Science, Physics, or a closely related field.\n\nstrong experience in Python programming.\n\nexperience with at least one deep-learning framework\na demonstrated record of academic publications\n\nresearch experience that aligns with the core focus of the research programme.\n\n\n\nContact:\nQueries about this post should be addressed to Alex Thiery at a.h.thiery@nus.edu.sg. We highly encourage prospective applicants to get in touch informally to discuss further details about the position first. This project is in collaboration with Jeremy Heng.\nThe positions will remain open until we find the right candidates."
  },
  {
    "objectID": "jobs/2024_diffusion/2024_diffusion.html",
    "href": "jobs/2024_diffusion/2024_diffusion.html",
    "title": "Research Fellow positions: Data-Assimilation and Generative Modeling",
    "section": "",
    "text": "Two Postdoctoral Research Fellow positions are available in the Department of Statistics & Data Science at the National University of Singapore (NUS). These positions are offered on a two-year contract basis, with the possibility of renewal subject to review.\nAlthough the research themes are flexible, our primary focus is on the design and analysis of statistical methods for data assimilation of high-dimensional time-series data and generative modeling. With the recent advances in diffusion-based models, concepts initially developed for the inference of time series and in the control theory literature can now be adapted for generative models. Conversely, the latest methodological developments in generative models (e.g., denoising diffusion, stochastic interpolants, flow-methods) can be applied to enhance the performance of traditional statistical tools used in data assimilation for high-dimensional time series models (e.g., ensemble Kalman filters, particle filters). In this project, we propose to explore the intersection of these two areas, aiming to enhance the robustness and accuracy of methods for data-assimilation of high-dimensional dynamical systems, as well as to develop new sampling algorithms.\n\nDetails:\n\nStart Date: Immediate\nDuration: 2 years (Renewable for up to an additional year)\nSalary Range: S$ 70k-90k / Year, plus benefits (Health Insurance / etc…)\nTravel: Generous budget for attending conferences\n\n\n\nRequirements:\nIdeal candidates should have:\n\na Ph.D. in Statistics, Computer Science, Physics, or a closely related field.\n\nstrong experience in Python programming.\n\nexperience with at least one deep-learning framework\na demonstrated record of academic publications\n\nresearch experience that aligns with the core focus of the research programme.\n\n\n\nContact:\nQueries about this post should be addressed to Alex Thiery at a.h.thiery@nus.edu.sg. We highly encourage prospective applicants to get in touch informally to discuss further details about the position first.\nThe positions will remain open until we find the right candidates."
  }
]