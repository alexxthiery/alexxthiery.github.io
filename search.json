[
  {
    "objectID": "notes_students/probability_basics/probability_basics.html",
    "href": "notes_students/probability_basics/probability_basics.html",
    "title": "Probability Basics",
    "section": "",
    "text": "Mean and Expectation\nStandard Deviation, Variances and Covariances\nRandom Vectors\nGaussian distributions\n\n\n\n\nThe mean or expectation of a random variable \\(X\\) is denoted as \\(\\mathbb{E}[X]\\)l. For a discrete random variable,\n\\[\n\\mathbb{E}[X] = \\sum_{x} x \\cdot \\mathbb{P}(X = x)\n\\]\nand for a continuous random variable,\n\\[\n\\mu_X = \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\n\\]\nwhere \\(f(x)\\) is the probability density function of \\(X\\). It’s important to note that in some cases, particularly for distributions with “heavy” tails, the mean might not be well-defined. This situation arises in distributions like the Cauchy distribution, where the tails of the distribution do not decay rapidly enough to yield a finite expectation. In such cases, the integrals or sums used to define the mean do not converge.\nIndependence is a fundamental concept in probability theory, referring to the relationship between two random variables. Two random variables, \\(X\\) and \\(Y\\), are said to be independent if the occurrence of an event related to \\(X\\) does not influence the probability of an event related to \\(Y\\), and vice versa. Mathematically, \\(X\\) and \\(Y\\) are independent if and only if for every pair of events \\(A\\) and \\(B\\), the probability that both \\(X\\) belongs to \\(A\\) and \\(Y\\) belongs to \\(B\\) is the product of their individual probabilities. This can be expressed as:\n\\[\n\\mathbb{P}(X \\in A \\; \\text{ and } \\; Y \\in B) = \\mathbb{P}(X \\in A) \\cdot \\mathbb{P}(Y \\in B).\n\\]\nThis can equivalently be expressed as the fact that, for any two functions \\(F(\\cdot)\\) and \\(G(\\cdot)\\), the following identity holds\n\\[\n\\mathbb{E}[F(X) \\cdot G(Y)] \\; = \\; \\mathbb{E}[F(X)] \\cdot \\mathbb{E}[G(Y)].\n\\]\nThis definition implies that knowing the outcome of \\(X\\) provides no information about the outcome of \\(Y\\), and this lack of influence is a key characteristic of independent random variables. One extremely important remark is that, for two random variables \\(X\\) and \\(Y\\), the expectation of the sum equals the sum of the expectation,\n\\[\n\\mathbb{E}[X+Y] = \\mathbb{E}[X] + \\mathbb{E}[Y],\n\\]\nas soon as all these quantities exists. This holds even if the two random variables are not independent.\n\n\n\nThe variance of a random variable \\(X\\), denoted as \\(\\text{Var}(X)\\), measures the spread of its values. It is defined as\n\\[\n\\begin{align}\n\\text{Var}(X) &= \\mathbb{E}[(X - \\mu_X )^2] = \\mathbb{E}[X^2] - \\mu_X ^2\\\\\n&= \\int_{-\\infty}^{\\infty} (x - \\mu_X)^2 \\cdot f(x) \\, dx\n\\end{align}\n\\]\nThe standard deviation is the square root of the variance, denoted as \\(\\sigma_X = \\sqrt{\\text{Var}(X)}\\). The notion of covariance measures the linear relationship between two random variables \\(X\\) and \\(Y\\). It is defined as\n\\[\n\\begin{align}\n\\text{Cov}(X, Y)\n&= \\mathbb{E}[(X - \\mu_X )(Y - \\mu_Y )] = \\mathbb{E}[X \\, Y] -  \\mu_X  \\, \\mu_Y\\\\\n&= \\int_{-\\infty}^{\\infty} (x - \\mu_X)(y-\\mu_Y) \\cdot f(x, y) \\, dx\n\\end{align}\n\\]\nwhere \\(f(x,y)\\) is the joined density of the pair of random variables \\((X,Y)\\). The correlation is defined as a normalized version of the covariance,\n\\[\n\\text{Corr}(X,Y) = \\textrm{Cov}\\left\\{ \\frac{X - \\mu_X}{\\sigma_X}, \\frac{Y - \\mu_Y}{\\sigma_Y}\\right\\}\n\\]\nand always satisfies \\(-1 \\leq \\text{Corr}(X,Y) \\leq 1\\), as is easily proved (exercise). Note that if \\(X\\) and \\(Y\\) are independent, then \\(\\text{Cov}(X, Y) = 0\\). However, zero covariance does not imply independence and it is a good exercise to construct such a counter-example. Standard manipulations reveal that for two random variables \\(X\\) and \\(Y\\) we have\n\\[\n\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y) + \\text{Cov}(X, Y),\n\\]\nwhich is indeed the equivalent of the identity \\((x+y)^2 = x^2 + y^2 + 2xy\\). Importantly, if the two random variables \\(X\\) and \\(Y\\) are independent, the variance of the sum equals the sum of the variances, \\(\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y)\\). This also shows that for \\(N\\) independent and identically distributed random variables \\(X_1, \\ldots, X_N\\), we have that\n\\[\n\\text{Var}\\left\\{ \\frac{X_1 + \\ldots + X_N}{N} \\right\\} \\; = \\; \\frac{\\text{Var}(X)}{N}.\n\\]\n\n\n\nA random vector is a vector of random variables. For a random vector \\(\\mathbf{X}\\) in \\(\\mathbb{R}^d\\), the mean of \\(\\mu = \\mathbf{X} \\ in \\mathbb{R}^d\\) is a vector in \\(\\mathbb{R}^d\\), each component of which is the mean of one of its \\(d\\) components. The covariance matrix, \\(\\Sigma \\in \\mathbb{R}^{d,d}\\), of \\(\\mathbf{X}\\) is a \\(d \\times d\\) matrix defined by\n\\[\n\\Sigma_{ij} = \\text{Cov}(X_i, X_j)\n=\n\\mathbb{E}[(X_i - \\mu_{X_i}) \\, (X_j - \\mu_{X_j})]\n\\]\nwhere \\(X_i\\) and \\(X_j\\) are the \\(i\\)-th and \\(j\\)-th components of \\(\\mathbf{X}\\), respectively. Each element \\(\\Sigma_{ij}\\) represents the covariance between the \\(i\\)-th and \\(j\\)-th components of the vector \\(\\mathbf{X}\\). If the components are independent, the covariance matrix is diagonal. Furthermore, the covariance matrix, when it exists, is always a symmetric and positive semi-definite matrix.\n\n\n\nThe Gaussian distribution, also known as the normal distribution, holds a central place in statistics, probability, and applied mathematics due to several key reasons. Firstly, its mathematical properties are well-understood and conducive to analytical work. Secondly, the Central Limit Theorem states that the sum of a large number of independent, identically distributed variables will approximately follow a Gaussian distribution, regardless of the original distribution. This makes it a fundamental tool for inferential statistics. Furthermore, Gaussian distributions arise naturally in numerous contexts due to random noise and errors often tending to distribute normally. Lastly, since Gaussian distributions are extremely tractable, its properties allow for convenient modeling in various fields.\n\n\nA univariate Gaussian (or normal) distribution for a random variable \\(X\\) is characterized by its mean \\(\\mu\\) and variance \\(\\sigma^2\\). Its probability density function is given by\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{ -\\frac{(x - \\mu)^2}{2\\sigma^2}\\right\\}\n\\]\nThe constant \\(1/\\sqrt{2\\pi\\sigma^2}\\) is often written as \\(1/\\mathcal{Z}\\) where \\(\\mathcal{Z} = \\sqrt{2\\pi\\sigma^2}\\) is referred to as the “normalization factor”. It ensures that the density \\(f(x)\\) integrates to one.\n\n\n\nA multivariate Gaussian distribution for a random vector \\(\\mathbf{X}\\) in \\(\\mathbb{R}^n\\) is characterized by a mean vector \\(\\boldsymbol{\\mu}\\) and a covariance matrix \\(\\Sigma\\). Its probability density function is\n\\[\nf(\\mathbf{x}) = \\frac{1}{\\sqrt{(2\\pi)^n |\\Sigma|}} \\exp\\left\\{ -\\frac{1}{2} \\langle (\\mathbf{x} - \\boldsymbol{\\mu}), \\Sigma^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\rangle \\right\\}\n\\]\nand \\(|\\Sigma|\\) is the determinant of the covariance matrix \\(\\Sigma\\). Crucially, the inverse of the covariance matrix is inside the dot product \\(\\langle (\\mathbf{x} - \\boldsymbol{\\mu}), \\Sigma^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\rangle\\). In general, zero correlation between variables does not imply their independence. However, this principle has a notable exception in the case of Gaussian distributions. If two vectors \\(X,Y\\) are such that \\((X,Y)\\) is a Gaussian vector, then zero correlation implies independence. Note that, it is still possible that \\(X\\) and \\(Y\\) are both Gaussian vectors with zero correlation and yet are not independent; this may happen with the joint vector \\((X,Y)\\) is not Gaussian (although both \\(X\\) and \\(Y\\) are)."
  },
  {
    "objectID": "notes_students/probability_basics/probability_basics.html#mean-expectation",
    "href": "notes_students/probability_basics/probability_basics.html#mean-expectation",
    "title": "Probability Basics",
    "section": "",
    "text": "The mean or expectation of a random variable \\(X\\) is denoted as \\(\\mathbb{E}[X]\\)l. For a discrete random variable,\n\\[\n\\mathbb{E}[X] = \\sum_{x} x \\cdot \\mathbb{P}(X = x)\n\\]\nand for a continuous random variable,\n\\[\n\\mu_X = \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\n\\]\nwhere \\(f(x)\\) is the probability density function of \\(X\\). It’s important to note that in some cases, particularly for distributions with “heavy” tails, the mean might not be well-defined. This situation arises in distributions like the Cauchy distribution, where the tails of the distribution do not decay rapidly enough to yield a finite expectation. In such cases, the integrals or sums used to define the mean do not converge.\nIndependence is a fundamental concept in probability theory, referring to the relationship between two random variables. Two random variables, \\(X\\) and \\(Y\\), are said to be independent if the occurrence of an event related to \\(X\\) does not influence the probability of an event related to \\(Y\\), and vice versa. Mathematically, \\(X\\) and \\(Y\\) are independent if and only if for every pair of events \\(A\\) and \\(B\\), the probability that both \\(X\\) belongs to \\(A\\) and \\(Y\\) belongs to \\(B\\) is the product of their individual probabilities. This can be expressed as:\n\\[\n\\mathbb{P}(X \\in A \\; \\text{ and } \\; Y \\in B) = \\mathbb{P}(X \\in A) \\cdot \\mathbb{P}(Y \\in B).\n\\]\nThis can equivalently be expressed as the fact that, for any two functions \\(F(\\cdot)\\) and \\(G(\\cdot)\\), the following identity holds\n\\[\n\\mathbb{E}[F(X) \\cdot G(Y)] \\; = \\; \\mathbb{E}[F(X)] \\cdot \\mathbb{E}[G(Y)].\n\\]\nThis definition implies that knowing the outcome of \\(X\\) provides no information about the outcome of \\(Y\\), and this lack of influence is a key characteristic of independent random variables. One extremely important remark is that, for two random variables \\(X\\) and \\(Y\\), the expectation of the sum equals the sum of the expectation,\n\\[\n\\mathbb{E}[X+Y] = \\mathbb{E}[X] + \\mathbb{E}[Y],\n\\]\nas soon as all these quantities exists. This holds even if the two random variables are not independent."
  },
  {
    "objectID": "notes_students/probability_basics/probability_basics.html#standard-deviation-variances-and-covariances",
    "href": "notes_students/probability_basics/probability_basics.html#standard-deviation-variances-and-covariances",
    "title": "Probability Basics",
    "section": "",
    "text": "The variance of a random variable \\(X\\), denoted as \\(\\text{Var}(X)\\), measures the spread of its values. It is defined as\n\\[\n\\begin{align}\n\\text{Var}(X) &= \\mathbb{E}[(X - \\mu_X )^2] = \\mathbb{E}[X^2] - \\mu_X ^2\\\\\n&= \\int_{-\\infty}^{\\infty} (x - \\mu_X)^2 \\cdot f(x) \\, dx\n\\end{align}\n\\]\nThe standard deviation is the square root of the variance, denoted as \\(\\sigma_X = \\sqrt{\\text{Var}(X)}\\). The notion of covariance measures the linear relationship between two random variables \\(X\\) and \\(Y\\). It is defined as\n\\[\n\\begin{align}\n\\text{Cov}(X, Y)\n&= \\mathbb{E}[(X - \\mu_X )(Y - \\mu_Y )] = \\mathbb{E}[X \\, Y] -  \\mu_X  \\, \\mu_Y\\\\\n&= \\int_{-\\infty}^{\\infty} (x - \\mu_X)(y-\\mu_Y) \\cdot f(x, y) \\, dx\n\\end{align}\n\\]\nwhere \\(f(x,y)\\) is the joined density of the pair of random variables \\((X,Y)\\). The correlation is defined as a normalized version of the covariance,\n\\[\n\\text{Corr}(X,Y) = \\textrm{Cov}\\left\\{ \\frac{X - \\mu_X}{\\sigma_X}, \\frac{Y - \\mu_Y}{\\sigma_Y}\\right\\}\n\\]\nand always satisfies \\(-1 \\leq \\text{Corr}(X,Y) \\leq 1\\), as is easily proved (exercise). Note that if \\(X\\) and \\(Y\\) are independent, then \\(\\text{Cov}(X, Y) = 0\\). However, zero covariance does not imply independence and it is a good exercise to construct such a counter-example. Standard manipulations reveal that for two random variables \\(X\\) and \\(Y\\) we have\n\\[\n\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y) + \\text{Cov}(X, Y),\n\\]\nwhich is indeed the equivalent of the identity \\((x+y)^2 = x^2 + y^2 + 2xy\\). Importantly, if the two random variables \\(X\\) and \\(Y\\) are independent, the variance of the sum equals the sum of the variances, \\(\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y)\\). This also shows that for \\(N\\) independent and identically distributed random variables \\(X_1, \\ldots, X_N\\), we have that\n\\[\n\\text{Var}\\left\\{ \\frac{X_1 + \\ldots + X_N}{N} \\right\\} \\; = \\; \\frac{\\text{Var}(X)}{N}.\n\\]"
  },
  {
    "objectID": "notes_students/probability_basics/probability_basics.html#random-vectors-mean-vector-and-covariance-matrix",
    "href": "notes_students/probability_basics/probability_basics.html#random-vectors-mean-vector-and-covariance-matrix",
    "title": "Probability Basics",
    "section": "",
    "text": "A random vector is a vector of random variables. For a random vector \\(\\mathbf{X}\\) in \\(\\mathbb{R}^d\\), the mean of \\(\\mu = \\mathbf{X} \\ in \\mathbb{R}^d\\) is a vector in \\(\\mathbb{R}^d\\), each component of which is the mean of one of its \\(d\\) components. The covariance matrix, \\(\\Sigma \\in \\mathbb{R}^{d,d}\\), of \\(\\mathbf{X}\\) is a \\(d \\times d\\) matrix defined by\n\\[\n\\Sigma_{ij} = \\text{Cov}(X_i, X_j)\n=\n\\mathbb{E}[(X_i - \\mu_{X_i}) \\, (X_j - \\mu_{X_j})]\n\\]\nwhere \\(X_i\\) and \\(X_j\\) are the \\(i\\)-th and \\(j\\)-th components of \\(\\mathbf{X}\\), respectively. Each element \\(\\Sigma_{ij}\\) represents the covariance between the \\(i\\)-th and \\(j\\)-th components of the vector \\(\\mathbf{X}\\). If the components are independent, the covariance matrix is diagonal. Furthermore, the covariance matrix, when it exists, is always a symmetric and positive semi-definite matrix."
  },
  {
    "objectID": "notes_students/probability_basics/probability_basics.html#gaussian-distributions",
    "href": "notes_students/probability_basics/probability_basics.html#gaussian-distributions",
    "title": "Probability Basics",
    "section": "",
    "text": "The Gaussian distribution, also known as the normal distribution, holds a central place in statistics, probability, and applied mathematics due to several key reasons. Firstly, its mathematical properties are well-understood and conducive to analytical work. Secondly, the Central Limit Theorem states that the sum of a large number of independent, identically distributed variables will approximately follow a Gaussian distribution, regardless of the original distribution. This makes it a fundamental tool for inferential statistics. Furthermore, Gaussian distributions arise naturally in numerous contexts due to random noise and errors often tending to distribute normally. Lastly, since Gaussian distributions are extremely tractable, its properties allow for convenient modeling in various fields.\n\n\nA univariate Gaussian (or normal) distribution for a random variable \\(X\\) is characterized by its mean \\(\\mu\\) and variance \\(\\sigma^2\\). Its probability density function is given by\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{ -\\frac{(x - \\mu)^2}{2\\sigma^2}\\right\\}\n\\]\nThe constant \\(1/\\sqrt{2\\pi\\sigma^2}\\) is often written as \\(1/\\mathcal{Z}\\) where \\(\\mathcal{Z} = \\sqrt{2\\pi\\sigma^2}\\) is referred to as the “normalization factor”. It ensures that the density \\(f(x)\\) integrates to one.\n\n\n\nA multivariate Gaussian distribution for a random vector \\(\\mathbf{X}\\) in \\(\\mathbb{R}^n\\) is characterized by a mean vector \\(\\boldsymbol{\\mu}\\) and a covariance matrix \\(\\Sigma\\). Its probability density function is\n\\[\nf(\\mathbf{x}) = \\frac{1}{\\sqrt{(2\\pi)^n |\\Sigma|}} \\exp\\left\\{ -\\frac{1}{2} \\langle (\\mathbf{x} - \\boldsymbol{\\mu}), \\Sigma^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\rangle \\right\\}\n\\]\nand \\(|\\Sigma|\\) is the determinant of the covariance matrix \\(\\Sigma\\). Crucially, the inverse of the covariance matrix is inside the dot product \\(\\langle (\\mathbf{x} - \\boldsymbol{\\mu}), \\Sigma^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\rangle\\). In general, zero correlation between variables does not imply their independence. However, this principle has a notable exception in the case of Gaussian distributions. If two vectors \\(X,Y\\) are such that \\((X,Y)\\) is a Gaussian vector, then zero correlation implies independence. Note that, it is still possible that \\(X\\) and \\(Y\\) are both Gaussian vectors with zero correlation and yet are not independent; this may happen with the joint vector \\((X,Y)\\) is not Gaussian (although both \\(X\\) and \\(Y\\) are)."
  },
  {
    "objectID": "posts/reverse_and_tweedie/reverse_and_tweedie.html",
    "href": "posts/reverse_and_tweedie/reverse_and_tweedie.html",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "",
    "text": "Video\n\n\nImagine a scalar diffusion process \\(\\{X_t\\}_{t=0}^T\\) defined on the interval \\([0,T]\\),\n\\[dX = \\mu(X) \\, dt + \\sigma \\, dW\\]\nwhere \\(\\mu(X)\\) is the drift term, \\(\\sigma\\) is the diffusion coefficient, and \\(dW\\) is a Wiener process. Denote the distribution of this process at time \\(t\\) (\\(0 \\leq t \\leq T\\)) as \\(p_t(dx)\\) with an initial distribution of \\(p_0(dx)\\). Now, what happens if we reverse time and examine the process backward? In other words, consider the time-reversed process \\(\\overleftarrow{X}\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIntuitively, the process \\(\\overleftarrow{X}\\) is also a diffusion on the interval \\([0, T]\\), but with an initial distribution \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). To gain intuition, consider an Euler discretization of the forward process:\n\\[X_{t+\\delta} = X_{t} + \\mu(X_t)\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, \\mathbf{n} \\tag{1}\\]\nwhere \\(\\mathbf{n}\\sim \\mathcal{N}(0,1)\\) represents a noise term independent from \\(X_{t}\\), and \\(\\delta \\ll 1\\) is a time increment. Re-arranging terms and making the approximation \\(\\mu(X_t) \\approx \\mu(X_{t+\\delta})\\) gives that\n\\[X_{t} \\approx X_{t+\\delta} - \\mu(X_{t+\\delta})\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, (-\\mathbf{n}). \\tag{2}\\]\nThis seems to suggest that the time-reversed process follows the dynamics \\(d\\overleftarrow{X} = -\\mu(\\overleftarrow{X}) \\, dt + \\sigma \\, dW\\) started from \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). However, this conclusion is incorrect because this would suggest that the time-reversed of a standard Brownian motion (where \\(\\mu(x) \\equiv 0\\)) starting at zero is also a Brownian motion starting at \\(p_T(dx) = \\mathcal{N}(0,T)\\), which is clearly not the case. The flaw in this argument lies in assuming that the noise term \\(Z\\) is independent of \\(X_{t+\\delta}\\), which is not true, rendering the Euler discretization argument invalid.\nDeriving the dynamics of the backward process in a rigorous manner is not straightforward (Anderson 1982) (Haussmann and Pardoux 1986). What follows is a heuristic derivation that proceeds by estimating the mean and variance of \\(X_{t}\\) given \\(X_{t+\\delta} = x_{t+\\delta}\\), assuming \\(\\delta \\ll 1\\). Here, \\(x_{t+\\delta}\\) is treated as a fixed and constant value, and we are only interested in the conditional distribution of \\(X_t\\) given \\(x_{t+\\delta}\\). Bayes’ law gives\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta}) \\propto p_t(x) \\, \\exp\\left\\{ -\\frac{\\left( x_{t+\\delta} - [x + \\mu(x) \\, \\delta] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\},\\]\nwhere the exponential term corresponds to the transition of the forward diffusion for \\(\\delta \\ll 1\\). Using the 1st order approximation\n\\[p_t(x) \\approx p_t(x_{t+\\delta}) \\,\\exp\\left( \\langle \\nabla \\log p_t(x_{t+\\delta}), (x-x_{t+\\delta})\\rangle\\right),\\]\neliminating multiplicative constants and higher-order error terms, we obtain:\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta})\n\\propto\n\\exp\\left\\{ -\\frac{\\left( x - [x_{t+\\delta} - \\mu(x_{t+\\delta}) \\delta\n+ {\\color{red} \\sigma^2 \\, \\nabla \\log p_t(x_{t+\\delta}) \\, \\delta} ] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\}.\\]\nFor \\(\\delta \\ll 1\\), this is transition of the reverse diffusion\n\\[\nd\\overleftarrow{X}_t = -\\mu(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\]\nThe notation \\(B\\) is used to emphasize that this Brownian motion is distinct from the one used in the forward diffusion. The additional drift term, denoted as \\({\\color{red} \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X})}\\), is intuitive: it pushes the reverse diffusion toward regions where the forward diffusion spent a significant amount of time, i.e., where \\(p_t(dx)\\) is large. The popular “denoising diffusion models” (Ho, Jain, and Abbeel 2020) can be seen as discretizations of this backward process, employing various techniques to estimate the additional drift term from data."
  },
  {
    "objectID": "posts/reverse_and_tweedie/reverse_and_tweedie.html#reversing-a-diffusion",
    "href": "posts/reverse_and_tweedie/reverse_and_tweedie.html#reversing-a-diffusion",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "",
    "text": "Video\n\n\nImagine a scalar diffusion process \\(\\{X_t\\}_{t=0}^T\\) defined on the interval \\([0,T]\\),\n\\[dX = \\mu(X) \\, dt + \\sigma \\, dW\\]\nwhere \\(\\mu(X)\\) is the drift term, \\(\\sigma\\) is the diffusion coefficient, and \\(dW\\) is a Wiener process. Denote the distribution of this process at time \\(t\\) (\\(0 \\leq t \\leq T\\)) as \\(p_t(dx)\\) with an initial distribution of \\(p_0(dx)\\). Now, what happens if we reverse time and examine the process backward? In other words, consider the time-reversed process \\(\\overleftarrow{X}\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIntuitively, the process \\(\\overleftarrow{X}\\) is also a diffusion on the interval \\([0, T]\\), but with an initial distribution \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). To gain intuition, consider an Euler discretization of the forward process:\n\\[X_{t+\\delta} = X_{t} + \\mu(X_t)\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, \\mathbf{n} \\tag{1}\\]\nwhere \\(\\mathbf{n}\\sim \\mathcal{N}(0,1)\\) represents a noise term independent from \\(X_{t}\\), and \\(\\delta \\ll 1\\) is a time increment. Re-arranging terms and making the approximation \\(\\mu(X_t) \\approx \\mu(X_{t+\\delta})\\) gives that\n\\[X_{t} \\approx X_{t+\\delta} - \\mu(X_{t+\\delta})\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, (-\\mathbf{n}). \\tag{2}\\]\nThis seems to suggest that the time-reversed process follows the dynamics \\(d\\overleftarrow{X} = -\\mu(\\overleftarrow{X}) \\, dt + \\sigma \\, dW\\) started from \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). However, this conclusion is incorrect because this would suggest that the time-reversed of a standard Brownian motion (where \\(\\mu(x) \\equiv 0\\)) starting at zero is also a Brownian motion starting at \\(p_T(dx) = \\mathcal{N}(0,T)\\), which is clearly not the case. The flaw in this argument lies in assuming that the noise term \\(Z\\) is independent of \\(X_{t+\\delta}\\), which is not true, rendering the Euler discretization argument invalid.\nDeriving the dynamics of the backward process in a rigorous manner is not straightforward (Anderson 1982) (Haussmann and Pardoux 1986). What follows is a heuristic derivation that proceeds by estimating the mean and variance of \\(X_{t}\\) given \\(X_{t+\\delta} = x_{t+\\delta}\\), assuming \\(\\delta \\ll 1\\). Here, \\(x_{t+\\delta}\\) is treated as a fixed and constant value, and we are only interested in the conditional distribution of \\(X_t\\) given \\(x_{t+\\delta}\\). Bayes’ law gives\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta}) \\propto p_t(x) \\, \\exp\\left\\{ -\\frac{\\left( x_{t+\\delta} - [x + \\mu(x) \\, \\delta] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\},\\]\nwhere the exponential term corresponds to the transition of the forward diffusion for \\(\\delta \\ll 1\\). Using the 1st order approximation\n\\[p_t(x) \\approx p_t(x_{t+\\delta}) \\,\\exp\\left( \\langle \\nabla \\log p_t(x_{t+\\delta}), (x-x_{t+\\delta})\\rangle\\right),\\]\neliminating multiplicative constants and higher-order error terms, we obtain:\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta})\n\\propto\n\\exp\\left\\{ -\\frac{\\left( x - [x_{t+\\delta} - \\mu(x_{t+\\delta}) \\delta\n+ {\\color{red} \\sigma^2 \\, \\nabla \\log p_t(x_{t+\\delta}) \\, \\delta} ] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\}.\\]\nFor \\(\\delta \\ll 1\\), this is transition of the reverse diffusion\n\\[\nd\\overleftarrow{X}_t = -\\mu(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\]\nThe notation \\(B\\) is used to emphasize that this Brownian motion is distinct from the one used in the forward diffusion. The additional drift term, denoted as \\({\\color{red} \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X})}\\), is intuitive: it pushes the reverse diffusion toward regions where the forward diffusion spent a significant amount of time, i.e., where \\(p_t(dx)\\) is large. The popular “denoising diffusion models” (Ho, Jain, and Abbeel 2020) can be seen as discretizations of this backward process, employing various techniques to estimate the additional drift term from data."
  },
  {
    "objectID": "posts/reverse_and_tweedie/reverse_and_tweedie.html#denoising-score-matching-tweedie-formula",
    "href": "posts/reverse_and_tweedie/reverse_and_tweedie.html#denoising-score-matching-tweedie-formula",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "Denoising Score Matching & Tweedie formula",
    "text": "Denoising Score Matching & Tweedie formula\n\n\n\n\nMaurice Tweedie (1919–1996)\n\n\n\nThe previous section shows that a quantity often referred to as the “score” in machine learning (ML) literature, i.e. \\(\\mathcal{S}_t(x) \\equiv \\nabla_x \\log p_t(x)\\), naturally arises when a diffusion process runs backward in time. Interestingly, it is worth noting that since the beginning of times statisticians have been using the term “score” to refer to the other derivative, i.e. the derivative with respect to a model’s parameter, which is a completely different object!\nConsider a Brownian diffusion process \\(dX = \\sigma \\, dW\\) initiated from a distribution \\(\\mu(dx) = p_0(dx)\\). If this process is ran forward for a duration \\(\\delta&gt;0\\), we have:\n\\[X_{\\delta} = X_0 + \\mathcal{N}(0, \\sigma^2 \\, \\delta)\\]\nwhere \\(X_0 \\sim \\mu(dx)\\). Now, focusing on a specific sample \\(y = X_{\\delta}\\), the backward dynamics \\(d\\overleftarrow{X} = \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X}) , dt + \\sigma \\, dB\\) suggests that for sufficiently small \\(\\delta\\), the following approximation holds:\n\\[\n\\mathbb{E}[X_0 \\, | X_{\\delta} = y] \\; { \\color{\\red} \\approx} \\; y + \\nabla \\log p_{\\delta}(y) \\, \\sigma^2 \\delta.\n\\tag{3}\\]\nMaybe surprisingly, in the case of Brownian dynamics (no drift), this relationship holds exactly, even for arbitrarily large time increments \\(\\delta&gt;0\\). This observation attributed in (Efron 2011) to Maurice Tweedie (1919–1996) has a straightforward proof. Specifically, if \\(X \\sim \\mu(dx)\\), then \\(Y = X + \\mathcal{N}(0, \\Gamma^2)\\) has a density given by:\n\\[\np_Y(dy) = \\int \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx\n\\]\nwhere \\(\\rho_{\\Gamma}(z) \\propto \\exp[-z^2/(2 \\Gamma^2)]\\) is a centred Gaussian with variance \\(\\Gamma^2\\). It follows that\n\\[\n\\frac{ \\mathbb{E}[X \\, | Y = y]-y}{\\Gamma^2} = \\frac{ \\int \\left( \\frac {x-y }{\\Gamma^2} \\right)\\, \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx }{\\int \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx}\n=\n\\nabla_y \\log \\left\\{ \\int \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx \\right\\}\n\\]\nsince \\(\\nabla_y \\rho_{\\Gamma}(y-x) = (x-y)/\\Gamma^2\\). This leads to Tweedie’s formula:\n\\[\n\\mathbb{E}[X \\, | Y = y] \\; {\\color{red} =} \\; y + \\Gamma^2 \\, \\nabla \\log p_Y(y),\n\\]\nwhich is exactly the same as Equation 3 but with an equality sign: no approximation needed! Interestingly, this is what Machine-Learners often refer to as “denoising score matching” (Vincent 2011): if we have access to a large number of samples \\((X_i,Y_i)\\), where \\(X_i \\sim \\mu(dx)\\) and \\(Y_i = X_i + \\mathcal{N}(0, \\Gamma^2)\\), and fit a regression model \\(F_{\\theta}\\) by minimizing the mean-squared error \\(\\theta \\mapsto \\mathbb{E} \\|X - F_{\\theta}(Y)\\|^2\\), then Tweedie formula shows that in the limit of infinite samples and with sufficient flexibility in the regression model \\(F_{\\theta_\\star}(y) = y + \\Gamma^2 \\nabla \\log p_Y(y)\\). This allows one to estimate \\(\\nabla \\log p_Y\\) from data, which is often a good approximation of \\(\\nabla \\log \\mu\\) if the variance \\(\\Gamma^2\\) of the added noise is not too large. Indeed, things can go bad if \\(\\Gamma^2\\) is very small and the number of training data is not large, no free lunch!"
  },
  {
    "objectID": "posts/hello_world/index.html",
    "href": "posts/hello_world/index.html",
    "title": "Hello World",
    "section": "",
    "text": "Just testing whether Latex is working correctly: \\[\\frac{1}{2}\\left( \\int \\exp\\left\\{ -\\frac{x^2}{2}\\right\\} \\, dx \\right)^2 \\approx 22/7\\]\nHere is a reference (MacKay 2003) and below is an animation:\n\n\nVideo\n\n\n\n\n\n\nReferences\n\nMacKay, David JC. 2003. Information Theory, Inference and Learning Algorithms. Cambridge university press."
  },
  {
    "objectID": "posts/index_blog.html",
    "href": "posts/index_blog.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Blogposts (vaguely) related to research, notes for students, announcements, things I would like to write down to understand better and/or not forget, etc… Comments, corrections, suggestions are welcome!\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n12-06-2023\n\n\nReverse diffusions, Score & Tweedie\n\n\nSDE,Generative-Model\n\n\n\n\n06-06-2023\n\n\nHello World\n\n\ntesting\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes_DRAFT/SSM/SSM.html#state-space-models",
    "href": "notes_DRAFT/SSM/SSM.html#state-space-models",
    "title": "State Space Models",
    "section": "State Space Models:",
    "text": "State Space Models:\nConsider a Markov chain \\((X_t, Y_t)\\) with \\(X_t \\in \\mathcal{X}\\) and \\(Y_t \\in \\mathcal{Y}\\) with initial distribution \\(p(X_0 \\in dx_0) = \\mu_0(x_0) \\, dx_0\\) and described by the dynamics\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nx_{t+1} &\\sim f(x_{t+1} | x_t) \\\\\ny_{t+1} &\\sim g(y_{t+1} | x_{t+1})\n\\end{aligned}\n\\right.\n\\end{align}\n\\tag{1}\\]\nfor some transition and observation densities \\(f(\\cdot | \\cdot)\\) and \\(g(\\cdot | \\cdot)\\). These functions could be time-dependent but we will assume not for lightening notations."
  },
  {
    "objectID": "notes_DRAFT/SSM/SSM.html#filtering",
    "href": "notes_DRAFT/SSM/SSM.html#filtering",
    "title": "State Space Models",
    "section": "Filtering:",
    "text": "Filtering:\nConsider an empirical approximation of the filtering distribution \\(p(x_t | y_{0:t})\\) using \\(N \\geq 1\\) particles \\((x^1_{t}, \\ldots, x^N_{t})\\),\n\\[\np(x_t | y_{0:t})\n\\approx\n\\sum_{i=1}^N w_{i,t} \\, \\delta_{x^i_{t}}(x_t).\n\\]\nThe filtering distribution at time \\((t+1)\\) follows the recursion:\n\\[\np(x_{t+1} | y_{0:t+1}) \\propto \\int p(x_{t} | y_{0:t}) \\, f(x_{t+1} | x_t) \\, g(y_{t+1} | x_t) \\, dx_t.\n\\]\nThis is the \\(x_{t+1}\\)-marginal of the joint density proportional to \\(p(x_{t} | y_{0:t}) \\, f(x_{t+1} | x_t) \\, g(y_{t+1} | x_t)\\). One can approximate this joint density using importance sampling with a proposal \\(p(x_{t} | y_{0:t}) \\, q_t(x_{t+1} | x_t)\\), where \\(q_t(x_{t+1} | x_t)\\) is any reasonable Markov kernel. The choice of \\(q_t(x_{t+1} | x_t) \\equiv f(x_{t+1} | x_t)\\) leads to the bootstrap particle filter. Alternatively, \\(q_t(x_{t+1} | x_t) = p(x_{t+1} | x_t, y_{t+1})\\) can yield better estimates, but is often hard to implement, though approximating it can be a viable strategy.\nUsing these methods, the filtering distribution at time \\((t+1)\\) can be approximated by a new set of particles \\(\\sum_{i=1}^N w_{i,t+1} \\, \\delta_{x^i_{t+1}}(x_{t+1})\\), where \\(x^{i}_{t+1} \\sim q_t(x^{i}_{t+1} | x^{i}_{t})\\) and weights are calculated as:\n\\[\nw^i_{t+1}  \\propto \\frac{f(x^{i}_{t+1} | x^{i}_{t}) \\, g(y_{t+1} | x^i_{t+1})}{q_t(x^{i}_{t+1} | x^{i}_{t})}.\n\\]"
  },
  {
    "objectID": "notes_DRAFT/SSM/SSM.html#smoothing",
    "href": "notes_DRAFT/SSM/SSM.html#smoothing",
    "title": "State Space Models",
    "section": "Smoothing:",
    "text": "Smoothing:\nWhen estimating parameters in time-series models, it is often crucial to evaluate sums like\n\\[\n\\sum_{t=0}^{t-1} \\mathbb{E}[ s_t(x_t, x_{t+1}) | y_{0:T}]\n\\]\nwhere \\(s_t(\\cdot, \\cdot)\\) is a specific function. For instance, in the Expectation-Maximization (EM) algorithm, we have \\(s_t(x_t, x_{t+1}) = \\log[ f(x_{t+1}|x_t) , g(y_{t+1}|x_{t+1})]\\). Essentially, this means it is important to accurately approximate the \\((x_t, x_{t+1})\\)-marginals of the smoothing distribution.\n\nForward only and Fixed-Lag approximation:\nA standard filtering approach on path-space, often referred to as the “poor man’s smoother”, can be used to approximate the full smoothing distribution. However, this method often leads to path degeneracy, especially for smoothing marginals at time \\(t \\ll T\\), which are typically very poor. In cases where the State-Space Model (SSM) has a “forgetting property”, the fixed-lag approximation can be useful. This is expressed as:\n\\[\np(x_t | y_{0:T}) \\; \\approx \\; p(x_t | y_{0:{t+L}}).\n\\]\nHere, \\(L\\) is the lag parameter, usually chosen to be quite small. To implement this fixed-lag approximation, one simply needs to run a filtering method and track the last \\(L\\) ancestors; that it straightforward since the SSM in Equation 1 can readily be extended to a SSM describing the dynamics of \\((x_{t-L}, \\ldots, x_{t-1}, x_t)\\).\n\n\nForward-Filtering Backward Smoothing:\nThe smoothing distribution satisfy the following backward recursion\n\\[\n\\begin{align}\np(x_t| y_{0:T})\n&=\n\\int p(x_t, x_{t+1} | y_{0:T}) \\, dx_{t+1}\\\\\n&=\n\\underbrace{p(x_t | y_{1:t})}_{\\text{(filtering)}} \\, \\int \\, \\underbrace{p(x_{t+1} | y_{0:T})}_{\\text{(smoothing)}} \\, \\frac{f(x_{t+1}|x_t)}{p(x_{t+1} | y_{1:t})} \\, dx_{t+1}.\n\\end{align}\n\\tag{2}\\]\nTo exploit this backward recursion, one can first a standard filtering methods to obtain particle approximations of all the filtering distributions. It is then straightforward to discretize Equation 2 to either:\n\ngenerate a single trajectory from the smoothing distribution starting from the final time \\(T\\). Generating this single smoothing trajectory can be implementing in \\(\\mathcal{O}(N \\, T)\\).\nreweight the particle approximations of the filtering distributions to obtain particle approximation of the marginal smoothing distributions of \\(x_t\\) or \\((x_t, x_{t+1})\\). This procedure can be implemented in \\(\\mathcal{O}(N^2 \\, T)\\)"
  },
  {
    "objectID": "notes_DRAFT/SSM/SSM_references.html",
    "href": "notes_DRAFT/SSM/SSM_references.html",
    "title": "State Space Models: References",
    "section": "",
    "text": "Monte-Carlo methods for SSM:\n\n(Cappé, Moulines, and Rydén 2009)\n(Douc, Moulines, and Stoffer 2014)\n(Chopin, Papaspiliopoulos, et al. 2020)\n\n(Särkkä and Svensson 2023)"
  },
  {
    "objectID": "notes_DRAFT/SSM/SSM_references.html#books",
    "href": "notes_DRAFT/SSM/SSM_references.html#books",
    "title": "State Space Models: References",
    "section": "",
    "text": "Monte-Carlo methods for SSM:\n\n(Cappé, Moulines, and Rydén 2009)\n(Douc, Moulines, and Stoffer 2014)\n(Chopin, Papaspiliopoulos, et al. 2020)\n\n(Särkkä and Svensson 2023)"
  },
  {
    "objectID": "notes_DRAFT/SSM/SSM_references.html#web",
    "href": "notes_DRAFT/SSM/SSM_references.html#web",
    "title": "State Space Models: References",
    "section": "Web:",
    "text": "Web:\n\nReference webpage maintained by Arnaud Doucet.\nReference webpage maintained by Pierre Del Moral."
  },
  {
    "objectID": "notes_DRAFT/SSM/SSM_references.html#smoothing",
    "href": "notes_DRAFT/SSM/SSM_references.html#smoothing",
    "title": "State Space Models: References",
    "section": "Smoothing:",
    "text": "Smoothing:\n\nFixed Lag Smoothing:\n\n(Kitagawa and Sato 2001): proposes the fixed-lag smoothing approach\n\n\nForward Filtering-Backward Sampling:\n\n(Godsill, Doucet, and West 2004) generates a smoothing trajectory in \\(\\mathcal{O}(N \\, T)\\)"
  },
  {
    "objectID": "publications/index_pubs.html#theory-methods",
    "href": "publications/index_pubs.html#theory-methods",
    "title": "Alexandre Thiéry",
    "section": "THEORY & METHODS:",
    "text": "THEORY & METHODS:\n\n\n\n Normalizing flow regularization for photoacoustic tomography   Chao Wang and Alexandre H. Thiery, (2024)   Inverse Problems, 2024    (Arxiv)   (Journal)  \n\n\n Out-of-Distribution Detection with a Single Unconditional Diffusion Model   Heng, A., Thiery, A. H., Soh, H., (2024)   NeuRIPS (2024)    (Arxiv) \n\n\n Doubly Adaptive Importance Sampling   van den Boom, W., Cremaschi, A., Thiery, A.H, (2024)   Submitted    (Arxiv) \n\n\n Ensemble Kalman Filtering Meets Gaussian Process SSM for Non-Mean-Field and Online Inference   Lin, Z., Sun, Y.,Yin, F., Thiery, A.H, (2023)   IEEE Transactions on Signal Processing    (Arxiv) \n\n\n GIT-Net: Generalized Integral Transform for Operator Learning   Chao Wang and Alexandre H. Thiery, (2023)   Transactions on Machine Learning Research, 2023    (Arxiv)   (Journal)  \n\n\n Computational Doob’s H-transforms for Online Filtering of Discretely Observed Diffusions   Nicolas Chopin, Andras Fulop, Jeremy Heng, Alexandre H. Thiery, (2023)   International Conference on Machine Learning (ICML) 2023    (Arxiv) \n\n\n Conditional sequential Monte Carlo in high dimensions   Axel Finke and Alexandre H. Thiery, (2023)   Annals of Statistics 2023, In Press    (Arxiv)   (Journal)  \n\n\n Pretrained equivariant features improve unsupervised landmark discovery   Rahul Rahaman, Atin Ghosh and Alexandre H. Thiery, (2022)   International Conference of Pattern Recognition (ICPR) 2022    (Arxiv) \n\n\n Manifold lifting: scaling MCMC to the vanishing noise regime   Khai Xiang Au, Matthew Graham, Alexandre H. Thiery, (2022)   JRSSB 2022    (Arxiv)   (Journal)  \n\n\n A discrete Bouncy Particle Sampler   Chris Sherlock and Alexandre H. Thiery, (2022)   Biometrika, Volume 109, Issue 2 (2022)    (Arxiv)   (Journal)  \n\n\n A Generalized & Robust Framework For Timestamp Supervision in Temporal Action Segmentation   Rahul Rahaman, Dipika Singhania, Alexandre H. Thiery, Angela Yao, (2022)   European Conference on Computer Vision (ECCV) 2022    (Journal)  \n\n\n Manifold Markov chain Monte Carlo methods for Bayesian inference in a wide class of diffusion models   Matthew Graham, Alexandre H. Thiery, Alex Beskos, (2022)   JRSSB, Volume 84 (4), 2022    (Arxiv)   (Journal)  \n\n\n Sequential Ensemble Transform for Bayesian Inverse Problems   Aaron Myers, Alexandre H. Thiery, Kainan Wang and Tan Bui-Tanh, (2021)   Journal of Computational Physics, Volume 427 (2021)    (Arxiv)   (Journal)  \n\n\n On Data-Augmentation and Consistency-Based Semi-Supervised Learning   Atin Ghosh and Alexandre H. Thiery, (2021)   ICLR 2021    (Arxiv) \n\n\n Uncertainty Quantification and Deep Ensembles   Rahul Rahaman and Alexandre H. Thiery, (2021)   NeuRIPS 2021    (Arxiv) \n\n\n On importance-weighted autoencoders   Axel Finke and Alexandre H. Thiery, (2019)   Tech Report    (Arxiv) \n\n\n A scalable optimal-transport based local particle filter   Matthew Graham and Alexandre H. Thiery, (2019)   Tech Report    (Arxiv) \n\n\n Particle Filter efficiency under limited communication   Deborshee Sen and Alexandre H. Thiery, (2019)   Tech Report    (Arxiv) \n\n\n Error Bounds for Sequential Monte Carlo Samplers for Multimodal Distributions   Daniel Paulin, Ajay Jasra and Alexandre H. Thiery, (2019)   Bernoulli, Volume 25, Number 1 (2019)    (Arxiv)   (Journal)  \n\n\n Asymptotic Analysis of the Random-Walk Metropolis Algorithm on Ridged Densities   Alex Beskos, Gareth Roberts, Alexandre H. Thiery and Natesh Pillai, (2018)   Annals of Applied Probability, Volume 28, Number 5 (2018)    (Arxiv)   (Journal)  \n\n\n On Coupling Particle Filter Trajectories   Deborshee Sen, Alexandre H. Thiery, Ajay Jasra, (2018)   Statistics and Computing, Volume 28, Number 2 (2018)    (Arxiv)   (Journal)  \n\n\n Levy statistics of interacting Rydberg gases   Thibault Vogt, Jingshan Han, Alexandre H. Thiery, Wenhui Li, (2017)   Physical Review A, Volume 95, Number 5 (2017)    (Arxiv)   (Journal)  \n\n\n Pseudo-marginal Metropolis–Hastings using averages of unbiased estimators   Chris Sherlock, Alexandre H. Thiery and Anthony Lee, (2017)   Biometrika, Volume 104, Number 3 (2017)    (Arxiv)   (Journal)  \n\n\n On the Convergence of Adaptive Sequential Monte Carlo Methods   Alex Beskos, Ajay Jasra, Nikolas Kantas, Alexandre H. Thiery, (2016)   Annals of Applied Probability, Volume 26, Number 2 (2016)    (Arxiv)   (Journal)  \n\n\n Consistency and fluctuations for stochastic gradient Langevin dynamics   Yee Whye Teh, Alexandre H. Thiery, Sebastian Vollmer, (2016)   Journal of Machine Learning Research, Volume 17 (2016)    (Arxiv)   (Journal)  \n\n\n On the efficiency of pseudo-marginal random walk Metropolis algorithms   Chris Sherlock, Alexandre H. Thiery, Gareth Roberts, Jeff Rosenthal, (2015)   Annals of Statistics, Volume 43, Number 1 (2015)    (Arxiv)   (Journal)  \n\n\n On non-negative unbiased estimators   Pierre Jacob, Alexandre H. Thiery, (2015)   Annals of Statistics, Volume 43, Number 2 (2015)    (Arxiv)   (Journal)  \n\n\n Efficiency of delayed-acceptance random walk Metropolis algorithms   Chris Sherlock, Alexandre H. Thiery and Andrew Golightly, (2015)   Annals of Statistics (In Press)    (Arxiv)   (Journal)  \n\n\n Noisy gradient flow from a random walk in Hilbert space   Natesh Pillai, Andrew Stuart, Alexandre H. Thiery, (2014)   Stochastic Partial Differential Equations: Analysis and Computations, Volume 2, Number 2 (2014)    (Arxiv)   (Journal)  \n\n\n Optimal Scaling and Diffusion Limits for the Langevin Algorithm in High Dimensions   Natesh Pillai, Andrew Stuart, Alexandre H. Thiery, (2012)   Annals of Applied Probability, Volume 22, Number 6 (2012)    (Arxiv)   (Journal)  \n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/index_pubs.html#applications",
    "href": "publications/index_pubs.html#applications",
    "title": "Alexandre Thiéry",
    "section": "APPLICATIONS",
    "text": "APPLICATIONS\n\n\n\n Three-Dimensional Structural Phenotype of the Optic Nerve Head as a Function of Glaucoma Severity   Braeu, F.A., Chuangsuwanich, T., Tun, T.A., Perera, S.A., Husain, R., Kadziauskienė, A., Schmetterer, L., Thiéry, A.H., Barbastathis, G., Aung, T. and Girard, M.J.A., (2023)   JAMA Ophthalmology, 2023    (Arxiv)   (Journal)  \n\n\n Towards Label-Free 3D Segmentation of Optical Coherence Tomography Images of the Optic Nerve Head Using Deep Learning   Devalla,S.K., Pham, T.H., Panda, S.K, Zhang,L., Subramanian,G., Swaminathan,A., Chin,Z.Y, Rajan,M., Mohan,S., Krishnadas,R., Senthil,V., Leon,J.M, Tun,T.A., Cheng,C.Y., Schmetterer, L., Perera,S., Aung,T., Thiery,A.H., Girard,M.J.A., (2023)   Biomedical Optics Express, Vol. 11, Issue 11    (Arxiv)   (Journal)  \n\n\n Geometric Deep Learning to Identify the Critical 3D Structural Features of the Optic Nerve Head for Glaucoma Diagnosis   Braeu, F., Thiery, A.H, Tun, T.A., Kadziauskiene, A., Barbastathis, G., Aung, T., and Girard. M.J.A., (2023)   American Journal of Ophthalmology, 2023    (Arxiv)   (Journal)  \n\n\n Medical Application of Geometric Deep Learning for the Diagnosis of Glaucoma   Thiery, A.H, Braeu, F., Tun, T.A., Aung, T., Girard. M.J.A., (2023)   Translational Vision Science and Technology, 2023    (Arxiv)   (Journal)  \n\n\n Detection of m6A from direct RNA sequencing using a Multiple Instance Learning framework   Hendra C, Pratanwanich PN, Wan YK, Goh WS, Thiery A, Göke J+., (2022)   Nature Methods (2022)    (Arxiv)   (Journal)  \n\n\n Describing the Structural Phenotype of the Glaucomatous Optic Nerve Head Using Artificial Intelligence   Panda, S.K., Cheong, H., Tun,T.A., Devella, S.K., Krishnadas, R., Buist, M.L., Perera, S., Cheng, C-Y., Aung, T., Thiery A.H., Girard, M.J.A., (2022)   American Journal of Ophthalmology, 2022    (Arxiv)   (Journal)  \n\n\n The Three-Dimensional Structural Configuration of the Central Retinal Vessel Trunk and Branches as a Glaucoma Biomarker   Satish K Panda, Haris Cheong, Tin A Tun, Thanadet Chuangsuwanich, Aiste Kadziauskiene, Vijayalakshmi Senthil, Ramaswami Krishnadas, Martin L Buist, Shamira Perera, Ching-Yu Cheng, Tin Aung, Alexandre H Thiery, Michaël JA Girard, (2022)   American Journal of Ophthalmology, 2022    (Journal)  \n\n\n Deep Learning Algorithms to Isolate and Quantify the Structures of the Anterior Segment in Optical Coherence Tomography Images   Pham, T.H, Devalla,S.K., Ang, A., Da,S.Z., Thiery,A.H., Boote,C., Cheng,C.Y., Koh,V., Girard, M.J.A., (2021)   British Journal of Ophthalmology    (Arxiv)   (Journal)  \n\n\n Identification of differential RNA modifications from nanopore direct RNA sequencing with xPore   Ploy N. Pratanwanich, Fei Yao, Ying Chen, Casslynn W.Q. Koh, Christopher Hendra, Polly Poon, Yeek Teck Goh, Phoebe M. L. Yap, Choi Jing Yuan, Wee Joo Chng, Sarah Ng, Alexandre Thiery, W.S. Sho Goh, Jonathan Goeke, (2021)   Nature Biotechnology, 2021    (Arxiv)   (Journal)  \n\n\n Beyond quadratic error: Case-study of a multiple criteria approach to the performance assessment of numerical forecasts of solar irradiance in the tropics   Verbois, H., Blanc, P., Huva, R., Saint-Drenan, Y-M, Rusydi, A.. Thiery, A., (2020)   Renewable and Sustainable Energy Reviews, Volume 117, (2020)    (Journal)  \n\n\n NanoVar: Accurate Characterization of Patients Genomic Structural Variants Using Low-Depth Nanopore Sequencing   Tham, C.Y, Tirado-Magallanes, R., Goh, Y., Fullwood, M. J., Koh, B.T.H. , Wang, W., Ng, C.H, Chng, W.J., Thiery, A.H., Tenen, D.G, Benoukraf, (2020)   Genome Biology (2020)    (Arxiv)   (Journal)  \n\n\n DeshadowGAN: A Deep Learning Approach to Remove Shadows from Optical Coherence Tomography Images   Cheong, H., Devalla, S.K., Pham, T.H., Liang, Z., Tun, T.A., Wang, X., Perera, S., Schmetterer, L., Tin, A., Boote, C., Thiery, A.H., Girard, M.J.A., (2020)   Translational Vision Science & Technology    (Arxiv)   (Journal)  \n\n\n Deep Learning Algorithms to Isolate and Quantify the Structures of the Anterior Segment in Optical Coherence Tomography Images   Pham, T.H, Devalla,S.K., Ang, A., Da, S.Z., Thiery, A.H., Boote,C., Cheng,C.Y., Koh,V., Girard, M.J.A, (2020)   British Journal of Ophthalmology, 2020    (Arxiv)   (Journal)  \n\n\n A Deep Learning Approach to Denoise Optical Coherence Tomography Images of the Optic Nerve Head   Devalla SK, Subramanian G, Pham TH, Wang X, Perera S, Tun TA, Aung T, Schmetterer L, Thiery A.H., Girard MJA, (2019)   Scientific Reports (2019)    (Arxiv)   (Journal)  \n\n\n Glaucoma management in the era of artificial intelligence   Devalla S.K., Liang Z., Pham T.H., Boote, C., Strouthidis, N.G., Thiery A.H., Girard M.J.A., (2019)   British Journal of Ophthalmology (2019)    (Journal)  \n\n\n DRUNET: A Dilated-Residual U-Net Deep Learning Network to Digitally Stain Optic Nerve Head Tissues in Optical Coherence Tomography Images   Devalla SK, Renukanand PK, Sreedhar BK, Perera SA, Mari JM, Chin KS, Tun TA, Strouthidis N, Aung T, Thiery A.H., Girard MJA, (2018)   Biomedical Optics Express, Vol. 9, Issue 7 (2018)    (Arxiv)   (Journal)  \n\n\n Probabilistic forecasting of day-ahead solar irradiance using quantile gradient boosting   Verbois, H., Rusydi, A., Thiery, A.H., (2018)   Solar Energy 173, 313-327 (2018)    (Journal)  \n\n\n\nNo matching items"
  },
  {
    "objectID": "about/about.html",
    "href": "about/about.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Email: a.h.thiery [at] nus.edu.sg\nOffice: Building S16, Office #06-113"
  },
  {
    "objectID": "about/about.html#address",
    "href": "about/about.html#address",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Email: a.h.thiery [at] nus.edu.sg\nOffice: Building S16, Office #06-113"
  },
  {
    "objectID": "about/about.html#qualifications",
    "href": "about/about.html#qualifications",
    "title": "Alexandre Thiéry",
    "section": "Qualifications",
    "text": "Qualifications\n\nPh.D., Probability & Statistics, Warwick University, 2009-2013.\nEcole Normale Superieure of Paris, Mathematics, 2005-2009\n\nResearch Assistant, Statslab (Cambridge, UK)\nMSc (Probability & Finance), University of Paris VI\nMSc (Partial Differential Equations & Modeling), University of Paris VI"
  },
  {
    "objectID": "about/about.html#employment-history",
    "href": "about/about.html#employment-history",
    "title": "Alexandre Thiéry",
    "section": "Employment history",
    "text": "Employment history\n\nAssociate Professor, Department of Statistics & Data Sciences, NUS, 2020–present.\n\nAffiliate, NUS Artificial Intelligence Institute (NAII), 2025–present\nAffiliate, NUS Centre for Data Science and Machine Learning, 2021–present\nAffiliate, NUS Institute of Data Science, 2028–present\nAffiliate, NUS Graduate School for Integrative Sciences and Engineering, 2017–present\n\nAssistant Professor, Department of Statistics & Probability, NUS, 2014–2019.\nResearch Fellow, Department of Statistics & Probability, NUS, 2013"
  },
  {
    "objectID": "about/about.html#leadership",
    "href": "about/about.html#leadership",
    "title": "Alexandre Thiéry",
    "section": "Leadership",
    "text": "Leadership\n\nDeputy Director of Institute for Mathematical Sciences, 7/2020 – 12/2023"
  },
  {
    "objectID": "about/about.html#service",
    "href": "about/about.html#service",
    "title": "Alexandre Thiéry",
    "section": "Service",
    "text": "Service\n\nArea Chair for AISTAT (2023–2025), ACML (2023), NeurIPS (2023, 2024), ICLR (2024, 2025)\nAssociate Editor for Statistics & Computing (2020–2022)"
  },
  {
    "objectID": "about/about.html#awards-and-honours",
    "href": "about/about.html#awards-and-honours",
    "title": "Alexandre Thiéry",
    "section": "Awards and honours",
    "text": "Awards and honours\n\nNUS Faculty of Sciences Dean’s Chair Associate Professor, 2022–2025\nNUS Faculty Teaching Excellence Award, 2022\nNUS Faculty Teaching Excellence Award, 2019\nNUS Faculty Teaching Excellence Award, 2018\nNUS Young Scientist Award. (Faculty of Science: 1 awardee per year), 2017\nNUS Young Investigator Award. (Faculty of Science: 3 awardees per year), 2016\nJohn Copas prize for the best PhD dissertation, 2013"
  },
  {
    "objectID": "about/about.html#industrial-activities",
    "href": "about/about.html#industrial-activities",
    "title": "Alexandre Thiéry",
    "section": "Industrial Activities",
    "text": "Industrial Activities\n\nCo-founder of Abyss Processing, start-up developing AI solutions for ophthalmology\nConsulting in the finance and health-care industries"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Google-Scholar\n  \n  \n    \n     Arxiv\n  \n  \n    \n     twitter\n  \n\n      \nAssociate Professor\nDepartment of Statistics & Data Science\nNational University of Singapore\n\n\n\nComputational Statistics & Machine Learning\nMonte-Carlo methods\nUncertainty Quantification\nInverse problems\n\n\n\n\n\nTwo Research Fellow positions: Data Assimilation\n\n\n\n\nOffice: Building S16, Office #06-113\nEmail: a.h.thiery [at] nus.edu.sg"
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Computational Statistics & Machine Learning\nMonte-Carlo methods\nUncertainty Quantification\nInverse problems"
  },
  {
    "objectID": "index.html#available-positions",
    "href": "index.html#available-positions",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Two Research Fellow positions: Data Assimilation"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Office: Building S16, Office #06-113\nEmail: a.h.thiery [at] nus.edu.sg"
  },
  {
    "objectID": "notes/MCMC_on_manifold/mcmc_manifold.html",
    "href": "notes/MCMC_on_manifold/mcmc_manifold.html",
    "title": "RWM & HMC on manifolds",
    "section": "",
    "text": "Consider a smooth manifold \\(\\mathcal{M}\\subset \\mathbb{R}^n\\) of dimension \\(d_{\\mathcal{M}} = (n-d)\\) defined as the zero set of a well-behaved “constraint” function \\(C: \\mathbb{R}^n \\to \\mathbb{R}^d\\),\n\\[\n\\mathcal{M}= \\{ x \\in \\mathbb{R}^n \\; \\text{such that} \\; C(x) = 0 \\}.\n\\]\nWe would like to use MCMC to sample from a probability distribution supported on \\(\\mathcal{M}\\) with density \\(\\pi(x)\\) with respect to the uniform Hausdorff measure on \\(\\mathcal{M}\\). It is relatively straightforward to adapt standard MCMC methods when dealing with simple manifolds such as a sphere or a torus since their geodesics and several other geometric quantities are analytically tractable. Maybe surprisingly, it is in fact relatively straightforward to design MCMC samplers on general implicitly defined manifold such as \\(\\mathcal{M}\\). The article (Zappa, Holmes-Cerfon, and Goodman 2018) explains these ideas beautifully.\n\nManifold Random Walk Metropolis-Hastings\nAssume that \\(x_n \\in \\mathcal{M}\\) is the current position of the MCMC chain. To generate a proposal \\(y_n \\in \\mathcal{M}\\) that will eventually be accepted or rejected, one can proceed very similarly to the standard RWM algorithm with Gaussian perturbations with variance \\(\\sigma^2\\). First, generate a vector \\(v \\in T_{x_n}\\) from a centred Gaussian distribution with covariance \\(\\sigma^2 \\, I\\) on the tangent space \\(T_{x_n}\\) to \\(\\mathcal{M}\\) at \\(x_n\\). To do so, it suffices for example to generate a standard Gaussian vector \\(z \\sim \\mathcal{N}(0, \\sigma^2 I_n)\\) in \\(\\mathbb{R}^n\\) and orthogonal-project it onto \\(T_{x_n}\\). Indeed, one cannot simply define the proposal as \\(x_n + v\\) since it would not necessarily lie on \\(\\mathcal{M}\\). Instead, one projects \\(x_n + v\\) back to \\(\\mathcal{M}\\). To do so, one needs to define the direction used for the projection and the manifold RWM algorithm uses \\(T_{x_n}^\\perp\\), for reasons that will become clear later. In other words, the proposal \\(y_n\\) is obtained by seeking a vector \\(w \\in T_{x_n}^{\\perp}\\) such that \\(x_n + v + w \\in \\mathcal{M}\\).\n\n\n\n\nProjection onto \\(\\mathcal{M}\\) from (Zappa, Holmes-Cerfon, and Goodman 2018)\n\n\n\nIf one calls \\(J_{x_n}\\) the Jacobian matrix of \\(C\\) at \\(x_n\\), i.e. the matrix whose rows are the gradients of the components of \\(C\\), this projection operation boils down to finding a vector \\(\\lambda \\in \\mathbb{R}^d\\) such that\n\\[\nC( \\, x_n + v + J_{x_n}^\\top \\lambda) = 0 \\in \\mathbb{R}^d.\n\\tag{1}\\]\nNote that Equation 1 is a non-linear equation in \\(\\lambda\\) that can have no solution, one solution or many solutions – this can seem like a fundamental roadblock to the design of a valid MCMC algorithm, but we will see that it is not! Before discussing in slightly more details the resolution of Equation 1, assume that a standard root-finding algorithm takes the pair \\((x_n+v, J_{x_n})\\) as input and attempts to produces the projection \\(y_n\\),\n\\[\n\\text{Proj}: \\quad (x_n+v, J_{x_n}) \\; \\underbrace{\\mapsto}_{\\text{root-finding}} \\; y_n \\in \\mathcal{M}.\n\\]\nThe algorithm will either converge to one of the possible solutions or fail. If the algorithm fails to converge, one can simply reject the proposal \\(y_n\\) and set \\(y_n = \\text{(Failed)}\\) and set \\(x_{n+1} = x_n\\). If the algorithm converges, this defines a valid proposal \\(y_n \\in \\mathcal{M}\\). To ensure reversibility, and it is one of the main novelty of the article (Zappa, Holmes-Cerfon, and Goodman 2018), one needs to verify that the reverse proposal \\(y_n \\mapsto x_n\\) is possible.\n\n\n\n\nReversibility check (Zappa, Holmes-Cerfon, and Goodman 2018)\n\n\n\nTo do so, note that the only possibility for the reverse move \\(y_n \\to x_n\\) to happen is if \\(x_n = \\text{Proj}(y_n + v', J_{y_n})\\) where\n\\[\nx_n-y_n \\;=\\; \\underbrace{v'}_{\\in T_{y_n}}  \\, + \\, \\underbrace{w'}_{\\in T_{y_n}^{\\perp}}.\n\\]\nThe uniqueness follows from the decomposition \\(\\mathbb{R}^n \\equiv T_{y_n} \\otimes T_{y_n}^{\\perp}\\). The reverse move is consequently possible if and only if the following reversibility check condition is satisfied,\n\\[\nx_n = \\text{Proj}(y_n + v', J_{y_n}).\n\\tag{2}\\]\nThis reversibility check is necessary as it is not guaranteed that the root-finding algorithm started from \\(y_n + v'\\) converges at all, or converges to \\(x_n\\) in the case when there are several solutions. If Equation 2 is not satisfied, the proposal \\(y_n\\) is rejected and one sets \\(x_{n+1} = x_n\\). On the other hand, if Equation 2 is satisfied, the proposal \\(y_n\\) is accepted with the usual Metropolis-Hastings probability\n\\[\n\\min \\left\\{1, \\frac{\\pi(y_n) \\, p(v'|x_n)}{\\pi(x_n) \\, p(v|x_n)} \\right\\}\n\\]\nwhere \\(p(v|x) = Z^{-1} \\, \\exp(-\\|v\\|^2 / 2 \\sigma^2)\\) denotes the Gaussian density on the tangent space \\(T_{x_n}\\) The above description defines a valid MCMC algorithm on \\(\\mathcal{M}\\) that is reversible with respect to the target distribution \\(\\pi(x)\\).\n\n\nProjection onto the manifold\nAs described above, the main difficulty is to solve the non-linear equation Equation 1 describing the projection of the proposal \\((x_n + v)\\) back to the manifold \\(\\mathcal{M}\\). The projection is along the space spanned by the columns of \\(J_{x_n}^\\top \\in \\mathbb{R}^{n,d}\\), i.e. find a vector \\(\\lambda \\in \\mathbb{R}^d\\) such that\n\\[\n\\Phi(\\lambda) = C( \\, x_n + v + J_{x_n}^\\top \\lambda) = 0 \\in \\mathbb{R}^d.\n\\]\nOne can use a standard Newton’s method to solve this equation started from \\(\\lambda_0=0\\). Setting for notational convenience \\(q(\\lambda) = x_n + v + J_{x_n}^T \\, \\lambda\\), this boils down to iterating\n\\[\n\\lambda_{k+1} - \\lambda_{k}\n=\n- \\left( J_{q(\\lambda_k)} \\, J_{x_n}^\\top \\right)^{-1} \\, \\Phi(\\lambda_k).\n\\]\nAs described in (Barth et al. 1995), it can sometimes be computationally advantageous to use a quasi-Newton method and use instead\n\\[\n\\lambda_{k+1} - \\lambda_{k}\n=\n- G^{-1} \\, \\Phi(\\lambda_k)\n\\]\nwith fixed positive definite matrix \\(G = J_{x_n} \\, J_{x_n}^\\top\\) since one can then pre-compute a decomposition of \\(G\\) and use it to solve the linear systems at each iterations. In some recent and related work (Au, Graham, and Thiery 2022), we observed that the standard Newton method performed well in the settings we considered and there was most of the time no computational advantage to using a quasi-Newton method. In practice, the main computational bottleneck is to compute the Jacobian matrix \\(J_{x_n}\\), although it is problem-dependent and some structure can typically be exploited. In practice, only a relatively small number of iterations are performed and the root-finding algorithm is stopped as soon as \\(\\|\\Phi(\\lambda_k)\\|\\) is below a certain threshold. If the stepsize is small, i.e. \\(\\|v\\| \\ll 1\\), it is typically the case that the Newton’s method will converge to a solution in only a very small number of iterations – indeed, Newton’s method is quadratically convergent when close to a solution.\n\n\n\n\n30k RWM chains ran in parallel to explore a double torus.\n\n\n\nIn the figure above, I have implemented the RWM algorithm above described to sample from the uniform distribution supported on a double torus described by the constraint function \\(C: \\mathbb{R}^3 \\to \\mathbb{R}\\) given as\n\\[\nC(x,y,z) = (x^2 \\, (x^2 - 1) + y^2)^2+z^2-0.03.\n\\]\nThe figure shows \\(30,000\\) chains ran in parallel, which is straightforward to implement in practice with JAX (Bradbury et al. 2018). All the chains are initialized from the same position so that one can visualize the evolution of the density of particles.\n\n\n\n\nTuning of manifold-RWM\n\n\n\nOne can for example monitor the usual expected squared jump distance\n\\[\n\\textrm{(ESJD)} \\equiv \\mathbb{E}[\\|X_{n+1} - X_n\\|^2]\n\\]\nand maximize it to tune the RWM step-size; it would probably make slightly more sense to monitor the squared geodesic distances instead the naive squared norm \\(\\|X_{n+1} - X_n\\|^2\\), but that’s way to much hassle and would probably make only a negligible difference. In the figure above, I have plotted the expected squared jump distance as a function of the acceptance rate for different step-sizes. It is interesting to see a pattern extremely similar to the one observed in the standard RWM algorithm (Roberts and Rosenthal 2001): in this double torus example, the optimal acceptance rate is around \\(25\\%\\). Note that since the target distribution is uniform, the rate of acceptance is only very slightly lower than the proportion of successful reversibility checks.\n\n\nHamiltonian Monte Carlo (HMC) on manifolds\nWhile the Random Walk Metropolis-Hastings algorithm is interesting, exploiting gradient information is often necessary to design efficient MCMC samplers. Consider a single iteration of a standard Hamiltonian Monte Carlo (HMC) sampler targeting a density \\(\\pi(q)\\) on \\(q \\in \\mathbb{R}^n\\). The method proceeds by simulating from a dynamics that is reversible with respect to an extended target density \\(\\bar{\\pi}(q,p)\\) on \\(\\mathbb{R}^n \\otimes \\mathbb{R}^n\\) defined as\n\\[\n\\begin{aligned}\n\\bar{\\pi}(q,p)\n&\\propto \\pi(q) \\, \\exp \\left\\{ -\\frac{1}{2m} \\|p\\|^2 \\right\\}\\\\\n&= \\exp\\left\\{ \\log \\pi(q) - K(p) \\right\\}\n\\end{aligned}\n\\]\nfor a user-defined mass parameter \\(m &gt; 0\\). In general, the mass parameter is a positive definite matrix but generalizing this to manifolds is slightly less useful in practice. For a time-discretization step \\(\\varepsilon&gt; 0\\) and a current position \\((q_n,p_n)\\), the method proceeds by generating a proposal \\((q_{*},p_{*})\\) defined as\n\\[\n\\left\\{\n\\begin{aligned}\np_{n+1/2} &= p_n + \\frac{\\varepsilon}{2} \\nabla \\log \\pi(q_n)\\\\\nq_{*} &= q_n + \\varepsilon\\, m^{-1} \\, p_{n+1/2}\\\\\np_{*} &= p_{n+1/2} + \\frac{\\varepsilon}{2} \\nabla \\log \\pi(q_{*}).\n\\end{aligned}\n\\right.\n\\]\nThis proposal is accepted with probability \\(\\min\\left( 1, \\bar{\\pi}(q_*, p_*)/\\bar{\\pi}(q_n, p_n) \\right)\\). Indeed, in standard implementation, several leapfrog steps are performed instead of a single one. One can also choose to perform a single leapfrog step as above and only do a partial refreshment of the momentum after each leapfrog step – this may be more efficient or easier to implement when running a large number of HMC chains in parallel on a GPU for example. To adapt the HMC algorithm to sample from a density \\(\\pi(q)\\) supported on a manifold \\(\\mathcal{M}\\), one can proceed similarly to the RWM algorithm by interleaving additional projection steps. These projections are needed to ensure that the momentum vectors \\(p_n\\) remain in the right tangent spaces and the position vectors \\(q_n\\) remain on the manifold \\(\\mathcal{M}\\),\n\\[\n(q_n, p_n) \\; \\in \\; \\mathcal{M}\\times T_{q_n}.\n\\]\nAs in the RWM algorithm, reversibility checks need to be performed to ensure that the overall algorithm is reversible with respect to the target distribution \\(\\overline{\\pi}(q,p)\\). The resulting algorithm for generating a proposal \\((q_n, p_n) \\mapsto (q_*, p_*)\\) reads as follows:\n\\[\n\\left\\{\n\\begin{aligned}\n\\widetilde{p}_{n+1/2} &= p_n + \\frac{\\varepsilon}{2} \\nabla \\log \\pi(q_n)\\\\\np_{n+1/2} &=  \\textcolor{red}{\\text{orthogonal project $\\widetilde{p}_{n+1/2}$ onto $T_{q_n}$}} \\\\\n\\widetilde{q}_{*} &= q_n + \\varepsilon\\, m^{-1} \\, p_{n+1/2}\\\\\nq_{*} &=  \\textcolor{red}{\\text{Proj$(\\widetilde{q}_{*}, J_{q_n})$}}\\\\\n\\overline{p}_{n+1/2} &=  \\textcolor{red}{\\text{orthogonal project $(q_{*}-q_n) \\, m / \\varepsilon$ onto $T_{q_{*}}$}} \\\\\n\\widetilde{p}_{*} &= \\overline{p}_{n+1/2} + \\frac{\\varepsilon}{2} \\nabla \\log \\pi(q_{*})\\\\\np_{*} &=  \\textcolor{red}{\\text{orthogonal project $\\widetilde{p}_{*}$ onto $T_{q_{*}}$}}.\n\\end{aligned}\n\\right.\n\\]\nIf any of the projection operations fail, the proposal is rejected. If no failure occurs, a reversibility check is performed by running the algorithm backward starting from \\((q_*, -p_*)\\). If the reversibility check is successful, the proposal is accepted with the usual Metropolis-Hastings probability \\(\\min\\left( 1, \\bar{\\pi}(q_*, p_*)/\\bar{\\pi}(q_n, p_n) \\right)\\).\n\n\n\n\n5k HMC chains ran in parallel: the momentum is not refreshed\n\n\n\nThe article (Lelièvre, Rousset, and Stoltz 2019) provides a detailed description of several of these ideas along with detailed analysis and extensions.\n\n\n\n\n\nReferences\n\nAu, Khai Xiang, Matthew M Graham, and Alexandre H Thiery. 2022. “Manifold Lifting: Scaling MCMC to the Vanishing Noise Regime.” Journal of the Royal Statistical Society: Series B. https://arxiv.org/abs/2003.03950.\n\n\nBarth, Eric, Krzysztof Kuczera, Benedict Leimkuhler, and Robert D Skeel. 1995. “Algorithms for Constrained Molecular Dynamics.” Journal of Computational Chemistry 16 (10). Wiley Online Library: 1192–1209. https://doi.org/10.1002/jcc.540161003.\n\n\nBradbury, James, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, et al. 2018. “JAX: Composable Transformations of Python+NumPy Programs.” http://github.com/google/jax.\n\n\nLelièvre, Tony, Mathias Rousset, and Gabriel Stoltz. 2019. “Hybrid Monte Carlo Methods for Sampling Probability Measures on Submanifolds.” Numerische Mathematik 143 (2). Springer: 379–421. https://arxiv.org/abs/1807.02356.\n\n\nRoberts, Gareth O, and Jeffrey S Rosenthal. 2001. “Optimal Scaling for Various Metropolis-Hastings Algorithms.” Statistical Science 16 (4). Institute of Mathematical Statistics: 351–67. https://doi.org/10.1214/ss/1015346320.\n\n\nZappa, Emilio, Miranda Holmes-Cerfon, and Jonathan Goodman. 2018. “Monte Carlo on Manifolds: Sampling Densities and Integrating Functions.” Communications on Pure and Applied Mathematics 71 (12). Wiley Online Library: 2609–47. https://arxiv.org/abs/1702.08446."
  },
  {
    "objectID": "notes/DDPM_deterministic/DDPM_deterministic.html",
    "href": "notes/DDPM_deterministic/DDPM_deterministic.html",
    "title": "From Denoising Diffusion to ODEs",
    "section": "",
    "text": "Consider an empirical data distribution \\(\\pi_{\\mathrm{data}}\\). In order to simulate approximate samples from \\(\\pi_{\\mathrm{data}}\\), Denoising Diffusion Probabilistic Models (DDPM) simulate a forward diffusion process \\(\\{X_t\\}_{[0,T]}\\) on an interval \\([0,T]\\). The diffusion is initialized at the data distribution, i.e. \\(X_0 \\sim \\pi_{\\mathrm{data}}\\), and is chosen so that that the distribution of \\(X_T\\) is very close to a known and tractable reference distribution \\(\\pi_{\\mathrm{ref}}\\), e.g. a Gaussian distribution. Denote by \\(p_t(dx)\\) the marginal distribution at time \\(0 \\leq t \\leq T\\), i.e. \\(\\mathbb{P}(X_t \\in dx) = p_t(dx)\\). By choosing the forward distribution with simple and tractable transition probabilities, e.g. an Ornstein-Uhlenbeck, it is relatively easy to estimate \\(\\nabla \\log p_t(x)\\) from simulated data: this can be formulated as a simple regression problem. This allows one to simulate the diffusion backward in time and generate approximate samples from \\(\\pi_{\\mathrm{data}}\\). Why this is useful is another question…\nThe fact that the mapping from data-samples at time \\(t=0\\) to (approximate) Gaussian samples at time \\(t=T\\) is stochastic and described by diffusion processes is cumbersome. This would be much more convenient to build a deterministic mapping between the data-distribution \\(\\pi_{\\mathrm{data}}\\) and the Gaussian reference distribution \\(\\pi_{\\mathrm{ref}}\\): this would allows one to associate a likelihood to data samples and to easily “encode”/“decode” data-samples. To so this, one can try to replace diffusion by Ordinary Differential Equations."
  },
  {
    "objectID": "notes/DDPM_deterministic/DDPM_deterministic.html#likelihood-computation",
    "href": "notes/DDPM_deterministic/DDPM_deterministic.html#likelihood-computation",
    "title": "From Denoising Diffusion to ODEs",
    "section": "Likelihood computation",
    "text": "Likelihood computation\nWith the diffusion-ODE trick, we have just seen that it is possible to build a vector fields \\(F[0,T] \\times \\mathbb{R}^d \\to \\mathbb{R}^d\\) such that the forward ODE\n\\[\n\\frac{d}{dt} \\overrightarrow{Y_t} =\nF(t,\\overrightarrow{Y_t})\n\\qquad \\textrm{initialized at} \\qquad\n\\overrightarrow{Y}_0 \\sim \\pi_{\\mathrm{data}}\n\\tag{3}\\]\nand the backward ODE defined as\n\\[\n\\frac{d}{ds} \\overleftarrow{Y_s} =\n-F(T-s,\\overleftarrow{Y_s})\n\\qquad \\textrm{initialized at} \\qquad\n\\overleftarrow{Y}_0 \\sim \\pi_{\\mathrm{ref}}\n\\]\nare such that \\(\\overrightarrow{Y}_T \\approx \\pi_{\\mathrm{ref}}\\) and \\(\\overleftarrow{Y}_T \\approx \\pi_{\\mathrm{data}}\\).\nIn general, consider a vector field \\(F(t,x)\\) and a bunch of particles distributed according to a distribution \\(p_t\\) at time \\(t\\). If each particle follows the vector field for an amount of time \\(\\delta \\ll 1\\), the particles that were in the vicinity of some \\(x \\in \\mathbb{R}^d\\) at time \\(t\\) end up in the vicinity of \\(x + F(x,t) \\, \\delta\\) at time \\(t+\\delta\\). At the same time, a volume element \\(dx\\) around \\(x \\in \\mathbb{R}^d\\) gets stretch by a factor \\(1+\\delta \\, \\mathop{\\mathrm{Tr}}[\\mathrm{Jac}F(x,t)] = 1 + \\delta \\mathop{\\mathrm{div}}F(x,t)\\) while following the vector field \\(F\\), which means that the density of particles at time \\(t+\\delta\\) and around \\(x + F(x,t) \\, \\delta\\) equals \\(p_t(x) / [1 + \\delta \\mathop{\\mathrm{div}}F(x,t)]\\). In other words \\(\\log p_{t+\\delta}(x + F(x,t) \\, \\delta) \\approx \\log p_t(x) - \\delta \\, \\mathop{\\mathrm{div}}F(x,t)\\). This means that if we follows a trajectory of \\(\\tfrac{d}{dt} X_t = F(t,X_t)\\) one gets\n\\[\n\\log p_T(X_T) = \\log p_0(X_0) - \\int_{t=0}^{T} \\mathop{\\mathrm{div}}F(X_t,t) \\, dt.\n\\]\nThat is the Lagrangian description of the density \\(p_t\\) of particles. Indeed, one could directly get this identity by differentiating \\(p_t(X_t)\\) with respect to time while using the continuity Equation 1. When applied to the DDPM, this gives a way to assign likelihood the data samples, namely\n\\[\n\\log \\pi_{\\mathrm{data}}(x) = \\log \\pi_{\\mathrm{ref}}(\\overrightarrow{Y_T}) + \\int_{t=0}^{T} \\mathop{\\mathrm{div}}F(t, \\overrightarrow{Y_t})\\, dt\n\\]\nwhere \\(\\overrightarrow{Y_t}\\) is trajectory of the forward ODE Equation 3 initialized as \\(\\overrightarrow{Y_0} = x\\). Note that in high-dimensional setting, it may be computationally expensive to compute the divergence term \\(\\mathop{\\mathrm{div}}F(t, \\overrightarrow{Y_t})\\) since it typically is \\(d\\) times slower that a gradient computation; for this reason, it is often advocated to use the Hutchinson trace estimator to get an unbiased estimate of it at a much lower computational cost."
  },
  {
    "objectID": "notes/reverse_and_tweedie/reverse_and_tweedie.html",
    "href": "notes/reverse_and_tweedie/reverse_and_tweedie.html",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "",
    "text": "Imagine a scalar diffusion process \\(\\{X_t\\}_{t=0}^T\\) defined on the interval \\([0,T]\\),\n\\[dX = \\mu(X) \\, dt + \\sigma \\, dW\\]\nwhere \\(\\mu(X)\\) is the drift term, \\(\\sigma\\) is the diffusion coefficient, and \\(dW\\) is a Wiener process. Denote the distribution of this process at time \\(t\\) (\\(0 \\leq t \\leq T\\)) as \\(p_t(dx)\\) with an initial distribution of \\(p_0(dx)\\). Now, what happens if we reverse time and examine the process backward? In other words, consider the time-reversed process \\(\\overleftarrow{X}\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIntuitively, the process \\(\\overleftarrow{X}\\) is also a diffusion on the interval \\([0, T]\\), but with an initial distribution \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). To gain intuition, consider an Euler discretization of the forward process:\n\\[X_{t+\\delta} = X_{t} + \\mu(X_t)\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, \\mathbf{n} \\tag{1}\\]\nwhere \\(\\mathbf{n}\\sim \\mathcal{N}(0,1)\\) represents a noise term independent from \\(X_{t}\\), and \\(\\delta \\ll 1\\) is a time increment. Re-arranging terms and making the approximation \\(\\mu(X_t) \\approx \\mu(X_{t+\\delta})\\) gives that\n\\[X_{t} \\approx X_{t+\\delta} - \\mu(X_{t+\\delta})\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, (-\\mathbf{n}). \\tag{2}\\]\nThis seems to suggest that the time-reversed process follows the dynamics \\(d\\overleftarrow{X} = -\\mu(\\overleftarrow{X}) \\, dt + \\sigma \\, dW\\) started from \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). However, this conclusion is incorrect because this would suggest that the time-reversed of a standard Brownian motion (where \\(\\mu(x) \\equiv 0\\)) starting at zero is also a Brownian motion starting at \\(p_T(dx) = \\mathcal{N}(0,T)\\), which is clearly not the case. The flaw in this argument lies in assuming that the noise term \\(Z\\) is independent of \\(X_{t+\\delta}\\), which is not true, rendering the Euler discretization argument invalid.\nDeriving the dynamics of the backward process in a rigorous manner is not straightforward (Anderson 1982) (Haussmann and Pardoux 1986). What follows is a heuristic derivation that proceeds by estimating the mean and variance of \\(X_{t}\\) given \\(X_{t+\\delta} = x_{t+\\delta}\\), assuming \\(\\delta \\ll 1\\). Here, \\(x_{t+\\delta}\\) is treated as a fixed and constant value, and we are only interested in the conditional distribution of \\(X_t\\) given \\(x_{t+\\delta}\\). Bayes’ law gives\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta}) \\propto p_t(x) \\, \\exp\\left\\{ -\\frac{\\left( x_{t+\\delta} - [x + \\mu(x) \\, \\delta] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\},\\]\nwhere the exponential term corresponds to the transition of the forward diffusion for \\(\\delta \\ll 1\\). Using the 1st order approximation\n\\[p_t(x) \\approx p_t(x_{t+\\delta}) \\,\\exp\\left( \\langle \\nabla \\log p_t(x_{t+\\delta}), (x-x_{t+\\delta})\\rangle\\right),\\]\neliminating multiplicative constants and higher-order error terms, we obtain:\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta})\n\\propto\n\\exp\\left\\{ -\\frac{\\left( x - [x_{t+\\delta} - \\mu(x_{t+\\delta}) \\delta\n+ {\\color{red} \\sigma^2 \\, \\nabla \\log p_t(x_{t+\\delta}) \\, \\delta} ] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\}.\\]\nFor \\(\\delta \\ll 1\\), this is transition of the reverse diffusion\n\\[\nd\\overleftarrow{X}_t =  \\textcolor{red}{-}\\mu(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\tag{3}\\]\nThe notation \\(B\\) is used to emphasize that this Brownian motion is distinct from the one used in the forward diffusion. The additional drift term, denoted as \\({\\color{red} \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X})}\\), is intuitive: it pushes the reverse diffusion toward regions where the forward diffusion spent a significant amount of time, i.e., where \\(p_t(dx)\\) is large. Note the minus sign in front of the original drift term \\(\\mu(X) \\, dt\\) It interesting to note that if \\(X\\) is a Langevin diffusion reversible with respect to a probability density \\(\\pi(x)\\),\n\\[\ndX = \\tfrac{\\sigma^2}{2} \\nabla \\log \\pi(X) \\, dt + \\sigma \\, dB,\n\\]\nthen the reverse diffusion diffusion Equation 3 reads simply as\n\\[\nd\\overleftarrow{X}_t =  \\tfrac{\\sigma^2}{2} \\nabla \\log \\pi(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log [p_{T-t} / \\pi](\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\]\nNote that there is no minus sign in front of the original drift term. This highlight that, when at stationarity, the reverse diffusion indeed follows the same dynamics as the forward diffusion. More generally, consider a bridging sequence of distributions \\(\\pi_t(dx)\\) indexed by time parameter \\(t \\in [0,T]\\). If one can find a control function \\(c_t(x)\\) such that the controlled Langevin diffusion\n\\[\ndX_t = \\tfrac{\\sigma^2}{2} \\, \\nabla \\log \\pi_t(X_t) \\, dt  \\textcolor{blue}{+ c_t(X_t)} \\, dt + \\sigma \\, dB,\n\\]\nis such that \\(X_t \\sim \\pi_t(dx)\\) for all time \\(0 \\leq t \\leq T\\), then the reverse diffusion reads is\n\\[\nd\\overleftarrow{X}_t =  \\tfrac{\\sigma^2}{2} \\, \\nabla \\log \\pi_{T-t}(\\overleftarrow{X}_t) \\, dt  \\textcolor{red}{- c_{T-t}(\\overleftarrow{X}_t)} \\, dt + \\sigma \\, dB.\n\\]\nA discussion on how his relates to a generalizations of the Crooks fluctuation theorem can be found in this other note. The popular “denoising diffusion models” (Ho, Jain, and Abbeel 2020) can be seen as discretizations of the backward process Equation 3, employing various techniques to estimate the additional drift term from data."
  },
  {
    "objectID": "notes/reverse_and_tweedie/reverse_and_tweedie.html#reversing-a-diffusion",
    "href": "notes/reverse_and_tweedie/reverse_and_tweedie.html#reversing-a-diffusion",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "",
    "text": "Imagine a scalar diffusion process \\(\\{X_t\\}_{t=0}^T\\) defined on the interval \\([0,T]\\),\n\\[dX = \\mu(X) \\, dt + \\sigma \\, dW\\]\nwhere \\(\\mu(X)\\) is the drift term, \\(\\sigma\\) is the diffusion coefficient, and \\(dW\\) is a Wiener process. Denote the distribution of this process at time \\(t\\) (\\(0 \\leq t \\leq T\\)) as \\(p_t(dx)\\) with an initial distribution of \\(p_0(dx)\\). Now, what happens if we reverse time and examine the process backward? In other words, consider the time-reversed process \\(\\overleftarrow{X}\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIntuitively, the process \\(\\overleftarrow{X}\\) is also a diffusion on the interval \\([0, T]\\), but with an initial distribution \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). To gain intuition, consider an Euler discretization of the forward process:\n\\[X_{t+\\delta} = X_{t} + \\mu(X_t)\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, \\mathbf{n} \\tag{1}\\]\nwhere \\(\\mathbf{n}\\sim \\mathcal{N}(0,1)\\) represents a noise term independent from \\(X_{t}\\), and \\(\\delta \\ll 1\\) is a time increment. Re-arranging terms and making the approximation \\(\\mu(X_t) \\approx \\mu(X_{t+\\delta})\\) gives that\n\\[X_{t} \\approx X_{t+\\delta} - \\mu(X_{t+\\delta})\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, (-\\mathbf{n}). \\tag{2}\\]\nThis seems to suggest that the time-reversed process follows the dynamics \\(d\\overleftarrow{X} = -\\mu(\\overleftarrow{X}) \\, dt + \\sigma \\, dW\\) started from \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). However, this conclusion is incorrect because this would suggest that the time-reversed of a standard Brownian motion (where \\(\\mu(x) \\equiv 0\\)) starting at zero is also a Brownian motion starting at \\(p_T(dx) = \\mathcal{N}(0,T)\\), which is clearly not the case. The flaw in this argument lies in assuming that the noise term \\(Z\\) is independent of \\(X_{t+\\delta}\\), which is not true, rendering the Euler discretization argument invalid.\nDeriving the dynamics of the backward process in a rigorous manner is not straightforward (Anderson 1982) (Haussmann and Pardoux 1986). What follows is a heuristic derivation that proceeds by estimating the mean and variance of \\(X_{t}\\) given \\(X_{t+\\delta} = x_{t+\\delta}\\), assuming \\(\\delta \\ll 1\\). Here, \\(x_{t+\\delta}\\) is treated as a fixed and constant value, and we are only interested in the conditional distribution of \\(X_t\\) given \\(x_{t+\\delta}\\). Bayes’ law gives\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta}) \\propto p_t(x) \\, \\exp\\left\\{ -\\frac{\\left( x_{t+\\delta} - [x + \\mu(x) \\, \\delta] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\},\\]\nwhere the exponential term corresponds to the transition of the forward diffusion for \\(\\delta \\ll 1\\). Using the 1st order approximation\n\\[p_t(x) \\approx p_t(x_{t+\\delta}) \\,\\exp\\left( \\langle \\nabla \\log p_t(x_{t+\\delta}), (x-x_{t+\\delta})\\rangle\\right),\\]\neliminating multiplicative constants and higher-order error terms, we obtain:\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta})\n\\propto\n\\exp\\left\\{ -\\frac{\\left( x - [x_{t+\\delta} - \\mu(x_{t+\\delta}) \\delta\n+ {\\color{red} \\sigma^2 \\, \\nabla \\log p_t(x_{t+\\delta}) \\, \\delta} ] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\}.\\]\nFor \\(\\delta \\ll 1\\), this is transition of the reverse diffusion\n\\[\nd\\overleftarrow{X}_t =  \\textcolor{red}{-}\\mu(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\tag{3}\\]\nThe notation \\(B\\) is used to emphasize that this Brownian motion is distinct from the one used in the forward diffusion. The additional drift term, denoted as \\({\\color{red} \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X})}\\), is intuitive: it pushes the reverse diffusion toward regions where the forward diffusion spent a significant amount of time, i.e., where \\(p_t(dx)\\) is large. Note the minus sign in front of the original drift term \\(\\mu(X) \\, dt\\) It interesting to note that if \\(X\\) is a Langevin diffusion reversible with respect to a probability density \\(\\pi(x)\\),\n\\[\ndX = \\tfrac{\\sigma^2}{2} \\nabla \\log \\pi(X) \\, dt + \\sigma \\, dB,\n\\]\nthen the reverse diffusion diffusion Equation 3 reads simply as\n\\[\nd\\overleftarrow{X}_t =  \\tfrac{\\sigma^2}{2} \\nabla \\log \\pi(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log [p_{T-t} / \\pi](\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\]\nNote that there is no minus sign in front of the original drift term. This highlight that, when at stationarity, the reverse diffusion indeed follows the same dynamics as the forward diffusion. More generally, consider a bridging sequence of distributions \\(\\pi_t(dx)\\) indexed by time parameter \\(t \\in [0,T]\\). If one can find a control function \\(c_t(x)\\) such that the controlled Langevin diffusion\n\\[\ndX_t = \\tfrac{\\sigma^2}{2} \\, \\nabla \\log \\pi_t(X_t) \\, dt  \\textcolor{blue}{+ c_t(X_t)} \\, dt + \\sigma \\, dB,\n\\]\nis such that \\(X_t \\sim \\pi_t(dx)\\) for all time \\(0 \\leq t \\leq T\\), then the reverse diffusion reads is\n\\[\nd\\overleftarrow{X}_t =  \\tfrac{\\sigma^2}{2} \\, \\nabla \\log \\pi_{T-t}(\\overleftarrow{X}_t) \\, dt  \\textcolor{red}{- c_{T-t}(\\overleftarrow{X}_t)} \\, dt + \\sigma \\, dB.\n\\]\nA discussion on how his relates to a generalizations of the Crooks fluctuation theorem can be found in this other note. The popular “denoising diffusion models” (Ho, Jain, and Abbeel 2020) can be seen as discretizations of the backward process Equation 3, employing various techniques to estimate the additional drift term from data."
  },
  {
    "objectID": "notes/reverse_and_tweedie/reverse_and_tweedie.html#denoising-score-matching-tweedie-formula",
    "href": "notes/reverse_and_tweedie/reverse_and_tweedie.html#denoising-score-matching-tweedie-formula",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "Denoising Score Matching & Tweedie formula",
    "text": "Denoising Score Matching & Tweedie formula\n\n\n\n\nMaurice Tweedie (1919–1996)\n\n\n\nThe previous section shows that a quantity often referred to as the “score” in machine learning (ML) literature, i.e. \\(\\mathcal{S}_t(x) \\equiv \\nabla_x \\log p_t(x)\\), naturally arises when a diffusion process runs backward in time. Interestingly, it is worth noting that since the beginning of times statisticians have been using the term “score” to refer to the other derivative, i.e. the derivative with respect to a model’s parameter, which is a completely different object!\nConsider a Brownian diffusion process \\(dX = \\sigma \\, dW\\) initiated from a distribution \\(p_0(dx)\\). If this process is ran forward for a duration \\(\\delta&gt;0\\), we have:\n\\[X_{\\delta} = X_0 + \\mathcal{N}(0, \\sigma^2 \\, \\delta)\\]\nwhere \\(X_0 \\sim p_0(dx)\\). Now, focusing on a specific sample \\(y = X_{\\delta}\\), the backward dynamics \\(d\\overleftarrow{X} = \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X}) \\, dt + \\sigma \\, dB\\) suggests that for sufficiently small \\(\\delta\\), the following approximation holds:\n\\[\n\\mathbb{E}[X_0 \\, | X_{\\delta} = y] \\; { \\color{\\red} \\approx} \\; y + \\nabla \\log p_{\\delta}(y) \\, \\sigma^2 \\delta.\n\\tag{4}\\]\nMaybe surprisingly, in the case of Brownian dynamics (no drift), this relationship holds exactly, even for arbitrarily large time increments \\(\\delta&gt;0\\). This observation attributed in (Efron 2011) to Maurice Tweedie (1919–1996) has a straightforward proof. Specifically, if \\(X \\sim p_0(dx)\\), then \\(Y = X + \\mathcal{N}(0, \\Gamma^2)\\) has a density given by:\n\\[\np_Y(dy) = \\int p_0(x) \\, \\rho_{\\Gamma}(y-x) \\, dx\n\\]\nwhere \\(\\rho_{\\Gamma}(z) \\propto \\exp[-z^2/(2 \\Gamma^2)]\\) is a centred Gaussian with variance \\(\\Gamma^2\\). It follows that\n\\[\n\\frac{ \\mathbb{E}[X \\, | Y = y]-y}{\\Gamma^2}\n=\n\\frac{ \\int \\left( \\frac {x-y }{\\Gamma^2} \\right)\\, p_0(x) \\, \\rho_{\\Gamma}(y-x) \\, dx }{\\int p_0(x) \\, \\rho_{\\Gamma}(y-x) \\, dx}.\n\\]\nSince \\(\\nabla_y \\log \\rho_{\\Gamma}(y-x) = (x-y)/\\Gamma^2\\), this also read:\n\\[\n\\frac{ \\mathbb{E}[X \\, | Y = y]-y}{\\Gamma^2}\n=\n\\nabla_y \\log \\left\\{ \\int p_0(x) \\, \\rho_{\\Gamma}(y-x) \\, dx \\right\\}.\n\\]\nThis leads to Tweedie’s formula:\n\\[\n\\mathbb{E}[X \\, | Y = y] \\; {\\color{red} =} \\; y + \\Gamma^2 \\, \\nabla \\log p_Y(y),\n\\]\nwhich is exactly the same as Equation 4 but with an equality sign: no approximation needed! Interestingly, this is what Machine-Learners often refer to as “denoising score matching” (Vincent 2011): if we have access to a large number of samples \\((X_i,Y_i)\\), where \\(X_i \\sim p_0(dx)\\) and \\(Y_i = X_i + \\mathcal{N}(0, \\Gamma^2)\\), and fit a regression model \\(F_{\\theta}\\) by minimizing the mean-squared error \\(\\theta \\mapsto \\mathbb{E} \\|X - F_{\\theta}(Y)\\|^2\\), then Tweedie formula shows that in the limit of infinite samples and with sufficient flexibility in the regression model \\(F_{\\theta_\\star}(y) = y + \\Gamma^2 \\nabla \\log p_Y(y)\\). This allows one to estimate \\(\\nabla \\log p_Y\\) from data, which is often a good approximation of \\(\\nabla \\log p_0\\) if the variance \\(\\Gamma^2\\) of the added noise is not too large. Indeed, things can go bad if \\(\\Gamma^2 \\ll 1\\) and the number of training data is not large enough, no free lunch!"
  },
  {
    "objectID": "notes/sanov/sanov.html",
    "href": "notes/sanov/sanov.html",
    "title": "Sanov’s Theorem",
    "section": "",
    "text": "Sanov’s Theorem\nConsider a random variable \\(X\\) on the finite alphabet \\(\\{a_1, \\ldots, a_K\\}\\) with \\(\\mathbb{P}(X=a_k) = p_k\\). For \\(N \\gg 1\\), consider a sequence \\((x_1, \\ldots, x_N)\\) obtained by sampling \\(N\\) times independently from \\(X\\) and set\n\\[\n\\widehat{p}_k = \\frac{1}{N} \\, \\sum_{i=1}^N \\, \\mathbf{1} {\\left( x_i = a_k \\right)}\n\\]\nthe proportion of \\(a_k\\) within this sequence. In other words, the empirical distribution obtained from the samples \\((x_1, \\ldots, x_N)\\) reads\n\\[\n\\widehat{p} = \\sum_{k=1}^K \\, \\widehat{p}_k \\, \\delta_{a_k}.\n\\]\nIndeed, the LLN indicates that \\(\\widehat{p}_k \\to p_k\\) as \\(N \\to \\infty\\), and it is important to estimate the probability that \\(\\widehat{p}_k\\) significantly deviates from \\(p_k\\). To this end, note that for another probability vector \\(q=(q_1, \\ldots, q_K)\\) the probability that\n\\[\n(\\widehat{p}_1, \\ldots, \\widehat{p}_K) \\; = \\; (q_1, \\ldots, q_K)\n\\]\nis straightforward to compute and reads\n\\[\n\\mathbb{P}(\\widehat{p} = q) \\; = \\; \\binom{N}{N q_1, \\ldots, N q_K} \\, p_1^{N q_1} \\ldots p_R^{N q_K}.\n\\]\nStirling’s approximation \\(m! \\asymp m \\, \\ln(m)\\) then gives that\n\\[\n\\mathbb{P}(\\widehat{p} = q) \\; \\asymp \\;\n\\exp {\\left( -N \\cdot D_{\\text{KL}}(q,p) \\right)}\n\\]\nwhere \\(D_{\\text{KL}}(q,p) = \\sum_{k=1}^K q_k \\, \\log[q_k / p_k]\\) is the Kullback–Leibler divergence of \\(q\\) from \\(p\\). In other words, as soon as \\(q \\neq p\\), the probability of observing \\(\\widehat{p} \\approx q\\) falls exponentially quickly to zero. With the language of Large Deviations, one can make this statement slightly more precise, rigorous and general, but it is essentially the content of Sanov’s Theorem.\n\n\nRare events happen in the least unlikely manner\nGiven a list of mutually exclusive events \\(E_1, \\ldots, E_R\\) and the knowledge that at least one of these events has taken place, the probability that the event \\(E_k\\) was the one that happened is \\(\\mathbb{P}(E_k) / [\\mathbb{P}(E_1) + \\ldots + \\mathbb{P}(E_R)]\\). The implication is that if all the events are rare, that is \\(p_k \\approx e^{-N \\, I_k} \\ll 1\\), and it is known that one event has indeed occurred, there is a high probability that the event with the smallest \\(I_k\\) value was the one that happened: the rare event took place in the least unlikely manner.\nConsider an iid sequence \\((X_1, \\ldots, X_N)\\) of \\(N \\gg 1\\) discrete real-valued random variables with \\(\\mathbb{P}(X = a_k) = p_k\\) and mean \\(\\mathbb{E}(X) \\in \\mathbb{R}\\). Suppose one observes the rare event\n\\[\n\\frac{1}{N} \\sum_{i=1}^N x_i  \\geq \\mu\n\\tag{1}\\]\nfor some level \\(\\mu\\) significantly above \\(\\mathbb{E}(X)\\). Naturally, the least unlikely way for this to happen is if \\((x_1 + \\ldots + x_N) / N \\, \\approx \\, \\mu\\). Furthermore, one may be interested in the empirical distribution \\(\\widehat{p}\\) associated to the sequence \\((x_1, \\ldots, x_N)\\) when the rare event Equation 1 does happen. The least unlikely empirical distribution is the one that minimizes \\(D_{\\text{KL}}(\\widehat{p}, p)\\) under the constraint that\n\\[\n\\sum_{i=1}^K a_k \\, \\widehat{p}_k = \\mu.\n\\tag{2}\\]\nThe function \\(\\widehat{p} \\mapsto D_{\\text{KL}}(\\widehat{p}, p)\\) is convex and the introduction of Lagrange multipliers shows that the solution to this constraint minimization problem is given by the Boltzmann distribution \\(p_{\\beta_\\mu}\\) defined as\n\\[\np_{\\beta_\\mu}(a_k) = \\frac{ p_k \\, e^{-\\beta_{\\mu} \\, a_k} }{Z(\\beta_{\\mu})}.\n\\tag{3}\\]\nThe parameter \\(\\beta_{\\mu} \\in \\mathbb{R}\\) is chosen so that the constraint Equation 2 be satisfied and the minus sign is to follow the “physics” convention. Note in passing that, in fact, the joint function \\((\\widehat{p},p) \\mapsto D_{\\text{KL}}(\\widehat{p}, p)\\) is convex! As usual, if one defines the log-partition function as \\(\\Phi(\\beta) = -\\log Z(\\beta)\\), with\n\\[\nZ(\\beta) \\; = \\; \\sum_{k=1}^K \\, p_k \\, e^{-\\beta \\, a_k},\n\\]\none obtains that the constraint is equivalent to requiring \\(\\mu = \\frac{d}{d \\beta} \\Phi(\\beta) \\mid_{\\beta = \\beta_\\mu}\\). Furthermore, since \\(\\Phi\\) is smooth and strictly concave, the function \\(\\beta \\mapsto \\left&lt; \\mu, \\beta \\right&gt; - \\Phi(\\beta)\\) is convex and the condition \\(\\mu = \\frac{d}{d \\beta} \\Phi(\\beta) \\mid_{\\beta = \\beta_\\mu}\\) is equivalent to setting\n\\[\n\\beta_{\\mu} \\; = \\; \\mathop{\\mathrm{argmin}}_{\\beta \\in \\mathbb{R}} \\; \\left&lt; \\mu, \\beta \\right&gt; - \\Phi(\\beta).\n\\]\nNaturally, one can now also estimate the probability of the event \\(\\mathbb{P}[(X_1 + \\ldots + X_N)/N \\approx \\mu]\\) happening since one now knows that it is equivalent (on a log scale) to \\(\\exp[-N \\, D_{\\text{KL}}(p_{\\beta_\\mu}, p)]\\). Algebra gives\n\\[\n\\begin{align}\nD_{\\text{KL}}(p_{\\beta_\\mu}, p)\n&= \\Phi(\\beta_\\mu) - \\left&lt; \\mu, \\beta_\\mu \\right&gt;\\\\\n&= \\max_{\\beta \\in \\mathbb{R}} \\; \\Phi(\\beta) - \\left&lt; \\mu, \\beta \\right&gt;.\n\\end{align}\n\\]\nAs a sanity check, note that since \\(\\Phi(0)=0\\), we have that \\(D_{\\text{KL}}(p_{\\beta_\\mu}, p) \\geq 0\\), as required. The statement that\n\\[\n\\frac{1}{N} \\, \\log \\mathbb{P} {\\left\\{ \\frac{X_1 + \\ldots + X_N}{N} \\; \\approx \\; \\mu \\right\\}}  \\; = \\; - I(\\mu)\n\\]\nwith a (Large Deviation) rate function given by\n\\[\nI(\\mu) \\; = \\; \\max_{\\beta \\in \\mathbb{R}} \\; \\Phi(\\beta) - \\left&lt; \\mu, \\beta \\right&gt;\n\\]\nis more or less the content of Cramer’s Theorem. The rate function \\(I(\\mu)\\) and the function \\(\\log Z( \\textcolor{red}{-}\\beta)\\) are related by a Legendre transform.\n\n\nExample: averaging uniforms…\nNow, to illustrate the above discussion, consider \\(N=10\\) iid uniform random variables on the interval \\([0,1]\\). It is straightforward to simulate these \\(N=10\\) uniforms conditioned on the event that their mean exceeds the level \\(\\mu = 0.7\\), which is a relatively rare event. When this is done repetitively and the empirical distribution is plotted, the resulting distribution is as follows:\n\n\n\n\nMean of \\(10\\) uniforms conditioned on being larger than \\(\\mu = 0.7\\)\n\n\n\nIndeed, the distribution in blue is (very close to) the Boltzmann distribution with density \\(\\mathcal{D}_{\\beta}(x) = e^{-\\beta \\, x} / Z(\\beta)\\) with \\(\\beta \\in \\mathbb{R}\\) chosen so that \\(\\int_{0}^{1} x \\, \\mathcal{D}_{\\beta}(dx) = \\mu\\)."
  },
  {
    "objectID": "notes/Gaussian_Assimilation/gaussian_assimilation_smoothing.html",
    "href": "notes/Gaussian_Assimilation/gaussian_assimilation_smoothing.html",
    "title": "Ensemble Kalman Smoother (EnKS)",
    "section": "",
    "text": "Consider a linear-Gaussian state space model with \\(\\mathbb{R}^{D_x}\\)-valued dynamics \\(X_{t+1} \\sim F \\, X_t + \\mathcal{N}(0,Q)\\) and \\(\\mathbb{R}^{D_y}\\)-valued observations \\(Y_t \\sim H X_t + \\mathcal{N}(0,R)\\). Assuming a Gaussian initial distribution, the filtering distributions \\(\\mathbb{P}(X_t \\in dx \\, | Y_{1:t}) \\equiv \\mathcal{N}(\\mu_{t|t}, P_{t|t})\\) are Gaussian and can be sequentially computed with the Kalman Filter. Similarly, the predictive distributions \\(\\mathbb{P}(X_{t+1} \\in dx \\, | Y_{1:t}) \\equiv \\mathcal{N}(\\mu_{t+1|t}, P_{t+1|t})\\) are straightforward to obtain from the filtering distributions: \\(\\mu_{t+1|t} = F \\, \\mu_{t|t}\\) and \\(P_{t+1|t} = F \\, P_{t|t} \\, F^\\top + Q\\). Given observations \\(y_{1:T} \\equiv (y_1, \\ldots, y_T)\\) and \\(1 \\leq t \\leq T\\), the smoothing distributions \\(\\mathbb{P}(X_t \\in dx \\, | Y_{1:T}) \\equiv \\mathcal{N}(\\mu_{t|T}, P_{t|T})\\) can computed by performing a “backward pass”. Since everything is linear and Gaussian, it is just an exercise in Linear Algebra & Gaussian-conditioning, as described by the Rauch-Tung-Striebel (Rauch, Tung, and Striebel 1965) smoothing recursions. The backward recursion reads\n\\[\n\\left\\{\n\\begin{aligned}\n\\mu_{t|T}\n&= \\mu_{t|t} + B_t \\,  {\\left( \\mu_{t+1|T} - \\mu_{t+1|t} \\right)} \\\\\nP_{t|T}\n&=\nP_{t|t} + B_t  {\\left(  P_{t+1|T} - P_{t+1|t}  \\right)}  B^\\top_{t}\n\\end{aligned}\n\\right.\n\\tag{1}\\]\nand allows one to compute the smoothing means and covariances matrices \\((\\mu_{t|T}, P_{t|T})\\) for \\(1 \\leq t \\leq T\\) starting from the knowledge of \\((\\mu_{T|T}, P_{T|T})\\). In Equation 1, the smoothing gain matrix \\(B_t\\) is given by\n\\[\n\\begin{align}\nB_t &=\n\\mathop{\\mathrm{Cov}}(X_t, X_{t+1} \\, | y_{1:t}) \\, \\mathop{\\mathrm{Var}}(X_{t+1} \\, | y_{1:t})^{-1} \\\\\n&=\nP_{t|t} F^\\top \\,  {\\left( F \\, P_{t|t} \\, F^\\top + Q \\right)} ^{-1}.\n\\end{align}\n\\tag{2}\\]\nThe Ensemble Kalman Filter (EnKF) is a non-linear equivalent of the Kalman filter, and the purpose of these notes is to derive the equivalent “ensemble version” of the backward recursion Equation 1. For this purpose, it is important to understand slightly better the role of the smoothing gain matrix \\(B_t\\). Consider the pair of random variable \\((X^f_t, X^p_{t+1})\\) distributed according to the joint distribution between the filtering distribution at time \\(t\\) and the predictive distribution at time \\(t+1\\) in the sense that\n\\[\n(X^f_t, X^p_t) \\; \\underbrace{=}_{\\text{(law)}}\\; (X_t, X_{t+1} \\, \\mid \\, y_{1:t}).\n\\]\nThis means that \\(X^f_t \\sim \\mathcal{N}(\\mu_{t|t}, P_{t|t})\\) and \\(X^p_{t+1} \\sim \\mathcal{N}(\\mu_{t+1|t}, P_{t+1|t})\\) and \\(X^p_t = F \\, X^f_t + \\mathcal{N}(0, Q)\\). Furthermore, Equation 2 and the standard gaussian conditional probabilities formulas give that the conditional means and covariances are given by\n\\[\n\\left\\{\n\\begin{align}\n\\textrm{Mean} {\\left( X^f_t | (X^p_{t+1}=x_{t+1}) \\right)}  \n\\; &= \\;\n\\mu_{t|t} + B_t (x_{t+1} - \\mu_{t+1|t}) \\\\\n\\textrm{Cov} {\\left( X^f_t | (X^p_{t+1}=x_{t+1}) \\right)}  \n\\; &= \\;\nP_{t|t} - B_t \\, P_{t+1|t} \\, B_t^\\top.\n\\end{align}\n\\right.\n\\tag{3}\\]\nThe above expression for the conditional mean also shows that the matrix \\(B_t\\) is a minimizer of the loss\n\\[\nM \\; \\mapsto \\;\n\\mathbb{E} {\\left(  \\left\\| (X^f_t - \\mu_{t|t}) - B (X^p_{t+1} - \\mu_{t+1|t}) \\right\\|^2  \\right)}\n\\]\nover all matrices \\(M \\in \\mathbb{R}^{D_x, D_x}\\). Heuristically, this shows that the smoothing gain matrix \\(B_t\\) can easily be computed by regressing \\(X^f_t\\) against \\(X^p_{t+1}\\). We can use this remark to build an ensemble version of the backward recursion Equation 1. Recall that when running a EnKF for filtering the observations \\(y_{1:T}\\), the final stage proceeds in two steps:\n\nObtain an ensemble of particles \\(X^{i,p}_{T} = F \\, X^{i,f}_{T-1} + \\mathcal{N}(0,Q)\\) that approximate the predictive distribution \\(\\mathbb{P}(X_T | y_{1:T-1})\\).\n\nAssimilate the last observation \\(y_T\\) using the Kalman gain matrix \\(K_T\\) and the correction \\(\\Delta_T^i = K_T \\, (\\tilde{y}_{i,\\star} - H \\, X^{i,p}_T)\\) by setting \\[\nX^{i,s}_T = X^{i,p}_T + \\Delta_T^i.\n\\tag{4}\\] The particles \\(X^{i,s}_T\\) approximate the smoothing distribution \\(\\mathbb{P}(X_T | y_{1:T})\\).\n\nFollowing our discussion of the smoothing gain matrix \\(B_{t}\\) and Equation 4, it seems sensible to set\n\\[\n\\begin{align}\nX^{i,s}_{T-1}\n&= X^{i,f}_{T-1} + B_{T-1} \\, \\Delta^i_T\\\\\n&= X^{i,f}_{T-1} + B_{T-1} \\, (X^{i,s}_{T} - X^{i,p}_{T})\n\\end{align}\n\\tag{5}\\]\nand hope that the ensemble of updated particles \\(X^{i,s}_{T-1}\\) approximate the smoothing distribution \\(\\mathbb{P}(X_{T-1} | y_{1:T})\\). In words, the particle \\(X^{i,s}_{T-1}\\) is obtained by “pulling” the correction term \\(\\Delta^i_{T} = X^{i,s}_{T} - X^{i,p}_{T}\\) back to \\(X^{i,f}_{T-1}\\) through the “regression” smoothing gain matrix \\(B_{T-1}\\). To check that the particles \\(X^{i,s}_{T-1}\\) indeed approximate the smoothing distribution \\(\\mathbb{P}(X_{T-1} \\,|y_{1:T})\\), it suffices to compute the mean/variance and verify that they are matching the one given by Equation 1. Recall that Equation 3 gives that the filtering/predictive distributions satisfy\n\\[\nX^f_{T-1} = \\mu_{T-1|T-1} + B_{T-1} \\, (X^p_{T} - \\mu_{T|T-1}) + \\varepsilon_t\n\\]\nwhere \\(\\varepsilon_t \\sim \\mathcal{N}(0, P_{T-1|T-1} - B_{T-1} \\, P_{T|T-1} \\, B_{T-1}^\\top)\\) is independent from all other sources of randomness. Plugging this into Equation 5 gives that\n\\[\nX^{i,s}_{T-1}\n=\n\\mu_{T-1|T-1} + B_{T-1} \\, (X^{i,s}_{T} - \\mu_{T|T-1}) + \\varepsilon_t.\n\\]\nSince the \\(X^{i,s}_{T}\\) are distributed according to the smoothing distribution, i.e. \\(X^{i,s}_{T} \\sim \\mathcal{N}(\\mu_{T|T}, P_{T|T})\\), this immediately shows that \\(X^{i,s}_{T-1}\\) is Gaussian with\n\\[\n\\left\\{\n\\begin{align}\n\\textrm{Mean} &= \\mu_{T-1|T} = \\mu_{T-1|T-1} + B_{T-1} \\,  {\\left( \\mu_{T|T} - \\mu_{T|T-1} \\right)} \\\\\n\\textrm{Covariance} &= P_{T-1|T} = P_{T-1|T-1} + B_{T-1}  {\\left(  P_{T|T} - P_{T|T-1}  \\right)}  B^\\top_{T-1},\n\\end{align}\n\\right.\n\\]\nas it should. One can then iterate this construction to obtain particle approximations of the smoothing distributions \\(\\mathbb{P}(X_t | y_{1:T})\\) for \\(1 \\leq t \\leq T\\) by running a backward pass and recursively setting\n\\[\nX^{i,s}_t \\; = \\; X^{i,f}_t + B_t \\,  {\\left( X^{i,s}_{t+1} - X^{i,p}_{t+1} \\right)} .\n\\]\nThe ensemble of particles \\(X^{i,s}_t\\) approximates the smoothing distribution \\(\\mathbb{P}(X_t | y_{1:T})\\). In a nonlinear setting, it suffices to approximate the smoothing gain matrices with\n\\[\n\\widehat{B}_t = \\mathop{\\mathrm{Cov}} {\\left(  x^f_{t,i}, x^p_{t+1,i}  \\right)}  \\, \\mathop{\\mathrm{Var}} {\\left(  x^p_{t+1,i}  \\right)} ^{-1}.\n\\]\n[Experiments: TODO]\n\n\n\n\nReferences\n\nRauch, Herbert E, F Tung, and Charlotte T Striebel. 1965. “Maximum Likelihood Estimates of Linear Dynamic Systems.” AIAA Journal 3 (8): 1445–50."
  },
  {
    "objectID": "notes/Gaussian_Assimilation/gaussian_assimilation.html",
    "href": "notes/Gaussian_Assimilation/gaussian_assimilation.html",
    "title": "Gaussian Assimilation & the EnKF",
    "section": "",
    "text": "Assume a prior Gaussian prior distribution \\(\\pi_0 \\equiv \\mathcal{N}(m_0,P_0)\\) and a noisy observation \\(y_\\star \\in \\mathbb{R}^{D_y}\\) with\n\\[\ny_\\star = H x + \\xi\n\\qquad \\text{with} \\qquad\n\\xi \\sim \\mathcal{N}(0,R)\n\\]\nwhere \\(x \\in \\mathbb{R}^{D_x}\\) is an unknown quantity of interest. The posterior distribution \\(\\pi \\equiv \\mathcal{N}(m,P)\\) is Gaussian and is given by\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &= m_0 + P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}(y_\\star - H m)\\\\\nP &= P_0 - P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1} \\, H P_0,\n\\end{aligned}\n\\right.\n\\end{align}\n\\]\nas standard Gaussian conditioning shows it. This can also be written as\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &= m_0 + K \\, (y_\\star - H m_0)\\\\\nP &= (I - K \\, H) P_0,\n\\end{aligned}\n\\right.\n\\end{align}\n\\]\nfor Kalman Gain Matrix \\(K \\in \\mathbb{R}^{D_x,D_y}\\) defined as\n\\[\nK \\; = \\; P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}.\n\\tag{1}\\]\nwhich can also be expressed as\n\\[\nK \\; = \\; \\mathop{\\mathrm{Cov}}(X, HX) \\, \\mathop{\\mathrm{Var}}(Y)^{-1}.\n\\]\nfor \\(X \\sim \\pi_0\\) and \\(Y = HX + \\mathcal{N}(0,R)\\); this point of view can be a useful for establishing generalization to non-linear scenarios. The important remark is that the posterior covariance matrix \\(P \\in \\mathbb{R}^{D_x,D_x}\\) and the posterior mean \\(m \\in \\mathbb{R}^{D_x}\\) can also be expressed as\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &=  \\textcolor{red}{(I - K \\, H)} \\, m_0 +  \\textcolor{blue}{K} \\, y_\\star\\\\\nP &= \\;  \\textcolor{red}{(I - K \\, H)} \\, P_0 \\,  \\textcolor{red}{(I - K \\, H)^\\top} +  \\textcolor{blue}{K} R  \\textcolor{blue}{K^\\top}.\n\\end{aligned}\n\\right.\n\\end{align}\n\\tag{2}\\]\nThis shows that \\(P\\) is indeed positive semi-definite. More importantly, this gives a mechanism for transforming samples from the prior distributions into samples from the posterior distributions. Indeed, consider \\(N\\) iid samples from the prior distribution, \\(x_1, \\ldots, x_N \\sim \\pi_0(dx)\\), and set\n\\[\n\\widetilde{x}_i\n=\n\\textcolor{red}{(I - K \\, H)} \\, x_i +  \\textcolor{blue}{K}(y_\\star + \\xi_i)\n\\]\nfor iid noise terms \\(\\xi_i \\sim \\mathcal{N}(0,R)\\). From Equation 2 it is clear that \\(\\widetilde{x}_1, \\ldots, \\widetilde{x}_N\\) are iid samples from the Gaussian posterior distribution \\(\\pi = \\mathcal{N}(m,P)\\). It is more intuitive to write this as\n\\[\n\\widetilde{x}_i\n=\nx_i + K \\, (  \\textcolor{green}{ \\widetilde{y}_{i,\\star} } - H \\, x_i)\n\\]\nwhere \\(\\widetilde{y}_{i,\\star} \\in \\mathbb{R}^{D_y}\\) are fake observations that are obtained by perturbing the actual observation \\(y_\\star\\) with additive Gaussian noise terms with covariance \\(R\\),\n\\[\n\\textcolor{green}{\\widetilde{y}_{i,\\star} \\, = \\, y_\\star + \\xi_i}.\n\\]\n\n\nSuppose that we would like to estimate \\(x \\in \\mathbb{R}^{D_x}\\) from the noisy observation\n\\[\ny_\\star = \\mathcal{H}(x) + \\xi\n\\qquad \\text{with} \\qquad\n\\xi \\sim \\mathcal{N}(0,R)\n\\]\nand possibly-nonlinear observation operator \\(\\mathcal{H}: \\mathbb{R}^{D_x} \\to \\mathbb{R}^{D_y}\\). Assume that we also have \\(N\\) samples \\(x_1, \\ldots, x_N\\) generated from some (unkown) prior distribution. For example, these samples could come from another numerical procedure. In order to obtain \\(N\\) approximate samples from the posterior distribution, one can set\n\\[\n\\widetilde{x}_i\n=\nx_i + \\widehat{K} \\, [  \\textcolor{green}{ \\widetilde{y}_{i,\\star} } - \\mathcal{H}(x_i)]\n\\]\nfor fake observations \\( \\textcolor{green}{\\widetilde{y}_{i,\\star} } = y_\\star + \\xi_i\\). The approximate Kalman gain matrix \\(\\widehat{K}\\) is obtained by noting that in Equation 1 giving\n\\[\nK \\; = \\; P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}\n\\]\nwe have \\(P_0 H^\\top = \\mathop{\\mathrm{Cov}}(X,HX)\\) and \\(H P_0 H^\\top = \\mathop{\\mathrm{Var}}(HX)\\) for \\(X \\sim \\pi_0\\). This means that an approximate Kalman matrix can be obtained using empirical estimates of these covariance matrices:\n\\[\n\\widehat{K} \\; = \\; \\widehat{\\mathop{\\mathrm{Cov}}}([x_i]_i, [\\mathcal{H}(x_i)]_i) \\,  {\\left( \\widehat{\\mathop{\\mathrm{Var}}}([\\mathcal{H}(x_i)]_i) + R \\right)} ^{-1}.\n\\]\nThese updates form the basis of the Ensemble Kalman filter (EnKF), and very successful and scalable approach to data-assimilation of high-dimensional dynamical systems. This method is operationally employed across various weather forecasting centers across the globe.\n\n\n\n\nEnKF Bible by Geir Evensen\n\n\n\n\n\n\nConsider a jointly Gaussian random vector \\((X,Y) \\in \\mathbb{R}^{D_x + D_y}\\). To sample from the conditional distribution \\(X|Y=y_\\star\\), one can sample first sample \\((X,Y)\\) from the unconditional distribution and then set:\n\\[\n\\widetilde{X} \\; = \\; X + \\mathop{\\mathrm{Cov}}(X,Y) \\, \\mathop{\\mathrm{Var}}(Y)^{-1} (y_\\star - Y).\n\\]\nThe sample \\(\\widetilde{X}\\) is a sample from the conditional distribution \\(X|Y=y_\\star\\) and the proof is straightforward: it suffices to compute the first two moments pf the Gaussian vector \\(\\widetilde{X}\\) and check that they are equal to what they should be. This method is often called the Matheron’s rule and (Wilson et al. 2020) mentions that this “notion of conditioning by kriging was first presented by Matheron in the early 1970s. Naturally, this provides another avenue to derive the EnKF update equations, as has been described by a number of authors (Doucet 2010). Given samples \\(x_i\\) from the Gaussian prior, generate samples \\(y_i = \\mathcal{H}(x_i) + \\xi_i\\) for \\(\\xi_i \\sim \\mathcal{N}(0,R)\\): this gives pairs samples \\((x_i,y_i)\\) from the joint”prior”. One can obtain samples from the posterior by setting \\(\\widetilde{x}_i = x_i + \\mathop{\\mathrm{Cov}}(x,y) \\, \\mathop{\\mathrm{Var}}(y)^{-1} (y_\\star - y_i)\\). Naturally, the matrix \\(\\mathop{\\mathrm{Cov}}(x,y)\\) can be estimated as \\(\\widehat{\\mathop{\\mathrm{Cov}}}([x_i]_i, [\\mathcal{H}(x_i)]_i)\\) and \\(\\mathop{\\mathrm{Var}}(y)\\) is estimated as \\(\\widehat{\\mathop{\\mathrm{Var}}}([\\mathcal{H}(x_i)]_i) + R\\). This, indeed, is the EnKF update equation.\n\n\n\nInterestingly enough, the remarks above can be used to design in a relatively principled manner a derivative free optimizer (Huang et al. 2022). For example, assume one would like to minimize a functional of the type\n\\[\n\\Psi(x) \\; = \\; \\|y_\\star - \\mathcal{H}(x)\\|^2_{R^{-1}}.\n\\]\nOne can start with a cloud of particles \\(x_1, \\ldots, x_N\\) and keep updating them by assuming that one assimilates the noisy observation \\(y_\\star\\) generated from a postulated observation process\n\\[\ny_\\star = \\mathcal{H}(x) + \\varepsilon^{-1} \\, \\xi\n\\]\nfor \\(\\xi \\sim \\mathcal{N}(0,R)\\) and \\(\\varepsilon\\ll 1\\) a “step-size”. Each assimilation step steers the cloud of points towards the rights direction. Careful choice of the step-size \\(\\varepsilon\\) is often crucial, as in any optimization procedure. It is indeed related to Information-Geometric Optimization Algorithms (IGO): the article (Ollivier et al. 2017) is beautiful!"
  },
  {
    "objectID": "notes/Gaussian_Assimilation/gaussian_assimilation.html#ensemble-kalman-updates",
    "href": "notes/Gaussian_Assimilation/gaussian_assimilation.html#ensemble-kalman-updates",
    "title": "Gaussian Assimilation & the EnKF",
    "section": "",
    "text": "Assume a prior Gaussian prior distribution \\(\\pi_0 \\equiv \\mathcal{N}(m_0,P_0)\\) and a noisy observation \\(y_\\star \\in \\mathbb{R}^{D_y}\\) with\n\\[\ny_\\star = H x + \\xi\n\\qquad \\text{with} \\qquad\n\\xi \\sim \\mathcal{N}(0,R)\n\\]\nwhere \\(x \\in \\mathbb{R}^{D_x}\\) is an unknown quantity of interest. The posterior distribution \\(\\pi \\equiv \\mathcal{N}(m,P)\\) is Gaussian and is given by\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &= m_0 + P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}(y_\\star - H m)\\\\\nP &= P_0 - P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1} \\, H P_0,\n\\end{aligned}\n\\right.\n\\end{align}\n\\]\nas standard Gaussian conditioning shows it. This can also be written as\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &= m_0 + K \\, (y_\\star - H m_0)\\\\\nP &= (I - K \\, H) P_0,\n\\end{aligned}\n\\right.\n\\end{align}\n\\]\nfor Kalman Gain Matrix \\(K \\in \\mathbb{R}^{D_x,D_y}\\) defined as\n\\[\nK \\; = \\; P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}.\n\\tag{1}\\]\nwhich can also be expressed as\n\\[\nK \\; = \\; \\mathop{\\mathrm{Cov}}(X, HX) \\, \\mathop{\\mathrm{Var}}(Y)^{-1}.\n\\]\nfor \\(X \\sim \\pi_0\\) and \\(Y = HX + \\mathcal{N}(0,R)\\); this point of view can be a useful for establishing generalization to non-linear scenarios. The important remark is that the posterior covariance matrix \\(P \\in \\mathbb{R}^{D_x,D_x}\\) and the posterior mean \\(m \\in \\mathbb{R}^{D_x}\\) can also be expressed as\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &=  \\textcolor{red}{(I - K \\, H)} \\, m_0 +  \\textcolor{blue}{K} \\, y_\\star\\\\\nP &= \\;  \\textcolor{red}{(I - K \\, H)} \\, P_0 \\,  \\textcolor{red}{(I - K \\, H)^\\top} +  \\textcolor{blue}{K} R  \\textcolor{blue}{K^\\top}.\n\\end{aligned}\n\\right.\n\\end{align}\n\\tag{2}\\]\nThis shows that \\(P\\) is indeed positive semi-definite. More importantly, this gives a mechanism for transforming samples from the prior distributions into samples from the posterior distributions. Indeed, consider \\(N\\) iid samples from the prior distribution, \\(x_1, \\ldots, x_N \\sim \\pi_0(dx)\\), and set\n\\[\n\\widetilde{x}_i\n=\n\\textcolor{red}{(I - K \\, H)} \\, x_i +  \\textcolor{blue}{K}(y_\\star + \\xi_i)\n\\]\nfor iid noise terms \\(\\xi_i \\sim \\mathcal{N}(0,R)\\). From Equation 2 it is clear that \\(\\widetilde{x}_1, \\ldots, \\widetilde{x}_N\\) are iid samples from the Gaussian posterior distribution \\(\\pi = \\mathcal{N}(m,P)\\). It is more intuitive to write this as\n\\[\n\\widetilde{x}_i\n=\nx_i + K \\, (  \\textcolor{green}{ \\widetilde{y}_{i,\\star} } - H \\, x_i)\n\\]\nwhere \\(\\widetilde{y}_{i,\\star} \\in \\mathbb{R}^{D_y}\\) are fake observations that are obtained by perturbing the actual observation \\(y_\\star\\) with additive Gaussian noise terms with covariance \\(R\\),\n\\[\n\\textcolor{green}{\\widetilde{y}_{i,\\star} \\, = \\, y_\\star + \\xi_i}.\n\\]\n\n\nSuppose that we would like to estimate \\(x \\in \\mathbb{R}^{D_x}\\) from the noisy observation\n\\[\ny_\\star = \\mathcal{H}(x) + \\xi\n\\qquad \\text{with} \\qquad\n\\xi \\sim \\mathcal{N}(0,R)\n\\]\nand possibly-nonlinear observation operator \\(\\mathcal{H}: \\mathbb{R}^{D_x} \\to \\mathbb{R}^{D_y}\\). Assume that we also have \\(N\\) samples \\(x_1, \\ldots, x_N\\) generated from some (unkown) prior distribution. For example, these samples could come from another numerical procedure. In order to obtain \\(N\\) approximate samples from the posterior distribution, one can set\n\\[\n\\widetilde{x}_i\n=\nx_i + \\widehat{K} \\, [  \\textcolor{green}{ \\widetilde{y}_{i,\\star} } - \\mathcal{H}(x_i)]\n\\]\nfor fake observations \\( \\textcolor{green}{\\widetilde{y}_{i,\\star} } = y_\\star + \\xi_i\\). The approximate Kalman gain matrix \\(\\widehat{K}\\) is obtained by noting that in Equation 1 giving\n\\[\nK \\; = \\; P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}\n\\]\nwe have \\(P_0 H^\\top = \\mathop{\\mathrm{Cov}}(X,HX)\\) and \\(H P_0 H^\\top = \\mathop{\\mathrm{Var}}(HX)\\) for \\(X \\sim \\pi_0\\). This means that an approximate Kalman matrix can be obtained using empirical estimates of these covariance matrices:\n\\[\n\\widehat{K} \\; = \\; \\widehat{\\mathop{\\mathrm{Cov}}}([x_i]_i, [\\mathcal{H}(x_i)]_i) \\,  {\\left( \\widehat{\\mathop{\\mathrm{Var}}}([\\mathcal{H}(x_i)]_i) + R \\right)} ^{-1}.\n\\]\nThese updates form the basis of the Ensemble Kalman filter (EnKF), and very successful and scalable approach to data-assimilation of high-dimensional dynamical systems. This method is operationally employed across various weather forecasting centers across the globe.\n\n\n\n\nEnKF Bible by Geir Evensen\n\n\n\n\n\n\nConsider a jointly Gaussian random vector \\((X,Y) \\in \\mathbb{R}^{D_x + D_y}\\). To sample from the conditional distribution \\(X|Y=y_\\star\\), one can sample first sample \\((X,Y)\\) from the unconditional distribution and then set:\n\\[\n\\widetilde{X} \\; = \\; X + \\mathop{\\mathrm{Cov}}(X,Y) \\, \\mathop{\\mathrm{Var}}(Y)^{-1} (y_\\star - Y).\n\\]\nThe sample \\(\\widetilde{X}\\) is a sample from the conditional distribution \\(X|Y=y_\\star\\) and the proof is straightforward: it suffices to compute the first two moments pf the Gaussian vector \\(\\widetilde{X}\\) and check that they are equal to what they should be. This method is often called the Matheron’s rule and (Wilson et al. 2020) mentions that this “notion of conditioning by kriging was first presented by Matheron in the early 1970s. Naturally, this provides another avenue to derive the EnKF update equations, as has been described by a number of authors (Doucet 2010). Given samples \\(x_i\\) from the Gaussian prior, generate samples \\(y_i = \\mathcal{H}(x_i) + \\xi_i\\) for \\(\\xi_i \\sim \\mathcal{N}(0,R)\\): this gives pairs samples \\((x_i,y_i)\\) from the joint”prior”. One can obtain samples from the posterior by setting \\(\\widetilde{x}_i = x_i + \\mathop{\\mathrm{Cov}}(x,y) \\, \\mathop{\\mathrm{Var}}(y)^{-1} (y_\\star - y_i)\\). Naturally, the matrix \\(\\mathop{\\mathrm{Cov}}(x,y)\\) can be estimated as \\(\\widehat{\\mathop{\\mathrm{Cov}}}([x_i]_i, [\\mathcal{H}(x_i)]_i)\\) and \\(\\mathop{\\mathrm{Var}}(y)\\) is estimated as \\(\\widehat{\\mathop{\\mathrm{Var}}}([\\mathcal{H}(x_i)]_i) + R\\). This, indeed, is the EnKF update equation.\n\n\n\nInterestingly enough, the remarks above can be used to design in a relatively principled manner a derivative free optimizer (Huang et al. 2022). For example, assume one would like to minimize a functional of the type\n\\[\n\\Psi(x) \\; = \\; \\|y_\\star - \\mathcal{H}(x)\\|^2_{R^{-1}}.\n\\]\nOne can start with a cloud of particles \\(x_1, \\ldots, x_N\\) and keep updating them by assuming that one assimilates the noisy observation \\(y_\\star\\) generated from a postulated observation process\n\\[\ny_\\star = \\mathcal{H}(x) + \\varepsilon^{-1} \\, \\xi\n\\]\nfor \\(\\xi \\sim \\mathcal{N}(0,R)\\) and \\(\\varepsilon\\ll 1\\) a “step-size”. Each assimilation step steers the cloud of points towards the rights direction. Careful choice of the step-size \\(\\varepsilon\\) is often crucial, as in any optimization procedure. It is indeed related to Information-Geometric Optimization Algorithms (IGO): the article (Ollivier et al. 2017) is beautiful!"
  },
  {
    "objectID": "notes/wasserstein_langevin/wasserstein_langevin.html",
    "href": "notes/wasserstein_langevin/wasserstein_langevin.html",
    "title": "Wasserstein Gradients & Langevin Diffusions",
    "section": "",
    "text": "Overdamped Langevin Diffusion as a gradient flow on the space of probability distributions endowed with the \\(2\\)-Wasserstein metric\n\n\n\nConsider a target probability density \\(\\pi(x) = \\frac{\\overline{\\pi}(x)}{\\mathcal{Z}}\\) on \\(\\mathbb{R}^D\\) that is known up to a normalizing constant \\(\\mathcal{Z}&gt; 0\\). We also have a different probability density \\(p_0(x)\\). The goal is to gradually tweak \\(p_0(x)\\) so that it eventually matches \\(\\pi(x)\\). More concretely, we aim to perform a gradient descent on the space of probability distributions to reduce the functional\n\\[\n\\mathcal{F}(p) \\; = \\; D_{\\text{KL}} {\\left( p, \\pi \\right)}  \\; = \\; \\int p(x) \\, \\log  {\\left\\{ \\frac{p(x)}{\\overline{\\pi}(x)} \\right\\}}  \\, dx \\, + \\, \\textrm{(constant)}.\n\\]\nThis approach can be discretized: assume \\(N \\gg 1\\) particles \\(X_0^1, \\ldots, X_0^N \\in \\mathbb{R}^D\\) forming an empirical distribution that approximates \\(p_0(dx)\\),\n\\[\np_0(dx) \\; \\approx \\; \\frac{1}{N} \\sum_{i=1}^N \\, \\delta_{X_0^i}(dx).\n\\]\nDefine \\(X_{\\delta}^i = X_0^i + \\delta_t \\, \\mu(X_0^i)\\) where \\(\\delta_t \\ll 1\\) denotes a time discretization parameter and \\(\\mu:\\mathbb{R}^D \\to \\mathbb{R}^D\\) is a “drift” function. Finding a suitable ‘drift function’ is the main problem. According to the Fokker-Planck equation, the computed empirical distribution\n\\[\np_{\\delta_t}(dx) \\; \\approx \\; \\frac{1}{N} \\sum_{i=1}^N \\, \\delta_{X_{\\delta_t}^i}(dx)\n\\]\napproximates \\(p_{\\delta_t}(x)\\) given by\n\\[\n\\frac{p_{\\delta_t}(x)- p_0(x)}{\\delta_t} \\; = \\; -\\nabla \\cdot  {\\left[ \\mu(x) \\, p_0(x) \\right]} .\n\\tag{1}\\]\nWhat is the optimal drift function \\(\\mu: \\mathbb{R}^D \\to \\mathbb{R}^D\\) that ensures that \\(p_{\\delta_t}\\) comes as close as possible to \\(\\pi\\)? Typically, we select \\(\\mu:\\mathbb{R}^D \\to \\mathbb{R}^D\\) such that the quantity \\(\\mathcal{F}(p_{\\delta_t})\\) is minimized, provided that \\(p_{\\delta_t}\\) is not drastically different from \\(p_0\\). One method is to use the \\(L^2\\) Wasserstein distance and assume the constraint\n\\[\nD_{\\text{Wass}}(p_{0}, p_{\\delta_t}) \\approx \\int p_0(x) \\, \\| \\delta_t \\, \\mu(x) \\|^2 \\, dx \\leq \\varepsilon\n\\tag{2}\\]\nfor a parameter \\(\\varepsilon\\ll 1\\). More pragmatically, it is generally easier (eg. proximal methods) to minimize the joint objective\n\\[\n\\mathcal{F}(p_{\\delta_t}) + \\frac{1}{2 \\varepsilon} \\, D_{\\text{Wass}}(p_{0}, p_{\\delta_t}).\n\\tag{3}\\]\nBased on equations Equation 1 and Equation 2, a first-order expansion shows that the joint objective Equation 3 can be approximated by\n\\[\n\\begin{align}\n-\\int &\\nabla \\cdot \\Big\\{  \\textcolor{red}{[\\delta_t \\mu]}(x) \\, p_0(x) \\Big\\} \\, \\log  {\\left\\{  \\frac{p_0(x)}{\\overline{\\pi}(x)}  \\right\\}} \\, dx \\, \\\\ &\\qquad + \\qquad \\, \\frac{1}{2 \\varepsilon} \\, \\int p_0(x) \\, \\|  \\textcolor{red}{[\\delta_t \\, \\mu]}(x) \\|^2 \\, dx,\n\\end{align}\n\\tag{4}\\]\na relatively straightforward quadratic function of the drift function \\(\\mu: \\mathbb{R}^D \\to \\mathbb{R}^D\\). The optimal drift function, ie. the minimizer of Equation 4, is given by \\[\n\\mu(x) \\; = \\; - {\\left(  \\frac{\\varepsilon}{\\delta_t}  \\right)}  \\, \\nabla \\log  {\\left\\{  \\frac{p_0(x)}{\\overline{\\pi}(x) }  \\right\\}} .\n\\]\nPut simply, this suggests that we should select the drift function proportional to \\(-\\nabla \\log[p_0(x) / \\overline{\\pi}(x)]\\). To implement this scheme, we begin by sampling \\(N \\gg 1\\) particles \\(X_0^i \\sim p_0(dx)\\) and let evolve each particle according to the following differential equation\n\\[\n\\frac{d}{dt} X_t^i \\; = \\; - \\nabla \\log  {\\left\\{  \\frac{p_t(X_t^i) }{ \\overline{\\pi}(X_t^i) }  \\right\\}}\n\\]\nwhere \\(p_t\\) is the density of the set of particles at time \\(t\\). It is the usual diffusion-ODE trick for describing the evolution of the density of an overdamped Langevin diffusion,\n\\[\ndX_t \\; = \\; -\\nabla \\log \\overline{\\pi}(X_t) \\, dt \\; + \\; \\sqrt{2} \\, dW_t.\n\\]\nThis can be shown by writing down the associated Fokker-Planck equation. This heuristic discussion shows that minimizing \\(D_{\\text{KL}}(p, \\pi)\\) by introducing a gradient flow in the space of probability distributions with the Wasserstein metric essentially produces a standard overdamped Langevin diffusion. Indeed, transforming this heuristic discussion into a formal statement is not trivial: the constructive proof in (Jordan, Kinderlehrer, and Otto 1998) is now usually referred to as the JKO scheme.\nThe above derivation shows that the Wasserstein distance plays particularly well with minimizing functionals of the space of probability distributions. The same heuristic discussion shows that minimizing a functional of the type\n\\[\n\\mathcal{F}(p) \\; = \\; \\int \\Phi[p(x)] \\, \\nu(dx)\n\\]\nfor some cost function \\(\\Phi: (0, \\infty) \\to \\mathbb{R}\\) and distribution \\(\\nu(dx)\\) leads to choosing a drift function \\(\\mu:\\mathbb{R}\\to \\mathbb{R}\\) minimizing\n\\[\n\\int -\\nabla \\cdot \\Big\\{  \\textcolor{red}{[\\delta_t \\, \\mu]}(x) \\, p(x) \\Big\\} \\Phi'[p(x)] \\, \\nu(dx)\n\\, + \\, \\frac{1}{2 \\varepsilon} \\, \\int p_0(x) \\, \\|  \\textcolor{red}{[\\delta_t \\, \\mu]}(x) \\|^2 \\, dx.\n\\]\nThis can be approached identically to what as been done in the case of minimizing \\(D_{\\text{KL}}(p, \\pi)\\).\n\n\n\n\nReferences\n\nJordan, Richard, David Kinderlehrer, and Felix Otto. 1998. “The Variational Formulation of the Fokker–Planck Equation.” SIAM Journal on Mathematical Analysis 29 (1). SIAM: 1–17."
  },
  {
    "objectID": "notes/index_notes.html",
    "href": "notes/index_notes.html",
    "title": "Notes",
    "section": "",
    "text": "Notes for students, half-baked write-ups for myself, and probably other random maths/stats/ML texts. Certainly full of typos, comments welcome!\n\n\nNotes indexed by categories.\n\n\nDenoising Diffusion Probabilistic Models\n\nNoising and Reverse Ornstein-Uhlenbeck\nFrom Denoising Diffusion to ODEs\nReverse diffusions, Score & Tweedie\nVIASM mini-course on diffusions and flows\n\n\n\nInformation Theory\n\nReferences & Readings\nEntropy and Basic Definitions\nShannon Source Coding Theorem\nFano’s inequality\nShearer’s Lemma\n\n\n\nMonte-Carlo methods\n\nDeriving the Langevin MCMC algorithm\nGaussian Assimilation & the EnKF\nEnsemble Kalman Smoothers\nMCMC with deterministic proposals\nRWM & HMC on Manifolds\n\n\n\nProbability Misc\n\nAuxiliary variable trick\nSanov’s Theorem\nWasserstein Gradients & Langevin Diffusions\nPoisson Equation & Asymptotic Variance\nAveraging and Homogenization\nJoe Doob & Change of measures on path space\nGirsanov and importance sampling\nDoob, Girsanov and Bellman\nBasic Conformal Inference\nJarzynski and Crooks\nSelf-Avoiding-Walks\nSparse Gaussian Processes\n\n\n\nOther\n\nAdjoint Method for Sensitivities"
  },
  {
    "objectID": "notes/girsanov/girsanov.html",
    "href": "notes/girsanov/girsanov.html",
    "title": "Girsanov and Importance Sampling",
    "section": "",
    "text": "Igor Girsanov (1934 – 1967)\n\n\n\nLet \\(q(dx) \\equiv \\mathcal{N}(\\mu,\\Gamma)\\) be the Gaussian distribution with mean \\(\\mu \\in \\mathbb{R}^D\\) and covariances \\(\\Gamma \\in \\mathbb{R}^{D \\times D}\\). For a direction \\(u \\in \\mathbb{R}^D\\), consider the distribution \\(q^{u}(dx) \\equiv \\mathcal{N}(\\mu + \\Gamma^{1/2} \\, u, \\Gamma)\\), i.e. the same Gaussian distribution but shifted by an amount \\(\\Gamma^{1/2} \\, u\\). Algebra directly gives that\n\\[\n\\frac{q^{u}(x)}{q(x)}\n=\n\\exp {\\left\\{ - \\frac{1}{2} \\| u\\|^2 + \\left&lt; u, \\, \\Gamma^{-1/2}(x-\\mu) \\right&gt; \\right\\}} .\n\\tag{1}\\]\nWe will see that, not very surprisingly, a similar change-of-probability result holds in continuous time. On the time interval \\([0,T]\\), let \\(W_t\\) be a standard Brownian motion in \\(\\mathbb{R}^D\\) and \\(X_t\\) be the solution to the SDE\n\\[\ndX_t \\; = \\; b(X_t) \\, dt + \\sigma(X_t) \\, dW_t\n\\tag{2}\\]\nfor some drift \\(b: \\mathbb{R}^D \\to \\mathbb{R}^D\\) and diffusion \\(\\sigma: \\mathbb{R}^D \\to \\mathbb{R}^{D \\times D}\\) and initial distribution \\(\\mu_0(dx_0)\\). This SDE defines a probability measure \\(\\mathbb{P}\\) on the path-space \\(C([0,T]; \\mathbb{R}^D)\\), the space of continuous functions from \\([0,T]\\) to \\(\\mathbb{R}^D\\). Consider a perturbation drift function \\(u: \\mathbb{R}^D \\to \\mathbb{R}^D\\) and associated perturbed SDE given by\n\\[\ndX_t^u \\; = \\; b(X_t^u) \\, dt + \\sigma(X_t^u) \\,  {\\left\\{  dW_t +  \\textcolor{blue}{u(X_t^u) \\, dt}  \\right\\}} .\n\\tag{3}\\]\nThis perturbed SDE, started from the same initial distribution \\(\\mu_0(dx_0)\\), defines a probability measure \\(\\mathbb{P}^u\\) on the path-space \\(C([0,T]; \\mathbb{R}^D)\\) and it is often useful to understand the Radon-Nikodym derivative of \\(\\mathbb{P}^u\\) with respect to \\(\\mathbb{P}\\). I have never really liked the way this is usually derived, and also never really remember the result. It takes only a few lines of algebra to re-derive these results, at least informally. For this purpose, consider a simpler Euler discretization of the SDE with time-discretization \\(\\delta = T/N\\) for \\(N \\gg 1\\). Consider a discretized paths \\((x_0, x_{\\delta}, \\ldots, x_{T})\\) of Equation 2 obtained by iterating the update\n\\[\nx_{t_{k+1}} \\; = \\; x_{t_k} + b(x_{t_k})\\,\\delta + \\sigma(x_{t_k}) \\, (\\Delta W_{t_k})\n\\]\nwith \\(t_k = k\\delta\\) and \\(\\Delta W_{t_k} = W_{t_{k+1}} - W_{t_k}\\). The probability of observing such a path reads \\[\n\\frac{1}{\\mathcal{Z}} \\, \\mu_0(x_0) \\, \\exp {\\left\\{ -\\frac{1}{2 \\delta} \\sum_{k=0}^{N-1}\n\\|x_{t_{k+1}} - [x_{t_k} + b(x_{t_k})\\,\\delta]\\|^2_{\\Gamma^{-1}(x_{t_k}) } \\right\\}}\n\\]\nwith \\(\\Gamma(x) \\equiv \\sigma(x) \\sigma^\\top(x)\\) the volatility matrix and an irrelevant multiplicative constant \\(\\mathcal{Z}\\). One obtains a similar expression for a discretized path of the perturbed SDE Equation 3 and the ratio of these two quantities equals\n\\[\n\\frac{d \\widetilde{\\mathbb{P}}^{u}}{d \\widetilde{\\mathbb{P}}}(x) = \\exp {\\left\\{ \\sum_{k=0}^{N-1} -\\frac{\\delta}{2} \\|u(x_{t_k})\\|^2  +\n\\left&lt; x_{t_{k+1}}-x_{t_k}-b(x_{t_k})\\delta, \\sigma(x_{t_k}) \\, u(x_{t_k}) \\right&gt;_{\\Gamma^{-1}(x_{t_k})}  \\right\\}} .\n\\]\nwhere the tilde notation denotes the discretized version of the measures. Since\n\\[\nx_{t_{k+1}}-x_{t_k}-b(x_{t_k})\\delta = \\sigma(x_{t_k}) \\, \\Delta W_{t_k},\n\\] for a path \\(dx_t \\; = \\; b(x_t) \\, dt + \\sigma(x_t) \\, dW_t\\) and taking the limit \\(N \\to \\infty\\) gives\n\\[\n\\frac{d \\mathbb{P}^{u}}{d \\mathbb{P}}(x) \\; = \\; \\exp {\\left\\{\n-\\frac 12 \\, \\int_0^T \\|u(x_t)\\|^2 \\, dt + \\int_{0}^T u^\\top(x_t) \\, dW_t\n\\right\\}} .\n\\]\nSimilarly, for a path \\(dx^{u}_t \\; = \\; b(x^u_t) \\, dt + \\sigma(x^u_t) \\,  {\\left(  dW_t + u(x^u_t) \\right)} \\), we have\n\\[\n\\frac{d \\mathbb{P}}{d \\mathbb{P}^u}(x^u) \\; = \\; \\exp {\\left\\{\n-\\frac 12 \\, \\int_0^T \\|u(x^u_t)\\|^2 \\, dt - \\int_{0}^T u^\\top(x^u_t) \\, dW_t\n\\right\\}} .\n\\]\nThese results remain identical for time-dependent drift and volatility functions, as is clear from this non-rigorous argument. The above two formulas for \\(d\\mathbb{P}^u/d\\mathbb{P}(x)\\) and \\(d\\mathbb{P}/d\\mathbb{P}^u(x)\\) may be slightly confusing since they are not immediately recognizable as inverse of each other. Furthermore, these probability ratios evaluated along a path \\(x\\) or \\(x^u\\) are expressed in terms of the Brownian trajectory that defines them, which can be confusing. In short, this would be better to express \\(d\\mathbb{P}^u/d\\mathbb{P}(x)\\) directly in terms of the path \\(x\\), and not in terms of the Brownian motion \\(W_t\\), even though it is indeed equivalent. For these reasons, it is often convenient to use the following equivalent expressions:\n\\[\n\\left\\{\n\\begin{aligned}\n\\frac{d \\mathbb{P}^{u}}{d \\mathbb{P}}(x) &= \\exp {\\left\\{\n\\textcolor{blue}{-}\\frac 12 \\, \\int_0^T \\|u(x_t)\\|^2 \\, dt  \\textcolor{blue}{+} \\int_{0}^T u^\\top(x_t) \\, \\frac{dx_t - b(x_t) dt}{\\sigma(x_t)} \\right\\}} \\\\\n\\frac{d \\mathbb{P}}{d \\mathbb{P}^{(u)}}(x) &= \\exp {\\left\\{\n\\textcolor{blue}{+}\\frac 12 \\, \\int_0^T \\|u(x_t)\\|^2 \\, dt  \\textcolor{blue}{-} \\int_{0}^T u^\\top(x_t) \\, \\frac{dx_t - b(x_t) dt}{\\sigma(x_t)} \\right\\}}\n\\end{aligned}\n\\right.\n\\]\nFrom these expression, the fact that \\(d\\mathbb{P}^u/d\\mathbb{P}(x)\\) and \\(d\\mathbb{P}/d\\mathbb{P}^u(x)\\) are indeed inverse of each other is clear. Another entirely equivalent formulation, slightly more symmetrical again, is as follows. Consider the two measures \\(\\mathbb{P}^{(1)}\\) and \\(\\mathbb{P}^{(2)}\\) associated to\n\\[\ndX_t^{(i)} \\; = \\; b^{(i)}(X_t) \\, dt + \\sigma(X_t) \\, dW_t\n\\]\nfor two drift functions \\(b^{(1)}: \\mathbb{R}^D \\to \\mathbb{R}^D\\) and \\(b^{(2)}: \\mathbb{R}^D \\to \\mathbb{R}^D\\). Then, the Radon-Nikodym derivative between these two measures is given by\n\\[\n\\frac{d \\mathbb{P}^{(2)}}{d \\mathbb{P}^{(1)}}(x) = \\exp {\\left\\{\n-\\frac{1}{2}\\int_{0}^T  {\\left( \\frac{\\|b^{(2)}_t\\|^2 - \\|b^{(1)}_t\\|^2}{\\sigma^2_t} \\right)}  \\, dt\n+\n\\int_{0}^T \\left&lt;  \\frac{b^{(2)}_t - b^{(1)}_t}{\\sigma_t^2}, dx_t \\right&gt;\n\\right\\}}\n\\]\nwith the shorthand \\(b^{(i)}_t = b^{(i)}(x_t)\\) and \\(\\sigma_t = \\sigma(x_t)\\) and \\(\\|v\\|^2/\\sigma^2 = \\left&lt; v, [\\sigma \\sigma^\\top]^{-1} v \\right&gt;\\) and \\(\\left&lt; u,v \\right&gt; / \\sigma^2 = \\left&lt; u, [\\sigma \\sigma^\\top]^{-1} v \\right&gt;\\). Again, this follows immediately from a discretized version of the SDEs. As described below, these change of variables formulae are often useful when performing importance sampling on path-space. As a sanity check, one can see that in the case of a scalar Brownian motion \\(dX = \\sigma \\, dW\\) and drifted version of it \\(dX^u = \\sigma \\, dW + u \\, dt\\), we indeed have that \\(d\\mathbb{P}^u/d\\mathbb{P}(x)\\) has unit expectation under \\(\\mathbb{P}\\) since it is equivalent to the fact \\(\\mathbb{E}[\\exp(\\sigma \\, \\xi)] = \\exp(\\sigma^2/2)\\) for a standard Gaussian random variable \\(\\xi\\). Finally, note that the Kullback-Leibler divergence between \\(\\mathbb{P}\\) and \\(\\mathbb{P}^u\\) has a particularly simple form. Since \\(D_{\\text{KL}}(\\mathbb{P}, \\mathbb{P}^u) = \\mathbb{E}_{\\mathbb{P}} {\\left[ -\\log {\\left\\{ \\frac{d \\mathbb{P}^{u}}{d \\mathbb{P}}(X) \\right\\}}  \\right]} \\) one obtains\n\\[\nD_{\\text{KL}}(\\mathbb{P}, \\mathbb{P}^u) = \\frac12 \\mathbb{E} {\\left[  \\int_0^T \\|u(X_t)\\|^2 \\, dt  \\right]} .\n\\]\n\nImportance Sampling on path-space\nConsider a functional \\(\\Phi: C([0,T]; \\mathbb{R}^D) \\to \\mathbb{R}\\) on path-space; a typical example is\n\\[\n\\Phi(x) = \\exp {\\left\\{ \\int_0^T f(X_t) \\, dt \\, + \\, g(X_T) \\right\\}} .\n\\]\nSuppose that we would like to evaluate the expectation of \\(\\Phi\\) under the measure \\(\\mathbb{P}\\). Naive Monte-Carlo (MC) would require sampling \\(M\\) trajectories from Equation 2 and computing the average of \\(\\Phi\\) on these trajectories. To reduce the variance of this naive MC estimator, one can also use importance sampling by sampling \\(M\\) trajectories \\(x^{1,u}, \\ldots, x^{M,u}\\) from the measure \\(\\mathbb{P}^u\\) and compute the average\n\\[\n\\frac{1}{M} \\, \\sum_{i=1}^M \\Phi(x^{i,u}) \\, W(x^{i,u})\n\\]\nwith weights given by the Radon-Nikodym derivative\n\\[\nW(x^{i,u}) \\; = \\; \\exp {\\left\\{\n-\\frac 12 \\, \\int_0^T \\|u(x^{i,u}_t)\\|^2 \\, dt - \\int_{0}^T u^\\top(x^{i,u}_t) \\, dW_t\n\\right\\}} .\n\\]\nChoosing the optimal “control” function \\(u\\) that minimizes the variance of the estimator is not entirely straightforward, although this previous note already gives the answer. More on this in another note."
  },
  {
    "objectID": "notes/doob_transforms/doob.html",
    "href": "notes/doob_transforms/doob.html",
    "title": "Joe Doob & Change of measures on path-space",
    "section": "",
    "text": "Joseph Doob (1910 – 2004)\n\n\n\nConsider a continuous time Markov process \\(X_t\\) on the time interval \\([0,T]\\) and with value in the state space \\(\\mathcal{X}\\). This defines a probability \\(\\mathbb{P}\\) on the set of \\(\\mathcal{X}\\)-valued paths. Now, it is often the case that one has to consider a perturbed probability distribution \\(\\mathbb{Q}\\) defined as\n\\[\n\\frac{d\\mathbb{Q}}{d\\mathbb{P}}(x_{[0,T]}) = \\frac{1}{\\mathcal{Z}} \\, \\exp[g(X_T)]\n\\tag{1}\\]\nfor a (typically unknown) normalization constant \\(\\mathcal{Z}\\) and some function \\(g: \\mathcal{X}\\to \\mathbb{R}\\). For example, collecting a noisy observation \\(y_T \\sim \\mathcal{F}(X_T) + \\textrm{(noise)}\\) at time \\(T\\), the distribution \\(\\mathbb{Q}\\) defined with the log-likelihood function \\(g(x) = \\log \\mathbb{P}(y_T \\mid X_T=x)\\) describes the dynamics of the Markov process \\(X_t\\) conditioned on the observation \\(y_T\\); we will use this interpretation in the following since this is the most common use case and gives the most intuitive interpretation. Doob h-transforms are a powerful tool to describe the dynamics of the conditioned process.\nFor convenience, let us use the notation \\(\\mathbb{E}_x[\\ldots] \\equiv \\mathbb{E}[\\ldots \\mid x_t=x]\\). For a test function \\(\\varphi: \\mathcal{X}\\to \\mathbb{R}\\) and a time increment \\(\\delta &gt; 0\\), we have\n\\[\n\\begin{align*}\n\\mathbb{E}[\\varphi(x_{t+\\delta}) | x_t, y_T] &= \\mathbb{E}_{x_t}[\\varphi(x_{t+\\delta}) \\, \\exp(g(x_T)) ] \\, / \\, \\mathbb{E}_{x_t}[\\exp(g(x_T))]\\\\\n&= \\frac{ \\mathbb{E}_{x_t}[\\varphi(x_{t+\\delta}) \\, h(t+\\delta, x_{t+\\delta})] }{h(t, x)}.\n\\end{align*}\n\\tag{2}\\]\nWe have introduced the important function \\(h:[0,T] \\times \\mathcal{X}\\to \\mathbb{R}\\) defined as\n\\[\nh(t, x) \\; = \\; \\mathbb{E} {\\left[  \\exp[g(x_T)] \\mid x_t = x \\right]}  \\; = \\; \\mathbb{P}(y_T \\mid x_t = x).\n\\]\nOne can readily check that the function \\(h\\) satisfies the Kolmogorov equation\n\\[\n(\\partial_t + \\mathcal{L}) \\, h = 0\n\\]\nwith boundary condition \\(h(T,x) = \\exp[g(x)]\\). Furthermore, denoting by \\(\\mathcal{L}\\) the infinitesimal generator of the Markov process \\(X_t\\), we have:\n\\[\n\\begin{align*}\n\\mathbb{E}_{x_t}[\\varphi(x_{t+\\delta}) & h(t+\\delta, x_{t+\\delta}) ]\n\\; \\approx \\;\n\\varphi(x_t) h(t, x_t) \\\\\n&+ \\; \\delta \\, (\\partial_t + \\mathcal{L})[h \\, \\varphi] \\, (t, x_t)\n\\; + \\; o(\\delta).\n\\end{align*}\n\\tag{3}\\]\nThe infinitesimal generator \\(\\mathcal{L}^{\\star}\\) of the conditioned process is\n\\[\n\\mathcal{L}^{\\star} \\varphi(t, x_t) = \\lim_{\\delta \\to 0^+} \\; \\frac{\\mathbb{E}[\\varphi(x_{t+\\delta}) | x_t, y_T] - \\varphi(x_t)}{\\delta}.\n\\]\nPlugging Equation 3 within Equation 2 directly gives that\n\\[\n\\mathcal{L}^{\\star} \\varphi\\; = \\; \\mathcal{L}\\varphi+ \\frac{\\mathcal{L}[\\varphi\\, h]}{h} + \\varphi\\frac{\\partial_t h}{h}.\n\\]\nThe generator \\(\\mathcal{L}^{\\star}\\) describes the dynamics of the conditioned process. In fact, the same computation holds with a more general change of measure of the type \\[\n\\textcolor{green}{\\frac{d\\mathbb{Q}}{d\\mathbb{P}}(x_{[0,T]}) = \\frac{1}{\\mathcal{Z}} \\, \\exp {\\left\\{ \\int_{0}^T f(s, X_s) \\, ds + g(x_T) \\right\\}}  }\n\\tag{4}\\]\nfor some function \\(f:[0,T] \\times \\mathcal{X}\\to \\mathbb{R}\\). One can define the function \\(h\\) similarly as\n\\[\n\\textcolor{green}{ h(t, x_t) \\; = \\; \\mathbb{E} {\\left[  \\exp {\\left\\{ \\int_{t}^T f(X_s) \\, ds + g(x_T) \\right\\}}  \\mid x_t \\right]} }.\n\\tag{5}\\]\nThis function satisfies the Feynman-Kac formula \\((\\partial_t + \\mathcal{L}+ f) \\, h = 0\\) and one obtains entirely similarly that the probability distribution \\(\\mathbb{Q}\\) describes a Markov process with infinitesimal generator\n\\[\n\\textcolor{green}{\\mathcal{L}^{\\star} \\varphi\\; = \\; \\mathcal{L}\\varphi+ \\frac{\\mathcal{L}[h \\, \\varphi]}{h} +  {\\left(  \\frac{\\partial_t h} {h} + f \\right)}  \\, \\varphi.}\n\\tag{6}\\]\nTo see how this works, let us see a few examples:\n\nGeneral diffusions\nConsider a diffusion process\n\\[\ndX = b(X) \\, dt + \\sigma(X) \\, dW\n\\]\nwith generator \\(\\mathcal{L}\\varphi= b \\nabla \\varphi+ \\tfrac12 \\, \\sigma \\sigma^\\top : \\nabla^2 \\varphi\\) and initial distribution \\(\\mu_0(dx)\\). We are interested in describing the dynamics of the “conditioned” process given by the probability distribution \\(\\mathbb{Q}\\) defined in Equation 4. Algebra applied to Equation 5 then shows that\n\\[\n\\mathcal{L}^\\star \\varphi\\; = \\; \\mathcal{L}\\varphi+ \\underbrace{\\frac{\\varphi\\, (\\partial_t + \\mathcal{L}+ f)[h]}{h}}_{=0}\n+ \\sigma \\, \\sigma^\\top \\, (\\nabla \\log h) \\, \\nabla \\varphi\n\\]\nwhere the function \\(h\\) is described in Equation 5. Since \\((\\partial_t + \\mathcal{L}+ f) \\, h = 0\\), this reveals that the probability distribution \\(\\mathbb{Q}\\) describes a diffusion process \\(X^\\star\\) with dynamics\n\\[\ndX^\\star = b(X^\\star) \\, dt + \\sigma(X^\\star) \\,  {\\left\\{  dW +  \\textcolor{blue}{u^\\star(t, X^\\star)} \\, dt \\right\\}} .\n\\]\nThe additional drift term \\(\\sigma(X^\\star) \\,  \\textcolor{blue}{u^\\star(t, X^\\star)} \\, dt\\) is involves a “control” \\( \\textcolor{blue}{u^\\star(t, X^\\star)}\\) with \\[\n\\textcolor{blue}{u^\\star(t, x) = \\sigma^\\top(x) \\, \\nabla \\log h(t, x)}.\n\\tag{7}\\]\nNote that the initial distribution of the conditioned process is\n\\[\n\\mu_0^\\star(dx) = \\frac{1}{\\mathcal{Z}} \\, \\mu_0(dx) \\, h(0,x).\n\\]\nUnfortunately, apart from a few straightforward cases such as a Brownian motion or an Ornstein-Uhlenbeck process, the function \\(h\\) is generally intractable. However, there are indeed several numerical methods available to approximate it effectively.\n\n\nBrownian bridge\nWhat about a Brownian motion in \\(\\mathbb{R}^D\\) conditioned to hit the state \\(x_\\star \\in \\mathbb{R}^D\\) at time \\(t=T\\), i.e. a Brownian bridge? In that case, the function \\(h\\) is given by\n\\[\nh(t,x) = \\mathbb{P}(B_T = x_\\star \\mid B_t = x)\n=\n\\exp {\\left\\{ -\\frac{\\|x-x_\\star\\|^2}{2 \\, (T-t)} \\right\\}}  / \\mathcal{Z}_{T-t}\n\\]\nfor some irrelevant normalization constant \\(\\mathcal{Z}_{T-t}\\) that only depends on \\(T-t\\). Plugging this into Equation 7 gives that the conditioned Brownian \\(X^{\\star}\\) motion has dynamics\n\\[\ndX^\\star \\;=\\;  \\textcolor{blue}{- \\frac{X^\\star - x_\\star}{T-t} \\, dt} + dB.\n\\]\nThe additional drift term \\(-(X^\\star - x_\\star)/(T-t)\\) is intuitive: it points in the direction of \\(x^\\star\\) and gets increasingly large as \\(t \\to T\\).\n\n\nPositive Brownian motion\nWhat about a scalar Brownian conditioned to stay positive at all times? Let us consider \\(T\\) and let us condition first on the event that the Brownian motion stays positive within \\([0,T]\\) and later consider the limit \\(T \\to \\infty\\). The function \\(h\\) reads\n\\[\nh(t,x) = \\mathbb{P} {\\left( \\text{$B_t$ stays $&gt;0$ on $[t,T]$} \\mid B_t=x \\right)} .\n\\]\nThis can easily be calculated with the reflection principle. It equals\n\\[\nh(t,x) = 1 - 2 \\, \\mathbb{P}(B_T &lt; 0 \\mid B_T = x)\n=\n\\mathbb{P}(\\sqrt{T-t} \\, \\| \\xi \\| &lt; x)\n\\]\nfor a standard Gaussian \\(\\xi \\sim \\mathcal{N}(0,1)\\). Plugging this into Equation 7 gives that the additional drift term is\n\\[\n\\nabla \\log h(t,x) = \\frac{\\exp {\\left( -x^2 / (2 \\, (T-t)) \\right)} }{x} \\quad \\to \\quad \\frac{1}{x}\n\\]\nas \\(T \\to \\infty\\). This shows that a Brownian motion conditioned to stay positive at all times has a upward drift of size \\(1/x\\),\n\\[\ndX^\\star \\;=\\; \\frac{1}{X^{\\star}} + dB.\n\\]\nIncidentally, it is the dynamics of a Bessel process of dimension \\(d=3\\), i.e. the law of the modulus of a three-dimensional Brownian motion. More generally, if one conditions a Brownian motion to stay within a closed domain \\(\\mathcal{D}\\), the conditioned dynamics exhibit a repulsive drift term of size about \\(1/\\textrm{dist}(x, \\partial \\mathcal{D})\\) near the boundary \\(\\partial \\mathcal{D}\\) of the domain, as described below.\n\n\nBrownian motion staying in a domain\nWhat about a Brownian motion conditioned to stay within a domain \\(\\mathcal{D}\\) forever? As before, consider an time horizon \\(T\\) and define the function \\(h\\) as\n\\[\nh(t,x) = \\mathbb{P} {\\left( \\text{$B_t$ stays in $\\mathcal{D}$ on $[t,T]$} \\mid B_t=x \\right)} .\n\\]\nOne can see that the function \\(h\\) satisfies the PDE\n\\[\n(\\partial_t + \\Delta) \\, h = 0\n\\]\nand equals zero on the boundary \\(\\partial \\mathcal{D}\\) of the domain. Furthermore \\(h(t,x) \\to 1\\) as \\(t \\to T\\) for all \\(x \\in \\mathcal{D}\\). Consider the eigenfunctions \\(\\psi_k\\) of the negative Laplacian \\(-\\Delta\\) with Dirichlet boundary conditions on \\(\\partial \\mathcal{D}\\). Recall that \\(-\\Delta\\) is a positive operator with a discrete spectrum \\(\\lambda_1 \\leq \\lambda_2 \\leq \\ldots\\) of non-negative eigenvalues. The eigenfunction corresponding to the smallest eigenvalue \\(\\lambda_1\\) is the principal eigenfunction \\(\\psi_1\\) and it is standard that it is a positive function within the domain \\(\\mathcal{D}\\), as a “slight” generalization of the Perron-Frobenius in linear algebra shows it. Expanding \\(h\\) in the basis of eigenfunctions \\(\\psi_k\\) gives that\n\\[\nh(t,x) = \\underbrace{c_1 \\, e^{-\\lambda_1 \\, (T-t)} \\, \\psi_1(x)}_{\\textrm{dominant contribution}} + \\sum_{k \\geq 2} c_k \\, e^{-\\lambda_k \\, (T-t)} \\, \\psi_k(x).\n\\]\n\n\n\n\nEigenfunctions of the Laplacian\n\n\n\nSince we are interested in the regime \\(T \\to \\infty\\), it holds that\n\\[ \\nabla_x \\log h(t,x) \\; \\to \\; \\nabla \\log \\psi_1(x).\\]\nThis shows that the conditioned Brownian motion has a drift term expressed in terms of the principal eigenfunction \\(\\psi_1\\) of the Laplacian:\n\\[\ndX^\\star \\;=\\;  \\textcolor{blue}{ \\nabla \\log \\psi_1(X^\\star) \\, dt} + dB.\n\\]\nFor example, if \\(\\mathcal{D}\\equiv [0,L]\\) for a 1D Brownian motion, the principal eigenfunction is \\(\\psi_1(x) = \\sin(\\pi \\, x /L)\\). This shows that there is a upward drift of size \\(\\sim 1/x\\) near \\(x \\approx 0\\) and a downward drift of size \\(\\sim 1/(L-x)\\) near \\(x \\approx L\\)."
  },
  {
    "objectID": "notes/information_theory_fano/information_theory_fano.html",
    "href": "notes/information_theory_fano/information_theory_fano.html",
    "title": "Information Theory: Fano’s inequality",
    "section": "",
    "text": "Robert Fano (1917 – 2016)\n\n\n\n\nFano’s inequality\nConsider a three random variables forming a Markov chain,\n\\[\nX \\mapsto Y \\mapsto \\widehat{X}\n\\tag{1}\\]\nin the sense that \\(Y = \\textrm{function}(X, \\text{noise})\\) and \\(Z = \\textrm{function}(Y, \\text{noise})\\). Typical situations include:\n\nWe select a parameter \\(\\theta\\) for a probabilistic model \\(\\mathbb{P}_{\\theta}\\). Afterward, we collect data \\(X\\) from this model, and our goal is to estimate the parameter \\(\\theta\\) solely from the data \\(X\\).\nWe generate data \\(X\\), compress this data into \\(X_{\\text{zip}}\\), and then attempt to recover the original data \\(X\\) as closely as we can.\n\nSince each step in Equation 1 destroys some information (eg. data processing), it is important to measure how accurately \\(\\widehat{X}\\) estimates the initial input, \\(X\\). In other words, we want to know how much more information (expressed as ‘bits’) we need to reconstruct \\(X\\) using knowledge of \\(\\widehat{X}\\) alone, i.e. we would like to upper-bound \\(H(X \\, | \\widehat{X})\\). For this purpose, imagine an “error” variable \\(E\\) that indicates whether \\(\\widehat{X}\\) perfectly matches \\(X\\),\n\\[\nE = \\mathbf{1} {\\left( \\widehat{X} \\neq X \\right)} .\n\\]\nThe probability of error is \\(p_E = \\mathbb{P}(\\widehat{X} \\neq X)\\) and \\(E = \\text{Bern}(p_E)\\). To estimate \\(X\\) from \\(\\widehat{X}\\), we can start by learning if \\(\\widehat{X}\\) equals \\(X\\), which costs us \\(H(E | \\widehat{X}) \\leq H(E) = h_2(p_E)\\) ‘bits’ of information. If it turns out that \\(E=0\\), we are done asking. If we find that \\(E=1\\), however, we need to ask additional \\(H(X | \\widehat{X}, E)\\) questions. Crucially, \\(H(X | \\widehat{X}, E) \\leq H(X)\\), but also \\(H(X | \\widehat{X}, E) \\leq \\log_2(|\\mathcal{X}|-1)\\) since \\(X\\) can take any value in \\(\\mathcal{X}\\) except \\(\\widehat{X}\\) when \\(E=1\\). Writing this reasoning quantitatively gives Fano’s inequality:\n\\[\n\\begin{align}\nH(X | \\widehat{X})\n&\\leq h_2(p_E) + p_E \\, \\log_2(|\\mathcal{X}|-1).\n\\end{align}\n\\tag{2}\\]\nApparently, this inequality was first derived by Robert Fano in the 50s while teaching a Ph.D. seminar at MIT. In words: a large \\(H(X | \\widehat{X})\\) means that \\(\\widehat{X}\\) offers insufficient information about \\(X\\), and as a result, the probability of error \\(p_E\\) must be high.\n\n\nApplications:\n\nConverse of Shannon’s coding theorem"
  },
  {
    "objectID": "notes/on_Langevin_MCMC/on_Langevin_MCMC.html",
    "href": "notes/on_Langevin_MCMC/on_Langevin_MCMC.html",
    "title": "Deriving Langevin MCMC",
    "section": "",
    "text": "Julian Besag (1945 – 2010)\n\n\n\nConsider a target density \\(\\pi(x)\\) in \\(\\mathbb{R}^D\\). Since the Langevin diffusion\n\\[\ndX_t = \\nabla \\log \\pi(X_t) \\, dt + \\sqrt{2} \\, dW\n\\tag{1}\\]\nis reversible with respect to \\(\\pi\\), it is natural to use a Euler-Maruyama discretization of Equation 1 to build MCMC proposals: in a MCMC simulation and for a time discretization parameter \\(\\varepsilon&gt; 0\\), if the current position is \\(x \\in \\mathbb{R}^D\\), a proposal \\(y \\in \\mathbb{R}^D\\) can be generated as\n\\[\ny = x + \\varepsilon\\, \\nabla \\log \\pi(X_t) + \\sqrt{2 \\varepsilon} \\, \\xi\n\\]\nwith \\(\\xi \\sim \\mathcal{N}(0,I)\\) before being accepted-or-reject according to the usual Metropolis-Hastings ratio. This MCMC method, first proposed by Julian Besag in 1994, is commonly referred to as the Metropolis-Adjusted-Langevin-Algorithm (MALA). But how can one come-up with this proposal mechanism without knowing before hand the existence of this reversible Langevin diffusion Equation 1? While it is intuitively clear that following the direction of \\(\\nabla \\log \\pi\\) is not such a bad idea, i.e. one would like to move towards areas of “high probability mass”, where does this \\(\\sqrt{2}\\) comes from? Naturally, one could look at proposals of the type \\(y = x + \\nabla \\log \\pi(X_t) \\, \\varepsilon+ \\lambda \\, \\xi\\) for some free parameter \\(\\lambda &gt; 0\\) and study the behavior of the Metropolis-Hastings ratio in the regime \\(\\varepsilon\\to 0\\): as simple as it sounds, it is not entirely straightforward and requires quite a bit of algebra (do it!). Instead, I very much like the type of approaches described in (Titsias and Papaspiliopoulos 2018). To summarize, we would like to generate a MCMC proposal \\(y \\in \\mathbb{R}^D\\) that stays in the vicinity of the current position \\(x \\in \\mathbb{R}^D\\) while exploiting the knowledge of \\(\\nabla \\log \\pi(x)\\). One cannot simply approximate the target distribution as \\(\\pi(x) \\approx \\pi(x_k) e^{\\left&lt; \\nabla \\log \\pi(x_k), x-x_k \\right&gt;}\\) and sample from this approximation since it is typically does not define a probability distribution. Instead, consider the following extended target distribution\n\\[\n\\overline{\\pi}(x,z) \\, \\propto \\pi(x) \\, \\exp {\\left\\{ -\\frac{1}{2\\varepsilon}\\|z-x\\|^2 \\right\\}} .\n\\]\nIn other words, the Gaussian auxiliary variable \\(z \\in \\mathbb{R}^D\\) is centred at \\(x\\) and at distance about \\(\\sqrt{\\varepsilon}\\) of it. Now, given the current position \\(x_k\\), to generate a proposal \\(y_\\star\\) that stays in the vicinity of \\(x_k\\), one can proceed in two steps, in the spirit of a Gibbs-sampling approach:\n\nFirst, generate \\(z_\\star \\sim \\overline{\\pi}(dz | x_k) \\sim \\mathcal{N}(x_k, \\sqrt{\\varepsilon}I)\\)\nSecond, sample from \\(y_\\star \\sim \\overline{\\pi}(dx | z_\\star)\\).\n\nUnfortunately, the second step is typically not tractable. Nevertheless, the conditional density \\(\\overline{\\pi}(dx | z_\\star)\\) is concentrated in a \\(\\sqrt{\\varepsilon}\\)-neighborhood of \\(z_\\star\\) and a simple Gaussian approximation around \\((x_k, z_\\star)\\) should be enough for our purpose. We have:\n\\[\n\\begin{align}\n\\log \\overline{\\pi}(dx | z_\\star)\n&=\n\\log \\pi(x) - \\frac{1}{2 \\varepsilon} \\|x - z_\\star\\|^2 + \\textrm{(Cst)}\\\\\n&\\approx\n\\left&lt;  \\nabla \\log \\pi(x_k), x-x_k  \\right&gt; - \\frac{1}{2 \\varepsilon} \\|x - z_\\star\\|^2 + \\textrm{(Cst)}\\\\\n&=\n- \\frac{1}{2 \\varepsilon} \\|x - [z_\\star + \\varepsilon\\, \\nabla \\log \\pi(x_k)]\\|^2 + \\textrm{(Cst)}.\n\\end{align}\n\\]\nThis shows that the conditional \\(\\overline{\\pi}(dx | z_\\star)\\) can be approximated by a Gaussian distribution centred at \\([z_\\star + \\nabla \\log \\pi(x_k)]\\) and variance \\(\\varepsilon\\, I\\). This means that the final proposal \\(y \\in \\mathbb{R}^D\\) can be generated as \\(y \\sim z_\\star + \\nabla \\log \\pi(x_k) + \\xi\\) where \\(\\xi \\sim \\mathcal{N}(0,\\varepsilon)\\). But that is equivalent to setting\n\\[\ny \\sim x + \\varepsilon\\, \\nabla \\log \\pi(x_k) + \\sqrt{2 \\varepsilon} \\, \\xi\n\\]\nwith \\(\\xi \\sim \\mathcal{N}(0,I)\\) since \\(z_\\star \\sim \\mathcal{N}(x, \\sqrt{\\varepsilon} I)\\). It is exactly the MALA proposal. Naturally, one can also try to be slightly more clever and use an extended distribution\n\\[\n\\overline{\\pi}(x,z) \\, \\propto \\, \\pi(x) \\, \\exp {\\left\\{  -\\frac{1}{2\\varepsilon} \\left&lt; (z-x), M^{-1} \\, (z-x) \\right&gt;  \\right\\}}\n\\]\nfor some appropriate positive-definite “mass” matrix \\(M \\in \\mathbb{R}^{D,D}\\). Indeed, this immediately leads to preconditioned MALA methods. I really like this approach since it can be adapted and generalized to quite a few other situations!\n\n\n\n\nReferences\n\nTitsias, Michalis K, and Omiros Papaspiliopoulos. 2018. “Auxiliary Gradient-Based Sampling Algorithms.” Journal of the Royal Statistical Society Series B: Statistical Methodology 80 (4). Oxford University Press: 749–67."
  },
  {
    "objectID": "notes/averaging_homogenization/averaging_homogenization.html",
    "href": "notes/averaging_homogenization/averaging_homogenization.html",
    "title": "Averaging and homogenization",
    "section": "",
    "text": "Averaging\nConsider a pair of (coupled) Markov processes \\(X_t \\in \\mathcal{X}\\) and \\(Y_t \\in \\mathcal{Y}\\) with dynamics that can informally be described as\n\\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon}/dt &= F(X^{\\varepsilon},Y^{\\varepsilon}, W^X)\\\\\ndY^{\\varepsilon}/dt &=  \\textcolor{red}{\\varepsilon^{-1}} \\, G(X^{\\varepsilon},Y^{\\varepsilon}, W^Y)\\\\\n\\end{align}\n\\right.\n\\]\nfor two independent “noise” terms \\(W^X\\) and \\(W^Y\\) and a time-scale parameter \\(\\varepsilon\\ll 1\\). We assume that \\(X\\) is a slow component that moves by \\(\\mathcal{O}(\\delta)\\) in on the time interval \\([t, t+\\delta]\\). The scaling \\(\\varepsilon^{-1}\\) in the dynamics of fast process \\(Y^{\\varepsilon}\\) indicates that we expect the process \\(Y\\) to evolve on a time scale of order \\(\\mathcal{O}(\\varepsilon)\\). We are interested in the limit \\(\\varepsilon\\to 0\\) and hope to “average out” the fast process \\(Y^{\\varepsilon}\\) and be able to describe the slow (and interesting) process \\(X^{\\varepsilon}\\) without referring to the fast process. Informally, we would like to describe the process \\(X^{\\varepsilon}\\), in the limit \\(\\varepsilon\\to 0\\), as following an effective Markovian dynamics\n\\[\ndX/dt = \\overline{F}(X, W^X).\n\\]\nFor describing the averaging phenomenon, we typically assume some ergodicity conditions on the fast process \\(Y\\). Here, we assume that for each fixed \\(x \\in \\mathcal{X}\\), the fast process process \\(Y^{[x]}\\) with fixed slow-component \\(x \\in \\mathcal{X}\\), i.e.\n\\[\ndY^{[x]}/dt = G(x,Y^{[x]}, W^Y)\n\\]\nis ergodic with respect to some probability distribution \\(\\rho_x(dy)\\). Although the averaging phenomenon is quite general, it is somewhat easier to illustrate it for diffusion processes. In this case, let us assume that the slow process is given by\n\\[\ndX^{\\varepsilon} = \\mu(X^{\\varepsilon}, Y^{\\varepsilon}) \\, dt + \\sigma(X^{\\varepsilon}, Y^{\\varepsilon}) \\, dW^x.\n\\]\nFor \\(X^{\\varepsilon}_{t} = x\\) and for a time increment \\(\\delta \\ll 1\\), since the process \\(X^{\\varepsilon}\\) can be considered constant we have\n\\[\n\\begin{align}\nX^{\\varepsilon}_{t+\\delta} - x\n&\\approx \\;\n{\\left(  \\frac{ \\int_{t}^{t+\\delta} \\mu(x, Y^{\\varepsilon}) \\, dt}{\\delta}  \\right)}  \\, \\delta + \\\n{\\left(  \\frac{ \\int_{t}^{t+\\delta} \\sigma^2(x, Y^{\\varepsilon}) }{\\delta} \\right)} ^{1/2} \\, \\mathcal{N}(0, \\delta).\n\\end{align}\n\\]\nThis can be regarded as a time-discretization of the averaged process\n\\[\ndX \\, = \\; \\overline{\\mu}(X) \\, dt + \\overline{\\sigma}(X) \\, dW\n\\]\nfor averaged drift and volatility functions give by\n\\[\n\\left\\{\n\\begin{align}\n\\overline{\\mu}(x)\n&=\n\\int \\mu(x,y) \\, \\rho_x(dy) \\\\\n\\overline{\\sigma}^2(x)\n&= \\int \\sigma^2(x,y) \\, \\rho_x(dy).\n\\end{align}\n\\right.\n\\tag{1}\\]\nOne standard approach for proving this type of results is to write the Kolmogorov equations\n\\[\\frac{d}{dt}\\varphi^{\\varepsilon}(x,y,t) = \\mathcal{L}^{\\varepsilon} \\varphi^{\\varepsilon}(x,y,t)\\] for \\(\\varphi^{\\varepsilon}(x,y,t) = \\mathbb{E}[\\varphi(X^{\\varepsilon}_{t}, Y^{\\varepsilon}_{t}, t) | X^{\\varepsilon}_{0}=x, Y^{\\varepsilon}_{0}=y]\\) and perform a multiscale expansion (Hinch 1991) (Pavliotis and Stuart 2008) (Weinan 2011)\n\\[\n\\varphi^{\\varepsilon}(x,y,t)\n=\nA(x,t) + \\varepsilon B(x,y,t) + \\mathcal{O}(\\varepsilon^2).\n\\tag{2}\\]\nIndeed, the first order term \\(A(x,t)\\) is expected to not depend on the initial condition \\(y\\) since the process \\((X^{\\varepsilon}_t, Y^{\\varepsilon}_t)\\) forgets \\(Y^{\\varepsilon}_0 = y\\) on time scales of order \\(\\varepsilon\\) and we are interested in the regime \\(\\varepsilon\\to 0\\). From Equation 2 one can obtain the dynamics of the averaged process described by the function \\(A(x,t)\\). One finds that \\(A\\) is described by the averaged generator of the slow component, i.e. averaging \\(\\mathcal{L}^{X^{\\varepsilon}}\\) under \\(\\rho_x(dy)\\); this exactly gives Equation 1 in the case of diffusions. A typical example could be as follows:\n\\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon} &= \\mu(X^{\\varepsilon}, Y^{\\varepsilon}) dt + \\sigma(X^{\\varepsilon}, Y^{\\varepsilon}) \\, dW^X\\\\\ndY^{\\varepsilon} &= - \\textcolor{red}{\\varepsilon^{-1}} \\frac{ (Y^{\\varepsilon} - X^{\\varepsilon}) }{\\sigma^2} \\, dt + \\sqrt{2  \\textcolor{red}{\\varepsilon^{-1}} } \\, dW^Y.\\\\\n\\end{align}\n\\right.\n\\]\nThe fast process \\(Y^{\\varepsilon}_t\\) is a Ornstein-Uhlenbeck process sped-up by a factor \\(1/\\varepsilon\\) that will very rapidly oscillate around \\(X^{\\varepsilon}_t\\), with Gaussian fluctuations with variance \\(\\sigma^2&gt;0\\), ie:\n\\[\n\\rho_x(dy) \\; = \\; \\frac{ e^{-(y-x)^2/2} }{\\sqrt{2\\pi \\sigma^2}}\\, dy.\n\\]\nThis averaging phenomenon is relatively straightforward and not extremely surprising. More interesting is the homogenization phenomenon described in the next Section.\n\n\nHomogenization\nConsider the presence of an additional intermediate time scale \\(\\varepsilon^{-1/2}\\), \\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon}/dt &=  \\textcolor{blue}{\\varepsilon^{-1/2} H(X^{\\varepsilon},Y^{\\varepsilon})} \\, + \\,F(X^{\\varepsilon},Y^{\\varepsilon}, W^X)\\\\\ndY^{\\varepsilon}/dt &=  \\textcolor{red}{\\varepsilon^{-1}} \\, G(X^{\\varepsilon},Y^{\\varepsilon}, W^Y)\\\\\n\\end{align}\n\\right.\n\\] with the same assumption that for any fixed \\(x \\in \\mathcal{X}\\) the process \\(dY^{[x]}/dt = G(x,Y^{[x]}, W^Y)\\) is ergodic with respect to the probability distribution \\(\\rho_x(dy)\\). The same reasoning as in the averaging case shows that averaging the term \\(F(X^{\\varepsilon},Y^{\\varepsilon}, W^X)\\) is relatively straightforward and has the exact same expression: it suffices to average under \\(\\rho_x(dy)\\). This means that one can study instead\n\\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon}/dt &=  \\textcolor{blue}{\\varepsilon^{-1/2} H(X^{\\varepsilon},Y^{\\varepsilon})} \\, + \\, \\overline{F}(X^{\\varepsilon}, W^X)\\\\\ndY^{\\varepsilon}/dt &=  \\textcolor{red}{\\varepsilon^{-1}} \\, G(X^{\\varepsilon},Y^{\\varepsilon}, W^Y)\\\\\n\\end{align}\n\\right.\n\\]\nwith, informally, \\(\\overline{F}(x,w) = \\int F(x,y,w) \\, \\rho_x(dy)\\). The new interesting phenomenon is coming from the intermediate time scale \\(\\varepsilon^{-1/2}\\). Contrarily to the averaging phenomenon of the previous section that was only relying on a Law of Large Numbers, dealing with the intermediate time-scale requires exploiting a CLT and quantifying the rate of mixing of the fast process \\(Y^{[x]}\\) Note that since \\(\\varepsilon^{-1/2} \\gg 1\\), for the dynamics to not explode one needs the centering condition:\n\\[\n\\int_{\\mathcal{Y}} H(x,y) \\, \\rho_x(dy) = 0\n\\qquad \\textrm{for all } x \\in \\mathcal{X}.\n\\tag{3}\\]\nBecause of the centering condition*, the term \\( \\textcolor{blue}{\\varepsilon^{-1/2} H(X^{\\varepsilon},Y^{\\varepsilon})}\\) will contribute an additional noise term in the effective dynamics of the slow process. To describe this additional noise term, assume an ergodic central limit theorem (CLT) for the fast process \\(dY^{[x]}/dt = G(x,Y^{[x]}, W^Y)\\): for a test function \\(\\varphi: \\mathcal{Y}\\to \\mathbb{R}\\) with zero expectation under \\(\\rho_x(dy)\\) we have:\n\\[\n\\lim_{t \\to \\infty} \\; T^{-1/2}\n\\int_{t=0}^T \\, \\varphi(Y^{[x]}_t) \\, dt\n\\; = \\; \\mathcal{N}(0, V_x[\\varphi])\n\\tag{4}\\]\nfor asymptotic variance \\(V_x[\\varphi] \\geq 0\\). For a time increment \\(\\delta &gt; 0\\) and assuming \\(X^{\\varepsilon}_{t}=x\\) we have\n\\[\n\\begin{align}\nX^{\\varepsilon}_{t+\\delta} - x\n&\\approx  \\textcolor{blue}{ \\varepsilon^{-1/2} \\, \\int_{u=t}^{t+\\delta} H(X^{\\varepsilon}_u,Y^{\\varepsilon}_u)} \\, du \\, + \\, \\int_{u=t}^{t+\\delta} \\overline{F}(x, W^X_u) \\, du.\n\\end{align}\n\\tag{5}\\]\nThe second integral term is an averaging term that can be treated easily. Approximating the process \\(t \\mapsto Y^{\\varepsilon}_t\\) by \\(t \\mapsto Y^{[x]}_{t \\varepsilon^{-1}}\\), the first integral on the RHS of Equation 5 can be approximated as\n\\[\n\\begin{align}\n\\underbrace{\\varepsilon^{-1/2} \\int_{u=t}^{t+\\delta}  H(x,Y^{[x]}_{u \\, \\varepsilon^{-1}}) \\, du}_{\\textrm{CLT}} \\,\n+\n\\underbrace{\\int_{u=t}^{t+\\delta} \\varepsilon^{-1/2} \\partial_x H(x,Y^{[x]}_{u \\, \\varepsilon^{-1}}) \\, (X^{\\varepsilon}_u - x) \\, \\, du}_{\\textrm{(drift)}}.\n\\end{align}\n\\]\nAfter a time-rescaling, one can readily see that the first term is described by the CLT of Equation 4,\n\\[\n\\varepsilon^{-1/2} \\int_{u=t}^{t+\\delta}  H(x,Y^{[x]}_{u \\, \\varepsilon^{-1}}) \\, du\n\\approx V_x[H(x, \\cdot)]^{1/2} \\mathcal{N}(0, \\delta).\n\\]\nThe second term is further approximated as\n\\[\n\\begin{align}\n\\varepsilon^{-1} \\, &\\int_{u=t}^{t+\\delta}\\int_{v=t}^{t+\\delta}\n\\partial_x H(x,Y^{[x]}_{u \\, \\varepsilon^{-1}}) \\, H(x,Y^{[x]}_{v \\, \\varepsilon^{-1}}) \\, 1_{v&lt;u} \\, du \\, dv\\\\\n&=  {\\left(  \\frac{1}{\\delta \\varepsilon^{-1}} \\int_{u=t}^{t+\\delta \\varepsilon^{-1}}\\int_{v=t}^{t+\\delta \\varepsilon^{-1}} \\partial_x H(x,Y^{[x]}_{u}) \\, H(x,Y^{[x]}_{v}) \\, 1_{v&lt;u}  \\right)}  \\, \\delta,\n\\end{align}\n\\]\nthe second equality coming from the time-rescaling \\(t \\mapsto t \\varepsilon\\). The process \\(Y^{[x]}\\) mixes on scale \\(\\mathcal{O}(1)\\) so that the term inside bracket \\( {\\left( \\ldots \\right)} \\) converges to its expectation. Setting \\(T = \\delta \\, \\varepsilon^{-1} \\to \\infty\\), one obtains\n\\[\n\\begin{align}\nI(x) &=\n\\frac{1}{T} \\iint_{[0,T]^2}\n\\partial_x H(x,Y^{[x]}_{u}) \\, H(x,Y^{[x]}_{v}) \\, 1_{v&lt;u}  \\, du \\, dv \\\\\n&\\to\n\\int \\rho_x(dy) \\, H(x,y) \\,   {\\left\\{  \\int_{s=0}^{\\infty} \\mathbb{E}[\\partial_x H(\\hat{x}, Y^{[x]}_s) \\, |  Y^{[x]}_0=y] \\, ds  \\right\\}} .\n\\end{align}\n\\]\nIn conclusion, the fast-slow system\n\\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon} &=  \\textcolor{blue}{\\varepsilon^{-1/2} \\, H(X^{\\varepsilon}, Y^{\\varepsilon})} \\, dt + \\mu(X^{\\varepsilon}, Y^{\\varepsilon}) \\, dt + \\sigma(X^{\\varepsilon}, Y^{\\varepsilon}) \\, dW^x\\\\\ndY^{\\varepsilon} &=  \\textcolor{red}{\\varepsilon^{-1}} \\, G(X^{\\varepsilon},Y^{\\varepsilon}, W^Y) \\, dt\n\\end{align}\n\\right.\n\\]\ncan be described in the regime \\(\\varepsilon\\to 0\\) by the effective dynamics\n\\[\ndX =  \\textcolor{blue}{I(X) \\, dt + \\Gamma^{1/2}(X) \\, dW^{H}}\n+\n\\overline{\\mu}(X) \\, dt + \\overline{\\sigma}(X) \\, dW^X.\n\\]\nfor two independent Brownian motions \\(W^X\\) and \\(W^H\\). The volatility terms \\( \\textcolor{blue}{\\Gamma(x)}\\) comes from the CLT and the drift term \\( \\textcolor{blue}{I(x)}\\) comes from the self-interaction term:\n\\[\n\\left\\{\n\\begin{align}\n\\Gamma(x)\n&= \\lim_{T \\to \\infty} \\; \\mathop{\\mathrm{Var}} {\\left\\{ T^{-1/2} \\int_{0}^T H(x, Y^{[x]}_t) \\, dt \\right\\}} \\\\\n%\nI(x)\n&= \\lim_{T \\to \\infty} \\; \\frac{1}{T} \\iint_{0&lt;u&lt;v&lt;T} H(x, Y^{[x]}_u) \\, \\partial_x H(x, Y^{[x]}_v) \\, du \\, dv.\n\\end{align}\n\\right.\n\\tag{6}\\]\nFor the drift function, the scaling \\(T^{-1} \\int_{0&lt;u&lt;v&lt;T} (\\ldots)\\) may look a bit surprising at first sight as one may expect \\(T^{-2} \\int_{0&lt;u&lt;v&lt;T} (\\ldots)\\) instead. Note that since the process \\(Y^{[x]}\\) mixes on a time scale \\(\\mathcal{O}(1)\\) and the centering condition \\(\\int H(x, y) \\rho_x(dy)=0\\) holds, the expectation \\(\\mathbb{E}[H(x, Y^{[x]}_u) \\, \\partial_x H(x, Y^{[x]}_v)]\\) goes to zero as soon as \\(|u-v| \\gg 1\\). This means that only the subset \\(|u-v| = \\mathcal{O}(1)\\) of \\([0,T]^2\\) really matters in that double integral, hence the \\((1/T)\\) normalization factor.\n\n\nClosed form solution & Poisson equation:\nThe drift and volatility terms \\(\\Gamma(x)\\) and \\(I(x)\\) quantify the mixing properties of the fast process \\(Y^{[x]}\\). While formulas Equation 6 are intuitive, they can be difficult to deal with if one needs the exact expressions of the drift and volatility functions. Instead, they can also be expressed in terms of the solution to an appropriate Poisson equations.\n\\[\n\\begin{align}\nI(x) &=\n\\frac{1}{T} \\iint_{[0,T]^2}\nH(x,Y^{[x]}_{v}) \\, \\partial_x H(x,Y^{[x]}_{u}) \\, 1_{v&lt;u}  \\, du \\, dv \\\\\n&\\to\n\\int \\rho_x(dy) \\, H(x,y) \\, \\partial_{\\hat{x}}  {\\left\\{  \\int_{s=0}^{\\infty} \\mathbb{E}[H(\\hat{x}, Y^{[x]}_s) \\, |Y^{[x]}_0=y] \\, ds  \\right\\}} \\\\\n&=\n-\\int \\rho_x(dy) \\, H(x,y) \\, \\partial_{x} \\Phi(x,y)\\\\\n&= -\\left&lt; H(x, \\cdot), \\partial_x \\Phi(x, \\cdot) \\right&gt;_{\\rho_x}\n\\end{align}\n\\tag{7}\\]\nwhere the function \\(\\Phi(x,y)\\) is solution to the Poisson equation\n\\[\n\\mathcal{L}^{Y^{[x]}} \\Phi(x, \\cdot) = H(x, \\cdot)\n\\]\nfor all \\(x \\in \\mathcal{X}\\) and \\(\\mathcal{L}^{Y^{[x]}}\\) is the generator of the fast process \\(dY^{[x]}/dt = G(x,Y^{[x]}, W^Y)\\). The last equality in Equation 7 follows from the integral representation of the Poisson equation. Similarly, and also as explained here, the asymptotic variance term can also be expressed in terms of the function \\(\\Phi\\),\n\\[\n\\begin{align}\n\\Gamma(x)\n&= \\lim_{T \\to \\infty} \\; \\mathop{\\mathrm{Var}} {\\left\\{ T^{-1/2} \\int_{0}^T H(x, Y^{[x]}_t) \\, dt \\right\\}} \\\\\n&= -2 \\int_{\\mathcal{Y}} \\Phi(x, y) \\, H(x, y) \\, \\rho_x(dy)\\\\\n&= -2 \\left&lt; \\Phi, \\mathcal{L}^{Y^{[x]}} \\Phi \\right&gt;_{\\rho_x}.\n\\end{align}\n\\]\n\n\nExample: integrated OU process\nConsider a slow process obtained by integrating an OU process,\n\\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon} &= \\varepsilon^{-1/2} \\, Y^{\\varepsilon} \\, dt\\\\\ndY^{\\varepsilon} &= -\\lambda \\varepsilon^{-1}\\, Y^{\\varepsilon} \\, dt + \\sqrt{2 \\lambda/\\varepsilon} \\, dW^Y,\n\\end{align}\n\\right.\n\\]\nwhere \\(\\lambda &gt; 0\\) is just a fixed time-scaling parameter. The fast OU process mixes on time scales of order \\(\\mathcal{O}(\\varepsilon)\\) and has a standard Gaussian distribution as invariant distribution. Homogenization gives that in the regime \\(\\varepsilon\\to 0\\), the slow process can be approximated as\n\\[\ndX = \\sqrt{2/\\lambda} \\, dW\n\\tag{8}\\]\nsince the asymptotic variance is\n\\[\n\\mathop{\\mathrm{Var}} {\\left\\{  T^{-1/2} \\int_{t=0}^{T} Y_t \\, dt \\right\\}}\n\\to\n2 \\, \\int_{0}^{\\infty} C(r) \\, dr = 2/\\lambda\n\\]\nwhere \\(C(r) = \\mathbb{E}[Y_t Y_{t+r}] = \\exp[-\\lambda r]\\) is the autocorrelation function of the fast OU process, as explained here. The fact that the effective diffusion is (twice) the integrated autocorrelation of the fast process is an example of Green-Kubo relations.\n\n\nExample: Overdamped Langevin Dynamics\nThis example does not exactly fall within the homogenization result described in the previous section, but almost. Consider a potential \\(U\\) and the slow-fast dynamics:\n\\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon} &= \\varepsilon^{-1/2} \\, Y^{\\varepsilon} \\, dt\\\\\ndY^{\\varepsilon} &= - \\varepsilon^{-1}\\, [Y^{\\varepsilon}+\\varepsilon^{1/2}\\nabla U(X^{\\varepsilon})] \\, dt + \\sqrt{2 /\\varepsilon} \\, dW^Y.\n\\end{align}\n\\right.\n\\]\nFor any fixed value of \\(x \\in \\mathcal{X}\\), the fast OU-dynamics\n\\[\ndY = -[Y^{\\varepsilon}+\\varepsilon^{1/2}\\nabla U(x)] \\, dt + \\sqrt{2} \\, dW^Y\n\\]\nconverges to a Gaussian distribution with mean \\(-\\nabla U(x)\\) and unit variance. The same arguments as the previous section immediately give that, starting from \\(X^{\\varepsilon}_0=x\\), we have\n\\[\n\\varepsilon^{-1/2} \\, \\int_{0}^{\\delta} Y^{\\varepsilon} \\, dt\n\\; \\to \\;\n-\\nabla U(x) \\, \\delta + \\sqrt{2 \\delta} \\, \\mathcal{N}(0,1).\n\\]\nThe \\(\\sqrt{2}\\) terms comes from the OU asymptotic variance. this shows that the slow process converges as \\(\\varepsilon\\to 0\\) to the overdamped Langevin dynamics\n\\[\ndX = -\\nabla U(X) \\; + \\; \\sqrt{2} \\, dW.\n\\]\n\n\nExample: Stratonovich Corrections\nConsider a function \\(f: \\mathbb{R}\\to \\mathbb{R}\\) and the slow-fast system\n\\[\ndX^{\\varepsilon} = \\varepsilon^{-1/2} \\, f(X^{\\varepsilon}) \\, Y^{\\varepsilon} \\, dt\n\\]\nwhere \\(dY^{\\varepsilon} = -(\\lambda/\\varepsilon) Y^{\\varepsilon} + \\sqrt{2 \\lambda / \\varepsilon}\\) is a fast OU process mixing on scales of order \\(\\mathcal{O}(\\varepsilon)\\) and with standard centred Gaussian invariant distribution \\(\\rho(dy)\\).The discussion leading to Equation 8 suggest that the term \\(\\varepsilon^{-1/2} \\, Y^{\\varepsilon} \\, dt\\) can be heuristically be thought of as \\((2/\\lambda)^{1/2} \\, dW\\), which would imply that the effective dynamics for the slow-process is\n\\[\ndX = \\sqrt{2/\\lambda} \\, f(X) \\, dW.\n\\]\nWe will see that this heuristic is wrong! In order to obtain the effective dynamics of the slow process as \\(\\varepsilon\\to 0\\), since the generator of the fast-OU reads \\(\\mathcal{L}\\varphi= \\lambda [ -y\\,\\varphi_y + \\varphi_{yy}]\\), one can solve the Poisson equation \\(\\mathcal{L}\\Psi(x,y) = f(x)y\\) to obtain that \\(\\Phi(x,y) = -f(x)y/\\lambda\\). One already knows that \\(\\mathop{\\mathrm{Var}}[T^{-1}\\int_{[0,T]} Y_t \\, dt] = 2/\\lambda\\). The drift term is given by\n\\[\n\\begin{align}\nI(x) &= \\int f(x) \\partial_x \\Phi(x,y) \\, \\rho(dy)\\\\\n&= \\lambda^{-1} \\int f(x) f'(x) y^2 \\, \\rho(dy)\\\\\n&= \\lambda^{-1} f(x) f'(x).\n\\end{align}\n\\]\nPutting everything together gives that the effective slow dynamics reads\n\\[\n\\begin{align}\ndX &=  \\textcolor{blue}{ \\lambda^{-1} f'(X) f(X) \\, dt } + \\sqrt{2/\\lambda} \\, f(X) \\, dW\\\\\n&= \\sqrt{2/\\lambda} \\, f(X)  \\textcolor{red}{\\circ} dW\n\\end{align}\n\\]\nwhere \\( \\textcolor{red}{\\circ}\\) denotes Stratonovich integration.\n\n\nReadings\nThe book (Pavliotis and Stuart 2008) is beautiful, and I quite like the section on multiscale expansion in (Weinan 2011). For proving this type of results with the “martingale problem” approach (Stroock and Varadhan 1997), the lectures (Papanicolaou 1977) are nicely done.\n\n\n\n\n\nReferences\n\nHinch, E. J. 1991. Perturbation Methods. Cambridge University Press.\n\n\nPapanicolaou, George. 1977. “Martingale Approach to Some Limit Theorems.” In Papers from the Duke Turbulence Conference, Duke Univ., Durham, NC, 1977.\n\n\nPavliotis, Grigoris, and Andrew Stuart. 2008. Multiscale Methods: Averaging and Homogenization. Springer Science & Business Media.\n\n\nStroock, Daniel W, and SR Srinivasa Varadhan. 1997. Multidimensional Diffusion Processes. Vol. 233. Springer Science & Business Media.\n\n\nWeinan, E. 2011. Principles of Multiscale Modeling. Cambridge University Press."
  },
  {
    "objectID": "notes/information_theory_shearer_lemma/shearer_lemma.html",
    "href": "notes/information_theory_shearer_lemma/shearer_lemma.html",
    "title": "Shearer’s lemma",
    "section": "",
    "text": "The Shearer’s lemma (Chung et al. 1986) is concerned with a generalization of the sub-additivity of the Shannon Entropy,\n\\[\nH(X_1, \\ldots, X_N) \\; \\leq \\; H(X_1) + \\ldots + H(X_N).\n\\]\nInstead, consider an integer \\(t \\geq 1\\) and a family \\(S_1, \\ldots, S_K\\) of subsets of \\(\\{1, \\ldots, N\\}\\) such that any index \\(1 \\leq n \\leq N\\) appears in at least \\(t\\) of these subsets. Note that for a subset \\(S_i = \\{ \\alpha_1, \\ldots, \\alpha_{r_i}\\}\\) with \\(\\alpha_1 &lt; \\ldots &lt; \\alpha_{r_i}\\) we have\n\\[\n\\begin{align}\nH(X_{S_i}) &\\equiv H(X_{\\alpha_1}, \\ldots, X_{\\alpha_{r_i}})\\\\\n&= H(X_{\\alpha_1}) + H(X_{\\alpha_2} | X_{\\alpha_1}) + \\ldots + H(X_{\\alpha_{r_i}} | X_{\\alpha_{r_i-1}}, \\ldots, X_{\\alpha_1} ) \\\\\n&\\geq H(X_{\\alpha_1}) + H(X_{\\alpha_2} | X_{1:(\\alpha_2-1)}) + \\ldots + H(X_{\\alpha_{r_i}} | X_{1:\\alpha_{r_i}-1}).\n\\end{align}\n\\tag{1}\\]\nSince each index appears in at least \\(t\\) of the subsets, summing Equation 1 over all the subset \\(S_i\\) yields\n\\[\n\\sum_{i=1}^K H(X_{S_k}) \\geq t \\, \\sum_{i=1}^N H(X_i \\, | X_{1:(i-1)}) = t \\, H(X).\n\\]\nThis means that the following inequality holds,\n\\[\nH(X) \\leq \\frac{1}{t} \\, \\sum_{k=1}^K H(X_{S_k})\n\\]\nIndeed, the standard sub-additivity property of the entropy corresponds to the set \\(S_k = [k]\\) for \\(1 \\leq k \\leq N\\) and \\(t=1\\).\n\nApplication: projection on hyperplanes\nConsider a measurable set \\(A \\subset \\mathbb{R}^n\\) and call \\(A_k\\) the projection of \\(A\\) on the hyperplane \\(\\{x=(x_1, \\ldots, x_n) \\in \\mathbb{R}^n \\, : \\, x_k=0\\}\\). A Theorem of Loomis and Whitney (Loomis and Whitney 1949) states that the lebesgue measure \\(|A|\\) of the set \\(A\\) satisfies\n\\[\n|A| \\; \\leq \\; \\prod_{k=1}^n |A_k|^{1/(n-1)}.\n\\]\nIn other words, if all the projections \\(A_k\\) of the set \\(A\\) are small then, necessarily, the set \\(A\\) itself is small. To proceed, one can approximate this set \\(A\\) with the union \\(A_{\\varepsilon}\\) of small cubes of side \\(\\varepsilon\\) centred on \\(\\varepsilon\\, \\mathbb{Z}^n\\). If one can prove the statement for \\(A^{[\\varepsilon]}\\), the results follows from a standard approximation argument (ie. outer measure). Now, each cube can be indexed with a \\(n\\)-uple of integers \\((x_1, \\ldots, x_n) \\in \\mathbb{Z}^n\\), and one can consider the random variable \\(X=(X_1, \\ldots, X_n)\\) that is uniformly distributed on the set of cubes coordinates. Because \\(2^{H(X)} = |A^{[\\varepsilon]}| / \\varepsilon^n\\) and \\(2^{H(X_2, \\ldots, X_n)} = |A_1^{[\\varepsilon]}| / \\varepsilon^n\\) etc…, choosing the subsets \\(S_i=[1:n] \\setminus \\{i\\}\\) and \\(t = (n-1)\\) in Shearer’s Lemma immediately gives the conclusion.\n\n\n\n\n\nReferences\n\nChung, Fan RK, Ronald L Graham, Peter Frankl, and James B Shearer. 1986. “Some Intersection Theorems for Ordered Sets and Graphs.” Journal of Combinatorial Theory, Series A 43 (1). Academic Press: 23–37.\n\n\nLoomis, Lynn H, and Hassler Whitney. 1949. “An Inequality Related to the Isoperimetric Inequality.”"
  },
  {
    "objectID": "notes/HJB/HJB.html",
    "href": "notes/HJB/HJB.html",
    "title": "Doob, Girsanov and Bellman",
    "section": "",
    "text": "Richard Bellman (1920 – 1984)\n\n\n\n\nChange of measure\nConsider a diffusion in \\(\\mathbb{R}^D\\) with deterministic starting position \\(x_0 \\in \\mathbb{R}^D\\) and dynamics\n\\[\ndX_t = b(X_t)dt + \\sigma(X_t) \\, dW_t\n\\]\nfor a drift and volatility functions \\(b: \\mathbb{R}^D \\to \\mathbb{R}^D\\) and \\(\\sigma: \\mathbb{R}^D \\to \\mathbb{R}^{D \\times D}\\). On the time interval \\([0,T]\\), this defines a probability \\(\\mathbb{P}\\) on the path-space \\(C([0,T];\\mathbb{R}^D)\\). For two functions \\(f: [0,T] \\times \\mathbb{R}^D \\to \\mathbb{R}\\) and \\(g: \\mathbb{R}^D \\to \\mathbb{R}\\), consider the probability distribution \\(\\mathbb{Q}\\) defined as\n\\[\n\\frac{d \\mathbb{Q}}{d \\mathbb{P}} = \\frac{1}{\\mathcal{Z}} \\exp  {\\left\\{  \\int_0^T f(X_s) \\, ds + g(X_T)  \\right\\}}\n\\]\nwhere \\(\\mathcal{Z}\\) denotes the normalizing constant \\[\n\\mathcal{Z}\\; = \\; \\mathbb{E} {\\left[  \\exp  {\\left\\{  \\int_0^T f(X_s) \\, ds + g(X_T)  \\right\\}}   \\right]} .\n\\tag{1}\\]\nThe distribution \\(\\mathbb{Q}\\) places more probability mass on trajectories such that \\(\\int_0^T f(X_s) \\, ds + g(X_T)\\) is large. As described in these notes on Doob h-transforms, the tilted probability distribution \\(\\mathbb{Q}\\) can be described by a diffusion process \\(X^\\star\\) with dynamics\n\\[\ndX^\\star = b(X^\\star)dt + \\sigma(X^\\star) \\,  {\\left\\{  dW_t +  \\textcolor{blue}{u^\\star(t, X^\\star)} \\, dt  \\right\\}} .\n\\]\nThe control function \\( \\textcolor{blue}{u^\\star: [0,T] \\times \\mathbb{R}^D \\to \\mathbb{R}^D}\\) is of the gradient form\n\\[\n\\textcolor{blue}{u^\\star(t, x)} \\; = \\; \\sigma^\\top(x) \\, \\nabla \\log[  \\textcolor{green}{h(t,x)} ]\n\\tag{2}\\]\nand the function \\( \\textcolor{green}{h(t,x)}\\) is described by the conditional expectation,\n\\[\n\\textcolor{green}{h(t,x) = \\mathbb{E} {\\left[  \\exp  {\\left\\{  \\int_t^T f(X_s) \\, ds + g(X_T)  \\right\\}}   \\mid X_t = x  \\right]} }.\n\\]\nThe expression \\( \\textcolor{blue}{u^\\star(t, x)} \\; = \\; \\sigma^\\top(x) \\, \\nabla \\log[  \\textcolor{green}{h(t,x)} ]\\) is intuitive; to describe the tilted measure \\(\\mathbb{Q}\\) that places more probability mass on trajectories such that \\(\\exp {\\left\\{  \\int_0^T f(X_s) \\, ds + g(X_T)  \\right\\}} \\) is large, the optimal control \\(u^\\star(t,x)\\) should point towards promising states, i.e. states such that the expected “reward-to-go” quantity \\(\\mathbb{E} {\\left[  \\exp  {\\left\\{  \\int_t^T f(X_s) \\, ds + g(X_T)  \\right\\}}  \\mid X_t = x  \\right]} \\) is large.\n\n\nVariational Formulation\nTo obtain a variational description of the optimal control function \\( \\textcolor{blue}{u^\\star}\\), it suffices to express it as the solution of an optimization problem of the form\n\\[\nu^\\star \\; = \\; \\mathop{\\mathrm{argmin}}\\; \\textrm{Dist} {\\left( \\mathbb{P}^{u} \\, , \\, \\mathbb{Q} \\right)}\n\\]\nfor an appropriately chosen distance. The KL-divergence is natural since it elegantly deals with the intractable constant \\(\\mathcal{Z}\\) and the ratio \\(d \\mathbb{P}^{u} / d \\mathbb{Q}\\) is easy to compute. Girsanov Theorem gives that that for a controlled diffusion\n\\[\ndX^u = b(X^u)dt + \\sigma(X^u) \\,  {\\left\\{  dW_t +  \\textcolor{blue}{u(t, X^{u})} \\, dt  \\right\\}}\n\\tag{3}\\]\nwith associated path measure \\(\\mathbb{P}^u\\), we have that \\[\n\\frac{d\\mathbb{Q}}{d\\mathbb{P}^u}(X^u) =\n\\exp {\\left\\{  \\int_{0}^{T} -\\tfrac 12 \\|u(s,X^u_s)\\|^2 + f(X^u_s)  \\, ds - \\int_{0}^{T} u(s,X^u_s)^\\top \\, dW_s + g(X^u_T)  \\right\\}}  / \\mathcal{Z}.\n\\]\nMinimizing the \\(D_{\\text{KL}}(\\mathbb{P}^u,\\mathbb{Q})\\) shows that the optimal control and the normalization constant \\(\\mathcal{Z}\\) are:\n\\[\n(-\\log \\mathcal{Z}, \\, u^\\star)\n\\; = \\;\n(\\inf_{u} , \\, \\mathop{\\mathrm{argmin}}_{u}) \\;\n\\mathbb{E} {\\left[  \\int_{0}^{T} \\tfrac 12 \\|u(s,X^u_s)\\|^2 - f(X^u_s)  \\, ds - g(X^u_T)  \\right]} .\n\\]\nMinimizing this loss attempts to find a control that drives the quantity \\(\\int_{0}^{T} f(X^u_s) \\, ds + g(X^u_T)\\) large while keeping the control effort \\(\\int_{0}^{T} \\|u(s,X^u_s)\\|^2 \\, ds\\) small. Equivalently, this can be expressed as a maximization problem,\n\\[\n(\\log \\mathcal{Z}, \\, u^\\star)\n\\; = \\;\n(\\sup_{u} , \\, \\mathop{\\mathrm{argmax}}_{u}) \\;\n\\mathbb{E} {\\left[  \\int_{0}^{T} -\\tfrac 12 \\|u(s,X^u_s)\\|^2 + f(X^u_s)  \\, ds + g(X^u_T)  \\right]} .\n\\]\nNote that since \\(\\mathbb{P}^{u_{\\star}} = \\mathbb{Q}\\), the optimal control \\(u^\\star\\) is such that for any trajectory we have:\n\\[\n\\log \\mathcal{Z}\n\\; = \\;\n\\int_{0}^{T} -\\tfrac12 \\|u^\\star(s,X^{u^{\\star}}_s)\\|^2 + f(X^{u^{\\star}}_s) \\, ds + g(X^{u^{\\star}}_T) - \\int_{0}^{T} u^{\\star}(s,X^{u^{\\star}}_s)^\\top \\, dW_s .\n\\tag{4}\\]\n\n\n\n\nStochastic Control\nIn the previous section, there was nothing special about the starting point \\(x_0\\) and the time horizon \\(T&gt;0\\). This means that the same derivation gives the solution to the following stochastic optimal control problem. Consider the reward-to-go function (a.k.a. value function) defined as\n\\[\nV(t,x) = \\sup_u \\; \\mathbb{E} {\\left[  \\int_{t}^{T} -\\tfrac 12 \\|u(s,X^u_s)\\|^2 + f(X^u_s) \\, ds + g(X^u_T) \\mid X_t = x \\right]}\n\\]\nwhere \\(X^u\\) is the controlled diffusion Equation 3. We have that\n\\[\n\\begin{align}\nV(t,x)\n&= \\log \\mathbb{E} {\\left[  \\exp  {\\left\\{  \\int_t^T f(X_s) \\, ds + g(X_T)  \\right\\}}  \\mid X_t = x \\right]} \\\\\n&= \\log[  \\textcolor{green}{h(t, x)} ].\n\\end{align}\n\\]\nThis shows that optimal control \\(u^\\star\\) can also be expressed as\n\\[\nu^\\star(t,x) = \\sigma^\\top(x) \\nabla \\log[  \\textcolor{green}{ h(t,x) }]\n= \\sigma^\\top(x) \\, \\nabla V(t,x) .\n\\tag{5}\\]\nThe expression \\(\\sigma^\\top(x) \\, \\nabla V(t,x)\\) is intuitive: since we are trying to maximize the reward-to-go function, the optimal control should be in the direction of the gradient of the reward-to-go function.\n\n\nHamilton-Jacobi-Bellman\nFinally, let us mention that one can easily derive the Hamilton-Jacobi-Bellman equation for the reward-to-go function \\(V(t,x)\\). We have\n\\[\nV(t,x) = \\sup_u \\; \\mathbb{E} {\\left[  \\int_{t}^T C_s \\, ds + g(X^u_T) \\right]}\n\\]\nwith \\(C_s = -\\tfrac12 \\|u(s,X^u_s)\\|^2 + f(X^u_s)\\). For \\(\\delta \\ll 1\\), we have\n\\[\n\\begin{align}\nV(t,x)\n&\\; = \\;\n\\sup_u \\;  {\\left\\{  C_t \\, \\delta + \\mathbb{E} {\\left[  V(t+\\delta, X^u_{t+\\delta}) \\mid X^u_t=x \\right]}   \\right\\}}  + o(\\delta)\\\\\n&\\; = \\;\nV(t,x) + \\delta \\, \\sup_{u(t,x)} \\;  {\\left\\{  C_t + (\\partial_t + \\mathcal{L}+ \\sigma(x) \\, u(t,x) \\, \\nabla) \\, V(t,x) \\right\\}}  + o(\\delta)\n\\end{align}\n\\]\nwhere \\(\\mathcal{L}= b \\nabla + \\sigma \\sigma^\\top : \\nabla^2\\) is the generator of the uncontrolled diffusion. Since \\(C_t = -\\tfrac12 \\|u(t,x)\\|^2 + f(x)\\) is a simple quadratic function, the supremum over the control \\(u(t,x)\\) can be computed in closed form,\n\\[\n\\begin{align}\nu^\\star(t,x)\n&= \\mathop{\\mathrm{argmax}}_{z \\in \\mathbb{R}^D} \\; -\\tfrac12 \\|z\\|^2 + \\left&lt; z, \\sigma^\\top(x) \\nabla V(t,x)  \\right&gt;\\\\\n&= \\sigma^\\top(x) \\, \\nabla V(t,x),\n\\end{align}\n\\]\nas we already knew from Equation 5. This implies that the reward-to-go function \\(V(t,x)\\) satisfies the HJB equation\n\\[\n{\\left( \\partial_t + \\mathcal{L} \\right)} V + \\frac12 \\| \\sigma^\\top \\nabla V \\|^2 + f = 0\n\\tag{6}\\]\nwith terminal condition \\(V(T,x) = g(x)\\). Another route to derive Equation 6 is to simply use the fact that \\(V(t,x) = \\log h(t,x)\\); since the Feynman-Kac gives that the function \\(h(t,x)\\) satisfies \\((\\partial_t + \\mathcal{L}+ f) h = 0\\), the conclusion follows from a few lines of algebra by starting writing \\(\\partial_t V = h^{-1} \\, \\partial_t h = -h^{-1}(\\mathcal{L}+ f)[h]\\), expanding \\(\\mathcal{L}h\\) and expressing everything back in terms of \\(V\\). The term \\(\\|\\sigma^\\top \\nabla V\\|^2\\) naturally arises when expressing the diffusion term \\(\\sigma \\sigma^\\top : \\nabla^2 h\\) as a function of the second derivative of \\(V\\); it is the idea of the standard Cole-Hopf transformation.\nFinally, Ito’s lemma and Equation 6 give that for \\(t_1 &lt; t_2\\), the optimally controlled diffusion \\(X^{u^\\star}\\) satisfies:\n\\[\nV(t_2, X^{u^\\star}_{t_2})\n-\nV(t_1, X^{u^\\star}_{t_1}) =\n\\int_{t_1}^{t_2} \\tfrac12 \\, \\|u^\\star(s,X^{u^\\star}_s)\\|^2 - f(X^{u^\\star}_s) \\, ds\n+ \\int_{t_1}^{t_2} u^\\star(s,X^{u^\\star}_s)^\\top \\, dW_s.\n\\]\nNaturally, writing this expression in between time \\(t_1=0\\) and \\(t_2=T\\) gives the expression Equation 4 for the log-normalizing constant \\(\\log \\mathcal{Z}\\)."
  },
  {
    "objectID": "notes/MCMC_deterministic_proposals/MCMC_deterministic.html",
    "href": "notes/MCMC_deterministic_proposals/MCMC_deterministic.html",
    "title": "Metropolis-Hastings ratio with deterministic proposals",
    "section": "",
    "text": "Consider a probability density \\(\\pi(x)\\) on \\(\\mathbb{R}^d\\) and a (deterministic) function \\(F: \\mathbb{R}^d \\to \\mathbb{R}^d\\). Assume further that \\(F\\) is an involution in the sense that\n\\[\nF(F(x)) = x\n\\]\nfor all \\(x \\in \\mathbb{R}^d\\). To keep things simple since it is not really the point of this short note, suppose that \\(\\pi(x)&gt;0\\) everywhere and that \\(F\\) is smooth. This type of transformations can be used to define Markov Chain Monte Carlo algorithms, eg. the standard Hamiltonian Monte Carlo (HMC) algorithm. To design a MCMC scheme with this involution \\(F\\), one needs to answer the following basic question: suppose that \\(X \\sim \\pi(dx)\\) and the proposal \\(Y = F(X)\\) is constructed and accepted with probability \\(\\alpha(X)\\), how should the acceptance probability function \\(\\alpha: \\mathbb{R}^d \\to [0,1]\\) be chosen so that the resulting random variable \\[Z \\; = \\; Y \\, B + (1-B) \\, X\\] is also distributed according to \\(\\pi(dx)\\)? The Bernoulli random variable \\(B\\) is such that \\(\\mathbb{P}(B=1|X=x)=\\alpha(x)\\). In other words, for any test function \\(\\varphi: \\mathbb{R}^d \\to \\mathbb{R}\\), we would like \\(\\mathbb{E}[\\varphi(Z)] = \\mathbb{E}[\\varphi(X)]\\), which means that\n\\[\n\\int  {\\left\\{  \\varphi(F(x)) \\, \\alpha(x) + \\varphi(x) \\, (1-\\alpha(x))  \\right\\}}  \\, \\pi(dx) = \\int \\varphi(x) \\, \\pi(dx).\n\\tag{1}\\]\nRequiring for Equation 1 to hold for any test function \\(\\varphi\\) is easily seen to be equivalent to asking for the equation\n\\[\n\\alpha(x) \\, \\pi(x) \\; = \\; \\alpha(y) \\, \\pi(y) \\, |J_F(x)|\n\\]\nto hold for any \\(x \\in \\mathbb{R}^d\\) where \\(y=F(x)\\) and \\(J_F(x)\\) is the Jacobian of \\(F\\) at \\(x\\). Since \\(|J_F(y)| \\times |J_F(x)| = 1\\) because the function \\(F\\) is an involution, this also reads\n\\[\n\\alpha(x) \\, \\frac{\\pi(x) }{|J_F(x)|^{1/2}} \\; = \\;\n\\alpha(y) \\, \\frac{\\pi(y) }{|J_F(y)|^{1/2}}.\n\\]\nAt this point, it becomes clear to anyone familiar with the the correctness-proof of the usual Metropolis-Hastings algorithm that a possible solution is\n\\[\n\\alpha(x) \\; = \\; \\min {\\left\\{ 1, \\frac{\\pi(y) / |J_F(y)|^{1/2}}{\\pi(x) / |J_F(x)|^{1/2}} \\right\\}}\n\\]\nalthough there are indeed many other possible solutions. Since \\(|J_F(y)| \\times |J_F(x)| = 1\\), this also reads\n\\[\n\\alpha(x) \\; = \\; \\min {\\left\\{ 1, \\frac{\\pi(y)}{\\pi(x)} \\, |J_F(x)| \\right\\}} .\n\\tag{2}\\]\nOne can reach a similar conclusion by looking at the Radon-Nikodym ratio \\([\\pi(dx) \\otimes q(x,dy)] / [\\pi(dy) \\otimes q(y,dx)]\\) where \\(q(x,dy)\\) is the markov kernel described the deterministic transformation (Green 1995), but I do not find this approach significantly simpler. The very neat article (Andrieu, Lee, and Livingstone 2020) describes much more sophisticated and interesting generalizations. Indeed, Equation 2 is often used in the simpler case when \\(F\\) is volume preserving, i.e. \\(|J_F(x)|=1\\), as is the case for the Hamiltonian Monte Carlo (HMC). The discussion above was prompted by a student implementing a variant of this but with the wrong acceptance ratio \\(\\alpha(x) = \\min {\\left\\{ 1, \\frac{\\pi(y)}{\\pi(x)} \\, \\frac{|J_F(x)|}{|J_F(y)|} \\right\\}} \\) and us taking quite a bit of time to find the bug…\nNote that there are interesting and practical situations when the function \\(F\\) satisfies the involution property \\(F(F(x))=x\\) only when \\(x\\) belongs to a subset of the state-space. For instance, this can happen when implementing MCMC on a manifold \\(\\mathcal{M} \\subset \\mathbb{R}^d\\) and the function \\(F\\) involves a “projection” on the manifold \\(\\mathcal{M}\\), as for example described in the really interesting article (Zappa, Holmes-Cerfon, and Goodman 2018). In that case, it suffices to add a “reversibility check”, i.e. make sure that when applying \\(F\\) to the proposal \\(y=F(x)\\), one goes back to \\(x\\) in the sense that \\(F(y)=x\\). The acceptance probability in that case should be amended and expressed as\n\\[\n\\alpha(x) \\; = \\; \\min {\\left\\{ 1, \\frac{\\pi(y)}{\\pi(x)} \\, |J_F(x)| \\right\\}}  \\, \\mathbf{1} {\\left( F(y)=x \\right)} .\n\\]\nIn other words, if applying \\(F\\) to the proposal \\(y=F(x)\\) does not lead back to \\(x\\), the proposal is always rejected.\n\nSame, but without involution\nin some situations, the requirement for \\(F\\) to be an involution can seem cumbersome. What if we consider the more general situation of a smooth bijection \\(T: \\mathbb{R}^d \\to \\mathbb{R}^d\\) and its inverse \\(T^{-1}\\)? In that case, one can directly apply what has been described in the previous section: it suffices to consider an extended state-space \\((x,\\varepsilon)\\) obtained by including an index \\(\\varepsilon\\in \\{-1,1\\}\\) and the involution \\(F\\) defined as\n\\[\nF:\n\\left\\{\n\\begin{align}\n(x,\\varepsilon=+1) &\\mapsto (T(x), \\varepsilon=-1)\\\\\n(x,\\varepsilon=-1) &\\mapsto (T^{-1}(x), \\varepsilon=+1).\n\\end{align}\n\\right.\n\\tag{3}\\]\nThis allows one to define a Markov kernel that lets the distribution \\(\\overline{\\pi}(x, \\varepsilon) = \\pi(dx)/2\\) invariant. Things can even start to get a bit more interesting if a deterministic “flip” \\((x, \\varepsilon) \\mapsto (x, -\\varepsilon)\\) is applied after each application of the Markov kernel above describe: doing so avoids immediately coming back to \\(x\\) in the event the move \\((x,\\varepsilon) \\mapsto (T^{\\varepsilon}(x), -\\varepsilon)\\) is accepted. There are indeed quite a few papers exploiting this type of ideas.\n\n\nA mixture of deterministic transformations?\nTo conclude these notes, here is a small riddle whose answer I do not have. One can check that for any \\(c \\in \\mathbb{R}\\), the function \\(F_{c}(x) = c + 1/(x-c)\\) is an involution of the real line. This means that for any target density \\(\\pi(x)\\) on the real line, one can build the associated Markov kernel \\(M_c\\) defined as\n\\[\nM_c(x, dy) = \\alpha_c(x) \\, \\delta_{F_c(x)}(dy) + (1-\\alpha_c(x)) \\, \\delta_x(dy)\n\\]\nfor an acceptance probability \\(\\alpha_c(x)\\) described as above,\n\\[\n\\alpha_c(x) = \\min {\\left\\{ 1, \\frac{\\pi[F_c(x)]}{\\pi(x)} |F'_c(x)| \\right\\}} .\n\\]\nFinally, choose a \\(N \\geq 2\\) values \\(c_1, \\ldots, c_N \\in \\mathbb{R}\\) and consider the mixture of Markov kernels\n\\[\nM(x,dy) \\; = \\; \\frac{1}{N} \\sum_{i=1}^N M_{c_i}(x, dy).\n\\]\nThe Markov kernel \\(M(x, dy)\\) lets the distribution \\(\\pi\\) invariant since each Markov kernel \\(M_{c_i}(x, dy)\\) does, but it is not clear at all (to me) under what conditions the associated MCMC algorithm does converge to \\(\\pi\\). One can empirically check that if \\(N\\) is very small, things can break down quite easily. On the other, for \\(N\\) large, the mixuture of Markov kernels \\(M(x,dy)\\) empirically seems to behave as if it were ergodic with respect to \\(\\pi\\).\n\n\n\n\n\n\n\nFor \\(N=5\\) values \\(c_1, \\ldots, c_5 \\in \\mathbb{R}\\) chosen at random, the illustration aboves shows the empirical distribution of the associated Markov chain ran for \\(T=10^6\\) iterations and targeting the standard Gaussian distribution \\(\\pi(dx) \\equiv \\mathcal{N}(0,1)\\): the fit seems almost perfect.\n\n\n\n\n\nReferences\n\nAndrieu, Christophe, Anthony Lee, and Sam Livingstone. 2020. “A General Perspective on the Metropolis-Hastings Kernel.” arXiv Preprint arXiv:2012.14881.\n\n\nGreen, Peter J. 1995. “Reversible Jump Markov Chain Monte Carlo Computation and Bayesian Model Determination.” Biometrika 82 (4). Oxford University Press: 711–32.\n\n\nZappa, Emilio, Miranda Holmes-Cerfon, and Jonathan Goodman. 2018. “Monte Carlo on Manifolds: Sampling Densities and Integrating Functions.” Communications on Pure and Applied Mathematics 71 (12). Wiley Online Library: 2609–47."
  },
  {
    "objectID": "notes/information_theory_shannon_coding/information_theory_shannon_coding.html",
    "href": "notes/information_theory_shannon_coding/information_theory_shannon_coding.html",
    "title": "Shannon Source Coding Theorem",
    "section": "",
    "text": "Claude Shannon (1916 – 2001)\n\n\n\n\nTransmission through a noisy channel\nConsider a scenario involving a “noisy channel,” where a message \\((x_1,x_2, \\ldots)\\) expressed in an alphabet \\(\\mathcal{X}\\) is transmitted before being received as a potentially different and corrupted message \\((y_1, y_2,\\ldots)\\) expressed using a potentially different alphabet \\(\\mathcal{Y}\\). One can assume that letter \\(x \\in \\mathcal{X}\\) is transformed into \\(y \\in \\mathcal{Y}\\) with probability \\(p(x \\to y)\\) so that the matrix \\(M_{x,y} = [p(x \\to y)]_{(x,y) \\in \\mathcal{X}\\times \\mathcal{Y}}\\) has rows summing-up to one, and that the “letters” of the message \\((x_1 x_2 \\ldots)\\) are transmitted one by one and independently from each other (ie. there is no memory effect in the channel).\nNow, imagine I have a text that needs to be transmitted through this channel. Assume that the text is represented using \\(N\\) bits. My goal is to encode it in a way that introduces redundancy, utilizing the alphabet \\(\\mathcal{X}\\), so that it can be transmitted through the noisy channel. Eventually, I want it to be successfully decoded, allowing the original message to be recovered with minimal errors.\n\n\n\n\nA Mathematical Theory of Communication\n\n\n\nIf transmitting each letter from the alphabet \\(\\mathcal{X}\\) takes \\(1\\) unit of time, I need to estimate the overall time it will take to transmit the entire text of \\(N\\) bits. In cases where the channel does not completely destroy the information, one can use about any non-idiotic encoding and transmit the same text multiple times to increase the chances of accurately recovering the original message. This can be achieved by employing techniques such as majority voting to decode the received messages, which are all distorted versions of the original text.\nThe transmission rate represents the inverse of the time required to transfer a single bit of information:\n\\[\n\\textrm{R = (Transmission Rate)} = \\frac{1}{\\textrm{(average time it takes to transfer one bit)}}.\n\\]\nIn other words, it takes about \\(N \\times R\\) unit of times to transfer a text of \\(N\\) bits. Moreover, the error rate refers to the percentage of errors in the decoded message, indicating the fraction of erroneous bits among the \\(N\\) decoded bits. Naturally, there exists a tradeoff, and it is evident that one can reach a vanishing error rate if one is willing to allow an arbitrarily slow transmission rate (eg. majority voting after transmitting a very large number of times the same text). For example, if \\(\\mathcal{X}= \\mathcal{Y}= \\{0,1\\}\\) and bits are flipped with probability \\(p_{\\text{flip}} \\ll 1\\), transmitting the text \\((2K+1)\\) times would lead to a transmission rate of \\(R = 1/(2K+1)\\) and an error rate approximately equal to \\(p_{\\text{flip}}^{K+1}\\).\nThe groundbreaking discovery made by Shannon is that it is possible to achieve a vanishing error rate even when transmitting at a finite transmission rate. He also managed to identify this optimal transmission rate. Shannon’s paper (Shannon 1948) is beautifully written and surprisingly readable for a text written more than 50 years ago.\n\n\nVanishing error rate: Shannon Codebooks\nLet’s imagine that we have a piece of information encoded in a variable, \\(X\\). We send \\(X\\) through a noisy channel, and at the other end we receive a somewhat distorted message, \\(Y\\). So, how much of our original information actually was transmitted? To reconstruct our original message, \\(X\\), using our received message, \\(Y\\), we require an average of \\(H(X|Y)\\) additional bits of information. On average, \\(X\\) contains \\(H(X)\\) bits of information. So, if we encode \\(H(X)\\) bits of useful information in \\(X\\), the variable \\(Y\\) that is correlated with \\(X\\) still holds \\(I(X;Y) = H(X) - H(X \\, | Y)\\) bits of that original information. The quantity \\(I(X;Y)\\) is the mutual information between the random variables \\(X\\) and \\(Y\\). In a noisy channel that transmits one “letter” at a time, the conditional probabilities \\(p(x \\rightarrow y)\\) are fixed. However, we can optimize the distribution of incoming messages. For instance, we can choose to transmit letters that are less likely to be corrupted. This discussion suggests that on average, transmitting \\(N\\) symbols through the channel can provide up to \\(N \\times C\\) bits of information, where \\(C = \\max I(X;Y)\\), the maximization being over the distribution of \\(X\\) while keeping the conditional probabilities \\(p(x \\rightarrow y)\\) fixed. It may seem that this implies a noisy channel cannot transmit information at a rate higher than \\(C\\). This hypothesis was precisely proven by Claude Shannon, who further established that this transmission rate can indeed be reached.\nTo prove that this transmission rate is achievable, Shannon’s idea was to simultaneously encode blocks of letters. To put it simply, consider the \\(2^N\\) feasible blocks \\(\\{ t^{[1]}, \\ldots, t^{[2^N]} \\}\\) of \\(N\\) binary letters. Each block \\(t^{[i]} \\in \\{0,1\\}^N\\) has \\(N\\) binary letters, \\(t^{[i]} = (t_1^{[i]}, \\ldots, t_N^{[i]})\\). Associate to each of block \\(t^{[i]} \\in \\{0,1\\}^N\\) a codeword \\(x^{[i]} \\in \\mathcal{X}^K\\) of size \\(K\\) in the alphabet \\(\\mathcal{X}\\). The set of these \\(2^N\\) codewords is usually called the codebook,\n\\[\n\\mathcal{C}= \\left\\{ x^{[1]}, x^{[2]}, \\ldots, x^{[2^N]} \\right\\} \\; \\subset \\mathcal{X}^{K}\n\\tag{1}\\]\nTo transmit a block of \\(N\\) letters from the original text, this block is first transformed into its associated codeword \\(x=(x_1, \\ldots, x_K) \\in \\mathcal{X}^K\\). This codeword is then sent through the noisy channel, resulting in a received message \\((y_1, \\ldots, y_K) \\in \\mathcal{Y}^K\\). The objective is to design a codebook with enough redundancy so that one can reconstruct the original codeword from the received message \\((y_1, \\ldots, y_K)\\): the higher the ratio \\(K/N\\), the larger the redundancy and the easier it should be to achieve this goal. The transmission rate is defined as \\(R = \\frac{N}{K}\\) since transmitting a binary text of length \\(N\\) with vanishing errors takes \\(K\\) units of time.\nFor generating the codebook in Equation 1, Shannon adopted a simple approach consisting in generating each \\(x^{[i]}_k\\) for \\(1 \\leq i \\leq 2^N\\) and \\(1 \\leq k \\leq K\\) independently at random from some (encoding) distribution \\(p_{\\text{code}}(dx)\\). The choice of this encoding distribution can be optimized at a later stage.\nConsider the codeword \\(x^{[0]} = (x^{[0]}_1, \\ldots, x^{[0]}_K)\\). After being transmitted through the noisy channel, this gives rise to a message \\(y_{\\star}\\). The codeword \\(x^{[0]}\\) can be easily recovered if \\((x^{[0]}, y_\\star)\\) is typical while all the other pairs \\((x^{[i]}, y_\\star)\\) for \\(2 \\leq i \\leq 2^N\\) are atypical. Since there are about \\(2^{K \\, H(X | Y)}\\) elements \\(x \\in \\mathcal{X}^K\\) such that \\((x, y_\\star)\\) is typical, and each codeword was chosen approximately uniformly at random within its typical set of size \\(2^{K \\, H(X)}\\), the probability for a random codeword to be atypical is about\n\\[1-2^{-K \\, [H(X) - H(X|Y)]} = 1 - 2^{-K \\, I(X;Y)}\\]\nConsequently, the probability that all the other pairs \\((x^{[i]}, y_\\star)\\) for \\(2 \\leq i \\leq 2^N\\) are atypical is\n\\[\np_{\\text{success}} = (1 - 2^{-K \\, I(X;Y)})^{2^N-1} \\approx (1 - 2^{-K \\, I(X;Y)})^{2^{KR}}.\n\\]\nThe probability \\(p_{\\text{success}} \\to 1\\) as soon as \\(R &lt; I(X;Y)\\) as \\(N \\to \\infty\\). Furthermore, remembering that one were free to optimize the encoding distribution \\(p_{\\text{code}}(dx)\\), a vanishing error rate is possible as soon as the transmission \\(R\\) rate is lower than\n\\[\n\\text{(Channel Capacity)} = C \\equiv \\max_{p_{\\text{code}}} \\; I(X;Y).\n\\]\nTo sum-up, consider \\(p_{\\mathcal{C}, \\text{success}}\\) the success rate of the codebook \\(\\mathcal{C}\\), ie. the probability that a random codeword of \\(\\mathcal{C}\\) is successfully decoded when passing through the noisy channel. The reasoning above shows that the averaged success rate \\(p_{\\text{success}} = \\left&lt; p_{\\mathcal{C}, \\text{success}} \\right&gt;\\), i.e. averaging \\(p_{\\mathcal{C}, \\text{success}}\\) over all possible codebooks \\(\\mathcal{C}\\), converges to one as long as the transmission rate is below the channel capacity \\(C\\). This means that one can find at least one codebook that works well! This reasoning is an example of the “probabilistic method”… Indeed, one also expect most random codebook to work well!\n\n\nNo vanishing error below the channel capacity\nTo demonstrate that transmission at vanishing error-rate is impossible when the transmission rate exceeds the channel capacity, \\(C\\), we can utilize Fano’s inequality.\nImagine selecting a message \\(M\\) uniformly at random within \\(\\{0,1\\}^N\\) and encode this message into the sequence \\(X=(X_1, ..., X_K) \\in \\mathcal{X}^K\\). We send \\(X\\) through a channel with capacity \\(C\\) and receive a corresponding, though somewhat distorted, signal \\(Y=(Y_1, ..., Y_K)\\). Finally, we decode this received message into \\(\\widehat{M}\\), an estimate of our original message:\n\\[\nM \\rightarrow X \\rightarrow Y \\rightarrow \\widehat{M}.\n\\]\nFano’s inequality points out that the error probability, \\(p_E = \\mathbb{P}(\\widehat{M} \\neq M)\\) is such that\n\\[\n\\begin{align}\nH(M | \\widehat{M})\n&\\leq 1 + p_E \\, \\log_2(\\# \\textrm{possible values of } M)\\\\\n&= 1 + p_E \\, N\n\\end{align}\n\\]\nApplying the data-processing inequality to \\(M \\rightarrow X \\rightarrow Y \\rightarrow \\widehat{M}\\) proves:\n\\[\n\\begin{align}\nN &= H(M) = H(M | \\widehat{M}) + I(M; \\widehat{M}) \\\\\n& \\leq H(M | \\widehat{M}) + I(X; Y)\\\\\n& \\leq 1 + N \\, p_E + I(X; Y).\n\\end{align}\n\\]\nTo wrap up, recall that each received letter \\(Y_i\\) in the message (Y_1, , Y_K)$ depends solely on the corresponding letter \\(X_i\\) in the message sent through the channel. This implies that \\(I(X; Y) \\leq \\sum_{i=1}^K I(X_i; Y_i) \\leq K \\, C\\).This yields:\n\\[\nN \\leq 1 + N \\, p_E + K \\, C.\n\\]\nThis reveals that for the probability of error to go to zero, i.e. \\(p_E \\rightarrow 0\\) as \\(N \\rightarrow \\infty\\), the transmission rate \\(N/K\\) must be lower than \\(C\\).\n\n\nExperiment\nConsider the Binary Symmetric Channel (BSC) that randomly flips \\(0 \\mapsto 1\\) and \\(1 \\mapsto 0\\) with equal probability \\(0&lt;q&lt;1\\). The capacity of this channel is easily computed and equals \\(C = 1 - h_2(q)\\) where \\(h_2(q) = -[q \\, \\log_2(q) + (1-q) \\, \\log_2(1-q)]\\) is the binary entropy function: the optimal encoding distribution is\n\\[p_{\\text{code}}(0) = p_{\\text{code}}(2) = 1/2.\\]\nFor a flipping rate of \\(q=0.1\\) the channel capacity equals \\(C=0.53\\). To estimate the performance of the random Shannon codebook strategy, I chose \\(N=13\\) and several values of \\(K \\geq N\\). This means generating a random codebook \\(\\mathcal{C}= \\{x^{[1]}, \\ldots, x^{[2^N]}\\}\\) of size \\(2^{13} = 8192\\) consisting of random binary vectors of size \\(K\\). For a randomly chosen codeword \\(x^{[i]}\\), a received message \\(y_\\star\\) is generated by flipping each of the \\(K\\) coordinates of \\(x^{[i]}\\) independently with probability \\(q\\). In the BSC setting, it is easily seen that the codeword of \\(\\mathcal{C}\\) that was the most likely to have originated \\(y_{\\star}\\) is\n\\[\nx_\\star \\; = \\; \\mathop{\\mathrm{argmin}}_{x \\in \\mathcal{C}} \\; \\|x - y_\\star\\|_{L^2}.\n\\]\nThe nearest neighbor \\(x_\\star\\) can be relatively efficiently computed with a nearest-neighbor routine (eg. FAISS). The figure below reports the probability of error (i.e. “Block Error Rate”),\n\\[\n\\text{(Block Error Rate)} \\; = \\; \\mathbb{P}(x_\\star \\neq x^{[i]})\n\\]\nwhen the codeword \\(x^{[i]}\\) is chosen uniformly at random within the codebook.\n\n\n\n\n\n\n\nIt can be seen that, although the error rate does go to zero for low transmission rate, the choice of \\(K = N / C\\) where \\(C\\) is the channel capacity still yields a relatively large block error rate. This indicates that the block size \\(N=13\\) is still far too low for the “law of large number” arguments presented in the previous section to kick-in. I did try for \\(N=20\\) and a codebook of \\(2^{20} \\approx 10^6\\) and the performace was still not impressive. This shows that even though the Shannon codebook approaches is an elegant construction, it is far from being practically useful. It requires a very large codebook of size \\(2^N\\) and decoding requires doing a nearest-neighbors search that can become slow as \\(N\\) increases.\n\n\n\n\n\nReferences\n\nShannon, Claude Elwood. 1948. “A Mathematical Theory of Communication.” The Bell System Technical Journal 27 (3). Nokia Bell Labs: 379–423."
  },
  {
    "objectID": "notes/information_theory_basics/information_theory_entropy.html",
    "href": "notes/information_theory_basics/information_theory_entropy.html",
    "title": "Information Theory: Entropy and Basic Definitions",
    "section": "",
    "text": "Claude Shannon (1916 – 2001)\n\n\n\n\nShannon Entropy & Compression\nIf Alice chooses a number \\(X\\) uniformly at random from the set \\(\\{1,2, \\ldots, N\\}\\), Bob can use a simple “dichotomy” strategy to ask Alice \\(\\log_2(N)\\) binary Yes/No questions and correctly identify the number. This result can be generalized to non-uniform distributions (cf. Huffman codes, and also Kraft-McMillan inequality). If Alice chooses a number \\(X\\) from \\(\\{1,2, \\ldots, N\\}\\) with probabilities \\(\\mathbb{P}(X=k) = p_k\\), Bob can design a deterministic strategy to find the answer using, on average, about\n\\[\nH(X) = - \\sum_{k=1}^N p_k \\, \\log_2(p_k)\n\\tag{1}\\]\nbinary questions, ie. bits. To be more precise, there are strategies that require at most \\(H(X) + 1\\) questions on average, and none that can require less than \\(H(X)\\). Note that applying this remark to an iid sequence \\(X_{1:T} = (X_1, \\ldots, X_T)\\) and using the the fact that \\(H(X_1, \\ldots, X_T) = T \\, H(X)\\), this shows that one can exactly determining the sequence \\(X_{1:T}\\) with at most \\(T \\, H(X) + 1\\) binary questions on average. The quantity \\(H(X)\\) defined in Equation 1, known as the Shannon Entropy of the distribution \\((p_1, \\ldots, p_N)\\), also implies that there are strategies that can encode each integer \\(1 \\leq x \\leq N\\) as a binary string of length \\(L(x)\\) (i.e. with \\(L(x)\\) bits), with the expected length \\(\\mathbb{E}[L(X)]\\) approximately equal to \\(H(X)\\). It is because a sequence of binary questions can be thought of as a binary tree, etc…\nThis remark can be used for compression. Imagine a very long sequence \\((X_1, \\ldots, X_T)\\) of iid samples from \\(X\\). Encoding each \\(X_i\\) with \\(L(X_i)\\) bits, one should be able to encode the resulting sequence with\n\\[\nL(X_1) + \\ldots + L(X_T) \\approx T \\, \\mathbb{E}[L(X)] \\approx T \\cdot H(X)\n\\]\nbits. Can the usual zip compression algorithm do this? To test this, choose a probability distribution on \\(\\{1, \\ldots, N\\}\\), generate an iid sequence of length \\(T \\gg 1\\), compress this using the \\(\\texttt{gzip}(\\ldots)\\) command, and finally look at the size of the resulting file (in bits). I have done that a few times, with a few different values of \\(16 \\leq N \\leq 256\\) and a few random distributions on \\(\\{1, \\ldots, N\\}\\), and with \\(T = 10^6\\). The plot of size of the compressed files versus the Shannon entropy \\(H\\) looks as below:\n\n\n\n\n\n\n\nSeems like the zip-algorithm works almost optimally for compressing iid sequences.\n\n\nSequence of random variables\nNow consider a pair of discrete random variables \\((X,Y)\\). If Alice draws samples from this pair of rvs, one can ask \\(H(X,Y)\\) binary questions on average to exactly find out these values. To do that, one can ask \\(H(X)\\) questions to estimate \\(X\\), and once \\(X=x\\) is estimated, one can then ask about \\(H(Y|X=x) = -\\sum_y \\mathbb{P}(Y=y|X=x) \\, \\log_2(\\mathbb{P}(Y=y|X=x))\\) to estimate \\(Y\\). This strategy requires on average \\(H(X) + \\sum_x \\mathbb{P}(X=x) \\, H(Y|X=x)\\) binary questions and is actually optimal, showing that\n\\[\nH(X,Y) = H(X) + H(Y | X)\n\\tag{2}\\]\nwhere we have defined \\(H(Y | X) = \\sum_x \\mathbb{P}(X=x) \\, H(Y|X=x)\\).\nIndeed, one can generalize these concepts to more than two random variables. Iterating Equation 2 shows that the trajectory \\(X_{1:T} \\equiv (X_1, \\ldots, X_N)\\) of a stationary ergodic Markov chain can be estimated on average with \\(H(X_{1:T})\\) binary questions where\n\\[\n\\begin{align}\nH(X_{1:T})\n&= H(X_1) + H(X_2|X_1) + \\ldots + H(X_{T} | X_{t-1})\\\\\n&\\approx T \\, H(X_{k+1} | X_k)\\\\\n&= - T \\, \\sum_x \\pi(x) \\, \\sum_{y} p(x \\to y) \\, \\log_2[p(x \\to y)].\n\\end{align}\n\\]\nHere, \\(\\pi(dx)\\) is the equilibrium distribution of the Markov chain and \\(p(x \\to y)\\) are the transition probabilities.\nCan \\(\\texttt{gzip}(\\ldots)\\) compress Markovian trajectories and roughly achieve that level of compression? Indeed, one can test this by generating random Markov transition matrices (that are, with very high probability, ergodic with respect to an equilibrium distribution \\(\\pi\\)). Doing this with trajectories of length \\(10^4\\) (ie. quite short because it is quite slow to) on \\(\\{1, \\ldots, N\\}\\) with \\(2 \\leq N \\leq 64\\), one get the following results:\n\n\n\n\n\n\n\nIn red is the entropy estimated without using the Markovian structure and assuming that the \\(X_i\\) are iid samples. One can clearly see that the dependencies between successive samples are exploited for better compression. Nevertheless, the optimal compression rate given by the Shannon entropy is not reached. Indeed, \\(\\texttt{gzip}(\\ldots)\\) is not an optimal algorithm – it cannot even compress well enough the sequence \\((1,2,3,1,2,3,1,2,3,\\ldots)\\)!\n\n\nAsymptotic Equipartition Property (AEP)\nThe AEP is simple remark that gives a convenient way of reasoning about long sequences random variables \\(X_{1:T} = (X_1, \\ldots, X_T)\\) with \\(T \\gg 1\\). For example, assuming that the random variables \\(X_i\\) are independent and identically distributed as the random variable \\(X\\), the law of large numbers (LLN) gives that\n\\[\n-\\frac{1}{T} \\, \\log_2 p(X_{1:T}) = -\\frac{1}{T} \\, \\sum \\log_2 p(X_i) \\approx H(X).\n\\]\nThis means that any “typical” sequence has a probability about \\(2^{-T \\, H(X)}\\) of occurring, which also means that there are about \\(2^{T \\, H(X)}\\) such “typical” sequences. Indeed, one could use the LLN for Markov chains and obtain a similar statement for Markovian sequences. All this can be made rigorous with large deviation theory, for example. This also establishes the link with the “statistical physics” definition of entropy as the logarithm of the number of configurations… Set of the type\n\\[\nA_{\\varepsilon} = \\left\\{ x_{1:T} \\; : \\; \\left| -\\frac{1}{T} \\, \\log_2 p(x_{1:T}) - H(X)\\right| &lt; \\varepsilon\\right\\}\n\\]\nare usually called typical set: for any \\(\\varepsilon&gt; 0\\), the probability of \\(X_{1:T}\\) to belongs to \\(A_{\\varepsilon}\\) goes to one as \\(T \\to \\infty\\). For these reasons, it is often a good heuristic to think of a draw of \\((X_1, \\ldots, X_T)\\) as a uniformly distributed on the associated typical set. For example, if \\((X_1, \\ldots, X_N)\\) are \\(N\\) iid draws from a Bernoulli distribution with \\(\\mathbb{P}(X=1) = 1-\\mathbb{P}(X=0) =p\\), the set \\(A \\subset \\{0,1\\}^N\\) of sequences such that \\(x_1 + \\ldots + x_N = Np\\) has \\(\\binom{N}{Np} \\approx 2^{N \\, h_2(q)}\\) elements where \\(h_2(q) = -[q \\, \\log_2(q) + (1-q) \\, \\log_2(q)]\\) is the entropy of a \\(\\text{Bern}(q)\\) random variable.\n\n\nMutual information\nConsider a pair of random variables \\((X,Y)\\). Assuming that \\(X\\) stores (on average) \\(H(X)\\) bits of useful information, how much of this information can be extracted from \\(Y\\)? Let us call this quantity \\(I(X;Y)\\) since we will see in a second that this quantity is symmetric. If \\(Y\\) is independent from \\(X\\), no useful information about \\(X\\) is contained in \\(Y\\) and \\(I(X;Y) = 0\\). On the contrary, if \\(X=Y\\), the knowledge of \\(Y\\) already contains all the information about \\(Y\\) and \\(I(X;Y) = H(X) = H(Y)\\). If one knows \\(Y\\), one needs on average \\(H(X|Y)\\) binary questions (ie. bits of additional information) in order to determine \\(X\\) certainly and recover all the information contained in \\(X\\). This means that the knowledge of \\(Y\\) already contains \\(I(X;Y) = H(X) - H(X|Y)\\) useful bits of information about \\(X\\)! This quantity is called the mutual information of the two random variable \\(X\\) and \\(Y\\), and it has the good taste of being symmetric:\n\\[\n\\begin{align}\nI(X;Y) &= H(X) - H(X|Y) \\\\\n&= H(X) + H(Y) - H(X,Y).\n\\end{align}\n\\]\nNaturally, one can define conditional version of it by setting \\(I(X;Y \\, |Z) = \\sum_{z} \\mathbb{P}(Z=z) \\, I(X_z | Y_z)\\) where \\((X_z, Y_z)\\) has the law of \\((X,Z)\\) conditioned on \\(Z=z\\). Since \\(I(X;Y \\,|Z)\\) is the reduction in uncertainty of \\(X\\) due to \\(Y\\) when \\(Z\\) is given, there are indeed situations when \\(I(X;Y \\, | Z)\\) is larger than \\(I(X;Y)\\) – it is to be contrasted to the intuitive inequality \\(H(X|Z) \\leq H(X)\\), which is indeed true. A standard such examples is when \\(X\\) and \\(Y\\) are independent \\(\\text{Bern}(1/2)\\) random variables and \\(Z = X+Y\\): a short computation gives that \\(I(X;Y \\, | Z) = 1/2\\) while, indeed, \\(I(X;Y) = 0\\). This definition of conditional mutual information leads to a chain-rule property,\n\\[\nI(X; (Y_1,Y_2)) = I(X;Y_1) + I(X;Y_2 | Y_1),\n\\]\nwhich can indeed be generalized to any number of variables. Furthermore, if the \\(Y_i\\) are conditionally independent given \\(X\\) (eg. if \\(X=(X_1, \\ldots, X_T)\\) and \\(Y_i\\) only depend on \\(X_i\\)), then the sub-additivity of the entropy readily gives that\n\\[\nI(X; (Y_1, \\ldots, Y_N)) \\leq \\sum_{i=1}^N I(X; Y_i).\n\\]\nImportantly, algebra shows that \\(I(X;Y)\\) can also be expressed as the Kullback-Leibler divergence between the joint distribution \\(\\mathbb{P}_{(X,Y)}\\) and the product of the marginals \\(\\mathbb{P}_X \\otimes \\mathbb{P}_Y\\),\n\\[\nI(X;Y) \\; = \\;\nD_{\\text{KL}} {\\left(  (X,Y) \\, \\| \\, X \\otimes Y \\right)} .\n\\]\nThis diagram from (MacKay 2003) nicely illustrate the different fundamental quantities \\(H(X)\\) and \\(H(X,Y)\\) and \\(H(Y|X)\\) and \\(I(X;Y)\\) and \\(H(X,Y)\\):\n\n\n\n\nFrom: Information Theory, Inference, and Learning Algorithms\n\n\n\nNaturally, if one considers three random variables \\(X \\mapsto Y \\mapsto Z\\) forming a “Markov chain”, we have the so-called data-processing inequality,\n\\[\nI(X;Z) \\leq I(X;Y)\n\\qquad \\text{and} \\qquad\nI(X;Z) \\leq I(Y;Z).\n\\]\nThe first inequality is clear since all the useful information contained in \\(Z\\) must be coming from \\(Y\\), and \\(Y\\) only contains \\(I(X;Y)\\) bits about \\(X\\). For the second inequality, note that if \\(Z\\) contains \\(I(Y;Z)\\) bits about \\(Y\\), and \\(Y\\) contains \\(H(Y;X)\\) bits about \\(X\\), then \\(Z\\) cannot contain more than \\(I(Y;Z)\\) bits of \\(X\\):\n\n\n\n\nData Processing Inequality for Markov \\(X \\to Y \\to Z\\)\n\n\n\n\n\n\n\n\nReferences\n\nMacKay, David JC. 2003. Information Theory, Inference and Learning Algorithms. Cambridge university press."
  },
  {
    "objectID": "notes/index_notes_as_list.html",
    "href": "notes/index_notes_as_list.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n05-04-2025\n\n\nSelf Avoiding Walks\n\n\nmonte-carlo\n\n\n\n\n22-02-2025\n\n\nJarzynski and Crooks\n\n\nSDE,markov\n\n\n\n\n08-12-2024\n\n\nBasic Conformal Inference\n\n\nconformal\n\n\n\n\n30-07-2024\n\n\nVIASM mini-course on diffusions and flows\n\n\ndiffusion\n\n\n\n\n11-06-2024\n\n\nDoob, Girsanov and Bellman\n\n\nSDE,markov\n\n\n\n\n03-06-2024\n\n\nGirsanov and Importance Sampling\n\n\nSDE,markov\n\n\n\n\n14-05-2024\n\n\nJoe Doob & Change of measures on path-space\n\n\nSDE,markov\n\n\n\n\n09-03-2024\n\n\nRWM & HMC on manifolds\n\n\nMCMC,manifold\n\n\n\n\n18-12-2023\n\n\nMetropolis-Hastings ratio with deterministic proposals\n\n\nauxiliary-variable\n\n\n\n\n28-11-2023\n\n\nAveraging and homogenization\n\n\ndiffusion\n\n\n\n\n18-11-2023\n\n\nEnsemble Kalman Smoother (EnKS)\n\n\nenkf,data-assimilation\n\n\n\n\n11-11-2023\n\n\nAsymptotic variance & Poisson Equation\n\n\nmarkov\n\n\n\n\n23-10-2023\n\n\nGaussian Assimilation & the EnKF\n\n\nenkf,data-assimilation\n\n\n\n\n19-10-2023\n\n\nDeriving Langevin MCMC\n\n\nMCMC\n\n\n\n\n16-10-2023\n\n\nWasserstein Gradients & Langevin Diffusions\n\n\ndiffusion\n\n\n\n\n09-10-2023\n\n\nSanov’s Theorem\n\n\nLargeDeviation\n\n\n\n\n03-10-2023\n\n\nAuxiliary variable trick\n\n\nauxiliary-variable\n\n\n\n\n02-10-2023\n\n\nShearer’s lemma\n\n\ninfoTheory\n\n\n\n\n30-09-2023\n\n\nInformation Theory: References and Readings\n\n\ninfoTheory\n\n\n\n\n26-09-2023\n\n\nShannon Source Coding Theorem\n\n\ninfoTheory\n\n\n\n\n23-09-2023\n\n\nInformation Theory: Fano’s inequality\n\n\ninfoTheory\n\n\n\n\n23-09-2023\n\n\nInformation Theory: Entropy and Basic Definitions\n\n\ninfoTheory\n\n\n\n\n02-07-2023\n\n\nFrom Denoising Diffusion to ODEs\n\n\nDDPM,score\n\n\n\n\n02-07-2023\n\n\nDenoising Diffusion Probabilistic Models (DDPM)\n\n\nDDPM,score\n\n\n\n\n12-06-2023\n\n\nReverse diffusions, Score & Tweedie\n\n\nSDE,score\n\n\n\n\n01-01-2023\n\n\nNotes\n\n\nindex\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/DDPM/DDPM.html",
    "href": "notes/DDPM/DDPM.html",
    "title": "Denoising Diffusion Probabilistic Models (DDPM)",
    "section": "",
    "text": "Setting & Goals\nConsider \\(N\\) samples \\(\\mathcal{D}\\equiv \\{x_i\\}_{i=1}^N\\) in \\(\\mathbb{R}^D\\) from an unknown data distribution \\(\\pi_{\\mathrm{data}}(dx)\\). We would like to build an mechanism that can generate other samples from this data distribution. Implicitly, this means that we will be fitting a statistical model to this finite set of samples and have an algorithmic procedure to generate samples from the fitted probabilistic model. This type of models that directly define a stochastic procedure that generates data are called implicit probabilistic models in the ML literature.\n\n\nOrnstein–Uhlenbeck Noising process\nDenoising Diffusion Probabilistic Models (DDPMs) work as follows. Consider a diffusion process \\(\\{ X_t \\}_{t=0}^T\\) that starts from the data distribution \\(p_0(dx) \\equiv \\pi_{\\mathrm{data}}(dx)\\) at time \\(t=0\\). The notation \\(p_t(dx)\\) refers to the marginal distribution of the diffusion at time \\(0 \\leq t \\leq T\\). Assume furthermore that at time \\(t=T\\), the marginal distribution is (very close to) a reference distribution \\(p_T(dx) = \\pi_{\\mathrm{ref}}(dx)\\) that is straightforward to sample from. Typically, \\(\\pi_{\\mathrm{ref}}(dx)\\) is an isotropic Gaussian distribution. This diffusion process is often called the noising process since it transform the data distribution into a reference measure that can be thought of as “pure noise”. It is often chosen as an Ornstein–Uhlenbeck (OU) diffusion,\n\\[\ndX = - \\frac12 X \\, dt + dW.\n\\tag{1}\\]\nThis diffusion is reversible with respect to, and quickly converges to, the reference distribution \\(\\pi_{\\mathrm{ref}} = \\mathcal{N}(0, I)\\) and has the good taste of having simple transition densities: the law of \\(X_{t+s}\\) given that \\(X_t = x_t\\) is the same as \\(e^{-s/2} x_t + \\sqrt{1-e^{-s}} \\, \\mathbf{n}\\), which we write as\n\\[\n\\alpha_s \\, x + \\sigma_s \\, \\mathbf{n}\n\\]\nwith \\(\\alpha_s^2 + \\sigma_s^2 = 1\\) and \\(\\alpha_s = e^{-s/2}\\) and \\(\\mathbf{n}\\sim \\pi_{\\mathrm{ref}} = \\mathcal{N}(0,I)\\). The forward probability transitions are tractable and given by \\[\n\\mathbb{P}(X_{t+s} \\in dy \\, | \\, X_t = x )\n\\; \\propto \\;\n\\exp {\\left\\{ -\\frac{(y - \\alpha_s \\, x)^2}{2 \\, \\sigma^2_s} \\right\\}}  \\, dy.\n\\tag{2}\\]\nThis also means that one can directly generate samples from \\(p_t(dx)\\) by first choosing a data samples \\(x_i\\) from the data distribution \\(p_{\\mathrm{data}} \\equiv p_0\\) and blend it with noise by setting \\(x_i^{(t)} = \\alpha_t \\, x_i + \\sigma_t \\, \\mathbf{n}\\). In other words, one does not need to simulate the diffusion to generate samples from the data distribution, i.e. the method is “simulation free”.\n\n\nThe reverse diffusion\nIn order to generate samples from the data distribution, the DDPM strategy consists in sampling from the Gaussian reference measure \\(\\pi_{\\mathrm{ref}}\\) at time \\(t=T\\) and simulate the OU process backward in time. In other words, one would like to simulate from the reverse process \\(\\overleftarrow{X}_t\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIn other words, the reverse process is distributed as \\(\\overleftarrow{X}_0 \\sim \\pi_{\\mathrm{ref}}\\) at time \\(t=0\\) and, crucially, we have that \\(\\overleftarrow{X}_T \\sim \\pi_{\\mathrm{data}}\\). Furthermore, and as explained in this note, the reverse diffusion follows the dynamics\n\\[\nd\\overleftarrow{X}_t = {\\color{red} + }\\frac12 \\overleftarrow{X}_t \\, dt\n\\; {\\color{red} + \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\;\n+ dB\n\\tag{3}\\]\nwhere \\(B\\) is another Wiener process. I have used the notation \\(B\\) to emphasize that there is no link between this Wiener process and the one used to simulate the forward process. Note that if the initial data distribution \\(p_0 \\equiv \\pi_{\\mathrm{data}}\\) were equal to the reference measure \\(\\pi_{\\mathrm{ref}}\\), i.e. \\(p_0 = p_T = \\pi_{\\mathrm{ref}}\\) then it is easy to see that the reverse diffusion would have exactly teh same dynamics as the forward diffusion. In order to simulate the reverse diffusion, one needs to be able to estimate the new term \\({\\color{red}\\nabla \\log p_{T-t}(x)}\\) called the score. If one can estimate the score, it is straightforward to simulate the reverse process \\(\\overleftarrow{X}_t\\) and obtain samples from the data distribution. Estimating the score is the crux of the game.\n\n\nDenoising to estimating the score\nIn practice, the score is unknown and one has to build an approximation of it\n\\[\\mathcal{S}^\\theta(t,x) \\; \\approx \\; \\nabla_x \\log p_t(x).\\]\nThe approximate score \\(\\mathcal{S}^\\theta(t,x)\\) is often parametrized by a neural network with parameters \\(\\theta \\in \\Theta\\). Since\n\\[\n\\log p_t(x_t) \\; = \\; \\log \\int \\; p(x_t \\mid x_0)\\; \\pi_{\\mathrm{data}}(d x_0)\n\\]\nit follows that \\(\\nabla_x \\log p_t(x)\\) is given by\n\\[\n\\nabla_{x_t} \\log p_t(x_t)\n=\n\\frac{\\int \\;  \\textcolor{blue}{ \\Big\\{ \\nabla_{x_t} \\log p(x_t \\mid x_0) \\Big\\} } \\; p(x_t \\mid x_0) \\, \\pi_{\\mathrm{data}}(d x_0)}{\\int \\; p(x_t \\mid x_0)\\; \\pi_{\\mathrm{data}}(d x_0)}\n\\]\nwhich is just a conditional expectation:\n\\[\n\\nabla_{x_t} \\log p_t(x_t)\n\\;=\\;\n\\mathbb{E}[  \\textcolor{blue}{ \\nabla_{x_t} \\log p(x_t \\mid x_0) } \\mid x_t]\n\\tag{4}\\]\nand Equation 4 is true for any diffusion process, not only the OU process. Nevertheless, when applied to the OU process of Equation 1 with tractable transition probabilities \\(p(x_t \\mid x_0)\\) as described in Equation 2, algebra gives that\n\\[\n\\nabla_{x_t} \\log p_t(x_t) \\; = \\; -\\frac{x_t - \\alpha_t \\,  \\textcolor{green}{\\widehat{x}_0(x_t,t)}}{\\sigma_t^2}\n\\tag{5}\\]\nwhere \\(\\widehat{x}_0(x_t,t)\\) is a “denoising” estimate of the initial position \\(x_0\\) given a noisy estimate \\(X_t=x_t\\) at time \\(t\\), i.e.\n\\[\n\\textcolor{green}{ \\widehat{x}_0(x_t,t) \\; = \\; \\mathbb{E}[X_0  \\; \\mid \\; X_t = x_t] }.\n\\]\nNote that this derivation fundamentally relies on the tractability of the transition density of the OU process, although any other linear Gaussian diffusion could have been used. For simplifying notation, I will often write \\(\\widehat{x}_0(x_t, t)\\) as \\(\\widehat{x}_0(x_t)\\) when it is clear that \\(x_t\\) is a sample obtained at time \\(0 \\leq t \\leq T\\). Equation 5 means that to estimate the score, one only needs to train a denoising function \\(\\widehat{x}^{\\theta}_0: [0,T] \\times \\mathbb{R}^d \\to \\mathbb{R}^d\\) parametrized by a parameter \\(\\theta \\in \\Theta\\) so that it approximates the true conditional expectation,\n\\[\n\\widehat{x}^{\\theta}_0(t,x_t)\n\\; \\approx \\;\n\\widehat{x}_0(t,x_t) = \\mathbb{E}[X_0 \\mid X_t=x_t].\n\\]\nIt is a standard regression problem: take a bunch of pairs \\((x_0, x_t)\\) that can be generated as\n\\[\nx_0 \\sim \\pi_{\\mathrm{data}}\n\\qquad \\textrm{and} \\qquad\nx_t \\sim p(dx_t \\mid x_0) = \\alpha_t x_0 + \\sigma_t \\, \\mathbf{n}\n\\]\nand minimize the Mean Squared Error (MSE) loss, i.e.\n\\[\n\\theta \\mapsto \\mathbb{E} {\\left[  \\|\\widehat{x}^{\\theta}_0(t, x_t) - x_0\\|^2  \\right]} .\n\\]\nThis can be implemented with a stochastic gradient descent method or any other stochastic optimization procedure. Indeed, this implicitly relies on the standard fact that the expectation of a squared integrable random variable \\(Z\\) is the minimizer of the function \\(\\gamma \\mapsto \\mathbb{E}[(\\gamma-Z)^2]\\). The score is then defined as\n\\[\n\\mathcal{S}^{\\theta}(t,x) \\; = \\; -\\frac{x - \\alpha_t \\, \\widehat{x}^{\\theta}_0(t,x)}{\\sigma^2_t}.\n\\]\nOne of the important take-away is that a denoiser, i.e. a function that estimates the conditional expectation of \\(X_0\\) given \\(X_t=x\\), can be upgraded to a sampler! Furthermore, for a given estimate \\(\\mathcal{S}^\\theta(t,x_t)\\) of the score function, one can easily obtain bounds on the quality of the resulting approximation of the data distribution. For example, if \\(q^{\\theta}\\) denotes the probability distribution on path-space induced by the reverse diffusion with approximate score \\(\\mathcal{S}^{\\theta}\\) while \\(p\\) denotes the actual probability distribution generated by reverse diffusion, the chain rule for the KL divergence gives\n\\[\nD_{\\text{KL}}(\\pi_{\\mathrm{data}}, q^{\\theta}_0) = D_{\\text{KL}}(p_0, q^{\\theta}_0)\n\\leq D_{\\text{KL}}(p, q^{\\theta}).\n\\]\nAs described in these notes, Girsanov’s theorem gives that\n\\[\nD_{\\text{KL}}(p, q^{\\theta}) = \\frac12 \\, \\mathbb{E} {\\left\\{ \\int_{0}^T \\|\\mathcal{S}^{\\theta}(t,X_t) - \\nabla_x \\log p_t(X_t) \\|^2 \\, dt \\right\\}} .\n\\]\nIf there were a time-dependent volatility \\(\\gamma_t\\) in the dynamics, i.e. \\(dX = \\mu(X) \\, dt + \\gamma_t \\, dW\\) instead of the one in Equation 1, the expression would remain essentially the same, with the only difference that the error term \\(\\|\\mathcal{S}^{\\theta}(t,X_t) - \\nabla_x \\log p_t(X_t) \\|^2\\) would be scaled by a factor \\(\\gamma^2_t\\). Indeed, this means that when the approximation of the score is perfect, i.e. \\(\\mathcal{S}^{\\theta}(t,x_t) \\equiv \\nabla_x \\log p_t(x_t)\\), the data distribution is perfectly recovered, as expected. When expressed in terms of the denoiser \\(\\widehat{x}\\), the upper-bound on \\(D_{\\text{KL}}(\\pi_{\\mathrm{data}}, q^{\\theta}_0)\\) reads\n\\[\n\\frac12 \\, \\mathbb{E} {\\left\\{ \\int_{0}^T  {\\left( \\frac{\\alpha_t}{\\sigma_t^2} \\right)} ^2 \\, \\|\\widehat{x}^{\\theta}_0(t,X_t) - \\mathbb{E}[X_0 \\mid X_t] \\|^2 \\, dt \\right\\}} .\n\\]\nSince \\((\\alpha_t / \\sigma_t) \\to \\infty\\) as \\(t \\to 0\\), this shows that it is probably a good idea to approximate denoiser \\(\\widehat{x}_0(t,x_t)\\) much better for small times \\(t \\to 0\\) than for large times \\(t \\to T\\).\n\n\nDenoiser: practical parametrization\nIn practice, it may not be efficient, or stable, to try to directly parametrize the denoiser \\(\\widehat{x}^\\theta_0(t, x_t)\\) with a neural network and simply minimize the loss\n\\[\n\\mathbb{E}\\|X_0 - \\widehat{x}^{\\theta}_0(t, X_t)\\|^2.\n\\tag{6}\\]\nFor example, for \\(t \\approx 0\\), we have that \\(\\widehat{x}_0(t,X_t) \\approx X_t \\approx X_0\\) so that it is very easy to reconstruct \\(X_0\\) from \\(X_t\\). On the contrary, for large \\(t\\), there is almost no information contained within \\(X_t\\) to reconstruct \\(X_0\\). This means that the typical value of the naive loss Equation 6 depends widely on \\(t\\), which makes it difficult to optimize: with this parametrization, since large values of the loss will be typically concentrated to large values of \\(t\\), the denoiser will not be accurate for \\(t \\approx 0\\), which is exactly the opposite than what is required, as discussed at the end of the previous section. Since \\(x_t = \\alpha_t \\, x_0 + \\sigma_t \\, \\mathbf{n}\\), one can defined the Signal-to-Noise-Ratio as\n\\[\\mathrm{SNR}(t) = \\frac{\\alpha_t}{\\sigma_t}.\\]\nIn order to normalize by the reconstruction difficulty, it makes more sense to train a denoiser by minimizing the loss:\n\\[\n\\mathbb{E} {\\left[  \\mathrm{SNR}^2(t) \\times \\|X_0 - \\widehat{x}^{\\theta}_0(t, X_t)\\|^2  \\right]} .\n\\]\nIt turns out that it is entirely equivalent to minimizing the loss\n\\[\n\\mathbb{E} {\\left[  \\| \\mathbf{n}- \\widehat{\\mathbf{n}}^{\\theta}(t, X_t)\\|^2  \\right]} .\n\\]\nwhere \\(x_t = \\alpha_t \\, x_0 + \\sigma_t \\, \\mathbf{n}\\) while the denoiser \\(\\widehat{x}^{\\theta}_0\\) and noise estimator \\(\\widehat{\\mathbf{n}}^{\\theta}\\) are parametrized so that\n\\[\nx_t = \\alpha_t \\, \\widehat{x}^{\\theta}_0(t, x_t) + \\sigma_t \\, \\widehat{\\mathbf{n}}^{\\theta}(t,x_t).\n\\]\nThat is one of the reasons why most of the papers on DDPM are parametrizing the denoiser \\(\\widehat{x}_0\\) by building instead a “noise estimator” \\(\\widehat{\\mathbf{n}}^{\\theta}\\) with a neural network and setting\n\\[\n\\widehat{x}^{\\theta}_0(t,x_t) = \\frac{x_t - \\sigma_t \\, \\widehat{\\mathbf{n}}^{\\theta}(t,x_t)}{\\alpha_t}.\n\\tag{7}\\]\nSince \\(\\alpha_t \\to 1\\) and \\(\\sigma_t \\to 0\\) for \\(t \\to 0\\), this also implicitly ensures that \\(\\widehat{x}_0(t,x_t) \\approx x_t\\) for \\(t \\to 0\\), as required. With the parametrization Equation 7, the score is given by\n\\[\n\\mathcal{S}^{\\theta}(t,x_t) \\, = \\, -\\frac{\\widehat{\\mathbf{n}}^{\\theta}(t,x_t)}{\\sigma_t},\n\\]\nwhich can still lead to instability issues for small times \\(t \\to 0\\), although to a lesser extent. It is unlikely that there is an easy way to avoid these issues since, as soon as the data distribution is supported on a (neighbourhood of) lower dimensional manifold, the actual score function indeed does become very large for \\(t \\to 0^+\\) since a strong “drift” is required to bring the diffusion back to the manifold.\n\n\nThe “denoising” diffusion\nOnce the denoiser \\(\\widehat{x}_0(\\ldots)\\) trained, the reverse diffusion defined \\(\\overleftarrow{X}_s = X_{T-s}\\) has to be simulated. Plugging Equation 5 back in the expression of the dynamics of the reverse diffusion shows that\n\\[\nd\\overleftarrow{X}_s =\n-\\frac12 \\, \\frac{1}{\\tanh((T-s)/2)} \\,\n{\\left(  \\overleftarrow{X}_s - \\frac{\\widehat{x}_0(\\overleftarrow{X}_s)}{\\cosh((T-s)/2)}  \\right)}\n\\; + \\;\ndB\n\\tag{8}\\]\nThis dynamics is intuitive: as \\(s \\to T\\) we have \\(\\cosh((T-s)/2) \\to 1\\) and \\(\\varepsilon^2 \\equiv \\tanh((T-s)/2) \\sim (T-s)/2 \\to 0\\) so that the dynamics is similar to\n\\[\nd\\overleftarrow{X}_s =\n-\\frac12 \\, \\frac{1}{\\varepsilon^2} \\,\n{\\left(  \\overleftarrow{X}_s - \\widehat{x}_0 \\right)}  + dB.\n\\]\nThat is an Ornstein-Uhlenbeck process that converges quickly, i.e. on time-scale of order \\(\\mathcal{O}(\\varepsilon^2)\\), towards a Gaussian distribution centred at \\(\\widehat{x}_0\\) and with variance \\(\\varepsilon^2\\).\nTo discretize the reverse dynamics Equation 8 on a small interval \\([\\overline{s}, \\overline{s}+\\delta]\\), one can for example consider the slightly simplified (linear) dynamics\n\\[\nd\\overleftarrow{X}_s =\n-\\frac12 \\, \\frac{1}{\\tanh((T-s)/2)} \\,\n{\\left(  \\overleftarrow{X}_s - \\mu \\right)}  + dB.\n\\]\nHere, \\(\\mu = \\widehat{x}_0(\\overleftarrow{X}_{\\overline{s}}) / \\cosh((T-\\overline{s})/2)\\) with \\(\\overleftarrow{X}_{\\overline{s}} = \\overleftarrow{x}_{\\overline{s}}\\). Algebra gives that, conditioned upon \\(\\overleftarrow{X}_{\\overline{s}} = \\overleftarrow{x}_{\\overline{s}}\\), we have\n\\[\n\\left\\{\n\\begin{aligned}\n\\mathbb{E}[ \\overleftarrow{X}_{\\overline{s} + \\delta} ]\n\\quad &= \\quad \\mu + \\lambda \\, (\\overleftarrow{x}_{\\overline{s}} - \\mu)\\\\\n\\mathop{\\mathrm{Var}}[ \\overleftarrow{X}_{\\overline{s} + \\delta} ]\n\\quad &= \\quad\n\\tanh {\\left( \\frac{T-\\overline{s}-\\delta}{2} \\right)}  \\, (1-\\lambda^2)\n\\end{aligned}\n\\right.\n\\]\nwhere the coefficient \\(0&lt;\\lambda&lt;1\\) is given by\n\\[\n\\lambda = \\frac{\\sinh(T-\\overline{s}-\\delta)}{\\sinh(T-\\overline{s})}.\n\\]\nThis discretization is more stable and efficient than a naive Euler-Maruyama approach since the drift term can get large as \\(s \\to T\\). WIth the above discretization, one can easily simulate the reverse diffusion on \\([0,T]\\) and generate approximate samples from \\(\\pi_{\\mathrm{data}}\\).\nIn the animation below, the method described in these notes was used. This very small example trains in less than a minute on an old laptop without a GPU. The noise estimator was parametrized with an MLP with \\(\\mathrm{elu}(\\ldots)\\) non-linearity and two hidden-layers with size \\(H=128\\). It was very useful to add a few Fourier features as input of the network, I could not make this work properly without.\n\n\nVideo\n\n\nThe literature on DDPM is enormous and still growing!"
  },
  {
    "objectID": "notes/Poisson_Eq_Asymp_Var/Poisson_Eq_Asymp_Var.html",
    "href": "notes/Poisson_Eq_Asymp_Var/Poisson_Eq_Asymp_Var.html",
    "title": "Asymptotic variance & Poisson Equation",
    "section": "",
    "text": "Consider a continuous time Markov process \\(X_t\\) on \\(\\mathbb{R}^D\\) that is ergodic with respect to the probability distribution \\(\\pi(dx)\\). A Langevin diffusion is a typical example. Call \\(\\mathcal{L}\\) the generator of this process so that for a test function \\(\\varphi: \\mathbb{R}^D \\to \\mathbb{R}\\) we have\n\\[\n\\varphi(X_t) = \\varphi(X_0) + \\int_{s=0}^t \\mathcal{L}\\varphi(X_s) \\, ds + \\textrm{($M_t \\equiv$ martingale)}.\n\\tag{1}\\]\nNow, assume further that \\(\\mathbb{E}_{\\pi}[\\varphi(X)] = 0\\) and that a Central Limit Theorem holds,\n\\[\n\\frac{1}{\\sqrt{T}} \\int_{s=0}^T \\varphi(X_s) \\, ds \\; \\to \\; \\mathcal{N}(0, \\sigma^2).\n\\tag{2}\\]\nHow can one estimate the asymptotic variance \\(\\sigma^2\\)?\n\nApproach I: Integrated autocovariance\nOne can directly try to compute the second moment of Equation 2 and obtain that\n\\[\n\\sigma^2 \\; = \\; \\lim_{T \\to \\infty} \\;\n\\frac{1}{T} \\, \\iint_{0 \\leq s,t \\leq T} \\mathbb{E}[\\varphi(X_s) \\varphi(X_t)] \\, ds \\, dt\n\\]\nSince \\(\\mathbb{E}[\\varphi(X_s) \\varphi(X_t)]\\) falls quickly to zero as \\(|s-t| \\to 0\\) and defining the auto-covariance at lag \\(r &gt; 0\\) as\n\\[\nC(r) = \\mathbb{E}[\\varphi(X_t) \\varphi(X_{t+r})],\n\\]\none obtains that an expression of the asymptotic as the integrated autocovariance function,\n\\[\n\\sigma^2 \\; = \\; 2 \\, \\int_{r=0}^\\infty C(r) \\, dr.\n\\tag{3}\\]\nIn the MCMC literature, this relation is often expressed as\n\\[\n\\sigma^2 \\; = \\; \\mathop{\\mathrm{Var}}_{\\pi}[\\varphi] \\, \\times \\, \\textrm{(IACT)}\n\\]\nwhere the integrated autocorrelation function is defined as\n\\[\n\\textrm{(IACT)} = 2 \\, \\int_{r=0}^\\infty \\rho(r) \\, dr.\n\\]\nfor autocorrelation at lag \\(r\\geq 0\\) defined as \\(\\rho(r) \\equiv \\mathop{\\mathrm{Corr}}[\\varphi(X_t), \\varphi(X_{t+r})]\\). The slower the autocorrelation function \\(\\rho(r)\\) falls to zero as \\(r \\to \\infty\\), the larger the asymptotic variance \\(\\sigma^2\\). Although Equation 3 is very intuitive, it can be difficult to estimate the autocorrelation function.\n\n\nApproach II: Poisson Equation\nUnder relatively general and mild conditions, since the expectation of \\(\\varphi\\) under the invariant distribution \\(\\pi\\) is zero and the Markov process is ergodic with respect to \\(\\pi\\), there exists a function \\(\\Phi: \\mathbb{R}^D \\to \\mathbb{R}\\) such that\n\\[\n\\mathcal{L}\\Phi \\; = \\; \\varphi.\n\\tag{4}\\]\nEquation 4 is called a Poisson Equation since \\(\\mathcal{L}\\) is often a Laplacian-like operator (eg. diffusion-type processes). Equation 1 gives that\n\\[\n\\frac{1}{\\sqrt{T}} \\int_{s=0}^T \\varphi(X_s) \\, ds\n\\; = \\;\n\\frac{M_T}{\\sqrt{T}} +  {\\left\\{  \\frac{\\Phi(X_T) - \\Phi(X_0)}{\\sqrt{T}}  \\right\\}}\n\\]\nwhere \\(M_T\\) is the martingale and \\([\\Phi(X_T) - \\Phi(X_0)]/\\sqrt{T}\\) typically vanishes as \\(T \\to \\infty\\) and can be neglected. For computing the asymptotic variance, it suffices to estimate \\(\\mathbb{E}(M_T^2)\\). And using the martingale property, it equals \\(\\int_{s=0}^T \\mathbb{E}(dM_t)^2\\). Also, since \\(M_t = \\varphi(X_t) - \\varphi(X_0) - \\int_{s=0}^t \\mathcal{L}\\varphi(X_s) \\, ds\\), algebra gives that\n\\[\n\\frac{1}{\\varepsilon} \\, \\mathbb{E} {\\left[  (M_{t+\\varepsilon} - M_t)^2  \\right]}  \\approx 2 \\mathbb{E} {\\left[  (\\Gamma \\Phi)(X_t)  \\right]}\n\\]\nwhere the so-called carré du champ \\((\\Gamma \\Phi)\\) is defined as\n\\[\n2 \\, (\\Gamma \\Phi)(X_t)\n\\; = \\;  {\\left(  \\mathcal{L}(\\Phi^2) - 2 \\Phi \\mathcal{L}\\Phi \\right)} (X_t)\n\\; = \\; \\lim_{\\varepsilon\\to 0} \\; \\frac{1}{\\varepsilon} \\mathop{\\mathrm{Var}}(\\Phi(X_{t+\\varepsilon}) \\, | \\, X_t).\n\\]\nThis shows that the asymptotic variance satisfies\n\\[\n\\sigma^2\n\\; = \\;\n\\lim_{T \\to \\infty} \\frac{2}{T} \\int_{s=0}^T \\Gamma \\Phi(X_s) \\, ds\n\\; = \\;\n2 \\, \\int_{x \\in \\mathbb{R}^D} \\Gamma \\Phi(x) \\, \\pi(dx).\n\\]\nFinally, since \\(\\int (\\mathcal{L}\\Phi^2)(x) \\, \\pi(dx) = 0\\), this can equivalently be written as\n\\[\n\\sigma^2\n\\; = \\;\n-2 \\, \\int_{x \\in \\mathbb{R}^D} \\Phi(x) \\, \\mathcal{L}\\Phi(x) \\, \\pi(dx)\n\\; = \\; 2 \\, \\mathcal{D}(\\Phi)\n\\tag{5}\\]\nwhere \\(\\mathcal{D}(\\Phi)\\) is the so-called Dirichlet form. In summary, we have just shown that the asymptotic variance of the additive functional \\(T^{-1/2} \\, \\int_0^T \\varphi(X_s) \\, ds\\) is given by two times the Dirichlet form \\(\\mathcal{D}(\\Psi)\\) where \\(\\Phi\\) is solution to the Poisson equation \\(\\mathcal{L}\\Phi = \\varphi\\). Note that this implies that the generator \\(\\Phi\\) is a negative operator in the sense that for a test function \\(\\Phi\\) we have that\n\\[\n\\left&lt; \\Phi, \\mathcal{L}\\Phi \\right&gt;_{\\pi} \\; \\leq \\; 0\n\\]\nwhere we have used the dot-product notation \\(\\left&lt; f,g \\right&gt;_{\\pi} = \\int f(x) g(x) \\pi(dx)\\).\n\n\nPoisson equation: Integral representation\nIt is often useful to think of the generator \\(\\mathcal{L}\\) as an infinite dimensional equivalent of a standard negative definite symmetric matrix/operator \\(M \\in \\mathbb{R}^{n,n}\\). And since \\(M^{-1} = -\\int_{t=0}^{\\infty} \\exp(tM) \\, dt\\), as can be seen by diagonalizing \\(M\\), one can expect the following equation to hold,\n\\[\n\\mathcal{L}^{-1} \\; = \\; -\\int_{t=0}^{\\infty} e^{t \\, \\mathcal{L}} \\, dt.\n\\]\nThat is just another way of writing that the solution \\(\\Phi\\) to the Poisson equation \\(\\mathcal{L}\\Phi = \\varphi\\), with the centering condition \\(\\mathbb{E}_{\\pi}[\\Phi(X)]=0\\) for picking one solution out of the many possible solutions to the Poisson equation differing from each other by an additive constant, can be expressed as\n\\[\n\\Phi(x) \\, = \\, -\\int_{t=0}^{\\infty} \\mathbb{E}[\\varphi(X_t)|X_0=x] \\, dt.\n\\tag{6}\\]\nEquation 6 is easily proved with Equation 1 by writing\n\\[\n\\Phi(x)-\\Phi(X_T) = -\\int_{t=0}^\\infty \\varphi(X_t) \\, dt + \\textrm{(martingale)}\n\\]\nand by taking expectation from both sides and noticing that \\(\\mathbb{E}[\\Phi(X_T)] \\to 0\\) thanks to the assumed centering condition \\(\\mathbb{E}_{\\pi}[\\Phi(X)]=0\\). Note that this remarks allows to give another derivation of Equation 5 starting from the integrated autocovariance formulation Equation 3. Indeed, note that\n\\[\n\\begin{align}\n\\sigma^2 &= 2 \\, \\int_{r=0}^{\\infty} C(r) \\, dr\\\\\n&=\n2 \\, \\int_{x \\in \\mathbb{R}^D} \\varphi(x)  {\\left\\{  \\int_{r=0}^{\\infty}  \\mathbb{E}[\\varphi(X_t) | X_0=x] \\, dr  \\right\\}}  \\, \\pi(dx)\\\\\n&=\n-2 \\, \\int_{x \\in \\mathbb{R}^D} \\varphi(x) \\Phi(x) \\, \\pi(dx)\n=\n-2 \\left&lt; \\Phi, \\mathcal{L}\\Phi \\right&gt;_{\\pi}.\n\\end{align}\n\\]\n\n\nExample: OU process\nConsider a OU process that is ergodic with respect to the standard Gaussian density \\(\\pi(x) = e^{-x^2/2} / \\sqrt{2\\pi}\\),\n\\[\ndX\n\\; = \\;\n-\\varepsilon^{-1}X \\, dt + \\sqrt{2 \\, \\varepsilon^{-1}} \\, dW.\n\\]\nThat’s a standard OU process accelerated by a factor \\(\\varepsilon^{-1} &gt; 0\\). Its generator reads\n\\[\n\\mathcal{L}\\, = \\, \\varepsilon^{-1} [-x \\, \\partial_x + \\partial_{xx}].\n\\]\nThe function \\(\\varphi(x)=x\\) is such that \\(\\pi(\\varphi)=0\\) and a solution to the Poisson equation \\(\\mathcal{L}\\Phi = \\varphi\\) is \\(\\Phi(x) = -\\varepsilon\\, x\\). This shows that the asymptotic variance is\n\\[\n\\sigma^2\n\\; = \\;\n2 \\, \\int_{x \\in \\mathbb{R}^D} \\varepsilon x^2 \\, \\pi(dx) \\; = \\; 2 \\varepsilon.\n\\]\nAs expected, accelerating the OU process by a factor \\(\\varepsilon^{-1}\\) means reducing the variance by a factor \\(\\varepsilon\\)."
  },
  {
    "objectID": "notes/information_theory_references/information_theory_references.html",
    "href": "notes/information_theory_references/information_theory_references.html",
    "title": "Information Theory: References and Readings",
    "section": "",
    "text": "Books\n\n“Elements of information theory” by T. M. Cover and J. A. Thomas – perfect intro book to the topic.\n“Information Theory, Inference, and Learning Algorithms” by David J.C. MacKay\n“Information Theory From Coding to Learning” by Yury Polyanskiy and Yihong Wu\n\n\n\nLecture Notes & Articles\n\n“A Mathematical Theory of Communication” by C. Shannon (2948) – entertaining and readable, even 70+ years later!\n“Lecture Notes on Statistics and Information Theory” by John Duchi\n“Information-theoretic methods for high-dimensional statistics” by Yihong Wu"
  },
  {
    "objectID": "notes/auxiliary_variable_trick/auxiliary_variable_trick.html",
    "href": "notes/auxiliary_variable_trick/auxiliary_variable_trick.html",
    "title": "Auxiliary variable trick",
    "section": "",
    "text": "Consider a complicated distribution on the state space \\(x \\in \\mathcal{X}\\) given by\n\\[\n\\pi(x) = \\frac{1}{\\mathcal{Z}} \\, e^{C(x) + B(x)}\n\\]\nfor a “complicated” functions \\(C(x)\\) and a simpler one \\(B(x)\\). In some situations, it is possible to introduce an auxiliary random variable \\(a \\in \\mathcal{A}\\) and an extended probability distribution \\(\\pi(x,a)\\) on the extended space \\(\\mathcal{X}\\times \\mathcal{A}\\),\n\\[\n\\pi(x,a) = \\pi(x) \\,  \\textcolor{red}{\\pi(a | x)} = \\frac{1}{\\mathcal{Z}} e^{C(x) + B(x)} \\,  \\textcolor{red}{e^{-C(x) + D(x, a)}},\n\\]\nwith a tractable conditional probability \\(\\pi(a | x)\\). This extended target distribution \\(\\pi(x,a) = (1/\\mathcal{Z}) \\, \\exp[B(x) + D(x,a)]\\) can be often be easier to explore, for example when \\(a\\) is continuous while \\(x\\) is discrete, or to analyze, since the “complicated” term \\(C(x)\\) has disappeared. Furthermore, there are a number of scenarios when the variable \\(x\\) can be averaged out of the extended distribution, i.e. the distribution\n\\[\n\\pi(a) = \\frac{1}{\\mathcal{Z}} \\, \\int_{x \\in \\mathcal{X}} e^{B(x) + D(x,a)}\n\\]\ncan be evaluated exactly.\n\nSwendsen–Wang algorithm\nConsider a set of edges \\(\\mathcal{E}\\) on a graph with vertices \\(\\{1, \\ldots, N\\}\\). The Ising model is defined as \\[\n\\pi(x) \\propto \\exp  {\\left\\{  \\sum_{(i,j) \\in \\mathcal{E}} \\beta x_i x_j  \\right\\}}\n\\]\nfor spin configurations \\(x=(x_1, \\ldots, x_N) \\in \\{-1,1\\}^N\\). The term \\(\\exp[\\beta x_i x_j]\\) couples the two spins \\(x_i\\) and \\(x_j\\) for each edge \\((i,j) \\in \\mathcal{E}\\). The idea of the Swendsen–Wang_algorithm is to introduce an auxiliary variable \\(u_{i,j}\\) for each edge \\((i,j) \\in \\mathcal{E}\\) that is uniformly distributed on the interval \\([0, \\exp(\\beta x_i x_j)]\\), i.e.\n\\[\n\\pi(u_{i,j} | x) \\; = \\; \\frac{ \\mathbf{1}  {\\left\\{  0 &lt; u_{i,j} &lt; \\exp[\\beta x_i x_j]  \\right\\}}  }{\\exp[\\beta x_i x_j] }\n\\]\nIt follows that the extended distribution on \\(\\{-1,1\\}^N \\times (0,\\infty)^{|\\mathcal{E}|}\\) reads\n\\[\n\\pi(x,u) = \\frac{1}{Z} \\prod_{(i,j) \\in \\mathcal{E}} \\; \\mathbf{1}  {\\left\\{  0 &lt; u_{i,j} &lt; \\exp[\\beta x_i x_j]  \\right\\}}\n\\]\nfor \\(x=(x_1, \\ldots, x_N)\\) and \\(u = (u_{i,j})_{(i,j) \\in \\mathcal{E}}\\): the coupling term \\(\\exp[\\beta x_i x_j]\\) has disappeared. Furthermore, it is straightforward to sample from the conditional distribution \\(\\pi(u | x)\\) and, perhaps surprisingly, it is also relatively straightforward to sample from the other conditional distribution \\(\\pi(x | u)\\) – this boils down to finding the connect components of the graph on \\(\\{1, \\ldots, N\\}\\) with an edge \\(i \\sim j\\) present if \\(u_{i,j} &gt; e^{-\\beta}\\) and flipping a fair coin for setting each connected component to \\(\\pm 1\\). This leads to an efficient Gibbs sampling scheme to sample from the extended distribution.\n\n\n\n\nSwendsen-Wang MCMC algorithm at critical temperature\n\n\n\n\n\nGaussian Integral trick: Curie-Weiss model\nFor an inverse temperature \\(\\beta &gt; 0\\), consider the distribution on \\(x \\in \\{-1,1\\}^N\\)\n\\[\n\\pi(x) = \\frac{1}{\\mathcal{Z}(\\beta)} \\, e^{\\beta \\, N \\, m^2}\n\\]\nwhere the magnetization of the system of spins \\(x=(x_1, \\ldots, x_N)\\) is defined as\n\\[\nm = \\frac{x_1 + \\ldots + x_N}{N}.\n\\]\nThe distribution \\(\\pi(x)\\) for \\(\\beta \\gg 1\\) favours configurations with a magnetization close to \\(+1\\) or \\(-1\\). The normalization constant (i.e. partition function) \\(\\mathcal{Z}(\\beta)\\) is a sum of \\(2^N\\) terms,\n\\[\n\\mathcal{Z}(\\beta) = \\sum_{s_1 \\in \\{ \\pm 1\\} } \\ldots \\sum_{s_N \\in \\{ \\pm 1\\} } \\exp  {\\left\\{  \\frac{\\beta}{N}  {\\left(  \\sum_{i=1}^N x_i  \\right)} ^2 \\right\\}} .\n\\]\nIt is not difficult to estimate \\(\\log \\mathcal{Z}(\\beta)\\) as \\(N \\to \\infty\\) with combinatorial arguments. Nevertheless, another way to proceed is as follows. One can introduce a Gaussian auxiliary random variable \\(\\pi(a | x) = \\mathcal{N}(\\alpha \\,  {\\left( \\sum_i x_i \\right)}  , \\sigma^2)\\) with mean \\(\\mu = \\alpha \\,  {\\left( \\sum_i x_i \\right)} \\) and variance \\(\\sigma^2\\): the parameters \\(\\alpha\\) and \\(\\sigma^2 &gt; 0\\) can then be judiciously chosen to cancel the bothering term \\(\\exp[\\frac{\\beta}{N} \\, m^2]\\). This approach is often called the a Hubbard-Stratonovich transformation. The bothering “coupling” term disappears when when choosing \\(\\frac{\\alpha^2}{2 \\sigma^2} = \\frac{\\beta}{N}\\). With such a choice, it follows that\n\\[\n\\pi(x, a) = \\frac{1}{\\mathcal{Z}(\\beta)} \\, \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp {\\left\\{  - \\frac{a^2}{2 \\sigma^2} + \\frac{\\alpha}{\\sigma^2} a \\,  {\\left( \\sum_i x_i \\right)}  \\right\\}} .\n\\]\nAveraging out the \\(x_i \\in \\{-1, +1\\}\\) gives that the partition function reads\n\\[\n\\mathcal{Z}(\\beta) \\; = \\;\n\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\,\n\\int_{a = -\\infty}^{a=\\infty} \\,\n\\exp {\\left\\{ -\\frac{a^2}{2 \\sigma^2} +  \\textcolor{red}{N} \\, \\log[ 2 \\, \\cosh(\\alpha a / \\sigma^2)] \\right\\}} .\n\\]\nIn order to use the method of steepest descent, it would be useful to have an integrand of the type \\(\\exp[N \\times (\\ldots)]\\). One can choose \\(1/(2 \\, \\sigma^2) = \\beta \\, N\\) and \\(\\alpha = 1/N\\). This gives\n\\[\n\\mathcal{Z}(\\beta) \\; = \\;\n\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\,\n\\int_{a = -\\infty}^{a=\\infty} \\,\n\\exp  {\\left\\{   \\textcolor{red}{N}  {\\left[  -\\beta \\, a^2 + \\log[ 2 \\, \\cosh(2 \\beta a )] \\right]}  \\right\\}}  \\, da\n\\]\nfrom which one directly obtains that:\n\\[\n\\lim_{N \\to \\infty} \\; -\\frac{\\log \\mathcal{Z}(\\beta)}{N}\n\\; = \\;\n\\min_{a \\in \\mathbb{R}} \\; \\Big\\{ \\beta a^2 - \\log[2 \\, \\cosh(2 \\beta a)] \\Big\\}.\n\\]\n\n\nSherrington–Kirkpatrick model\nConsider the distribution on \\(x \\in \\{-1,1\\}^N\\)\n\\[\n\\pi(x) = \\frac{1}{\\mathcal{Z}(\\beta)} \\,\n\\exp {\\left\\{ \\frac 12 \\, \\sum_{i,j} W_{ij} x_i x_j \\right\\}}\n=\n\\frac{1}{\\mathcal{Z}(\\beta)} \\,\n\\exp {\\left\\{ \\frac 12 \\, \\left&lt; x, W x \\right&gt; \\right\\}}\n\\]\nwhere the \\(w_{ij}\\) are some fixed weights with \\(w_{ij} = w_{ji}\\). We assume that the matrix \\(W = [W_{ij}]_{ij}\\) is positive definite: this can be achieved by adding \\(\\lambda \\, I_N\\) to it if necessary, which does not change the distribution \\(\\pi\\). As described in (Zhang et al. 2012), although I would not be surprised if similar ideas appeared in the physics literature much earlier, one can introduce a Gaussian auxiliary random variable \\(a = (a_1, \\ldots, a_N)\\) so that \\(\\pi(a | x)\\) has mean \\(Fx\\) and covariance \\(\\Gamma\\). In other words,\n\\[\n\\pi(a | x) = \\frac{1}{(2 \\pi)^{d/2} \\, |\\Gamma|^{1/2}}\n\\, \\exp {\\left\\{ -\\frac 12 \\left&lt; (a - Fx), \\Gamma^{-1} (a - Fx) \\right&gt; \\right\\}} .\n\\]\nIn order to cancel-out the \\(\\left&lt; x, W, x \\right&gt;\\) it suffices to make sure that \\(F^\\top \\, \\Gamma^{-1} \\, F = W\\). There are a number of possibilities, the simplest approaches being perhaps\n\\[\n(F,\\Gamma) = (W^{1/2}, I_N)\n\\quad \\text{or} \\quad\n(F,\\Gamma) = (I, W^{-1})\n\\quad \\text{or} \\quad\n(F,\\Gamma) = (W, W).\n\\]\nIn any case, the joint distribution reads\n\\[\n\\pi(x,a) \\; = \\; \\frac{1}{\\mathcal{Z}} \\, \\frac{1}{(2 \\pi)^{d/2} \\, |\\Gamma|^{1/2}} \\,\n\\exp {\\left\\{ -\\frac{1}{2} \\left&lt; a, \\Gamma^{-1} a \\right&gt; + \\left&lt; x, F^\\top \\Gamma^{-1} \\, a \\right&gt; \\right\\}} .\n\\]\nIndeed, one can try to implement some Gibbs-style update in order to explain this joint distribution since both \\(\\pi(x|a)\\) and \\(\\pi(a|x)\\) are straightforward to sample from: it is indeed related Restricted Boltzmann Machine models. One can also average-out the spins \\(x_i \\in \\{-1,1\\}\\) and obtain that\n\\[\n\\mathcal{Z}= \\int_{\\mathbb{R}^N}\n\\exp {\\left\\{  \\sum_{i=1}^N \\log {\\left( 2 \\, \\cosh([F^\\top \\Gamma^{-1} \\, a]_i) \\right)}  \\right\\}}  \\, \\mathcal{D}_{\\Gamma}(da)\n\\]\nwhere \\(\\mathcal{D}_{\\Gamma}\\) is the density of a centred Gaussian distribution with covariance \\(\\Gamma\\). [TODO: add SMC experiments to estimate \\(\\mathcal{Z}\\)].\n\n\n\n\n\nReferences\n\nZhang, Yichuan, Zoubin Ghahramani, Amos J Storkey, and Charles Sutton. 2012. “Continuous Relaxations for Discrete Hamiltonian Monte Carlo.” Advances in Neural Information Processing Systems 25."
  },
  {
    "objectID": "teaching/teaching.html",
    "href": "teaching/teaching.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "DSA5104: Machine Learning and Predictive Modelling, NUS (2024)\nST3247: Simulation, NUS (2024)\n\nAssignment on Self-Avoiding Walks\n\nYSC4216: Machine Learning, Yale-NUS (2024)\nDSA4212: Large Scale optimization, NUS (2017 – 2025)\nYSC4236: Optimization for Large Scale Inference, Yale-NUS, (2022)\nYSC3252: Computational Statistics, Yale-NUS, (2022)\nDSA5843: Learning from Data: Neural Networks, NUS (2020–2021)\nST3233: Applied Times Series, NUS, (2016, 2019, 2021)\nST4240: Data Mining, NUS, (2014–2017)\nST4231: Computationally Intensive Statistics, NUS (2014–2015)\nMultivariable Calculus II, Chennai (CMI), (2018)"
  },
  {
    "objectID": "teaching/teaching.html#semester-long-courses",
    "href": "teaching/teaching.html#semester-long-courses",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "DSA5104: Machine Learning and Predictive Modelling, NUS (2024)\nST3247: Simulation, NUS (2024)\n\nAssignment on Self-Avoiding Walks\n\nYSC4216: Machine Learning, Yale-NUS (2024)\nDSA4212: Large Scale optimization, NUS (2017 – 2025)\nYSC4236: Optimization for Large Scale Inference, Yale-NUS, (2022)\nYSC3252: Computational Statistics, Yale-NUS, (2022)\nDSA5843: Learning from Data: Neural Networks, NUS (2020–2021)\nST3233: Applied Times Series, NUS, (2016, 2019, 2021)\nST4240: Data Mining, NUS, (2014–2017)\nST4231: Computationally Intensive Statistics, NUS (2014–2015)\nMultivariable Calculus II, Chennai (CMI), (2018)"
  },
  {
    "objectID": "teaching/teaching.html#lecture-series",
    "href": "teaching/teaching.html#lecture-series",
    "title": "Alexandre Thiéry",
    "section": "Lecture Series:",
    "text": "Lecture Series:\n\nSummer school on Statistics & ML, VIASM July 2024\n\nSlides available here\n\nSummer school on Bayesian statistics (5h Lecture), VIASM July 2023"
  },
  {
    "objectID": "teaching/teaching.html#notes",
    "href": "teaching/teaching.html#notes",
    "title": "Alexandre Thiéry",
    "section": "Notes:",
    "text": "Notes:\n\nProbability Basics"
  },
  {
    "objectID": "teaching/DSA4212/assignments.html",
    "href": "teaching/DSA4212/assignments.html",
    "title": "DSA4212 Assignments",
    "section": "",
    "text": "Traveling Salesman Problem [90%]\nGoal: The Traveling Salesman Problem (TSP) is a classic problem in combinatorial optimization. Given a list of cities and the distances between each pair of cities, the problem is to find the shortest possible route that visits each city exactly once and returns to the origin city. You are tasked with implementing a few methods to solve the TSP and compare their performance. Your report should not include any actual code. Instead, it should describe the problem and provide a discussion of the results. You are encouraged to compare your results with the best known solutions.\nPossible References:\n\nWikipedia\nTSPLIB\n\n\n\n\nThompson problem [90%]\nGoal: The Thompson problem is a classic problem in computational geometry. Consider N = 300 points \\(P_1, P_2, \\ldots, P_N \\in \\mathbb{R}^3\\) on the unit sphere: for \\(1 \\leq i \\leq N\\), we have \\(\\|P_i\\| = 1\\). How should these points be placed so that the quantity\n\\[\nE = \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\frac{1}{\\|P_i - P_j\\|}\n\\]\nis minimized. Note that it is the norm, and not the norm squared, in the denominator. Your report should not include any actual code. Instead, it should describe the problem and provide a discussion of the results. You are encouraged to compare your results with the best known solutions.\nPossible References:\n\nWikipedia\n\n\n\n\nImage classification with convolutional neural networks [90%]\nGoal: Implement a simple convolutional neural network in Python and use it to classify images from the CIFAR-10 dataset. For this purpose, you will implement a small CNN on a GPU (eg. Google Colab) – the actual details of the CNN is not the emphasis of this assignment and you should not spend too much time optimizing the architecture. Instead, you will study:\n\nthe influence of the choice of optimizer\n\nthe effect of the learning rate on the speed of convergence and the final test accuracy\n\nwhether using a learning rate schedule improves performance\n\nthe effect of the batch size on the speed of convergence and the final test accuracy\ninfluence and importance of data augmentation\n\nYour report should not include any actual code. Instead, it should describe your experiments and their results, and provide a discussion of the findings.\nPossible References:\n\nSuper-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates\n\nHow to train your ResNet\nDeep Learning Tuning Playbook\n\n\n\n\nAutomatic differentiation [90%]\nGoal: Implement a simple version of automatic differentiation in Python. Your implementation should be able to easily compute the gradient of the composition of any number of (simple) multivariate functions. While your report should not include any actual code, it should describe the mechanics of your implementation and provide examples of its use.\nPossible References:\n\nAutomatic Differentiation\n\nAndrej Karpathy\n\nAri Seff\n\nChris Olah\n\n\n\n\nCollaborative Filtering [90%]\nGoal: Implement a collaborative filtering algorithm in Python from scratch – you should not use any high-level library such as surprise since this would defeat the purpose of the assignment. For testing your implementation, you will use the song dataset available here. You are tasked to implement the matrix factorization algorithm discussed in class, propose improvements, and compare your implementations with a number of baselines approaches. The challenge of this assignment is to implement the algorithm in scalable manner.\n\n\n\nNatural Evolution Strategies [100%]\nThe class of algorithms known as NaturalEvolutionStrategies (NES) has demonstrated significant potential in various fields of applied mathematics and machine learning. In this project, you will provide an overview of NES and apply these approaches to three optimization problems of your choice. Your report should not include any actual code. Instead, it should describe what NES are, how they work, and provide a discussion of the results.\nPossible References:\n\nInformation-geometric optimization algorithms: A unifying pic- ture via invariance principles\n\nEvolution Strategies as a Scalable Alternative to Reinforcement Learning\n\nEvolution Strategies\nNatural Evolution Strategies\n\n\n\n\nWord Embeddings [100%]\nGoal: Implement a simple word embedding in Python (from scratch) and use it to find the most similar words to a given word. Your report should not include any actual code. Instead, it should describe how word embeddings work, how you implemented it, and provide a discussion of the results. For this purpose, you need to come up with a dataset, as well as evaluation metrics to assess the quality of your embeddings.\nPossible References:\n\nAndrew Ng\n\nThe illustrated Word2Vec\n\n\n\n\nPortfolio Optimization [100%]\nGoal: You are tasked with implementing the mean-variance portfolio optimization framework. Your objective is to either maximize returns for a given level of risk or minimize risk for a desired return level. For this assignment:\n\nData Collection: Download historical financial data to estimate the expected returns, volatilities (standard deviations) of asset returns, and the covariance matrix of asset returns. You are required to optimize a portfolio of at least 50 stocks\nOptimization: You must use the CVXPY Python library to implement and solve the mean-variance optimization. The goal is to determine the optimal portfolio weights that balance risk and return.\n\nEfficient Frontier: Calculate and plot the efficient frontier, which represents the set of optimal portfolios for different risk-return trade-offs.\nStability of Solutions: Study how stable the solutions are by testing your results across different time windows and datasets. Consider the impact of changes in volatility and return estimates on portfolio stability.\n\nDiscussion: Reflect on the limitations of the mean-variance framework, particularly with respect to the sensitivity of the results with respect to the inputs.\n\n\n\n\nShampoo optimizer [100%]\nGoal: Shampoo is a very recent second-order-like optimizer that has shown great promise in training deep neural networks such as Large Language Models. The optimizer was named “Shampoo” as it attempts to build a good preconditioner for the optimization problem. The purpose of this assignment is to implement the Shampoo optimizer in Python + Jax and compare its performance with other optimizers such as Adam, SGD, and RMSprop. Your report should not include any actual code. Instead, it should describe how Shampoo works, how you implemented it, and provide a discussion of the results. It is not necessary to understand all the theoretical details of Shampoo, but you should be able to explain the main ideas behind it.\nReferences:\n- Shampoo: Preconditioned Stochastic Tensor Optimization\n- A New Perspective on Shampoo’s Preconditioner\n- SOAP: improving and stabilizing shampoo using adam\n- A Distributed Data-Parallel PyTorch Implementation\n\n\n\nTransformer architecture [100%]\nGoal: Implement a simple transformer in Python and from scratch, ie. not use the Transformer(...) function of a high-level library.\nSome examples of simple task that can be used to test the transformer are:\n\nSequence Reversal: reversing a sequences of numbers, [1,3,2,4,5,3] -&gt; [3,5,4,2,3,1].\n\nBasic Arithmetic: learning simple arithmetic operation is a surprisingly challenging task for a transformer. For example, given a sequence of numbers and arithmetic operations, the model should output the result of the operations. [1,3,2,\"+\",9,4,2]-&gt;[1,0,7,4]. [1,3,2,\"+\",9,4,2]-&gt;[1,0,7,4]\n\nCopying Task: A simple task where the model’s objective is to output the same sequence it receives as input. This might seem trivial but is a good starting point: [1,3,2,4,5,3]-&gt;[1,3,2,4,5,3]\n\nSorting Numbers: This can be very challenging for a transformer, especially if the sequence is long. [1,3,2,4,5,3]-&gt;[1,2,3,3,4,5]\n\nCharacter-Level Text Generation: Generating text one character at a time, based on a given prompt or starting character.\n\nYour report should not include any actual code. Instead, it should describe how a transformer works, how you implemented it, and provide a discussion of the results.\nPossible References:\n\nThe Annotated Transformer\n\nJAX: Transformers and Multi-Head Attention\nGPT in 60 lines of code\nThe Transformer Family\nThe Illustrated Transformer\npytorch transformer from scratch\n\nTransformers: Zero to Hero\n\n\n\n\nTopic of your choice\nPropose a topic and contact me to discuss it and for approval."
  },
  {
    "objectID": "people/index_people.html",
    "href": "people/index_people.html",
    "title": "Research team",
    "section": "",
    "text": "Ling Min Hao PhD (2023-present): Machine Learning methods for Uncertainty Quantification alternative splicing RNA data – co-supervised by Jonathan Göke (GIS)\nZhidi Lin Research Fellow (2024-present): High-Dimensional models time-series model and data-assimilation.\nHai Dang Dau Research Fellow (2024-present): High-Dimensional models time-series model and data-assimilation, diffusion models\nHangao Liang MSc (2024-present): Monte-Carlo methods with path-space methods."
  },
  {
    "objectID": "people/index_people.html#phdpostdoc-co-supervision",
    "href": "people/index_people.html#phdpostdoc-co-supervision",
    "title": "Research team",
    "section": "",
    "text": "Ling Min Hao PhD (2023-present): Machine Learning methods for Uncertainty Quantification alternative splicing RNA data – co-supervised by Jonathan Göke (GIS)\nZhidi Lin Research Fellow (2024-present): High-Dimensional models time-series model and data-assimilation.\nHai Dang Dau Research Fellow (2024-present): High-Dimensional models time-series model and data-assimilation, diffusion models\nHangao Liang MSc (2024-present): Monte-Carlo methods with path-space methods."
  },
  {
    "objectID": "people/index_people.html#alumni",
    "href": "people/index_people.html#alumni",
    "title": "Research team",
    "section": "Alumni",
    "text": "Alumni\n\nWang Chao Research Fellow (2021-2023): Bayesian inverse problems, generative models, data-driven priors.\nChristopher Hendra PhD (2019-2022): Genomics, Nanopore sequencing – co-supervised by Jonathan Göke (GIS). Chris is now a Senior Scientist at MSD.\nKhai Xiang Au PhD (2019-2023): PDE constrained Bayesian inverse problems, uncertainty quantification, variational inference. Khai is now working as a data scientist at American Express.\nRahul Rahaman PhD(2018-2022): Bayesian inference, Uncertainty Quantification, Deep-Learning. Rahul is now an Applied Research Scientist at Amazon.\nAtin Ghosh PhD (2017-2021): Deep Learning for Glaucoma Understanding, representation learning, generative models, semi-supervised learning. Atin is now an Applied Research Scientist at Amazon.\nSe-In Jang Research Fellow (2019-2021): Computer vision and application to ophthalmology. Se-In is now a research fellow in the Center for Advanced Medical Computing and Analysis and the Gordon Center for Medical Imaging, Massachusetts General Hospital (MGH) and Harvard Medical School.\nAxel Finke Research Fellow (2017-2020): Sequential Monte Carlo, MCMC, algorithms for high-dimensional problems; applications in finance, economics, ecology and molecular biology. Axel is now an assistant professor at Loughborough University (UK).\nZuozhu Liu Research Fellow (2019-2020): Bayesian inference, deep generative models, 3D vision and medical applications. Zuozhu is now Assistant Professor at the Zhejiang University-University of Illinois at Urbana-Champaign Institute.\nMatt Graham Research Fellow (2017-2020): Approximate inference methods, MCMC, approximate Bayesian computation, numerical simulation. Matt is now a research data scientist in the Advanced Research Computing Centre at University College London.\nWillem van den Boom Research Fellow (2018-2019): After a position of Senior Research Fellow in the Division of Biomedical Data Science at the Yong Loo Lin School of Medicine at NUS, Willem now works at Google.\nKhai Sing Chin Research Associate (2017-2018): Khai Sing is now working in the finance industry.\nDeborshee Sen PhD (NUS, 2014-2017). Winner of the 2017 DSAP NUS best researcher award. After a postdoc at Duke University and a position as an assistant Professor at Bath University, Deborshee now works as a Research Scientist at Amazon.\nDaniel Paulin Research Fellow (NUS, 2014-2015). Daniel is now an Associate Professor at the Nanyang Technological University,\nEge Muzaffer PhD (NUS, 2016): Bayesian inverse problems and Sequential Monte Carlo. Ege is now a Machine Learning EngineerMachine Learning Engineer Ubisoft RedLynx"
  },
  {
    "objectID": "people/index_people.html#msc",
    "href": "people/index_people.html#msc",
    "title": "Research team",
    "section": "MSc",
    "text": "MSc\n\nQuang Huy Nguyen MSc (2018-2019): representation learning, robust models for image segmentation.\nAugustin Hoff (NUS, 2016-2017). Deep Neural Networks and Features Extraction. Augustin is now a Senior Data Scientist at MAIF.\nMajdi Rabia (NUS, 2016-2017). Numerical Method for Backward-Stochastic-Differential-Equations. Majdi is Co-founder and CTO @Fairphonic.\nBenjamin Scellier (NUS, 2015). Deep Learning. After his MSc at NUS, Benjamin joined Yoshua Bengio’s Group as a PhD. He is now a principal research scientist at Rain"
  },
  {
    "objectID": "jobs/2024_koopman/2024_koopman.html",
    "href": "jobs/2024_koopman/2024_koopman.html",
    "title": "Research Fellow positions: Data-Assimilation",
    "section": "",
    "text": "[Edit 28/02/2025: The positions have been filled. Thank you for your interest.]\nTwo Postdoctoral Research Fellow positions are currently open in the Department of Statistics & Data Science at the National University of Singapore (NUS). These are two-year contracts, with the possibility of renewal upon review.\nWhile the research themes are flexible, our primary interest lies in the design and analysis of statistical methods for data-assimilation of high-dimensional time-series data. We anticipate applying tools from reinforcement learning, Koopman operators, and unsupervised representation learning, supplementing the more conventional techniques in state-space modeling (particle filters, EnKF, variational approaches, etc…)\n\nDetails:\n\nStart Date: Immediate\nDuration: 2 years (Renewable for up to an additional year)\nSalary Range: S$ 70k-100k / Year, plus benefits (Health Insurance / etc…)\nTravel: Generous budget for attending conferences\n\n\n\nRequirements:\nIdeal candidates should have:\n\na Ph.D. in Statistics, Computer Science, Physics, or a closely related field.\n\nstrong experience in Python programming.\n\nexperience with at least one deep-learning framework\na demonstrated record of academic publications\n\nresearch experience that aligns with the core focus of the research programme.\n\n\n\nContact:\nQueries about this post should be addressed to Alex Thiery at a.h.thiery@nus.edu.sg. We highly encourage prospective applicants to get in touch informally to discuss further details about the position first. This project is in collaboration with Jeremy Heng."
  },
  {
    "objectID": "jobs/2024_koopman/2024_control/2024_controlled.html",
    "href": "jobs/2024_koopman/2024_control/2024_controlled.html",
    "title": "Research Fellow positions: Data-Assimilation and Generative Modeling",
    "section": "",
    "text": "Two Postdoctoral Research Fellow positions are available in the Department of Statistics & Data Science at the National University of Singapore (NUS). These positions are offered on a two-year contract basis, with the possibility of renewal subject to review.\nAlthough the research themes are flexible, our primary focus is on the design and analysis of statistical methods for data assimilation of high-dimensional time-series data and generative modeling. With the recent advances in diffusion-based models, concepts initially developed for the inference of time series and in the control theory literature can now be adapted for generative models. Conversely, the latest methodological developments in generative models (e.g., denoising diffusion, stochastic interpolants, flow-methods) can be applied to enhance the performance of traditional statistical tools used in data assimilation for high-dimensional time series models (e.g., ensemble Kalman filters, particle filters). In this project, we propose to explore the intersection of these two areas, aiming to enhance the robustness and accuracy of methods for data-assimilation of high-dimensional dynamical systems, as well as to develop new sampling algorithms.\n\nDetails:\n\nStart Date: Immediate\nDuration: 2 years (Renewable for up to an additional year)\nSalary Range: S$ 70k-90k / Year, plus benefits (Health Insurance / etc…)\nTravel: Generous budget for attending conferences\n\n\n\nRequirements:\nIdeal candidates should have:\n\na Ph.D. in Statistics, Computer Science, Physics, or a closely related field.\n\nstrong experience in Python programming.\n\nexperience with at least one deep-learning framework\na demonstrated record of academic publications\n\nresearch experience that aligns with the core focus of the research programme.\n\n\n\nContact:\nQueries about this post should be addressed to Alex Thiery at a.h.thiery@nus.edu.sg. We highly encourage prospective applicants to get in touch informally to discuss further details about the position first. This project is in collaboration with Jeremy Heng.\nThe positions will remain open until we find the right candidates."
  },
  {
    "objectID": "jobs/2024_diffusion/2024_diffusion.html",
    "href": "jobs/2024_diffusion/2024_diffusion.html",
    "title": "Research Fellow positions: Data-Assimilation and Generative Modeling",
    "section": "",
    "text": "Two Postdoctoral Research Fellow positions are available in the Department of Statistics & Data Science at the National University of Singapore (NUS). These positions are offered on a two-year contract basis, with the possibility of renewal subject to review.\nAlthough the research themes are flexible, our primary focus is on the design and analysis of statistical methods for data assimilation of high-dimensional time-series data and generative modeling. With the recent advances in diffusion-based models, concepts initially developed for the inference of time series and in the control theory literature can now be adapted for generative models. Conversely, the latest methodological developments in generative models (e.g., denoising diffusion, stochastic interpolants, flow-methods) can be applied to enhance the performance of traditional statistical tools used in data assimilation for high-dimensional time series models (e.g., ensemble Kalman filters, particle filters). In this project, we propose to explore the intersection of these two areas, aiming to enhance the robustness and accuracy of methods for data-assimilation of high-dimensional dynamical systems, as well as to develop new sampling algorithms.\n\nDetails:\n\nStart Date: Immediate\nSalary Range: S$ 80k-90k / Year, plus benefits (Health Insurance / etc…)\nTravel: Generous budget for attending conferences\n\n\n\nRequirements:\nIdeal candidates should have:\n\na Ph.D. in Statistics, Computer Science, Physics, or a closely related field.\n\nstrong experience in Python programming.\n\nexperience with at least one deep-learning framework\na demonstrated record of academic publications\n\nresearch experience that aligns with the core focus of the research programme.\n\n\n\nContact:\nQueries about this post should be addressed to Alex Thiery at a.h.thiery@nus.edu.sg. We highly encourage prospective applicants to get in touch informally to discuss further details about the position first.\nThe positions will remain open until we find the right candidates."
  },
  {
    "objectID": "notes/VIASM_2024/VIASM_2024.html",
    "href": "notes/VIASM_2024/VIASM_2024.html",
    "title": "VIASM mini-course on diffusions and flows",
    "section": "",
    "text": "The slides for this short course on diffusion models (denoising diffusions, probability flows) and other flow methods (stochastic interpolants, flow-matching) are available here. There are a few animations, so loading the slides may be slow…"
  },
  {
    "objectID": "notes_students/probability_basics/probability_basics.html#mean-and-expectation",
    "href": "notes_students/probability_basics/probability_basics.html#mean-and-expectation",
    "title": "Probability Basics",
    "section": "",
    "text": "The mean or expectation of a random variable \\(X\\) is denoted as \\(\\mathbb{E}[X]\\)l. For a discrete random variable,\n\\[\n\\mathbb{E}[X] = \\sum_{x} x \\cdot \\mathbb{P}(X = x)\n\\]\nand for a continuous random variable,\n\\[\n\\mu_X = \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\n\\]\nwhere \\(f(x)\\) is the probability density function of \\(X\\). It’s important to note that in some cases, particularly for distributions with “heavy” tails, the mean might not be well-defined. This situation arises in distributions like the Cauchy distribution, where the tails of the distribution do not decay rapidly enough to yield a finite expectation. In such cases, the integrals or sums used to define the mean do not converge.\nIndependence is a fundamental concept in probability theory, referring to the relationship between two random variables. Two random variables, \\(X\\) and \\(Y\\), are said to be independent if the occurrence of an event related to \\(X\\) does not influence the probability of an event related to \\(Y\\), and vice versa. Mathematically, \\(X\\) and \\(Y\\) are independent if and only if for every pair of events \\(A\\) and \\(B\\), the probability that both \\(X\\) belongs to \\(A\\) and \\(Y\\) belongs to \\(B\\) is the product of their individual probabilities. This can be expressed as:\n\\[\n\\mathbb{P}(X \\in A \\; \\text{ and } \\; Y \\in B) = \\mathbb{P}(X \\in A) \\cdot \\mathbb{P}(Y \\in B).\n\\]\nThis can equivalently be expressed as the fact that, for any two functions \\(F(\\cdot)\\) and \\(G(\\cdot)\\), the following identity holds\n\\[\n\\mathbb{E}[F(X) \\cdot G(Y)] \\; = \\; \\mathbb{E}[F(X)] \\cdot \\mathbb{E}[G(Y)].\n\\]\nThis definition implies that knowing the outcome of \\(X\\) provides no information about the outcome of \\(Y\\), and this lack of influence is a key characteristic of independent random variables. One extremely important remark is that, for two random variables \\(X\\) and \\(Y\\), the expectation of the sum equals the sum of the expectation,\n\\[\n\\mathbb{E}[X+Y] = \\mathbb{E}[X] + \\mathbb{E}[Y],\n\\]\nas soon as all these quantities exists. This holds even if the two random variables are not independent."
  },
  {
    "objectID": "notes_students/probability_basics/probability_basics.html#standard-deviation-and-variances-and-covariances",
    "href": "notes_students/probability_basics/probability_basics.html#standard-deviation-and-variances-and-covariances",
    "title": "Probability Basics",
    "section": "",
    "text": "The variance of a random variable \\(X\\), denoted as \\(\\text{Var}(X)\\), measures the spread of its values. It is defined as\n\\[\n\\begin{align}\n\\text{Var}(X) &= \\mathbb{E}[(X - \\mu_X )^2] = \\mathbb{E}[X^2] - \\mu_X ^2\\\\\n&= \\int_{-\\infty}^{\\infty} (x - \\mu_X)^2 \\cdot f(x) \\, dx\n\\end{align}\n\\]\nThe standard deviation is the square root of the variance, denoted as \\(\\sigma_X = \\sqrt{\\text{Var}(X)}\\). The notion of covariance measures the linear relationship between two random variables \\(X\\) and \\(Y\\). It is defined as\n\\[\n\\begin{align}\n\\text{Cov}(X, Y)\n&= \\mathbb{E}[(X - \\mu_X )(Y - \\mu_Y )] = \\mathbb{E}[X \\, Y] -  \\mu_X  \\, \\mu_Y\\\\\n&= \\int_{-\\infty}^{\\infty} (x - \\mu_X)(y-\\mu_Y) \\cdot f(x, y) \\, dx\n\\end{align}\n\\]\nwhere \\(f(x,y)\\) is the joined density of the pair of random variables \\((X,Y)\\). The correlation is defined as a normalized version of the covariance,\n\\[\n\\text{Corr}(X,Y) = \\textrm{Cov}\\left\\{ \\frac{X - \\mu_X}{\\sigma_X}, \\frac{Y - \\mu_Y}{\\sigma_Y}\\right\\}\n\\]\nand always satisfies \\(-1 \\leq \\text{Corr}(X,Y) \\leq 1\\), as is easily proved (exercise). Note that if \\(X\\) and \\(Y\\) are independent, then \\(\\text{Cov}(X, Y) = 0\\). However, zero covariance does not imply independence and it is a good exercise to construct such a counter-example. Standard manipulations reveal that for two random variables \\(X\\) and \\(Y\\) we have\n\\[\n\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y) + \\text{Cov}(X, Y),\n\\]\nwhich is indeed the equivalent of the identity \\((x+y)^2 = x^2 + y^2 + 2xy\\). Importantly, if the two random variables \\(X\\) and \\(Y\\) are independent, the variance of the sum equals the sum of the variances, \\(\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y)\\). This also shows that for \\(N\\) independent and identically distributed random variables \\(X_1, \\ldots, X_N\\), we have that\n\\[\n\\text{Var}\\left\\{ \\frac{X_1 + \\ldots + X_N}{N} \\right\\} \\; = \\; \\frac{\\text{Var}(X)}{N}.\n\\]"
  },
  {
    "objectID": "notes_students/probability_basics/probability_basics.html#random-vectors",
    "href": "notes_students/probability_basics/probability_basics.html#random-vectors",
    "title": "Probability Basics",
    "section": "",
    "text": "A random vector is a vector of random variables. For a random vector \\(\\mathbf{X}\\) in \\(\\mathbb{R}^d\\), the mean of \\(\\mu = \\mathbf{X} \\ in \\mathbb{R}^d\\) is a vector in \\(\\mathbb{R}^d\\), each component of which is the mean of one of its \\(d\\) components. The covariance matrix, \\(\\Sigma \\in \\mathbb{R}^{d,d}\\), of \\(\\mathbf{X}\\) is a \\(d \\times d\\) matrix defined by\n\\[\n\\Sigma_{ij} = \\text{Cov}(X_i, X_j)\n=\n\\mathbb{E}[(X_i - \\mu_{X_i}) \\, (X_j - \\mu_{X_j})]\n\\]\nwhere \\(X_i\\) and \\(X_j\\) are the \\(i\\)-th and \\(j\\)-th components of \\(\\mathbf{X}\\), respectively. Each element \\(\\Sigma_{ij}\\) represents the covariance between the \\(i\\)-th and \\(j\\)-th components of the vector \\(\\mathbf{X}\\). If the components are independent, the covariance matrix is diagonal. Furthermore, the covariance matrix, when it exists, is always a symmetric and positive semi-definite matrix."
  },
  {
    "objectID": "notes_DRAFT/optimization/duality.html",
    "href": "notes_DRAFT/optimization/duality.html",
    "title": "Perturbed Optimization and Duality",
    "section": "",
    "text": "The Dual Problem\n(Ekeland and Temam 1999) describe the concept of perturbed optimization and how it relates to duality theory. It is interesting to derive standard Lagrange and Fenchel duality from this perspective. Consider a function \\(f: E \\to \\bar{\\mathbb{R}}\\) from \\(E = \\mathbb{R}^n\\) to the extended real numbers \\(\\bar{\\mathbb{R}} = \\mathbb{R}\\cup \\{-\\infty, +\\infty\\}\\). We are interested minimizing \\(f\\). We introduce a perturbation function \\(\\Phi: E \\times F \\to \\bar{\\mathbb{R}}\\) where \\(F=\\mathbb{R}^m\\) and such that\n\\[\nf(x) = \\Phi(x, 0) \\quad \\text{for all } x \\in E.\n\\]\nOur purpose is to compute \\(v(0)\\) where the value function \\(v: F \\to \\bar{\\mathbb{R}}\\) is defined as\n\\[\nv(y) = \\inf_{x \\in E} \\Phi(x, y) \\quad \\text{for all } y \\in F.\n\\]\nThe convex conjugate function \\(\\Phi^*: E^* \\times F^* \\to \\bar{\\mathbb{R}}\\) is defined as\n\\[\n\\Phi^*(p, q) = \\sup_{x \\in E, y \\in F} \\; \\; \\left&lt; p,x \\right&gt; + \\left&lt; q,y \\right&gt; - \\Phi(x, y)\n\\]\nso that \\(\\Phi(x,y) + \\Phi^*(p,q) \\geq \\left&lt; p,x \\right&gt; + \\left&lt; q,y \\right&gt;\\) for all \\((x,y,p,q)\\). In particular, we have\n\\[\n\\Phi(x,0) + \\Phi^*(0,q) \\geq 0\n\\]\nfor all \\(x \\in E\\) and \\(q \\in F^*\\). Since algebra directly shows that \\(\\Phi^*(0,q) = v^*(q)\\), we have the fundamental inequality:\n\\[\nv(0) = \\inf_{x \\in E} \\; f(x) \\; \\geq \\; \\sup_{q \\in F^*} \\; -v^*(q) = v^{**}(0).\n\\tag{1}\\]\nThis is the so-called dual problem. Note that it does indeed depend on the perturbation function \\(\\Phi\\) and not only on the function \\(f\\). If one can analyze the conjugate function \\(v^*\\), then one can obtain lower bounds on the infimum of \\(f\\). As first sight, Equation 1 may seem like a trivial result since for any function \\(F\\) we have that \\(F(0) + F^*(0) \\geq 0\\) and getting hold of the conjugate function \\(v^*\\) does not seem to be an easy task; we will see that this is not the case. One says that strong duality holds if\n\\[\nv(0) = v^{**}(0).\n\\tag{2}\\]\n\n\nConvexity and Strong Duality\nIf one assumes that the objective function \\(f(x)\\) is convex and the perturbation function \\(\\Phi(x,y)\\) is jointly convex in \\((x,y)\\), then the value function \\(v\\) is also convex. Indeed, the function \\(y \\mapsto \\inf_x C(x,y)\\) is convex as soon as the function \\(C(x,y)\\) is convex in \\((x,y)\\). Since a function \\(g\\) equals its biconjugate \\(g^{**}(x)\\) at a point \\(x\\) as soon its subdifferential \\(\\partial g(x)\\) is nonempty, this shows that strong duality @#eq-strong-duality holds as soon as \\(\\partial v(0) \\neq \\emptyset\\). Furthermore, in a finite dimensional space, a convex function is continuous and admits a subdifferential at any point within the interior of its domain. This shows that if \\(0\\) belongs to the interior of the domain of \\(v\\), then strong duality holds. Since a convex function is continuous at \\(x\\) if it is bounded in a neighborhood of \\(x\\), strong duality holds as soon as \\(v\\) is bounded in a neighborhood of \\(0\\). By definition of the value function we have that \\(v(y) \\leq \\Phi(x,y)\\). This shows that strong duality holds as soon \\(v(0)\\) is finite and there exists \\(x_0 \\in E\\) such that the function \\(\\Phi_{x_0}: y \\mapsto \\Phi(x_0, y)\\) is continuous at \\(0\\).\n\n\nFenchel Duality\nConsider the case of a function to minimize of the type:\n\\[\n\\inf_x \\; f(x) = G(x) + H( A x)\n\\]\nfor a linear operator \\(A: E \\to F\\) and two convex functions \\(G: E \\to \\bar{\\mathbb{R}}\\) and \\(H: F \\to \\bar{\\mathbb{R}}\\). One can consider the perturbation function\n\\[\\Phi(x,y) = G(x) + H(Ax - y).\\]\nAlgebra gives that \\(\\Phi^*(0,q) = G^*(A^\\top q) + H^*(-q)\\) so that the dual problem reads\n\\[\n\\sup_{q \\in F^*} \\; -G^*(A^\\top q) - H^*(-q).\n\\]\nBy the previous discussion, strong duality holds, for example, as soon as the primal problem is finite and there exists \\(x_0\\) such that \\(G(x_0) &lt; \\infty\\) and the function \\(H\\) is continuous at \\(A x_0\\).\n\n\n\n\n\nReferences\n\nEkeland, Ivar, and Roger Temam. 1999. Convex Analysis and Variational Problems. SIAM."
  },
  {
    "objectID": "notes_DRAFT/natural_grad/natur_grad.html",
    "href": "notes_DRAFT/natural_grad/natur_grad.html",
    "title": "Natural Gradient",
    "section": "",
    "text": "Shun’ichi Amari, founder of modern information geometry\n\n\n\nConsider a parametric family of probability distributions \\(q_{\\theta}(x)\\) indexed by \\(\\theta \\in \\Theta \\subseteq \\mathbb{R}^d\\). We are interested in minimizing a functional \\(\\theta \\mapsto J(q_{\\theta})\\), typically the Kullback-Leibler divergence between \\(q_{\\theta}\\) and a target distribution \\(\\pi(x)\\). One could indeed set \\(F(\\theta) = J(q_{\\theta})\\) and use a variation of the gradient descent algorithm to minimize \\(F\\), i.e. \\(\\theta_{t+1} = \\theta_t - \\alpha_t \\nabla F(\\theta_t)\\) for some step size \\(\\alpha_t\\). Although simple, this approach has the important drawback of depending very much one the choice of the parametrization of the family \\(q_{\\theta}\\). Instead, one could for example consider an update of the type:\n\\[\n\\theta_{t+1} = \\mathop{\\mathrm{argmin}}_{\\theta \\in \\Theta} \\;  {\\left\\{   \\textcolor{blue}{J(q_{\\theta})} + \\frac{1}{\\alpha_t} \\, D(q_{\\theta} \\| q_{\\theta_t}) \\right\\}}\n\\]\nfor step size \\(\\alpha_t\\) and some divergence \\(D\\) often taken to be the Kullback-Leibler divergence. This has the advantage of beging entirely independent of the parametrization of the family \\(q_{\\theta}\\). Unfortunately, the right hand side of the above equation is not tractable in general. A middle ground consists in considering\n\\[\n\\theta_{t+1} = \\mathop{\\mathrm{argmin}}_{\\theta \\in \\Theta} \\;  {\\left\\{   \\textcolor{blue}{F(\\theta_t) + \\left&lt; \\theta - \\theta_t, \\nabla_{\\theta} F(\\theta_t) \\right&gt;} + \\frac{1}{\\alpha_t} \\, D(q_{\\theta} \\| q_{\\theta_t}) \\right\\}} .\n\\tag{1}\\]\nFurthermore, in the case \\(D\\) is the Kullback-Leibler divergence, a Taylor expansion of the divergence around \\(\\theta_t\\) gives\n\\[\nD_{\\text{KL}}(q_{\\theta} \\| q_{\\theta_t}) \\approx \\frac{1}{2} \\left&lt; (\\theta - \\theta_t), \\mathcal{I}(\\theta_t) (\\theta - \\theta_t) \\right&gt; \\, + \\, \\mathcal{O}(\\|\\theta - \\theta_t\\|^3)\n\\]\nwhere \\(\\mathcal{I}(\\theta_t)\\) is the Fisher information matrix at \\(\\theta_t\\):\n\\[\n\\begin{align*}\n\\mathcal{I}(\\theta_t)\n&=\n\\mathbb{E}_{q_{\\theta_t}} {\\left[  \\nabla_{\\theta} \\log q_{\\theta_t}(X) \\, \\nabla_{\\theta} \\log q_{\\theta_t}^\\top(X) \\right]} \\\\\n&=\n- \\mathbb{E}_{q_{\\theta_t}}  {\\left[ \\nabla_{\\theta}^2 \\log q_{\\theta_t} \\right]} .\n\\end{align*}\n\\]\nReplacing \\(D(q_{\\theta} \\| q_{\\theta_t})\\) by \\(\\frac{1}{2} \\left&lt; (\\theta - \\theta_t), \\mathcal{I}(\\theta_t) (\\theta - \\theta_t) \\right&gt;\\) in Equation 1 gives the so-called natural gradient update rule (Amari 1998):\n\\[\n\\theta_{t+1} = \\theta_t - \\alpha_t \\, \\mathcal{I}(\\theta_t)^{-1} \\, \\nabla_{\\theta} F(\\theta_t).\n\\]\nIn other words, the inverse of the Fisher information matrix acts as a preconditioner.\n\n\n\n\nReferences\n\nAmari, Shun-Ichi. 1998. “Natural Gradient Works Efficiently in Learning.” Neural Computation 10 (2): 251–76."
  },
  {
    "objectID": "notes/conformal_inference/conformal.html",
    "href": "notes/conformal_inference/conformal.html",
    "title": "Basic Conformal Inference",
    "section": "",
    "text": "Unfortunately, I am totally ignorant about conformal inference. However, in today’s seminar, I attended a very interesting talk on the topic, and I think it’s time I try implementing the most basic version of it. It seems like a useful concept, and I might even explain it next semester in my simulation class. What I’ll describe below is the simplest version of conformal inference. There appear to be many extensions and variations of it, most of which I don’t yet understand. For now, I just want to spend a few minutes implementing it myself to ensure I grasp the basic idea.\nConsider the (simulated) 1D dataset \\(\\mathcal{D} = \\{x_i, y_i\\}_{i=1}^N\\) below; our goal is to build prediction confidence intervals \\([L(x), U(x)]\\) for the target variable \\(y\\) given a new input \\(x\\). Crucially, we would like these predictions to be well-calibrated in the sense that \\(y \\in [L(x), U(x)]\\) with probability \\(90\\%\\), say.\n\n\n\n\n1D regression dataset\n\n\n\nI am lazy so I will be using a simple KNN regressor to predict the target variable \\(y\\) given a new input \\(x\\). For this purpose, split the dataset \\(\\mathcal{D}\\) into two parts \\(\\mathcal{D}_{\\text{train}}\\) and \\(\\mathcal{D}_{\\text{Cal}}\\). The regressor is fitted on \\(\\mathcal{D}_{\\text{train}}\\). To calibrate the prediction intervals, compute the residuals \\(r_i = |y_i - \\hat{y}_i|\\) on the calibration set \\(\\mathcal{D}_{\\text{Cal}}\\), where \\(\\hat{y}_i = \\hat{y}(x_i)\\) is the prediction of the regressor on \\(x_i\\). One can then compute the \\(90\\%\\) quantile \\(\\gamma_{90\\%}\\) of the residuals: with probability \\(90\\%\\) we have that \\(y_i \\in [\\hat{y}_i - \\gamma_{90\\%}, \\hat{y}_i + \\gamma_{90\\%}]\\) on the calibration set, and this can be used to build the prediction intervals, as displayed below:\n\n\n\n\n1D regression dataset: basic conformal inference\n\n\n\nNot terribly impressive, but at least it is entirely straightforward to implement and it has the correct (marginal) coverage: for a new pair \\((X,Y)\\) coming from the same distribution as the training data, the probability that \\(Y\\) falls within the prediction interval is indeed \\(90\\%\\), up to a bit of nitpicking. Note that it is much much less impressive than saying that\n\\[\\mathbb{P} {\\left( Y \\in [\\hat{y}(x) - \\gamma_{90\\%}, \\hat{y}(x) + \\gamma_{90\\%}] \\; | \\; X=x \\right)}  = 90\\%,\\]\nwhich is clearly not true as can be seen from the figure above, but it is a good start. As a matter of fact, I’ve learned today from the very nice talk that without other assumptions, it is impossible to design a procedure that would guarantee the above so-called conditional coverage (Lei and Wasserman 2014). But let’s face it, the figure above is terribly unimpressive. Nevertheless, one can indeed make it slightly less useless by calibrating using a different strategy. For example, I can use the training set to estimate the Mean Absolute Deviation (MAD) of the residuals \\(\\sigma(x) = \\mathbb{E}[ |Y - \\hat{y}(x) | \\; | \\; X=x]\\) (again with a naive KNN regressor) and use the calibration set to estimate the \\(90\\%\\) quantile \\(\\gamma_{90\\%}\\) of the quantities \\(|y_i - \\hat{y}_i| / \\sigma(x_i)\\). This allows one to produce calibrated prediction intervals of the type \\([\\hat{y}_i - \\gamma_{90\\%} \\sigma(x_i), \\hat{y}_i + \\gamma_{90\\%} \\sigma(x_i)]\\), which are displayed below:\n\n\n\n\n1D regression dataset: less useless conformal inference\n\n\n\nIt is slightly more useful, and it is again surprisingly straightforward to implement, literally 5 lines of code. I think I will have to read more about this in the future and I am pretty sure I will introduce the idea to the next batch of students!\n\nReadings:\n\nThe introduction paper (Lei et al. 2018) is really good\nI am really curious about (Gibbs, Cherian, and Candès 2023) and it’s next on my reading list\n\n\n\n\n\n\nReferences\n\nGibbs, Isaac, John J Cherian, and Emmanuel J Candès. 2023. “Conformal Prediction with Conditional Guarantees.” arXiv Preprint arXiv:2305.12616.\n\n\nLei, Jing, Max G’Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. 2018. “Distribution-Free Predictive Inference for Regression.” Journal of the American Statistical Association 113 (523). Taylor & Francis: 1094–1111.\n\n\nLei, Jing, and Larry Wasserman. 2014. “Distribution-Free Prediction Bands for Non-Parametric Regression.” Journal of the Royal Statistical Society Series B: Statistical Methodology 76 (1). Oxford University Press: 71–96."
  },
  {
    "objectID": "notes_DRAFT/markov/markov.html",
    "href": "notes_DRAFT/markov/markov.html",
    "title": "Convergence of Markov Processes",
    "section": "",
    "text": "Andrey Markov (1856 - 1922)"
  },
  {
    "objectID": "notes_DRAFT/markov/coupling.html",
    "href": "notes_DRAFT/markov/coupling.html",
    "title": "Convergence of Markov Processes",
    "section": "",
    "text": "Andrey Markov (1856 - 1922)"
  },
  {
    "objectID": "notes_DRAFT/markov/drift.html",
    "href": "notes_DRAFT/markov/drift.html",
    "title": "Convergence of Markov Processes",
    "section": "",
    "text": "Andrey Markov (1856 - 1922)"
  },
  {
    "objectID": "notes_DRAFT/markov/sobolev.html",
    "href": "notes_DRAFT/markov/sobolev.html",
    "title": "Convergence of Markov Processes",
    "section": "",
    "text": "Andrey Markov (1856 - 1922)"
  },
  {
    "objectID": "notes_DRAFT/markov/isoperimetric.html",
    "href": "notes_DRAFT/markov/isoperimetric.html",
    "title": "Convergence of Markov Processes",
    "section": "",
    "text": "Andrey Markov (1856 - 1922)"
  },
  {
    "objectID": "notes_DRAFT/markov/conductance.html",
    "href": "notes_DRAFT/markov/conductance.html",
    "title": "Convergence of Markov Processes",
    "section": "",
    "text": "Andrey Markov (1856 - 1922)"
  },
  {
    "objectID": "notes_DRAFT/markov/spectral.html",
    "href": "notes_DRAFT/markov/spectral.html",
    "title": "Convergence of Markov Processes",
    "section": "",
    "text": "Andrey Markov (1856 - 1922)"
  },
  {
    "objectID": "notes/jarzynski/jarzynski.html",
    "href": "notes/jarzynski/jarzynski.html",
    "title": "Jarzynski and Crooks",
    "section": "",
    "text": "Christopher Jarzynski and Gavin Crooks\n\n\n\nConsider a sequence of densities on \\(\\mathbb{R}^D\\) indexed by time parameter \\(t \\in [0,T]\\),\n\\[\n\\pi_t(x) \\; = \\; \\frac{ e^{-U_t(x)}}{Z_t}\n\\]\nwhere \\(U_t: \\mathbb{R}^D \\to \\mathbb{R}\\) is time-dependent potential function and \\(Z_t\\) is the normalizing constant. We are in fact really interested in studying the final density \\(\\pi_T\\) and the bridging sequence of densities \\(\\pi_t\\) is just a tool to get there, starting from an initial and tractable density \\(\\pi_0\\). If one initializes a particle \\(X_0 \\sim \\pi_0\\) and evolves it according to the Langevin dynamics\n\\[\ndX_t \\; = \\; -\\nabla U_t(X_t) \\, dt + \\sqrt{2} \\, dW_t\n\\]\none can hope that the distribution of \\(X_T\\) will be close to \\(\\pi_T\\). This would be the case if one evolved the particle according \\(dX_t \\; = \\; -\\gamma \\nabla U_t(X_t) \\, dt + \\sqrt{2 \\gamma} \\, dW_t\\) and let \\(\\gamma \\to \\infty\\) since in that case \\(X_t\\) would be distributed according to \\(\\pi_t\\) for all \\(t\\). Can one correct the distribution of \\(X_T\\) with importance sampling weights?\nI like the approach presented in (Vargas et al. 2024) and these notes are my attempt to understand it. One very fruitful idea that has been used in a number of works in the Monte-Carlo literature is to look at a probability distribution of interest as the marginal of a joint distribution and to carry out computations and build numerical methods on the joint distribution (Del Moral, Doucet, and Jasra 2006). Indeed, there is a lot of flexibility in the choice of the joint distribution.\nHere, we can also consider the diffusion process \\(Y_T\\) that runs backward in times and that is initialized according to \\(\\pi_T\\) and follows the same Langevin dynamics. Again, one expects the distribution of \\(Y_t\\) to be close to \\(\\pi_t\\). It is more intuitive to discuss discretized version of the process. For a time discretization \\(\\delta = T/N\\), we have\n\\[\n\\left\\{\n\\begin{aligned}\nx_{t + \\delta} &= x_t - \\nabla U_t(x_t) \\, \\delta + \\sqrt{2 \\delta} \\, \\xi_t\\\\\ny_{t} &= y_{t + \\delta} - \\nabla U_{t + \\delta}(y_{t + \\delta}) \\, \\delta + \\sqrt{2 \\delta} \\, \\xi_t\n\\end{aligned}\n\\right.\n\\]\nwhere \\(\\xi_t \\sim \\mathcal{N}(0,I)\\) are i.i.d. standard Gaussian random variables. Let us continue with these discretized versions and denote by \\(\\mathbb{P}^{X}\\) and \\(\\mathbb{P}^{Y}\\) the probability measures associated with the discretized processes. The crucial remark is that the marginal distribution of \\(\\mathbb{P}^Y\\) at time \\(T\\) is our distribution of interest \\(\\pi_T\\). For a discretized path \\(\\underline{z} = (z_0, z_{\\delta}, \\ldots, z_{T})\\) we have:\n\\[\n\\begin{aligned}\n\\mathbb{P}^X(\\underline{z}) &=\n\\pi_0(z_0) \\, \\exp {\\left\\{ -\\frac{1}{4 \\delta} \\sum_{k=0}^{N-1} \\|z_{t_{k+1}} - [z_{t_k} - \\nabla U_{t_k}(z_{t_k})\\,\\delta]\\|^2 \\right\\}} \\\\\n\\mathbb{P}^Y(\\underline{z}) &=\n\\pi_T(z_T) \\, \\exp {\\left\\{ -\\frac{1}{4 \\delta} \\sum_{k=0}^{N-1} \\|z_{t_{k}} - [z_{t_{k+1}} - \\nabla U_{t_{k+1}}(z_{t_{k+1}})\\,\\delta]\\|^2 \\right\\}} .\n\\end{aligned}\n\\]\nOne can compute the ratio \\(\\mathbb{P}^Y(z) / \\mathbb{P}^X(z)\\) and examine its limit as \\(N \\to \\infty\\). Algebra gives:\n\\[\n\\frac{d \\mathbb{P}^Y}{d \\mathbb{P}^X}(\\underline{z}) =\n\\frac{\\pi_T(z_T)}{\\pi_0(z_0)} \\,\n\\exp {\\left\\{ \\sum_{k=0}^{N-1} \\left&lt; z_{t_{k+1}} - z_{t_k}, \\frac{\\nabla U_{t_k}(z_{t_k}) + \\nabla U_{t_{k+1}}(z_{t_{k+1}})}{2}  \\right&gt; + \\mathcal{O}(\\delta^2) \\right\\}} .\n\\]\nOne could probably use some Stratonovich calculus to study this, but I always forget these things, so let’s use Ito instead. Write\n\\[\\frac{\\nabla U_{t_k}(z_{t_k}) + \\nabla U_{t_{k+1}}(z_{t_{k+1}})}{2}\n\\approx\n\\nabla U_{t_k}(z_{t_k}) + \\frac{1}{2} \\mathrm{Hess}_{U_{t_{k+1}}} (z_{t_{k+1}}) (z_{t_{k+1}} - z_{t_k})\n+\n\\frac12 \\, \\partial_t U_{t_k}(z_{t_k}) \\, \\delta.\n\\]\nThe term \\(\\partial_t U_{t_k}(z_{t_k}) \\, \\delta\\) is too small to matter in the limit \\(N \\to \\infty\\) and Ito formula \\(d U_t(z_t) = \\partial_t U_t(z_t) \\, dt + \\left&lt; \\nabla U_t(z_t), dz_t \\right&gt; + \\frac{1}{2} \\left&lt; dx, \\mathrm{Hess}_{U_t}(z_t) \\, dz_t \\right&gt;\\) shows that in the limit \\(N \\to \\infty\\):\n\\[\n\\frac{d \\mathbb{P}^Y}{d \\mathbb{P}^X}(\\underline{z})\n=\n\\frac{\\pi_T(z_T)}{\\pi_0(z_0)} \\,\n\\exp {\\left\\{  U_T(z_T) - U_0(z_0) - \\int_0^T \\partial_t U_t(z_t) \\, dt \\right\\}} .\n\\]\nSince \\(\\pi_t(z_t) = \\exp(-U_t(z_t)) / Z_t\\), this gives the Crooks relation:\n\\[\n\\frac{d \\mathbb{P}^Y}{d \\mathbb{P}^X}(\\underline{z})\n=\n\\frac{Z_0}{Z_T} \\,\n\\exp {\\left\\{  - \\int_0^T \\partial_t U_t(z_t) \\, dt \\right\\}} .\n\\]\nIntegrating over trajectories of \\(X_t\\), since \\(\\mathbb{E}_{X}[(d \\mathbb{P}^Y / d \\mathbb{P}^X)(X)] = 1\\), one obtains the Jarzynski equality \\[\n\\frac{Z_T}{Z_0} \\; = \\; \\mathbb{E}_{X}  {\\left\\{  \\exp {\\left\\{  - \\int_0^T \\partial_t U_t(X_t) \\, dt \\right\\}}  \\right\\}}\n\\]\nwhich is indeed also central to sequential Monte-Carlo methods. As described in (Vargas et al. 2024), the same approach can be used to slightly generalize the Crooks relation. Indeed, suppose that one instead consider the dynamics:\n\\[\ndX_t \\; = \\; -\\nabla U_t(X_t) \\, dt  \\textcolor{blue}{+ b_t(X_t) \\, dt} + \\sqrt{2} \\, dW_t\n\\]\nwhere \\(b: \\mathbb{R}^D \\to \\mathbb{R}^D\\) is a control function. One can consider the backward dynamics \\(Y_t\\) that is initialized according to \\(\\pi_T\\) and follows the dynamics \\(dY_t = -\\nabla U_t(Y_t) \\, dt  \\textcolor{red}{-} b(Y_t) \\, dt + \\sqrt{2} \\, dW_t\\) backward in time, i.e.\n\\[\n\\left\\{\n\\begin{aligned}\nx_{t + \\delta} &= x_t - \\nabla U_t(x_t) \\, \\delta + b_t(x_t) \\, \\delta + \\sqrt{2 \\delta} \\, \\xi_t\\\\\ny_{t} &= y_{t + \\delta} - \\nabla U_{t + \\delta}(y_{t + \\delta}) \\, \\delta  \\textcolor{red}{-} b_{t + \\delta}(y_{t + \\delta}) \\, \\delta + \\sqrt{2 \\delta} \\, \\xi_t.\n\\end{aligned}\n\\right.\n\\]\nThe minus sign for the backward dynamics \\(Y_t\\) is natural since one knows that this exactly gives the backward dynamics of the forward process \\(X_t\\) in the case when \\(X_t \\sim \\pi_t(dx)\\) for all time \\(0 \\leq t \\leq T\\). One can then follow the exact same steps, using that the quadratic variation is \\(\\left&lt; dz_t, dz_t \\right&gt; = 2 \\, dt\\), to obtain that\n\\[\n\\frac{d \\mathbb{P}^Y}{d \\mathbb{P}^X}(\\underline{z})\n=\n\\frac{Z_0}{Z_T} \\,\n\\exp {\\left\\{  \\int_0^T -\\partial_t U_t(z_t)  \\textcolor{blue}{+ \\nabla \\cdot b_t(z_t) - \\left&lt; \\nabla U_t(z_t), b_t(z_t) \\right&gt;} \\, dt \\right\\}} .\n\\]\nThis for example shows that, for \\(dX_t \\; = \\; -\\nabla U_t(X_t) \\, dt  \\textcolor{blue}{+ b_t(X_t) \\, dt} + \\sqrt{2} \\, dW_t\\) initialized according to \\(\\pi_0\\), we have:\n\\[\n\\frac{Z_T}{Z_0} \\; = \\; \\mathbb{E}_{X}  {\\left\\{  \\exp {\\left\\{  \\int_0^T -\\partial_t U_t(X_t)  \\textcolor{blue}{+ \\nabla \\cdot b_t(X_t) - \\left&lt; \\nabla U_t(X_t), b_t(X_t) \\right&gt;} \\, dt \\right\\}}  \\right\\}} .\n\\]\nThis generalization of the Crooks relation is also explored in (Albergo and Vanden-Eijnden 2024) where an alternative derivation by directly exploiting the Fokker-Planck equation. Crucially, (Albergo and Vanden-Eijnden 2024) note that, if the control function \\(b_t: \\mathbb{R}^D \\to \\mathbb{R}^D\\) is chosen so that\n\\[\n-\\partial_t U_t(x) + \\nabla \\cdot b_t(x) - \\left&lt; \\nabla U_t(x), b_t(x) \\right&gt;\n=\n\\frac{d}{dt} \\, \\log Z_t\n\\tag{1}\\]\nthen the term \\(\\int_0^T -\\partial_t U_t(X_t) + \\nabla \\cdot b_t(X_t) - \\left&lt; \\nabla U_t(X_t), b_t(X_t) \\right&gt; \\, dt\\) is indeed constant, which gives a zero-variance estimator of the free energy difference \\(\\log(Z_T/Z_0)\\). Indeed, it is a formidable challenge to solve the high-dimensional PDE Equation 1 and (Albergo and Vanden-Eijnden 2024) propose interesting PINNs-based methods to do so.\n\nSome References:\n\nThe original papers by Jarzynski (Jarzynski 1997) and Crooks (Crooks 1999).\nThe book (Stoltz, Rousset, et al. 2010) is excellent!\nThe two papers that prompted these notes: (Vargas et al. 2024) and (Albergo and Vanden-Eijnden 2024).\n\n\n\n\n\n\nReferences\n\nAlbergo, Michael S, and Eric Vanden-Eijnden. 2024. “Nets: A Non-Equilibrium Transport Sampler.” arXiv Preprint arXiv:2410.02711.\n\n\nCrooks, Gavin E. 1999. “Entropy Production Fluctuation Theorem and the Nonequilibrium Work Relation for Free Energy Differences.” Physical Review E 60 (3). APS: 2721.\n\n\nDel Moral, Pierre, Arnaud Doucet, and Ajay Jasra. 2006. “Sequential Monte Carlo Samplers.” Journal of the Royal Statistical Society Series B: Statistical Methodology 68 (3). Oxford University Press: 411–36.\n\n\nJarzynski, Christopher. 1997. “Nonequilibrium Equality for Free Energy Differences.” Physical Review Letters 78 (14). APS: 2690.\n\n\nStoltz, Gabriel, Mathias Rousset, et al. 2010. Free Energy Computations: A Mathematical Perspective. World Scientific.\n\n\nVargas, Francisco, Shreyas Padhy, Denis Blessing, and Nikolas Nüsken. 2024. “Transport Meets Variational Inference: Controlled Monte Carlo Diffusions.” ICLR 2024."
  },
  {
    "objectID": "teaching/SAW/SAW.html",
    "href": "teaching/SAW/SAW.html",
    "title": "ST3247 Assignments",
    "section": "",
    "text": "A 2D self-avoiding walk\n\n\n\n\nIntroduction\nA 2D self-avoiding random walk (SAW) is a path on the square lattice \\(\\mathbb{Z}^2\\) that does not visit the same site more than once. Unlike standard random walks, which allow revisits to previously occupied sites, self-avoiding walks impose a strict non-recrossing condition, making them a fundamental model in statistical mechanics, polymer physics, combinatorics and probability theory. Self-avoiding walks are particularly interesting because they provide insights into the behavior of polymers, have connections to critical phenomena in statistical physics, have led to new results in combinatorial mathematics and probability theory. From a mathematical perspective, SAWs are notoriously difficult to analyze due to the combinatorial explosion of possible paths. Even determining the number of possible SAWs of a given length on a lattice remains a challenging combinatorial problem. The difficulty arises from the correlations introduced by the self-avoidance constraint, making standard probabilistic techniques inapplicable.\nIn this assignment, you will focus on estimating the number \\(c_L\\) of self-avoiding walks of a given length \\(L\\) on the 2D square lattice, as illustrated in the figure above. This remains an open problem with no exact formula known up to this date. As this problem is important to many areas of mathematics, a number of advanced computational techniques have been developed to study them. The purpose of this project is for you to explore some of these techniques and deepen your understanding of Monte Carlo methods.\n\n\nProblem Statement\nConsider the two-dimensional square lattice, denoted as \\(\\mathbb{Z}^2 = \\{(x, y) : x, y \\in \\mathbb{Z}\\}\\). A self-avoiding walk (SAW) of length \\(L\\) is a sequence of \\(L+1\\) distinct vertices \\((x_0, y_0), (x_1, y_1), \\dots, (x_L, y_L)\\) in \\(\\mathbb{Z}^2\\) such that each successive vertex \\(z_{i+1} = (x_{i+1}, y_{i+1})\\) is a nearest neighbor of \\(z_i = (x_i, y_i)\\) for all \\(i = 0, 1, \\dots, L-1\\). We denote such a walk as \\(z_{0:L} = (z_0, z_1, \\dots, z_L)\\) and assume that it begins at the origin, i.e., \\(z_0 = (0, 0)\\). The number of self-avoiding walks of length \\(L\\) is denoted by \\(c_L\\), with known values \\(c_1 = 4\\) and \\(c_2 = 12\\). This quantity serves as the normalization constant for the uniform distribution \\(\\mathbb{P}_L\\) over all self-avoiding walks of length \\(L\\) since we have\n\\[\n\\mathbb{P}_L(z_{0:L}) = \\frac{1}{c_L} \\mathbf{1}\\{z_{0:L} \\text{ is a SAW of length } L\\}.\n\\]\nThis means that the Monte-Carlo techniques for evaluating normalization constants can be applied to estimate \\(c_L\\). The number of self-avoiding walks grows exponentially with \\(L\\), and it is known that \\(c_L \\approx \\mu^L\\), where \\(\\mu \\approx 2.6\\) is called the connective constant. However, the exact value of \\(\\mu\\) remains unknown. The main objective of this assignment is to estimate \\(\\mu\\), or equivalently, the quantity \\(\\lambda = \\log \\mu\\):\n\\[\n\\lambda = \\lim_{N \\to \\infty} \\; \\frac{1}{N} \\log c_N.\n\\]\nComputing \\(c_N\\) directly is infeasible for large \\(N\\) due to its exponential growth. Moreover, Monte Carlo methods are challenging to apply since the self-avoiding constraint makes uniform sampling difficult. For instance, in a standard random walk of length \\(L\\), there are \\(4^L\\) possible paths, but only a small fraction are self-avoiding. Given that \\(c_L \\approx \\mu^L\\) with \\(\\mu \\approx 2.6\\), the probability that a random walk is self-avoiding is approximately \\((\\mu/4)^L\\). For \\(L = 20\\), this probability is roughly \\(0.02\\%\\).\n\n\nOrganization of the report\nThe first few sections of your report should be as follows:\n\nIntroduction: Briefly introduce the problem and describe clearly what the main goal of the assignment is. This section is to make sure that you have understood the problem statement correctly. This section can be brief.\n\nBasic deterministic methods: Implement a method that can compute \\(c_N\\) for small values of \\(L\\), at least up to \\(L=10\\). This will be useful to validate your Monte Carlo methods.\nBasic Monte Carlo I: Implement a simple Monte-Carlo method that generates standard random walks of length \\(L\\) and estimates the fraction of self-avoiding walks.\nBasic Monte Carlo II: Implement a more sophisticated Monte-Carlo method that generates a walk of length \\(L\\) by sampling the next step \\(z_{i+1}\\) uniformly from the set of possible neighbors of \\(z_i\\) that do not lead to a self-intersection. If there is no such neighbor, which can happen, the walk remains still at \\(z_i\\) until the end of the walk, i.e. \\(z_{i+1} = z_i\\) until \\(i = L-1\\). Discuss how can this procedure be used to estimate \\(c_N\\).\n\nAfter the above \\(4\\) sections, you are then tasked with exploring more advanced Monte Carlo methods to estimate \\(\\lambda\\) and \\((\\log c_N)/N\\) for large values of \\(N\\).\n\n\nPractical Details\nSubmission: A single submission is required for each group. The submission is on canvas. The submission should include a report and the code. The code can be submitted as a zip file (not exceeding 10 MB) or as a link to a public repository (e.g., github). The report should be in PDF format and should not include the code. The report should be at most 10 pages long, including figures and references. It is entirely possible to obtain a full grade with a report that is significantly shorter than 10 pages: do not write a longer report just to meet the page limit. Although not compulsory, you are encouraged to use Latex to write the report – equations are straightforward to write and this produce neat reports. A template is available here and with your NUS email, you can get a free account on Overleaf for collaborative editing.\nCriteria: In this project, you will be evaluated on the following criteria:\n\nYour exploration and understanding of various Monte Carlo methods for estimating \\(\\lambda\\). You will be evaluated on the quality and breadth of your numerical experiments, your critical analysis of the results, as well as the extend to which you have attempted to explore the problem and the literature.\n50% of the grade is based on the content of the report. The clarity and organization of your report are important.\n50% of the grade is based on the quality of the code. The code should be well-documented and easy to read.\nIt is entirely fine to use a LLM-assistant to improve the quality of your writing and code, as well as for brainstorming ideas.\n\nAll sources (github repositories, papers, blog posts, etc.) should be properly cited. Failure to do so will be considered plagiarism. Although it is fine (and even encouraged) to discuss the assignments with other students/groups, the report and the code should be entirely your own work. You are encouraged to research extensively, experiment with different approaches, and explore creative ideas. A deep understanding of the methods used and the ability to explain them clearly is crucial. Do not just copy code from the internet without understanding it. This project is open-ended, and you are expected to explore the problem in depth.\nAs a last remark, the assignment is designed to be challenging. Do not be discouraged if you find it difficult. The goal is to learn and to improve your skills. If you are stuck, do not hesitate to ask for help and reach out to me. Furthermore, it may be a good idea to build a proper git repository to keep track of your progress and to showcase your work when applying for internships or jobs. There is a lot of room for creativity in this project and exploring new ideas is encouraged.\nDeadlines:\n\nFriday 28 March 2025: I am giving you the opportunity to submit a draft of your report; you can add a section with the title “Questions” where you can ask me specific questions about the project. I will provide feedback on the draft, and you can use this feedback to improve your final submission. The draft should include the first \\(4\\) sections of the report highlighted above. The draft should be submitted on canvas. Note that this draft is optional, but I highly recommend that you submit it. This draft will not be graded: its only purpose is to provide you with feedback and improve your final submission.\n\nFriday 25 April 2025: Final submission of the report and the code. The report should be submitted on canvas. The code can be submitted as a zip file (not exceeding 10 MB) or as a link to a public repository (e.g., github). The report should be in PDF format and should not include the code. The report should be at most 10 pages long, including figures and references. It is entirely possible to obtain a full grade with a report that is significantly shorter than 10 pages: do not write a longer report just to meet the page limit. Do include the name and student number of all group members in the report."
  },
  {
    "objectID": "people/index_people.html#postdoc-phd-msc-co-supervision",
    "href": "people/index_people.html#postdoc-phd-msc-co-supervision",
    "title": "Research team",
    "section": "",
    "text": "Ling Min Hao PhD (2023-present): Machine Learning methods for Uncertainty Quantification alternative splicing RNA data – co-supervised by Jonathan Göke (GIS)\nZhidi Lin Research Fellow (2024-present): High-Dimensional models time-series model and data-assimilation.\nHai Dang Dau Research Fellow (2024-present): High-Dimensional models time-series model and data-assimilation, diffusion models\nHangao Liang MSc (2024-present): Monte-Carlo methods with path-space methods."
  },
  {
    "objectID": "teaching/DSS5104/SAW.html",
    "href": "teaching/DSS5104/SAW.html",
    "title": "Pushing the Limits of Linear Regression",
    "section": "",
    "text": "Assignment: Pushing the Limits of Linear Regression\n\n\nOverview\nThe goal of this assignment is to explore how far one can push the predictive performance of linear regression through creative and rigorous feature engineering, applied to the task of predicting house prices. While modern machine learning tends to emphasize complex models like gradient boosting or neural networks, this assignment intentionally restricts you to linear regression—not to limit you, but to challenge your ability to understand data deeply, model relationships intelligently, and build interpretable models that are robust and insightful.\nYou will work with the dataset house_dataset.csv, which contains various attributes of houses and their sale prices. Your task is to construct a linear regression model but you are free—and encouraged—to apply any form of feature engineering that enhances the model’s expressiveness while preserving its linear form.\n\n\n\nModeling Constraint\nYou are limited to using linear regression as your modeling technique. However, this includes: - Ordinary least squares (OLS) - Regularized linear models (e.g., ridge or lasso, if properly justified) - Models built after transformation of inputs or the target (see below)\n\n\n\nCreative Freedom in Feature Engineering\nYou are free to be as creative as possible in transforming the features and/or the prediction target, provided that the final model remains linear. Possibilities include, but are not limited to:\n\nTransformations of features\nApply log, square root, polynomial terms, splines (e.g., piecewise linear or cubic), binning, or custom thresholds to expose non-linear patterns while retaining a linear modeling structure.\nInteractions\nBetween numerical variables, categorical variables, or combinations thereof\n\nHandling categorical data\nUse one-hot encoding, group rare categories together, or carefully apply target encoding (e.g., average price by category) while preventing data leakage, exploit location, etc..\nComposite features\nEngineer meaningful domain-informed variables like price per square foot, total number of bathrooms, room density, age of the house at time of sale, or binary indicators of luxury features\nDistance-based and neighborhood features Use K-nearest neighbors (KNN) to build local or similarity-based features such as the mean price of the k most similar houses (based on selected features)\nDimensionality reduction\nPCA or clustering-based grouping, if helpful\nTransforming the target\nPredicting log(price), price per square foot, or even price normalized by neighborhood average—all are valid, as long as your final predictions can be mapped back to actual prices for evaluation\n\nYour feature engineering should be guided by both statistical intuition and exploratory data analysis. Interpretability, parsimony, and understanding of the data should drive your decisions, not just error minimization.\n\n\n\nEvaluation Metric\nYour model will be evaluated using the Mean Absolute Percentage Error (MAPE):\n[ = _{i=1}^{n} | | ]\nWhere: - ( y_i ) is the actual sale price of the ( i )-th house - ( _i ) is your model’s predicted price - ( n ) is the number of observations\nThis metric captures the average relative error, which is especially appropriate when prices vary widely across the dataset. You may choose to model log(price) or other transformed targets internally, but your final predictions must be mapped back to raw price values for evaluation.\n\n\n\nTrain/Test Split and Validation Strategy\nYou are responsible for creating your own train/test split from the provided dataset. This split should simulate a real-world scenario where a model is trained on historical data and evaluated on unseen data. The test set must be held out entirely during the feature engineering, model selection, and training phases—it should be used only once, at the end, to evaluate your final model.\nThroughout the model development process, you should rely on cross-validation (e.g., k-fold or repeated k-fold) to assess model performance and guide feature selection. This helps ensure that your choices generalize beyond the specific training data and are not overfit to noise.\nBe especially mindful of data leakage. This includes: - Leaking information from the test set into training (e.g., through target encoding applied globally) - Creating features that implicitly use future or aggregate information not available at prediction time - Applying data transformations (e.g., scaling, imputing, encoding) using the entire dataset instead of fitting only on the training portion during cross-validation\n\n\n\nBenchmarking Against a Non-Linear Model\nTo contextualize the performance of your linear model, you should also build a well-tuned XGBoost model as a benchmark. This comparison is not meant to “beat” XGBoost, but rather to understand how close a thoughtfully engineered linear model can get to a strong non-linear model in terms of predictive accuracy.\nThe comparison should be done on the same held-out test set, using the same evaluation metric (MAPE). Briefly report the performance of XGBoost alongside your linear model, and reflect on the trade-offs: interpretability vs. accuracy, complexity vs. insight, and any differences in the types of features each model appears to exploit.\nThis benchmark will help you critically evaluate the value of your linear modeling choices and deepen your understanding of when and why simple models can perform competitively.\n\n\n\nDeliverables\nYou must submit the following:\n\nA written report in PDF format\n\nMaximum length: 10 pages (shorter reports are welcomed if concise and clear)\n\nThe report should contain no code, but should clearly explain your approach, methodology, key decisions, model diagnostics, and findings.\n\nInclude relevant plots or tables where helpful\n\nYour code\n\nSubmit either as a zip file containing all notebooks, scripts, and necessary resources\n\nOr provide a link to a GitHub repository\n\nYour code should be well-organized, reproducible, and allow easy verification of the results discussed in the report.\n\n\n\n\n\nWhy This Matters in Practice\nIn many real-world applications, a well-engineered linear model with thoughtful, domain-driven feature engineering is not just a pedagogical exercise—it is a preferred modeling approach in industry. This is especially true when interpretability, transparency, and robustness are essential.\nFor example: - In real estate, analysts and stakeholders often want to understand how specific features (like location, size, or renovations) affect price, not just get a black-box prediction. - In finance, risk models must often be auditable and explainable for regulatory compliance—making interpretable linear models with engineered features a standard. - In healthcare, treatment effect models, cost estimation, or hospital resource forecasting require clarity and justifiability in model behavior. - In public policy or urban planning, decision-makers need models that provide insight into the data, not just accuracy—understanding the role of variables like income, zoning, or infrastructure is critical.\nThis assignment trains you to think like a data scientist who not only builds accurate models but also extracts value and understanding from data—which is often the more important and lasting contribution."
  },
  {
    "objectID": "teaching/DSS5104/creative_linear_regression.html",
    "href": "teaching/DSS5104/creative_linear_regression.html",
    "title": "Pushing the Limits of Linear Regression",
    "section": "",
    "text": "Overview\nThe goal of this assignment is to explore how far one can push the predictive performance of linear regression through creative and rigorous feature engineering, applied to the task of predicting house prices. While modern machine learning tends to emphasize complex models like gradient boosting or neural networks, this assignment intentionally restricts you to linear regression—not to limit you, but to challenge your ability to understand data deeply, model relationships intelligently, and build interpretable models that are robust and insightful.\nYou will work with the dataset house_dataset.csv, which contains various attributes of houses and their sale prices. Your task is to construct a linear regression model but you are free—and encouraged—to apply any form of feature engineering that enhances the model’s expressiveness while preserving its linear form.\n\n\n\nModeling Constraint\nYou are limited to using linear regression as your modeling technique. However, this includes: - Ordinary least squares (OLS) - Regularized linear models (e.g., ridge or lasso, if properly justified) - Models built after transformation of inputs or the target (see below)\n\n\n\nCreative Freedom in Feature Engineering\nYou are free to be as creative as possible in transforming the features and/or the prediction target, provided that the final model remains linear. Possibilities include, but are not limited to:\n\nTransformations of features\nApply log, square root, polynomial terms, splines (e.g., piecewise linear or cubic), binning, or custom thresholds to expose non-linear patterns while retaining a linear modeling structure.\nInteractions\nBetween numerical variables, categorical variables, or combinations thereof\n\nHandling categorical data\nUse one-hot encoding, group rare categories together, or carefully apply target encoding (e.g., average price by category) while preventing data leakage, exploit location, etc..\nComposite features\nEngineer meaningful domain-informed variables like price per square foot, total number of bathrooms, room density, age of the house at time of sale, or binary indicators of luxury features\nDistance-based and neighborhood features Use K-nearest neighbors (KNN) to build local or similarity-based features such as the mean price of the k most similar houses (based on selected features)\nDimensionality reduction\nPCA or clustering-based grouping, if helpful\nTransforming the target\nPredicting log(price), price per square foot, or even price normalized by neighborhood average—all are valid, as long as your final predictions can be mapped back to actual prices for evaluation\n\nYour feature engineering should be guided by both statistical intuition and exploratory data analysis. Interpretability, parsimony, and understanding of the data should drive your decisions, not just error minimization.\n\n\n\nEvaluation Metric\nYour model will be evaluated using the Mean Absolute Percentage Error (MAPE):\n\\[\n\\text{MAPE} = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|\n\\]\nwhere \\(y_i\\) is the actual sale price of the \\(i\\)-th house, \\(\\hat{y}_i\\) is your model’s predicted price and \\(n\\) is the number of observations.\nThis metric captures the average relative error, which is especially appropriate when prices vary widely across the dataset. You may choose to model log(price) or other transformed targets internally, but your final predictions must be mapped back to raw price values for evaluation.\n\n\n\nTrain/Test Split and Validation Strategy\nYou are responsible for creating your own train/test split from the provided dataset. The test set must be held out entirely during the feature engineering, model selection, and training phases—it should be used only once, at the end, to evaluate your final model.\nThroughout the model development process, you should rely on cross-validation (e.g., k-fold or repeated k-fold) to assess model performance and guide feature selection. This helps ensure that your choices generalize beyond the specific training data and are not overfit to noise.\nBe especially mindful of data leakage. This includes:\n\nLeaking information from the test set into training (e.g., through target encoding applied globally)\n\nCreating features that implicitly use future or aggregate information not available at prediction time\n\nApplying data transformations (e.g., scaling, imputing, encoding) using the entire dataset instead of fitting only on the training portion during cross-validation\n\n\n\n\nBenchmarking Against a Non-Linear Model\nTo contextualize the performance of your linear model, you should also build a well-tuned XGBoost model as a benchmark. This comparison is not meant to “beat” XGBoost, but rather to understand how close a thoughtfully engineered linear model can get to a strong non-linear model in terms of predictive accuracy.\nThe comparison should be done on the same held-out test set, using the same evaluation metric (MAPE). Briefly report the performance of XGBoost alongside your linear model, and reflect on the trade-offs: interpretability vs. accuracy, complexity vs. insight, and any differences in the types of features each model appears to exploit.\nThis benchmark will help you critically evaluate the value of your linear modeling choices and deepen your understanding of when and why simple models can perform competitively.\n\n\n\nDeliverables\nYou must submit the following:\n\nA written report in PDF format\n\nMaximum length: 10 pages (shorter reports are welcomed if concise and clear)\n\nThe report should contain no code, but should clearly explain your approach, methodology, key decisions, model diagnostics, and findings.\n\nInclude relevant plots or tables where helpful\n\nYour code\n\nSubmit either as a zip file containing all notebooks, scripts, and necessary resources\n\nOr provide a link to a GitHub repository\n\nYour code should be well-organized, reproducible, and allow easy verification of the results discussed in the report.\n\n\n\n\n\nWhy This Matters in Practice\nIn many real-world applications, a well-engineered linear model with thoughtful, domain-driven feature engineering is not just a pedagogical exercise—it is a preferred modeling approach in industry. This is especially true when interpretability, transparency, and robustness are essential.\nFor example:\n\nIn real estate, analysts and stakeholders often want to understand how specific features (like location, size, or renovations) affect price, not just get a black-box prediction.\n\nIn finance, risk models must often be auditable and explainable for regulatory compliance—making interpretable linear models with engineered features a standard.\n\nIn healthcare, treatment effect models, cost estimation, or hospital resource forecasting require clarity and justifiability in model behavior.\n\nIn public policy or urban planning, decision-makers need models that provide insight into the data, not just accuracy—understanding the role of variables like income, zoning, or infrastructure is critical.\n\nThis assignment trains you to think like a data scientist who not only builds accurate models but also extracts value and understanding from data—which is often the more important and lasting contribution."
  },
  {
    "objectID": "teaching/DSS5104/DL-for-tabular-data.html",
    "href": "teaching/DSS5104/DL-for-tabular-data.html",
    "title": "Deep Learning for Tabular Data",
    "section": "",
    "text": "Overview\nThe goal of this assignment is to critically explore and evaluate recent deep learning methods for tabular data prediction. While deep learning has achieved dramatic success in domains like vision and language, tabular data, often structured, heterogeneous, and small-to-medium in size, has long remained a domain dominated by tree-based models such as XGBoost. However, in recent years, new deep learning architectures specifically tailored for tabular data have emerged, including TabNet, NODE, and FT-Transformer and a host of others. These models leverage the strengths of deep learning, such as representation learning and attention mechanisms, to tackle tabular prediction tasks.\nIn this assignment, you will evaluate the performance, robustness, and interpretability of a selection of these methods using publicly available implementations on a variety of non-trivial tabular prediction tasks. You are not expected to re-implement these models from scratch, but you are expected to understand how to use them properly and interpret their behavior.\n\n\n\nAssignment Objectives\n\nUnderstand and implement recent deep learning methods for tabular data\nCritically assess their performance across multiple datasets\nCompare their results against classical baselines like gradient boosting and random forests.\nAnalyze the strengths, weaknesses, and practical challenges of deep learning models in the tabular setting\nReflect on when and why these methods may be preferable, or not, to classical approaches\n\n\n\n\nDatasets and Prediction Tasks\nYou must choose at least five datasets for tabular regression and/or classification. These should be:\n\nNon-trivial in size, dimensionality, or complexity\nFrom diverse domains (e.g., finance, healthcare, retail, education, etc.)\nPublicly available\n\n\n\n\nKey Questions to Address\nIn your report, aim to answer:\n\nPerformance: How well do deep learning models perform relative to classical baselines? Are there specific datasets or characteristics where they shine or fail?\nScalability: How do training times and resource demands compare?\nSensitivity: How sensitive are results to hyperparameters, preprocessing, or random seed?\nGeneralization: Do deep models overfit more easily on small tabular datasets?\nPractical Utility: Would you recommend using these models in practice? Why or why not?\n\n\n\n\nDeliverables\n\nA written report in PDF format\n\nYour report should be at most 10 pages long, although a much shorter report is possible (and preferable if you are concise).\nNo code should be included in the report.\n\nYour code\n\nEither as a zip file or link to a GitHub repository\n\nClearly organized notebooks/scripts for:\n\nData preparation\nModel training/evaluation\nMetric reporting\nVisualizations and interpretability analysis\n\n\nMust be reproducible\n\n\n\n\n\nFinal Thoughts\nThis assignment encourages you to step beyond traditional modeling and engage with the frontier of machine learning for structured data. While deep learning for tabular data remains a debated and fast-evolving space, it is essential for practitioners to develop the critical skill of evaluating tools, not just using them.\nYou will gain hands-on experience that helps answer:\n\nAre these models ready for production use?\nWhen are they worth the added complexity?\nWhat types of problems benefit from deep tabular models?\n\nBy the end, you should be able to advocate for or against these methods with informed reasoning, grounded in your own experiments."
  },
  {
    "objectID": "teaching/DSS5104/molecular-prediction.html",
    "href": "teaching/DSS5104/molecular-prediction.html",
    "title": "Molecular Properties Prediction",
    "section": "",
    "text": "Overview\nThe goal of this assignment is to explore the use of deep learning, and in particular Graph Neural Networks (GNNs), for molecular property prediction. While traditional cheminformatics has relied on handcrafted features like molecular fingerprints, recent advances in graph-based learning allow us to learn directly from the molecular graph structure. This assignment challenges you to understand both paradigms, classical feature engineering and deep graph representation learning, and critically compare their strengths and limitations.\nYou will work with the Tox21 dataset , which consists of thousands of molecules labeled for activity against 12 different toxicity-related biological targets. Your task is to build and evaluate both traditional machine learning models and modern GNN-based approaches to predict molecular activity.\n\n\n\nDataset: Tox21\nThe Tox21 dataset (available on CANVAS) is a benchmark in computational toxicology. It includes:\n\nCompounds represented as SMILES strings\n12 binary classification tasks (e.g., activation of nuclear receptors or stress pathways)\n~8,000 molecules for training/testing\nMulti-label setup (a molecule can be active in multiple assays)\n\nThe dataset has been preprocessed and split into training, validation, and test sets.\n\n\n\nMolecular Representations\n\nSMILES (Simplified Molecular-Input Line-Entry System)\nSMILES is a textual representation of chemical structures, where atoms and bonds are encoded as ASCII strings (e.g., CC(=O)Oc1ccccc1C(=O)O for aspirin). While convenient for storage and parsing, SMILES must be converted to structured formats (e.g., molecular graphs) to be usable for learning, or features need to be extracted for classical ML.\n\n\nFrom SMILES to Graphs\nYou can use cheminformatics libraries like RDKit to convert a SMILES string into a molecular graph, where:\n\nNodes = atoms (with features like element, degree, charge)\nEdges = bonds (with features like bond type)\n\n\n\n\n\nModeling with GNNs\nGNNs operate directly on molecular graphs by passing messages between atoms, aggregating neighborhood information, and learning rich, hierarchical molecular representations. In this assignment, you may explore architectures such as:\n\nGraph Convolutional Networks (GCNs)\nMessage Passing Neural Networks (MPNNs)\nAttentiveFP or D-MPNN (ChemProp)\n\nYou do not have to analyse all of these architectures: choose one or two that you find interesting and implement them. These models typically include:\n\nAtom/bond featurization\nMessage passing layers\nA readout/pooling step to produce a fixed-size molecular embedding\nA final MLP for classification\n\nYou may use any high-level libraries of your choice to implement GNNs. You can also use existing implementations of the models you choose. The purpose of this assignment is not to implement the models from scratch, but to understand how GNNs work, how to train them and leverage them for molecular property prediction.\n\n\n\nAssignment Tasks\n\nPart 1: Classical ML Baseline with Molecular Fingerprints\n\nUse RDKit to compute ECFP4 (Extended Connectivity Fingerprints) for all molecules: these are features that characterize the local environment of each atom in the molecule. ECFP4 is a widely used fingerprint for molecular similarity and property prediction. You are free to also explore other types of fingerprints/features (e.g., MACCS keys, Morgan fingerprints) if you wish. For this, do not hesitate to read the literature and/or brainstorm with a LLM-assistant.\nTrain Random Forest or Gradient Boosting classifiers (e.g., XGBoost) using these fingerprints.\nReport performance on all 12 targets.\n\nThis will serve as a baseline for comparison with GNNs. You may also explore other classical models (e.g., SVM, logistic regression) or additional features (e.g., molecular descriptors) if you wish.\n\n\nPart 2: Graph Neural Network Modeling\n\nConvert each SMILES string into a molecular graph.\nBuild a GNN model using any of the recommended architectures.\nTrain and evaluate on the same prediction tasks and metrics.\n\nIf time permits:\n\ncombine GNNs with some of the classical features you have explored in Part 1.\ncompare to RNNs (eg. LSTM, biLSTM, 1D-CNN) that operate on the SMILES strings directly.\n\n\n\n\n\nEvaluation Metric\nUse Area Under the ROC Curve (AUC ROC) for each of the 12 targets, and report: - Individual AUC scores - Mean and median AUC across tasks\n\n\n\nDeliverables\nYou must submit the following:\n\nA written report (PDF)\n\nA report of at most 10 pages; shorter is better if concise\nClearly describe your methodology, experiments, results, and findings\n\nInclude key plots or tables where relevant (e.g., ROC curves, performance comparisons)\nDo not include code in the report\n\nCode\n\nSubmit as a GitHub repo or zip file\n\nShould include all scripts/notebooks for data loading, feature generation, model training, and evaluation\n\nMake your code clean and reproducible\n\n\n\n\n\nWhy This Matters in Practice\nDrug discovery increasingly depends on accurate, data-driven prediction of molecular properties. While traditional cheminformatics models still play a role, deep learning, especially GNNs, offers a powerful way to move beyond hand-designed descriptors."
  },
  {
    "objectID": "teaching/DSS5104/open_topic.html",
    "href": "teaching/DSS5104/open_topic.html",
    "title": "Open Topic",
    "section": "",
    "text": "Open-Ended Project Option\nYou may propose a project topic of your choice. Your proposal should:\n\nBriefly describe the topic and why it is interesting or relevant (e.g., personal interest, career goals, research plans).\nIf possible, specify a dataset you plan to use.\nOutline what techniques or methods you aim to explore or learn.\n\nThis is an opportunity to:\n\nLearn and study new models or approaches not covered in the lectures,\nExplore applications and methods relevant to your particular interests or future work,\n\nIf you choose this option, please discuss your proposal with me to ensure it is suitable for the assignment. For this purpose, upload your PDF proposal on CANVAS."
  },
  {
    "objectID": "teaching/DSS5104/DL-for-time-series.html",
    "href": "teaching/DSS5104/DL-for-time-series.html",
    "title": "Deep Learning for Time-Series Forecasting",
    "section": "",
    "text": "Overview\nThe goal of this assignment is to critically explore and evaluate recent deep learning methods for time-series prediction. While classical models like ARIMA, Prophet, and gradient boosting have long been standard for temporal forecasting, deep learning architectures, particularly those designed for sequential modeling, Deep Learning models have shown increasing promise, althoug it is still not clear if they are the best choice for all time-series problems.\nIn this assignment, you will evaluate the performance, scalability, and robustness of a selection of modern deep learning methods using publicly available implementations on a variety of real-world time-series forecasting tasks. You are not expected to implement models from scratch but must demonstrate an understanding of their proper application.\n\n\n\nAssignment Objectives\n\nUnderstand and apply recent deep learning models for time-series forecasting\nBenchmark their performance on multiple datasets\nCompare them against classical baselines that do not use deep learning\nAnalyze the advantages, limitations, and practical considerations of deep models in time-series settings\nReflect on when and why deep models outperform traditional methods—or don’t\n\n\n\n\nDatasets and Prediction Tasks\nYou must choose at least five time-series datasets. These should:\n\nInvolve forecasting over a meaningful horizon (multi-step if possible)\nCome from diverse domains (e.g., energy, finance, climate, retail, transportation)\nBe publicly available and reasonably complex\n\n\n\n\nKey Questions to Address\n\nPerformance: How do deep time-series models compare to classical models in accuracy? On which kinds of data do they excel or struggle? Are they really better than classical models?\nScalability: How do they perform in terms of training time, memory usage, etc…?\nRobustness: How sensitive are models to data volume, missing values, input noise, and hyperparameters?\nGeneralization: Do deep models overfit, especially on smaller datasets or with limited history?\nPracticality: Would you recommend deep models in real-world time-series applications? Under what conditions?\n\n\n\n\nDeliverables\n\nWritten Report (PDF)\n\nMax 10 pages (shorter is welcome if well-written and to the point)\n\nFocus on insights, comparisons, and analysis: no code in the report\n\nCode Submission\n\nOrganized scripts or notebooks (via GitHub or zip) for:\n\nData preprocessing and transformation\nModel training, evaluation, and tuning\nMetrics reporting and visualizations\n\n\nCode must be clear, well-documented, and reproducible\n\n\n\n\n\nFinal Thoughts\nThis assignment is your opportunity to engage with the cutting edge of time-series forecasting using deep learning. These models bring new capabilities but also new challenges. Through hands-on experimentation and thoughtful evaluation, you’ll build the ability to choose, justify, and defend modeling decisions in time-series contexts.\nBy the end, you should be able to answer:\n\nAre deep models worth the overhead in time-series problems? It is fine if you find that they are not!\nWhen are they most beneficial?\nWhat trade-offs do they introduce compared to classical approaches?"
  },
  {
    "objectID": "teaching/DSS5104/topics_list.html",
    "href": "teaching/DSS5104/topics_list.html",
    "title": "DSS5104 Assignment Topics",
    "section": "",
    "text": "Here is a list of possible topics for your assignment.\n\nPushing the limit of Linear Regression\nMolecular Properties Prediction\nDeep Learning for Time-Series Forecasting\nDeep Learning for Tabular Data\nOpen Topic\n\nAssignment group: You are reminded that assignments are to be done in groups of at most 5 students: each student in a group will be receiving the same mark. In all your reports, do include the list of all students, as well as their student numbers. Submission of the assignment is to be done via CANVAS.\nLate submissions will incur a penalty of \\(10\\%\\) per day late: if you original mark is, say, \\(81/100\\), and you are \\(6\\) days late, your mark will be \\(81 \\times 0.9^6 = 43\\)."
  },
  {
    "objectID": "notes/SAW/SAW.html",
    "href": "notes/SAW/SAW.html",
    "title": "Self Avoiding Walks",
    "section": "",
    "text": "A 2D self-avoiding walk\n\n\n\nThese notes present comments on the “Self-avoiding walks” assignment given to the “ST3247: Simulations” class. Most of the drafts that have been submitted so far describe variations of importance sampling. The purpose of these notes is to suggest directions for slightly more advanced Monte Carlo methods that can be used to estimate the connective constant \\(\\mu\\) of self-avoiding walks. These are only pointers and suggestions.\n\nThe problems and notations\nRecall that we are trying to estimate the connective constant \\(\\mu\\) of self-avoiding walks (SAW) in the 2D lattice \\(\\mathbb{Z}^2\\). If \\(c_L\\) denotes the number of SAWs of length \\(L\\), we have the following asymptotic behavior:\n\\[\nc_L \\; \\sim \\; A \\, \\mu^L \\, L^{\\gamma}\n\\]\nfor some unknown constants \\(A\\), \\(\\mu\\), and \\(\\gamma\\). The main objective of the assignment is to estimate \\(\\mu\\), which can also be expressed as the limit of \\(c_L^{1/L}\\) as \\(L \\to \\infty\\). As of today, the best known estimate is \\(\\mu \\approx 2.638158533032790(3)\\), which required several tens of thousand hours of CPU time to compute. To estimate \\(\\mu\\), one must approximate the number of SAWs of length \\(L\\) starting at the origin for large values of \\(L\\) if one hopes to get a good estimate.\nConsider a sequence \\(z_{0:L} = (z_0, z_1, \\dots, z_L)\\) of \\(L+1\\) distinct vertices in \\(\\mathbb{Z}^2\\) with \\(z_0 = (0,0)\\) and \\(\\|z_{k+1} - z_k\\|=1\\) for all \\(0 \\leq k \\leq L-1\\), i.e., a walk of length \\(L\\). For notational convenience, let us introduce the function \\(\\varphi^{\\textrm{walk}}(z_{:L})\\) that returns one if \\(z_{0:L}\\) is a correct walk of length \\(L\\), and zero otherwise. In particular, this function returns zero if two consecutive vertices are the same, or if the walk does not start at zero. Similarly, introduce the function \\(\\varphi^{\\textrm{SAW}}(z_{:L})\\) that returns one if \\(z_{0:L}\\) is a SAW of length \\(L\\). One can define two important probability mass functions:\n\\[\np^{\\textrm{walk}}_{L}(z_{0:L}) = \\frac{\\varphi_L^{\\textrm{walk}}(z_{0:L})}{4^L}\n\\qquad \\textrm{and} \\qquad\np^{\\textrm{SAW}}_{L}(z_{0:L}) = \\frac{\\varphi_L^{\\textrm{SAW}}(z_{0:L})}{c_L}.\n\\]\nThey describe the uniform distributions on all the walks of length \\(L\\) and all the SAWs of length \\(L\\), respectively.\n\n\nImportance sampling\nOne can approximate \\(c_L\\) with naive Monte Carlo by estimating the proportion \\(p_L\\) of walks that are SAWs,\n\\[\np_L = \\mathbb{E}_{p^{\\textrm{walk}}_{L}} \\left[ \\varphi_L^{\\textrm{SAW}}(z_{0:L}) \\right]\n=\n\\frac{1}{4^L} \\sum_{z_{0:L}} \\varphi_L^{\\textrm{SAW}}(z_{0:L}).\n\\]\nThis is an absolute disaster since the proportion of SAWs among all walks is extremely small. One can do significantly better using importance sampling. For this, consider a proposal distribution that starts at the origin and continues by choosing uniformly among the four neighbors of the last vertex that have not been visited yet. If there are no unvisited neighbors, the walk continues by standing still until length \\(L\\) is reached: the resulting path is not even a valid walk, so \\(p^{\\textrm{walk}}_{L}(z_{0:L}) = 0\\) as well as \\(p^{\\textrm{SAW}}_{L}(z_{0:L}) = 0\\). The probability mass function of the proposal distribution is easy to compute, so estimating \\(p_L\\) with importance sampling is straightforward. This is usually called the Rosenbluth method (Rosenbluth and Rosenbluth 1955). [Note to students: make it much clearer in your report that the Rosenbluth method is just importance sampling. Do note that even the “rejected” walks have to be taken into account!]\n\n\n\n\nImportance Sampling (Rosenbluth method)\n\n\n\nAs one can see, the quality quickly deteriorates as \\(L\\) increases. This is because the number of accepted walks is very small, and, among them, the importance weights are highly unequal.\n[Note to students: you should explain this much more clearly, and possibly explore this more quantitatively. The reason it is failing is not only that the number of accepted walks is small]\n\n\nRecursive formulation\nWe have just seen that importance sampling will not be able to estimate \\(c_L\\) for large values of \\(L\\). This makes accurate estimates of \\(\\mu\\) difficult to obtain this way.\nTo make progress, one can exploit the recursive structure of the problem. Let us define the concatenation of two walks. Given a first walk \\(z^{(A)}_{0:L_A}\\) and a second walk \\(z^{(B)}_{0:L_B}\\), one can define a new walk of length \\(L_A + L_B\\) by starting at the origin, following the \\(L_A\\) increments of the first walk, then the \\(L_B\\) increments of the second. The concatenation of two SAWs is not always a SAW. However, it is not hard to prove the following. Define \\(B(L_A, L_B) \\in (0,1)\\) as the probability that, when sampling SAWs \\(z^{(A)}_{0:L_A}\\) and \\(z^{(B)}_{0:L_B}\\) independently and uniformly at random, their concatenation is still a SAW. Then:\n\\[\nB(L_A, L_B) \\; = \\; \\frac{c_{L_A + L_B}}{c_{L_A} \\, c_{L_B}}.\n\\]\n[Note to students: it is OK for you to use this fact. It’s even better if you can prove it, but not absolutely necessary.]\nAssuming one can generate SAWs of length \\(L\\) uniformly at random ( a problem that will be discussed later), we can estimate \\(\\mu\\) in several ways:\n\nFor small values of \\(L_1\\), the number of SAWs \\(c_{L_1}\\) is known exactly (e.g., \\(c_1 = 4\\), \\(c_{10} = 44100\\)). Suppose one can generate SAWs of length \\(L_2 \\gg 1\\). One can then estimate \\(B(L_1, L_2)\\) empirically. Since \\(c_L \\; \\sim \\; A \\, \\mu^L \\, L^{\\gamma}\\), it follows that, for \\(L_1\\) fixed and \\(L_2 \\to \\infty\\), \\[\n\\frac{c_{L_1+L_2}}{c_{L_2}} \\approx \\mu^{L_1}.\n\\] Using the fact that \\(B(L_1, L_2) = c_{L_1+L_2} / (c_{L_2} c_{L_1})\\), one can then estimate \\(\\mu\\) from the estimate of \\(B(L_1, L_2)\\).\nAlternatively, one can estimate \\(c_L\\) for large \\(L\\) recursively. For example, starting from \\(c_{10} = 44100\\), estimate \\(B(10,10)\\) to compute \\(c_{20}\\), then use \\(B(20,20)\\) to compute \\(c_{40}\\), and so on. Using this method and about \\(5\\) hours of CPU time (see below for details) with \\(10,000\\) SAWs of lengths \\(10, 20, \\dots, 2560\\), I obtained \\(\\mu \\approx 2.643\\).\n\n\n\nGenerating SAWs\nThe previous discussion shows that, once we know how to generate uniform SAWs, we can estimate \\(\\mu\\) relatively easily. One of the most common methods is the pivot algorithm: see here for a nice visualization. The principle is simple: given a SAW, randomly select a pivot site and apply a symmetry operation (like rotation or reflection) to one part of the walk. If the resulting walk remains self-avoiding, accept it; otherwise, reject it. Repeating this process generates diverse, approximately uniform SAWs.\n[Note to students: explain this much more clearly if you decide to use it]\nIn short, the pivot algorithm updates a SAW by applying a symmetry operation to a subpath. Given a SAW \\(z_{0:L}\\), one can obtain another SAW by applying to it the pivot algorithm a (large) number of times. To obtain a nearly independent SAW of length \\(L\\) starting from \\(z_{0:L}\\), one typically need to apply about \\(L\\) pivot steps. While it can be slow for large \\(L\\), it is far more efficient than naive importance sampling.\n[Note to students: efficiently implementing the pivot algorithm is non-trivial, but LLM assistants can help a lot, and are actually quite useful for code optimization]\n\n\nSequential Monte Carlo\nTo estimate \\(c_L\\) for large \\(L\\), one can use Sequential Monte Carlo (SMC). The idea is to grow a population of \\(N\\) SAWs in parallel and estimate \\(c_L\\) by recursively estimating the ratios \\(c_{L+1}/c_L\\). Suppose you have \\(N\\) SAWs of length \\(L\\). Try to extend each SAW by choosing a neighbor of the last vertex that has not been visited yet. This is a form of importance sampling, giving \\(N\\) new walks of length \\(L+1\\) with associated weights (some of them being non-valid walks!). Then, resample \\(N\\) times from this weighted set to get \\(N\\) new SAWs of length \\(L+1\\) (with possible duplicates). Apply the pivot algorithm to eliminate these duplicates and generate more diverse SAWs.\n[Note to students: if you decide to use SMC, explain it much more clearly. It’s not entirely straightforward to understand or implement, but it is one of the most powerful and versatile Monte Carlo methods to this day. A good investment of your time if you decide to understand SMC]\n\n\n\n\nSequential Monte Carlo\n\n\n\n\n\nImproving the estimation of \\(\\mu\\)\nSuppose you have estimates of \\((\\log c_L)/L\\) for various \\(L\\). Since\n\\[\n\\frac{\\log c_L}{L} \\approx \\log A \\cdot \\frac{1}{L} + \\log \\mu + \\gamma \\cdot \\frac{\\log L}{L},\n\\]\nyou can fit a linear regression to estimate \\(\\log A\\), \\(\\log \\mu\\), and \\(\\gamma\\). I tried this approach using a naive and non-optimized SMC implementation with \\(N=1000\\) and \\(L=1000\\), running for 10 hours on a free (and bad) online CPU, and obtained \\(\\mu \\approx 2.6366\\).\n[Note to students: can you do much better?]\n\n\nRunning long simulations\nThe best known estimate of \\(\\mu\\) required several tens of thousands of CPU hours. While writing these notes, I was able to run simulations easily and for free using deepNote: it was my first time using it, and it was very user friendly. This allowed me to run simulations for 8 hours on a (free but slow) CPU without issue. Launch simulations in the evening and let them run overnight. [Note to students: for the more motivated ones, you can try writing GPU-friendly code to run simulations, possibly on Google Colab]\n\n\n\n\n\nReferences\n\nRosenbluth, Marshall N, and Arianna W Rosenbluth. 1955. “Monte Carlo Calculation of the Average Extension of Molecular Chains.” The Journal of Chemical Physics 23 (2). American Institute of Physics: 356–59."
  },
  {
    "objectID": "notes/sparse_GP/sparse_gp.html",
    "href": "notes/sparse_GP/sparse_gp.html",
    "title": "Sparse GP",
    "section": "",
    "text": "These notes are mainly for my own reference; I’m pretty clueless about GPs at the moment, and that needs to change. Read at your own risk; typos and mistakes are likely.\n\n\nVideo\n\n\nLet \\(f(\\cdot) \\sim \\mathrm{GP}(m, K)\\) denote a Gaussian Process (GP) prior with zero mean \\(m=0\\) and covariance kernel \\(K\\). Assume we observe \\(n \\gg 1\\) noisy measurements \\(y = (y_i){i=1}^n\\) of \\(f_i = f(x_i)\\) at input locations \\(x = (x_i){i=1}^n\\). The goal is to compute the posterior of \\(f = (f_i)_{i=1}^n\\) and to infer GP hyperparameters. The main challenge with GP models is the cubic complexity of the matrix inversion required to many of the posterior computations.\nSparse GPs are a class of approaches that aim to reduce this complexity by approximating the full GP posterior with a smaller set of so-called inducing variables \\(u=(u_1, \\ldots, u_m)\\) that entirely describe an approximate posterior distribution. Consider \\(m \\ll n\\) locations \\(z=(z_i)_{i=1}^m\\) called inducing points and set \\(u_i = f(z_i)\\) for the latent function values at the inducing points. The Gaussian random variables \\(u_i\\) can be used as inducing random variable; the choice of inducing points \\(z\\) defines a different set of inducing variables \\(u\\). In this setting, optimizing the inducing variables simply means optimizing the locations of the inducing points \\(z\\). The strategy is to approximate the posterior of \\((u,f)\\) with a tractable distribution \\(q(u,f)\\),\n\\[\np(u,f \\mid y) \\, = \\, \\frac{p(u) \\, p(f \\mid u) \\, p(y \\mid f)}{p(y)} \\; \\approx \\; q(u,f).\n\\]\nLater, we will see that setting \\(u_i = f(z_i)\\) is indeed not the only choice of inducing variables, but let’s keep it to this for now. We have \\(p(u) = N(0,K_u)\\) and \\(p(f \\mid u) = N(\\mu_{f|u}, K_{f|u})\\) where\n\\[\n\\left\\{\n\\begin{align*}\n\\mu_{f|u} &= K_{fu} K_u^{-1} u\\\\\nK_{f|u} &= K_f - K_{fu} K_u^{-1} K_{uf}.\n\\end{align*}\n\\right.\n\\tag{1}\\]\nwhere \\(K_u\\) is the covariance matrix of the inducing variables \\(u\\) and \\(K_{fu}\\) is the covariance matrix between \\(f\\) and \\(u\\). Evaluating \\(p(f \\mid u)\\) involves inverting \\(K_{f|u} \\in \\mathbb{R}^{n,n}\\), which typically scales as \\(\\mathcal{O}(n^3)\\), hence intractable for large \\(n\\). To approximate \\(p(u,f \\mid y)\\) with another distribution \\(q(u,f)\\), one can minimize the Kullback-Leibler divergence \\(D_{\\text{KL}}[q(u,f) \\| p(u,f \\mid y)]\\), i.e.\n\\[\n\\int q(u) \\, q(f|u) \\, \\log  {\\left\\{  \\frac{q(u) \\, q(f|u)}{p(u) \\,  \\textcolor{red}{p(f \\mid u)} \\, p(y \\mid f)}  \\right\\}}  \\, du \\, df \\, + \\, \\log p(y).\n\\]\nIt is not extremely helpful since the intractable term \\( \\textcolor{red}{p(f \\mid u)}\\) is present. However, (Titsias 2009) proposes to set \\(q(f|u) = p(f|u)\\), i.e. to consider an approximate posterior of the form:\n\\[\nq(u,f) = q(u) \\, p(f \\mid u).\n\\]\nNote that the correct posterior distribution is typically not of this form, although when the number of inducing points is large enough, this approximations becomes increasingly accurate. With this class of approximate posterior, the expectation \\(\\mathbb{E}[\\Phi(f) \\mid y]\\) of some functional \\(\\Phi\\) is approximated as \\(\\int \\mathbb{E}[\\Phi(f) \\mid u] \\, q(u) \\, du\\). For example, if \\(q(u) = \\mathcal{N}(\\mu_q, K_q)\\) is a Gaussian variational distribution, the posterior distribution of \\(f_\\star = f(x_\\star)\\) at a new location \\(x_\\star\\) is approximated as \\(K_{\\star,u} \\, K_{u}^{-1} \\mathcal{N}(\\mu_q, K_q) + K_{\\star|u}\\); it is a Gausian with\n\\[\n\\left\\{\n\\begin{align*}\n\\textrm{mean} &= K_{\\star,u} \\, K_{u}^{-1} \\mu_q\\\\\n\\textrm{cov} &= K_{\\star,u} \\, K_{u}^{-1} \\, K_q \\, K_{u}^{-1} K_{u,\\star} + K_{\\star|u}\n\\end{align*}\n\\right.\n\\]\nOptimizing the inducing variables is equivalent to minimizing the free energy quantity\n\\[\n\\mathcal{F}\\; \\equiv \\; \\int q(u) \\, p(f|u) \\, \\log  {\\left\\{  \\frac{q(u)}{p(u) \\, p(y \\mid f)}  \\right\\}}  \\, du \\, df,\n\\tag{2}\\]\nover the variational distribution \\(q(u)\\) and choice of inducing variables. For a fixed set of inducing variables (eg. set of inducing points), it is clear that the optimal variational distribution is given by\n\\[\n\\begin{align*}\nq_{\\star}(u)\n&= p(u) \\exp {\\left\\{  \\int p(f|u) \\, \\log  {\\left( p(y \\mid f) \\right)}  \\, df  \\right\\}}  / \\mathcal{Z}\\\\\n&= p(u) \\exp {\\left\\{  \\mathbb{E}[\\log p(y \\mid f) \\mid u]  \\right\\}}  / \\mathcal{Z}\n\\end{align*}\n\\tag{3}\\]\nfor some normalization constant \\(\\mathcal{Z}= \\int p(u) \\exp {\\left\\{  \\mathbb{E}[\\log p(y \\mid f) \\mid u]  \\right\\}}  \\, du\\); this can be seen by expressing Equation 2 as KL divergence, as similarly done for example when deriving the Coordinate Ascent Variational Inference (CAVI) method,\n\\[\n\\mathcal{F}= D_{\\text{KL}}[q(u) \\mid q_{\\star}(u)] \\, - \\, \\log \\mathcal{Z}.\n\\tag{4}\\]\nEquation 3 shows that \\(q_{\\star}(u)\\) is the prior \\(p(u)\\) weighted by a term that is large when the observations \\(y\\) are likely given \\(u\\), i.e. when \\(\\mathbb{E}[\\log p(y \\mid f) \\mid u]\\) is large.\n\nNystrom approximation\nBefore describing the simple and most important case of additive Gaussian noise, let’s give a brief reminder on the Nystrom approximation. The distribution of \\(f \\mid u\\) is Gaussian with mean \\(\\mu_{f|u} = K_{fu} K_u^{-1} u\\) and covariance \\(K_{f|u}\\). This means that \\(K_{fu} K_u^{-1} u + \\mathcal{N}(0, K_{f|u})\\) is distributed as the unconditional distribution \\(f \\sim \\mathcal{N}(0, K_f)\\). In particular:\n\\[\n\\begin{align*}\nK_f\n&= K_{fu} K_u^{-1} K_u K_u^{-1} K_{uf} + K_{f|u} \\\\\n&=  \\textcolor{red}{K_{fu} K_u^{-1} K_{uf}} + K_{f|u} \\\\\n&=  \\textcolor{blue}{\\widehat{K}^{u}_{f}} + K_{f|u}\n\\end{align*}\n\\]\nwhere \\( \\textcolor{blue}{\\widehat{K}^{u}_{f}} \\equiv K_{fu} K_u^{-1} K_{uf}\\) is the so-called Nystrom approximation of the covariance matrix \\(K_f\\) based on the inducing variable \\(u\\). This shows that the Nystrom approximation \\(\\widehat{K}^{u}_{f}\\) simply consists in ignoring the conditional variance term \\(K_{f|u}\\), and is thus an underestimate of the covariance matrix \\(K_f\\). Furthermore, if \\(u\\) is very informative of \\(f\\), then \\(K_{f|u}\\) is small and the Nystrom approximation is accurate.\n\n\nObservation with additive Gaussian noise\nThe case of additive Gaussian noise is particularly simple. Assume that\n\\[\ny_i = f_i + \\varepsilon_i\n\\qquad \\text{with} \\qquad\n\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n\\]\nwhere the noise terms \\(\\varepsilon_i\\) are independent. Since \\(f|u \\sim \\mathcal{N}(\\mu_{f|u}, K_{f|u})\\), algebra gives that\n\\[\\mathbb{E}[\\log p(y \\mid f) \\mid u] =\n\\log[ \\mathcal{N}(y; \\mu_{f|u}, \\sigma^2 \\, I) ] - \\frac{1}{2 \\sigma^2} \\, \\mathop{\\mathrm{Tr}}( K_{f|u} )\\]\nUsing that \\(\\mu_{f|u} = K_{fu} K_u^{-1} u\\) and the matrix inversion lemma it quickly follows that optimal variational distribution is \\(q_{\\star}(u) = \\mathcal{N}(\\mu_{\\star}, K_{\\star})\\) with\n\\[\n\\left\\{\n\\begin{aligned}\n\\mu_{\\star} &= K_{uf} \\,  {\\left( \\widehat{K}^{u}_{f} + \\sigma^2 I \\right)} ^{-1} \\, y\\\\\nK_{\\star} &= K_u - K_{uf} \\,  {\\left( \\widehat{K}^{u}_{f} + \\sigma^2 I \\right)} ^{-1} \\, K_{fu}.\n\\end{aligned}\n\\right.\n\\]\nIndeed, these are approximations of the exact condition moments,\n\\[\n\\left\\{\n\\begin{aligned}\n\\mu_{u|y} &= K_{uf} \\,  {\\left( K_{f} + \\sigma^2 I \\right)} ^{-1} \\, y\\\\\nK_{u|y} &= K_u - K_{uf} \\,  {\\left( K_{f} + \\sigma^2 I \\right)} ^{-1} \\, K_{fu}.\n\\end{aligned}\n\\right.\n\\]\nwhere the Nystrom approximation \\(\\widehat{K}^{u}_{f} \\approx K_f\\) is used instead. One then finds that \\(\\log \\mathcal{Z}= \\log \\mathcal{N}(y; 0, \\widehat{K}^u_f + \\sigma^2 I) - \\frac{1}{2\\sigma^2} \\mathop{\\mathrm{Tr}}(K_{f|u})\\). With the optimal variational distribution \\(q_\\star(u)\\), Equation 4 gives that the free energy is:\n\\[\n\\mathcal{F}\n= -\\log \\mathcal{N} {\\left( y; 0, \\; \\widehat{K}^{u}_{f} + \\sigma^2 I \\right)}  + \\frac{1}{2\\sigma^2} \\mathop{\\mathrm{Tr}}K_{f|u} \\\\\n\\]\nFurthermore, note that exact likelihood of the observations is \\(p(y) = \\mathcal{N} {\\left( y; 0, \\; K_f + \\sigma^2 I \\right)} \\) so that the free energy can be expressed as\n\\[\n\\mathcal{F}\n\\; = \\;\n-\\log \\widehat{p}^u(y) + \\frac{1}{2\\sigma^2} \\mathop{\\mathrm{Tr}}K_{f|u} \\\\\n\\]\nfor pseudo-likelihood \\(\\widehat{p}^u(y) = \\mathcal{N} {\\left( y; 0, \\; \\widehat{K}^{u}_{f} + \\sigma^2 I \\right)} \\). This shows that \\(D_{\\text{KL}}\\) is given by: \\[\n\\begin{align*}\nD_{\\text{KL}}&[q(u,f) \\mid p(u,f \\mid y)]\n=\n\\mathcal{F}+ \\log p(y) \\\\\n&=\n\\log \\frac{p(y)}{\\widehat{p}^u(y)}\n+\n\\frac{1}{2\\sigma^2} \\mathop{\\mathrm{Tr}}K_{f|u}.\n\\end{align*}\n\\]\nThe term \\(\\mathop{\\mathrm{Tr}}K_{f|u}\\) is just the sum of the conditional variances \\(\\mathop{\\mathrm{Var}} {\\left( f_i | u \\right)} \\) and can be thought of as a regularization term,\n\\[\nR = \\frac{1}{2\\sigma^2} \\mathop{\\mathrm{Tr}}K_{f|u} =\n\\frac12 \\, \\sum_{i=1}^n \\frac{\\mathop{\\mathrm{Var}} {\\left( f_i | u \\right)} }{\\sigma^2}.\n\\]\nAs the number of inducing variables \\(m\\) increases, the pseudo-likelihood becomes more accurate \\(\\widehat{p}^u(y) \\to p(y)\\), the conditional variances \\(\\mathop{\\mathrm{Var}} {\\left( f_i | u \\right)}  \\to 0\\) shrink to zero, and the KL divergence \\(D_{\\text{KL}}[q(u,f) \\mid p(u,f \\mid y)]\\) approaches zero.\nThe animation at the start of this note illustrates the effect of optimizing the location of the inducing points \\(z\\) with a very simple gradient descent. A few experiments show that it is worth being careful with the initial choice of inducing points. Inducing points chosen very far from the data essentially remain fixed during the optimization (ie. the gradient is very small). Initializing with k-means++ clustering of the data points seems to be a robust strategy and give an almost optimal choice of inducing points.\n\n\n\n\n\nReferences\n\nTitsias, Michalis. 2009. “Variational Learning of Inducing Variables in Sparse Gaussian Processes.” In Artificial Intelligence and Statistics, 567–74. PMLR."
  },
  {
    "objectID": "notes/adjoint_method/adjoint.html",
    "href": "notes/adjoint_method/adjoint.html",
    "title": "Adjoint method for sensitivities",
    "section": "",
    "text": "Lev Pontryagin (1908 - 1988)"
  },
  {
    "objectID": "notes/adjoint_method/adjoint.html#table-of-contents",
    "href": "notes/adjoint_method/adjoint.html#table-of-contents",
    "title": "Adjoint method for sensitivities",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nLinear Systems\nAdjoint Method\nPDE Inverse Problems\nControlled Diffusions\n\nThe adjoint method is, at its core, the same idea as backpropagation or reverse-mode automatic differentiation. In practice, though, it’s often helpful to understand how it works under the hood. Basic implementations of backprop can be sub-optimal or impractical, especially in settings like PDE-constrained optimization or stochastic optimal control.\n\nLinear Systems\nFor a parameter \\(\\theta \\in \\mathbb{R}^{d_\\theta}\\), consider \\(x = x(\\theta)\\) the solution of the linear system\n\\[\nA x = b,\n\\]\nwhere the matrix \\(A = A(\\theta) \\in \\mathbb{R}^{d_x \\times d_x}\\) and the vector \\(b = b(\\theta) \\in \\mathbb{R}^{d_x}\\) depend on \\(\\theta\\). This setup is typical when discretizing PDEs: \\(A\\) arises from the differential operator, and \\(b\\) from the source term. We are interested in a loss function of the type\n\\[\n\\mathcal{L} = F(\\theta, x),\n\\]\nwith \\(F: \\mathbb{R}^{d_\\theta} \\times \\mathbb{R}^{d_x} \\to \\mathbb{R}\\). We aim to compute the gradient of \\(\\mathcal{L}\\) with respect to \\(\\theta\\). Applying the chain rule,\n\\[\nD_\\theta \\mathcal{L}= F_\\theta - F_x A^{-1} \\left( A_\\theta \\, x - b_\\theta \\right).\n\\]\nThe notation \\(F_\\theta = \\nabla_\\theta F^\\top \\in \\mathbb{R}^{1, d_\\theta}\\) denotes the Jacobian of \\(F\\) with respect to \\(\\theta\\), and similarly for \\(F_x\\) and \\(A_\\theta\\) and \\(b_\\theta\\). When \\(d_x \\gg 1\\) and \\(d_\\theta \\gg 1\\), directly computing \\(A^{-1}\\) is infeasible. Naively evaluating \\(A^{-1} (A_\\theta x - b_\\theta)\\) would require \\(d_\\theta\\) linear solves, which is too expensive. A better approach is to first compute \\(\\lambda^\\top = F_x A^{-1}\\) by solving the adjoint system\n\\[\nA^\\top \\lambda = F_x^\\top.\n\\]\nThis requires only one linear solve. Once \\(\\lambda\\) is computed, the gradient simplifies to\n\\[\nD_\\theta \\mathcal{L}= F_\\theta - \\lambda^\\top (A_\\theta x - b_\\theta).\n\\]\n\n\nAdjoint Method\nNow consider a more general situation where \\(x \\in \\mathbb{R}^{d_x}\\) and \\(\\theta \\in \\mathbb{R}^{d_\\theta}\\) are related by an implicit equation\n\\[\n\\Phi(x, \\theta) = 0\n\\]\nfor some function \\(\\Phi: \\mathbb{R}^{d_x} \\times \\mathbb{R}^{d_\\theta} \\to \\mathbb{R}^{d_x}\\) that satisfies the usual conditions of the implicit function theorem. Differentiating gives \\(x_\\theta = -\\Phi_x^{-1} \\Phi_\\theta\\). As before, we want the gradient with respect to \\(\\theta\\) of \\(\\mathcal{L} = F(x, \\theta)\\). It equals \\(D_\\theta \\mathcal{L}= F_\\theta - F_x \\Phi_x^{-1} \\Phi_\\theta\\) and can also be expressed as \\(D_\\theta \\mathcal{L}= F_\\theta - \\lambda^\\top \\Phi_\\theta\\) where \\(\\lambda \\in \\mathbb{R}^{d_x}\\) is the solution of the adjoint system\n\\[\n\\Phi_x^\\top \\lambda = F_x^\\top.\n\\tag{1}\\]\nAnother way to present this computation is to note that, for any vector \\(\\lambda \\in \\mathbb{R}^{d_x}\\), we have\n\\[\n\\mathcal{L} = F(\\theta, x) - \\lambda^\\top \\Phi(x, \\theta)\n\\]\nsince \\(\\Phi(x, \\theta) = 0\\) for all choices of \\(\\theta\\). Introducing the “adjoint” variable \\(\\lambda\\) allows one to eliminate cumbersome terms in the gradient. Differentiate with respect to \\(\\theta\\) gives\n\\[\nD_\\theta \\mathcal{L}= F_\\theta - \\lambda^\\top \\Phi_\\theta +  {\\left(  F_x - \\lambda^\\top \\Phi_x \\right)}  x_\\theta.\n\\]\nThe term \\(x_\\theta \\in \\mathbb{R}^{d_x, d_\\theta}\\) is cumbersome, and we would like to eliminate it. To do this, it suffices to choose \\(\\lambda\\) so that the term \\(F_x - \\lambda^\\top \\Phi_x\\) vanishes, which is again the adjoint system Equation 1.\n\n\nPDE Inverse Problems\nLet us see how this works in the context of PDE-constrained optimization. Let \\(\\Omega \\subset \\mathbb{R}^d\\) be a domain and let \\(\\kappa: \\Omega \\to \\mathbb{R}\\) be a scalar field. Consider the PDE\n\\[\n\\nabla  {\\left(  e^{\\kappa(x)} \\nabla u  \\right)}  = f,\n\\tag{2}\\]\nwhere \\(f\\) is a given source term. The field \\(\\kappa\\) describes the diffusion, or permeability, properties of the medium. We are interested in the solution \\(u\\) of the PDE on a bounded domain \\(\\Omega \\subset \\mathbb{R}^d\\) with Dirichlet boundary conditions \\(u(x) = 0\\) for \\(x \\in \\partial \\Omega\\). For each field \\(\\kappa\\), the elliptic PDE determines a unique solution \\(u\\). We are interested in minimizing the quantity\n\\[\n\\mathcal{L}(\\kappa) = \\int_\\Omega F(u(x)) \\, dx,\n\\]\nfor some given function \\(F: \\mathbb{R} \\to \\mathbb{R}\\). A common case is\n\\[\n\\mathcal{L}(\\kappa) = \\frac{1}{2} \\, \\int_\\Omega \\left| u(x) - u^\\star(x) \\right|^2 \\, w(x) \\, dx,\n\\]\nwhere \\(u^\\star\\) is a target solution and \\(w(x)&gt;0\\) is a weight. The goal is to adjust the field \\(\\kappa\\) so that \\(u\\) matches \\(u^\\star\\) as closely as possible. To carry out the minimization of \\(\\mathcal{L}\\) with respect to \\(\\kappa\\), one needs the gradient of \\(\\mathcal{L}\\) with respect to \\(\\kappa\\). To that end, define the augmented functional\n\\[\n\\mathcal{L} = \\int_\\Omega F(u(x)) \\, dx - \\int_\\Omega \\lambda \\left( \\nabla \\cdot (e^{\\kappa} \\nabla u) - f \\right) \\, dx,\n\\]\nfor an auxiliary field \\(\\lambda : \\Omega \\to \\mathbb{R}\\) that will be chosen later. As before, a good choice of \\(\\lambda\\) can simplify the gradient expression. Let \\(\\delta \\kappa\\) be a perturbation of \\(\\kappa\\). This induces a perturbation \\(u + \\delta u\\) in the solution and the first order variation of \\(\\mathcal{L}\\) reads:\n\\[\n\\delta \\mathcal{L} = \\int_\\Omega F'(u) \\, \\delta u \\, dx - \\int_\\Omega \\lambda \\, \\nabla \\cdot (e^{\\kappa} \\nabla \\delta u) \\, dx - \\int_\\Omega \\lambda \\, \\nabla \\cdot (e^{\\kappa} \\delta \\kappa \\nabla u) \\, dx.\n\\]\nThe term involving \\(\\delta u\\) is inconvenient. Assuming \\(\\lambda\\) also satisfies Dirichlet boundary conditions, which we can indeed assume seems we are free to define \\(\\lambda\\) in any manner we want, we integrate by parts:\n\\[\n\\delta \\mathcal{L} = \\int_\\Omega \\left( F'(u) - \\nabla \\cdot (e^{\\kappa} \\nabla \\lambda) \\right) \\delta u \\, dx - \\int_\\Omega \\lambda \\, \\nabla \\cdot (e^{\\kappa} \\delta \\kappa \\nabla u) \\, dx.\n\\]\nTo eliminate the \\(\\delta u\\) term, choose \\(\\lambda\\) to satisfy the adjoint equation\n\\[\n\\nabla \\cdot (e^{\\kappa} \\nabla \\lambda) = F'(u),\n\\tag{3}\\]\nwith Dirichlet boundary conditions. Then, an integration by parts gives\n\\[\n\\begin{align*}\n\\delta \\mathcal{L}\n&= -\\int_\\Omega \\lambda \\, \\nabla \\cdot (e^{\\kappa} \\delta \\kappa \\nabla u) \\, dx\\\\\n&= \\int_\\Omega e^{\\kappa} \\, \\left&lt;  \\nabla u, \\nabla \\lambda  \\right&gt; \\, \\delta \\kappa \\, dx\n= \\left&lt; g, \\delta \\kappa \\right&gt;_{L^2(\\Omega)}.\n\\end{align*}\n\\]\nThis means that the \\(L^2\\) gradient of \\(\\mathcal{L}\\) with respect to \\(\\kappa\\) is given by:\n\\[\ng = e^{\\kappa} \\, \\left&lt;  \\nabla u, \\nabla \\lambda  \\right&gt;,\n\\]\nwhere \\(\\lambda: \\Omega \\to \\mathbb{R}\\) solves the adjoint system Equation 3. This shows that the gradient of the objective can be computed at the same computational cost as the solution the original PDE Equation 2. This expression can be used directly in gradient-based optimization schemes.\n\n\nControlled Diffusions\nConsider the ODE on \\([0, T]\\):\n\\[\n\\dot{x} = b(t, \\theta, x),\n\\tag{4}\\]\nwith initial condition \\(x(0) = \\mu(\\theta)\\), where \\(x \\in \\mathbb{R}^{d_x}\\) and \\(\\theta \\in \\mathbb{R}^{d_\\theta}\\). The drift term \\(b\\) is parameterized by \\(\\theta\\). We want the sensitivity of the functional\n\\[\n\\mathcal{L} = \\int_0^T f(t, \\theta, x(t)) \\, dt + g(\\theta, x(T)),\n\\]\nwhere \\(f\\) and \\(g\\) are given functions. As before, introduce an auxiliary function \\(\\lambda(t) \\in \\mathbb{R}^{d_x}\\) and write:\n\\[\n\\mathcal{L} = \\mathcal{L} - \\int_0^T \\lambda^\\top (\\dot{x} - b(t, \\theta, x)) \\, dt.\n\\]\nDifferentiating with respect to \\(\\theta\\) gives:\n\\[\n\\begin{aligned}\nD_\\theta \\mathcal{L}\n&= \\int_0^T \\left( f_\\theta + \\lambda^\\top b_\\theta \\right) dt + g_\\theta(\\theta, x(T)) + \\lambda^\\top(0) \\mu_\\theta \\\\\n&\\quad + \\left( g_x - \\lambda^\\top(T) \\right) x_\\theta(T)\n+ \\int_0^T \\left( f_x + \\dot{\\lambda}^\\top + \\lambda^\\top b_x \\right) x_\\theta(t) \\, dt.\n\\end{aligned}\n\\]\nTo eliminate the dependence on \\(x_\\theta(t)\\), choose \\(\\lambda\\) to satisfy the adjoint system:\n\\[\n\\begin{cases}\n\\dot{\\lambda}(t) = -\\nabla_x f - b_x^\\top \\lambda(t), \\\\\n\\lambda(T) = \\nabla_x g.\n\\end{cases}\n\\tag{5}\\]\nThis is a linear ODE with a given terminal condition that needs to be solved backward in time. This means that for computing the gradient of \\(\\mathcal{L}\\), one can first solve the forward ODE Equation 4 to obtain \\(x(t)\\), and then solve the adjoint system Equation 5 backward in time to obtain \\(\\lambda(t)\\). The gradient of \\(\\mathcal{L}\\) with respect to \\(\\theta\\) is then given by\n\\[\nD_\\theta \\mathcal{L}\n= \\int_0^T \\left( f_\\theta + \\lambda^\\top b_\\theta \\right) dt + g_\\theta(\\theta, x(T)) + \\lambda^\\top(0) \\mu_\\theta.\n\\tag{6}\\]\nThe term \\(\\lambda^\\top b_\\theta\\) is a vector jacobian product, and can be computed efficiently. This formulation is often referred to as the “continuous adjoint method” or “adjoint sensitivity analysis” or “optimize-then-discretize” and dates back to the work of (Pontryagin 1962). A naive implementation of backpropagation can be inefficient memory-wise since quantities such as \\(f_\\theta\\) would typically be stored along the forward pass. When \\(d_\\theta \\gg 1\\), as is for example the case when the drift is parameterized by a neural network, this can be impractical. Instead, it may be more efficient to store the forward trajectory \\(x(t)\\) only, and recompute all the other quantities during the backward pass. Similarly, if implicit methods are used to solve the ODE instead if a simple Euler scheme, backpropagation through the implicit solver can be tricky. Note that nothing really changes when considering a SDE with additive noise instead,\n\\[\ndx = b(t, \\theta, x) \\, dt + \\sigma(t) \\, dW_t.\n\\]\nExtremely informally, one can more or less apply the same reasoning as above to the controlled ODE with drift \\(b(t, \\theta, x) + \\sigma(t) \\, dW_t/dt\\). Again, it suffices to solve the SDE forward in time to obtains \\(x(t)\\) and then solve the same adjoint system Equation 5 backward in time to obtain \\(\\lambda(t)\\): it is still an ordinary differential equation. The gradient of \\(\\mathcal{L}\\) with respect to \\(\\theta\\) is then given by the same expression Equation 6. For SDEs with multiplicative noise, the adjoint system is slightly more complicated, but hardly changes the overall picture. Finally, note that if \\(f\\) and \\(g\\) do not depend on \\(\\theta\\) and one chooses \\(\\theta = x_0\\) and the initial condition \\(\\mu_{x_0} = x_0\\), Equation 6 shows that \\(D_{x_0} \\mathcal{L}=  {\\left( \\nabla_{x_0} \\mathcal{L} \\right)} ^\\top = \\lambda(0)^\\top\\). More generally, this shows that:\n\\[\n\\lambda(t) = \\nabla_{x(t)} \\,  {\\left\\{  \\int_t^T f(s, \\theta, x(s)) \\, ds + g(\\theta, x(T)) \\right\\}} .\n\\]"
  }
]