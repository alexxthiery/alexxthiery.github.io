[
  {
    "objectID": "posts/reverse_and_tweedie/reverse_and_tweedie.html",
    "href": "posts/reverse_and_tweedie/reverse_and_tweedie.html",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "",
    "text": "Video\n\n\nImagine a scalar diffusion process \\(\\{X_t\\}_{t=0}^T\\) defined on the interval \\([0,T]\\),\n\\[dX = \\mu(X) \\, dt + \\sigma \\, dW\\]\nwhere \\(\\mu(X)\\) is the drift term, \\(\\sigma\\) is the diffusion coefficient, and \\(dW\\) is a Wiener process. Denote the distribution of this process at time \\(t\\) (\\(0 \\leq t \\leq T\\)) as \\(p_t(dx)\\) with an initial distribution of \\(p_0(dx)\\). Now, what happens if we reverse time and examine the process backward? In other words, consider the time-reversed process \\(\\overleftarrow{X}\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIntuitively, the process \\(\\overleftarrow{X}\\) is also a diffusion on the interval \\([0, T]\\), but with an initial distribution \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). To gain intuition, consider an Euler discretization of the forward process:\n\\[X_{t+\\delta} = X_{t} + \\mu(X_t)\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, \\mathbf{n} \\tag{1}\\]\nwhere \\(\\mathbf{n}\\sim \\mathcal{N}(0,1)\\) represents a noise term independent from \\(X_{t}\\), and \\(\\delta \\ll 1\\) is a time increment. Re-arranging terms and making the approximation \\(\\mu(X_t) \\approx \\mu(X_{t+\\delta})\\) gives that\n\\[X_{t} \\approx X_{t+\\delta} - \\mu(X_{t+\\delta})\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, (-\\mathbf{n}). \\tag{2}\\]\nThis seems to suggest that the time-reversed process follows the dynamics \\(d\\overleftarrow{X} = -\\mu(\\overleftarrow{X}) \\, dt + \\sigma \\, dW\\) started from \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). However, this conclusion is incorrect because this would suggest that the time-reversed of a standard Brownian motion (where \\(\\mu(x) \\equiv 0\\)) starting at zero is also a Brownian motion starting at \\(p_T(dx) = \\mathcal{N}(0,T)\\), which is clearly not the case. The flaw in this argument lies in assuming that the noise term \\(Z\\) is independent of \\(X_{t+\\delta}\\), which is not true, rendering the Euler discretization argument invalid.\nDeriving the dynamics of the backward process in a rigorous manner is not straightforward (Anderson 1982) (Haussmann and Pardoux 1986). What follows is a heuristic derivation that proceeds by estimating the mean and variance of \\(X_{t}\\) given \\(X_{t+\\delta} = x_{t+\\delta}\\), assuming \\(\\delta \\ll 1\\). Here, \\(x_{t+\\delta}\\) is treated as a fixed and constant value, and we are only interested in the conditional distribution of \\(X_t\\) given \\(x_{t+\\delta}\\). Bayes’ law gives\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta}) \\propto p_t(x) \\, \\exp\\left\\{ -\\frac{\\left( x_{t+\\delta} - [x + \\mu(x) \\, \\delta] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\},\\]\nwhere the exponential term corresponds to the transition of the forward diffusion for \\(\\delta \\ll 1\\). Using the 1st order approximation\n\\[p_t(x) \\approx p_t(x_{t+\\delta}) \\,\\exp\\left( \\langle \\nabla \\log p_t(x_{t+\\delta}), (x-x_{t+\\delta})\\rangle\\right),\\]\neliminating multiplicative constants and higher-order error terms, we obtain:\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta})\n\\propto\n\\exp\\left\\{ -\\frac{\\left( x - [x_{t+\\delta} - \\mu(x_{t+\\delta}) \\delta\n+ {\\color{red} \\sigma^2 \\, \\nabla \\log p_t(x_{t+\\delta}) \\, \\delta} ] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\}.\\]\nFor \\(\\delta \\ll 1\\), this is transition of the reverse diffusion\n\\[\nd\\overleftarrow{X}_t = -\\mu(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\]\nThe notation \\(B\\) is used to emphasize that this Brownian motion is distinct from the one used in the forward diffusion. The additional drift term, denoted as \\({\\color{red} \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X})}\\), is intuitive: it pushes the reverse diffusion toward regions where the forward diffusion spent a significant amount of time, i.e., where \\(p_t(dx)\\) is large. The popular “denoising diffusion models” (Ho, Jain, and Abbeel 2020) can be seen as discretizations of this backward process, employing various techniques to estimate the additional drift term from data."
  },
  {
    "objectID": "posts/reverse_and_tweedie/reverse_and_tweedie.html#reversing-a-diffusion",
    "href": "posts/reverse_and_tweedie/reverse_and_tweedie.html#reversing-a-diffusion",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "",
    "text": "Video\n\n\nImagine a scalar diffusion process \\(\\{X_t\\}_{t=0}^T\\) defined on the interval \\([0,T]\\),\n\\[dX = \\mu(X) \\, dt + \\sigma \\, dW\\]\nwhere \\(\\mu(X)\\) is the drift term, \\(\\sigma\\) is the diffusion coefficient, and \\(dW\\) is a Wiener process. Denote the distribution of this process at time \\(t\\) (\\(0 \\leq t \\leq T\\)) as \\(p_t(dx)\\) with an initial distribution of \\(p_0(dx)\\). Now, what happens if we reverse time and examine the process backward? In other words, consider the time-reversed process \\(\\overleftarrow{X}\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIntuitively, the process \\(\\overleftarrow{X}\\) is also a diffusion on the interval \\([0, T]\\), but with an initial distribution \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). To gain intuition, consider an Euler discretization of the forward process:\n\\[X_{t+\\delta} = X_{t} + \\mu(X_t)\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, \\mathbf{n} \\tag{1}\\]\nwhere \\(\\mathbf{n}\\sim \\mathcal{N}(0,1)\\) represents a noise term independent from \\(X_{t}\\), and \\(\\delta \\ll 1\\) is a time increment. Re-arranging terms and making the approximation \\(\\mu(X_t) \\approx \\mu(X_{t+\\delta})\\) gives that\n\\[X_{t} \\approx X_{t+\\delta} - \\mu(X_{t+\\delta})\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, (-\\mathbf{n}). \\tag{2}\\]\nThis seems to suggest that the time-reversed process follows the dynamics \\(d\\overleftarrow{X} = -\\mu(\\overleftarrow{X}) \\, dt + \\sigma \\, dW\\) started from \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). However, this conclusion is incorrect because this would suggest that the time-reversed of a standard Brownian motion (where \\(\\mu(x) \\equiv 0\\)) starting at zero is also a Brownian motion starting at \\(p_T(dx) = \\mathcal{N}(0,T)\\), which is clearly not the case. The flaw in this argument lies in assuming that the noise term \\(Z\\) is independent of \\(X_{t+\\delta}\\), which is not true, rendering the Euler discretization argument invalid.\nDeriving the dynamics of the backward process in a rigorous manner is not straightforward (Anderson 1982) (Haussmann and Pardoux 1986). What follows is a heuristic derivation that proceeds by estimating the mean and variance of \\(X_{t}\\) given \\(X_{t+\\delta} = x_{t+\\delta}\\), assuming \\(\\delta \\ll 1\\). Here, \\(x_{t+\\delta}\\) is treated as a fixed and constant value, and we are only interested in the conditional distribution of \\(X_t\\) given \\(x_{t+\\delta}\\). Bayes’ law gives\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta}) \\propto p_t(x) \\, \\exp\\left\\{ -\\frac{\\left( x_{t+\\delta} - [x + \\mu(x) \\, \\delta] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\},\\]\nwhere the exponential term corresponds to the transition of the forward diffusion for \\(\\delta \\ll 1\\). Using the 1st order approximation\n\\[p_t(x) \\approx p_t(x_{t+\\delta}) \\,\\exp\\left( \\langle \\nabla \\log p_t(x_{t+\\delta}), (x-x_{t+\\delta})\\rangle\\right),\\]\neliminating multiplicative constants and higher-order error terms, we obtain:\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta})\n\\propto\n\\exp\\left\\{ -\\frac{\\left( x - [x_{t+\\delta} - \\mu(x_{t+\\delta}) \\delta\n+ {\\color{red} \\sigma^2 \\, \\nabla \\log p_t(x_{t+\\delta}) \\, \\delta} ] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\}.\\]\nFor \\(\\delta \\ll 1\\), this is transition of the reverse diffusion\n\\[\nd\\overleftarrow{X}_t = -\\mu(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\]\nThe notation \\(B\\) is used to emphasize that this Brownian motion is distinct from the one used in the forward diffusion. The additional drift term, denoted as \\({\\color{red} \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X})}\\), is intuitive: it pushes the reverse diffusion toward regions where the forward diffusion spent a significant amount of time, i.e., where \\(p_t(dx)\\) is large. The popular “denoising diffusion models” (Ho, Jain, and Abbeel 2020) can be seen as discretizations of this backward process, employing various techniques to estimate the additional drift term from data."
  },
  {
    "objectID": "posts/reverse_and_tweedie/reverse_and_tweedie.html#denoising-score-matching-tweedie-formula",
    "href": "posts/reverse_and_tweedie/reverse_and_tweedie.html#denoising-score-matching-tweedie-formula",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "Denoising Score Matching & Tweedie formula",
    "text": "Denoising Score Matching & Tweedie formula\n\n\n\n\nMaurice Tweedie (1919–1996)\n\n\n\nThe previous section shows that a quantity often referred to as the “score” in machine learning (ML) literature, i.e. \\(\\mathcal{S}_t(x) \\equiv \\nabla_x \\log p_t(x)\\), naturally arises when a diffusion process runs backward in time. Interestingly, it is worth noting that since the beginning of times statisticians have been using the term “score” to refer to the other derivative, i.e. the derivative with respect to a model’s parameter, which is a completely different object!\nConsider a Brownian diffusion process \\(dX = \\sigma \\, dW\\) initiated from a distribution \\(\\mu(dx) = p_0(dx)\\). If this process is ran forward for a duration \\(\\delta&gt;0\\), we have:\n\\[X_{\\delta} = X_0 + \\mathcal{N}(0, \\sigma^2 \\, \\delta)\\]\nwhere \\(X_0 \\sim \\mu(dx)\\). Now, focusing on a specific sample \\(y = X_{\\delta}\\), the backward dynamics \\(d\\overleftarrow{X} = \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X}) , dt + \\sigma \\, dB\\) suggests that for sufficiently small \\(\\delta\\), the following approximation holds:\n\\[\n\\mathbb{E}[X_0 \\, | X_{\\delta} = y] \\; { \\color{\\red} \\approx} \\; y + \\nabla \\log p_{\\delta}(y) \\, \\sigma^2 \\delta.\n\\tag{3}\\]\nMaybe surprisingly, in the case of Brownian dynamics (no drift), this relationship holds exactly, even for arbitrarily large time increments \\(\\delta&gt;0\\). This observation attributed in (Efron 2011) to Maurice Tweedie (1919–1996) has a straightforward proof. Specifically, if \\(X \\sim \\mu(dx)\\), then \\(Y = X + \\mathcal{N}(0, \\Gamma^2)\\) has a density given by:\n\\[\np_Y(dy) = \\int \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx\n\\]\nwhere \\(\\rho_{\\Gamma}(z) \\propto \\exp[-z^2/(2 \\Gamma^2)]\\) is a centred Gaussian with variance \\(\\Gamma^2\\). It follows that\n\\[\n\\frac{ \\mathbb{E}[X \\, | Y = y]-y}{\\Gamma^2} = \\frac{ \\int \\left( \\frac {x-y }{\\Gamma^2} \\right)\\, \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx }{\\int \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx}\n=\n\\nabla_y \\log \\left\\{ \\int \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx \\right\\}\n\\]\nsince \\(\\nabla_y \\rho_{\\Gamma}(y-x) = (x-y)/\\Gamma^2\\). This leads to Tweedie’s formula:\n\\[\n\\mathbb{E}[X \\, | Y = y] \\; {\\color{red} =} \\; y + \\Gamma^2 \\, \\nabla \\log p_Y(y),\n\\]\nwhich is exactly the same as Equation 3 but with an equality sign: no approximation needed! Interestingly, this is what Machine-Learners often refer to as “denoising score matching” (Vincent 2011): if we have access to a large number of samples \\((X_i,Y_i)\\), where \\(X_i \\sim \\mu(dx)\\) and \\(Y_i = X_i + \\mathcal{N}(0, \\Gamma^2)\\), and fit a regression model \\(F_{\\theta}\\) by minimizing the mean-squared error \\(\\theta \\mapsto \\mathbb{E} \\|X - F_{\\theta}(Y)\\|^2\\), then Tweedie formula shows that in the limit of infinite samples and with sufficient flexibility in the regression model \\(F_{\\theta_\\star}(y) = y + \\Gamma^2 \\nabla \\log p_Y(y)\\). This allows one to estimate \\(\\nabla \\log p_Y\\) from data, which is often a good approximation of \\(\\nabla \\log \\mu\\) if the variance \\(\\Gamma^2\\) of the added noise is not too large. Indeed, things can go bad if \\(\\Gamma^2\\) is very small and the number of training data is not large, no free lunch!"
  },
  {
    "objectID": "posts/hello_world/index.html",
    "href": "posts/hello_world/index.html",
    "title": "Hello World",
    "section": "",
    "text": "Just testing whether Latex is working correctly: \\[\\frac{1}{2}\\left( \\int \\exp\\left\\{ -\\frac{x^2}{2}\\right\\} \\, dx \\right)^2 \\approx 22/7\\]\nHere is a reference (MacKay 2003) and below is an animation:\n\n\nVideo\n\n\n\n\n\n\nReferences\n\nMacKay, David JC. 2003. Information Theory, Inference and Learning Algorithms. Cambridge university press."
  },
  {
    "objectID": "posts/index_blog.html",
    "href": "posts/index_blog.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Blogposts (vaguely) related to research, notes for students, announcements, things I would like to write down to understand better and/or not forget, etc… Comments, corrections, suggestions are welcome!\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n12-06-2023\n\n\nReverse diffusions, Score & Tweedie\n\n\nSDE,Generative-Model\n\n\n\n\n06-06-2023\n\n\nHello World\n\n\ntesting\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/index_pubs.html#theory-methods",
    "href": "publications/index_pubs.html#theory-methods",
    "title": "Alexandre Thiéry",
    "section": "THEORY & METHODS:",
    "text": "THEORY & METHODS:\n\n\n\n Computational Doob’s H-transforms for Online Filtering of Discretely Observed Diffusions   Nicolas Chopin, Andras Fulop, Jeremy Heng, Alexandre H. Thiery, (2023)   International Conference on Machine Learning (ICML) 2023    (Arxiv) \n\n\n Conditional sequential Monte Carlo in high dimensions   Axel Finke and Alexandre H. Thiery, (2023)   Annals of Statistics 2023, In Press    (Arxiv)   (Journal)  \n\n\n Pretrained equivariant features improve unsupervised landmark discovery   Rahul Rahaman, Atin Ghosh and Alexandre H. Thiery, (2022)   International Conference of Pattern Recognition (ICPR) 2022    (Arxiv) \n\n\n Manifold lifting: scaling MCMC to the vanishing noise regime   Khai Xiang Au, Matthew Graham, Alexandre H. Thiery, (2022)   JRSSB 2022    (Arxiv)   (Journal)  \n\n\n A discrete Bouncy Particle Sampler   Chris Sherlock and Alexandre H. Thiery, (2022)   Biometrika, Volume 109, Issue 2 (2022)    (Arxiv)   (Journal)  \n\n\n A Generalized & Robust Framework For Timestamp Supervision in Temporal Action Segmentation   Rahul Rahaman, Dipika Singhania, Alexandre H. Thiery, Angela Yao, (2022)   European Conference on Computer Vision (ECCV) 2022    (Journal)  \n\n\n Manifold Markov chain Monte Carlo methods for Bayesian inference in a wide class of diffusion models   Matthew Graham, Alexandre H. Thiery, Alex Beskos, (2022)   JRSSB, Volume 84 (4), 2022    (Arxiv)   (Journal)  \n\n\n Sequential Ensemble Transform for Bayesian Inverse Problems   Aaron Myers, Alexandre H. Thiery, Kainan Wang and Tan Bui-Tanh, (2021)   Journal of Computational Physics, Volume 427 (2021)    (Arxiv)   (Journal)  \n\n\n On Data-Augmentation and Consistency-Based Semi-Supervised Learning   Atin Ghosh and Alexandre H. Thiery, (2021)   ICLR 2021    (Arxiv) \n\n\n Uncertainty Quantification and Deep Ensembles   Rahul Rahaman and Alexandre H. Thiery, (2021)   NeuRIPS 2021    (Arxiv) \n\n\n On importance-weighted autoencoders   Axel Finke and Alexandre H. Thiery, (2019)   Tech Report    (Arxiv) \n\n\n A scalable optimal-transport based local particle filter   Matthew Graham and Alexandre H. Thiery, (2019)   Tech Report    (Arxiv) \n\n\n Particle Filter efficiency under limited communication   Deborshee Sen and Alexandre H. Thiery, (2019)   Tech Report    (Arxiv) \n\n\n Error Bounds for Sequential Monte Carlo Samplers for Multimodal Distributions   Daniel Paulin, Ajay Jasra and Alexandre H. Thiery, (2019)   Bernoulli, Volume 25, Number 1 (2019)    (Arxiv)   (Journal)  \n\n\n Asymptotic Analysis of the Random-Walk Metropolis Algorithm on Ridged Densities   Alex Beskos, Gareth Roberts, Alexandre H. Thiery and Natesh Pillai, (2018)   Annals of Applied Probability, Volume 28, Number 5 (2018)    (Arxiv)   (Journal)  \n\n\n On Coupling Particle Filter Trajectories   Deborshee Sen, Alexandre H. Thiery, Ajay Jasra, (2018)   Statistics and Computing, Volume 28, Number 2 (2018)    (Arxiv)   (Journal)  \n\n\n Levy statistics of interacting Rydberg gases   Thibault Vogt, Jingshan Han, Alexandre H. Thiery, Wenhui Li, (2017)   Physical Review A, Volume 95, Number 5 (2017)    (Arxiv)   (Journal)  \n\n\n Pseudo-marginal Metropolis–Hastings using averages of unbiased estimators   Chris Sherlock, Alexandre H. Thiery and Anthony Lee, (2017)   Biometrika, Volume 104, Number 3 (2017)    (Arxiv)   (Journal)  \n\n\n On the Convergence of Adaptive Sequential Monte Carlo Methods   Alex Beskos, Ajay Jasra, Nikolas Kantas, Alexandre H. Thiery, (2016)   Annals of Applied Probability, Volume 26, Number 2 (2016)    (Arxiv)   (Journal)  \n\n\n Consistency and fluctuations for stochastic gradient Langevin dynamics   Yee Whye Teh, Alexandre H. Thiery, Sebastian Vollmer, (2016)   Journal of Machine Learning Research, Volume 17 (2016)    (Arxiv)   (Journal)  \n\n\n On the efficiency of pseudo-marginal random walk Metropolis algorithms   Chris Sherlock, Alexandre H. Thiery, Gareth Roberts, Jeff Rosenthal, (2015)   Annals of Statistics, Volume 43, Number 1 (2015)    (Arxiv)   (Journal)  \n\n\n On non-negative unbiased estimators   Pierre Jacob, Alexandre H. Thiery, (2015)   Annals of Statistics, Volume 43, Number 2 (2015)    (Arxiv)   (Journal)  \n\n\n Efficiency of delayed-acceptance random walk Metropolis algorithms   Chris Sherlock, Alexandre H. Thiery and Andrew Golightly, (2015)   Annals of Statistics (In Press)    (Arxiv)   (Journal)  \n\n\n Noisy gradient flow from a random walk in Hilbert space   Natesh Pillai, Andrew Stuart, Alexandre H. Thiery, (2014)   Stochastic Partial Differential Equations: Analysis and Computations, Volume 2, Number 2 (2014)    (Arxiv)   (Journal)  \n\n\n Optimal Scaling and Diffusion Limits for the Langevin Algorithm in High Dimensions   Natesh Pillai, Andrew Stuart, Alexandre H. Thiery, (2012)   Annals of Applied Probability, Volume 22, Number 6 (2012)    (Arxiv)   (Journal)  \n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/index_pubs.html#applications",
    "href": "publications/index_pubs.html#applications",
    "title": "Alexandre Thiéry",
    "section": "APPLICATIONS",
    "text": "APPLICATIONS\n\n\n\n Three-Dimensional Structural Phenotype of the Optic Nerve Head as a Function of Glaucoma Severity   Braeu, F.A., Chuangsuwanich, T., Tun, T.A., Perera, S.A., Husain, R., Kadziauskienė, A., Schmetterer, L., Thiéry, A.H., Barbastathis, G., Aung, T. and Girard, M.J.A., (2023)   JAMA Ophthalmology, 2023    (Arxiv)   (Journal)  \n\n\n Towards Label-Free 3D Segmentation of Optical Coherence Tomography Images of the Optic Nerve Head Using Deep Learning   Devalla,S.K., Pham, T.H., Panda, S.K, Zhang,L., Subramanian,G., Swaminathan,A., Chin,Z.Y, Rajan,M., Mohan,S., Krishnadas,R., Senthil,V., Leon,J.M, Tun,T.A., Cheng,C.Y., Schmetterer, L., Perera,S., Aung,T., Thiery,A.H., Girard,M.J.A., (2023)   Biomedical Optics Express, Vol. 11, Issue 11    (Arxiv)   (Journal)  \n\n\n Geometric Deep Learning to Identify the Critical 3D Structural Features of the Optic Nerve Head for Glaucoma Diagnosis   Braeu, F., Thiery, A.H, Tun, T.A., Kadziauskiene, A., Barbastathis, G., Aung, T., and Girard. M.J.A., (2023)   American Journal of Ophthalmology, 2023    (Arxiv)   (Journal)  \n\n\n Medical Application of Geometric Deep Learning for the Diagnosis of Glaucoma   Thiery, A.H, Braeu, F., Tun, T.A., Aung, T., Girard. M.J.A., (2023)   Translational Vision Science and Technology, 2023    (Arxiv)   (Journal)  \n\n\n Detection of m6A from direct RNA sequencing using a Multiple Instance Learning framework   Hendra C, Pratanwanich PN, Wan YK, Goh WS, Thiery A, Göke J+., (2022)   Nature Methods (2022)    (Arxiv)   (Journal)  \n\n\n Describing the Structural Phenotype of the Glaucomatous Optic Nerve Head Using Artificial Intelligence   Panda, S.K., Cheong, H., Tun,T.A., Devella, S.K., Krishnadas, R., Buist, M.L., Perera, S., Cheng, C-Y., Aung, T., Thiery A.H., Girard, M.J.A., (2022)   American Journal of Ophthalmology, 2022    (Arxiv)   (Journal)  \n\n\n The Three-Dimensional Structural Configuration of the Central Retinal Vessel Trunk and Branches as a Glaucoma Biomarker   Satish K Panda, Haris Cheong, Tin A Tun, Thanadet Chuangsuwanich, Aiste Kadziauskiene, Vijayalakshmi Senthil, Ramaswami Krishnadas, Martin L Buist, Shamira Perera, Ching-Yu Cheng, Tin Aung, Alexandre H Thiery, Michaël JA Girard, (2022)   American Journal of Ophthalmology, 2022    (Journal)  \n\n\n Deep Learning Algorithms to Isolate and Quantify the Structures of the Anterior Segment in Optical Coherence Tomography Images   Pham, T.H, Devalla,S.K., Ang, A., Da,S.Z., Thiery,A.H., Boote,C., Cheng,C.Y., Koh,V., Girard, M.J.A., (2021)   British Journal of Ophthalmology    (Arxiv)   (Journal)  \n\n\n Identification of differential RNA modifications from nanopore direct RNA sequencing with xPore   Ploy N. Pratanwanich, Fei Yao, Ying Chen, Casslynn W.Q. Koh, Christopher Hendra, Polly Poon, Yeek Teck Goh, Phoebe M. L. Yap, Choi Jing Yuan, Wee Joo Chng, Sarah Ng, Alexandre Thiery, W.S. Sho Goh, Jonathan Goeke, (2021)   Nature Biotechnology, 2021    (Arxiv)   (Journal)  \n\n\n Beyond quadratic error: Case-study of a multiple criteria approach to the performance assessment of numerical forecasts of solar irradiance in the tropics   Verbois, H., Blanc, P., Huva, R., Saint-Drenan, Y-M, Rusydi, A.. Thiery, A., (2020)   Renewable and Sustainable Energy Reviews, Volume 117, (2020)    (Journal)  \n\n\n NanoVar: Accurate Characterization of Patients Genomic Structural Variants Using Low-Depth Nanopore Sequencing   Tham, C.Y, Tirado-Magallanes, R., Goh, Y., Fullwood, M. J., Koh, B.T.H. , Wang, W., Ng, C.H, Chng, W.J., Thiery, A.H., Tenen, D.G, Benoukraf, (2020)   Genome Biology (2020)    (Arxiv)   (Journal)  \n\n\n DeshadowGAN: A Deep Learning Approach to Remove Shadows from Optical Coherence Tomography Images   Cheong, H., Devalla, S.K., Pham, T.H., Liang, Z., Tun, T.A., Wang, X., Perera, S., Schmetterer, L., Tin, A., Boote, C., Thiery, A.H., Girard, M.J.A., (2020)   Translational Vision Science & Technology    (Arxiv)   (Journal)  \n\n\n Deep Learning Algorithms to Isolate and Quantify the Structures of the Anterior Segment in Optical Coherence Tomography Images   Pham, T.H, Devalla,S.K., Ang, A., Da, S.Z., Thiery, A.H., Boote,C., Cheng,C.Y., Koh,V., Girard, M.J.A, (2020)   British Journal of Ophthalmology, 2020    (Arxiv)   (Journal)  \n\n\n A Deep Learning Approach to Denoise Optical Coherence Tomography Images of the Optic Nerve Head   Devalla SK, Subramanian G, Pham TH, Wang X, Perera S, Tun TA, Aung T, Schmetterer L, Thiery A.H., Girard MJA, (2019)   Scientific Reports (2019)    (Arxiv)   (Journal)  \n\n\n Glaucoma management in the era of artificial intelligence   Devalla S.K., Liang Z., Pham T.H., Boote, C., Strouthidis, N.G., Thiery A.H., Girard M.J.A., (2019)   British Journal of Ophthalmology (2019)    (Journal)  \n\n\n DRUNET: A Dilated-Residual U-Net Deep Learning Network to Digitally Stain Optic Nerve Head Tissues in Optical Coherence Tomography Images   Devalla SK, Renukanand PK, Sreedhar BK, Perera SA, Mari JM, Chin KS, Tun TA, Strouthidis N, Aung T, Thiery A.H., Girard MJA, (2018)   Biomedical Optics Express, Vol. 9, Issue 7 (2018)    (Arxiv)   (Journal)  \n\n\n Probabilistic forecasting of day-ahead solar irradiance using quantile gradient boosting   Verbois, H., Rusydi, A., Thiery, A.H., (2018)   Solar Energy 173, 313-327 (2018)    (Journal)  \n\n\n\nNo matching items"
  },
  {
    "objectID": "about/about.html",
    "href": "about/about.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Email: a.h.thiery [at] nus.edu.sg\nOffice: Building S16, Office #06-113"
  },
  {
    "objectID": "about/about.html#address",
    "href": "about/about.html#address",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Email: a.h.thiery [at] nus.edu.sg\nOffice: Building S16, Office #06-113"
  },
  {
    "objectID": "about/about.html#qualifications",
    "href": "about/about.html#qualifications",
    "title": "Alexandre Thiéry",
    "section": "Qualifications",
    "text": "Qualifications\n\nPh.D., Probability & Statistics, Warwick University, 2009-2013.\nEcole Normale Superieure of Paris, Mathematics, 2005-2009\n\nResearch Assistant, Statslab (Cambridge, UK)\nMSc (Probability & Finance), University of Paris VI\nMSc (Partial Differential Equations & Modeling), University of Paris VI"
  },
  {
    "objectID": "about/about.html#employment-history",
    "href": "about/about.html#employment-history",
    "title": "Alexandre Thiéry",
    "section": "Employment history",
    "text": "Employment history\n\nAssociate Professor, Department of Statistics & Data Sciences, NUS, 2020–present.\nAffiliate, NUS Centre for Data Science and Machine Learning, 2021–present\nAffiliate, NUS Institute of Data Science, 2028–present\nAffiliate, NUS Graduate School for Integrative Sciences and Engineering, 2017–present\nAssistant Professor, Department of Statistics & Probability, NUS, 2014–2019.\nResearch Fellow, Department of Statistics & Probability, NUS, 2013"
  },
  {
    "objectID": "about/about.html#leadership",
    "href": "about/about.html#leadership",
    "title": "Alexandre Thiéry",
    "section": "Leadership",
    "text": "Leadership\n\nDeputy Director of Institute for Mathematical Sciences, 7/2020 – 12/2023"
  },
  {
    "objectID": "about/about.html#service",
    "href": "about/about.html#service",
    "title": "Alexandre Thiéry",
    "section": "Service",
    "text": "Service\n\nArea Chair for AISTAT (2023), ACML (2023), NeurIPS (2023), ICLR (2024)\nAssociate Editor for Statistics & Computing (2020–2022)"
  },
  {
    "objectID": "about/about.html#awards-and-honours",
    "href": "about/about.html#awards-and-honours",
    "title": "Alexandre Thiéry",
    "section": "Awards and honours",
    "text": "Awards and honours\n\nNUS Faculty of Sciences Dean’s Chair Associate Professor, 2022–2025\nNUS Faculty Teaching Excellence Award, 2022\nNUS Faculty Teaching Excellence Award, 2019\nNUS Faculty Teaching Excellence Award, 2018\nNUS Young Scientist Award. (Faculty of Science: 1 awardee per year), 2017\nNUS Young Investigator Award. (Faculty of Science: 3 awardees per year), 2016\nJohn Copas prize for the best PhD dissertation, 2013"
  },
  {
    "objectID": "about/about.html#industrial-activities",
    "href": "about/about.html#industrial-activities",
    "title": "Alexandre Thiéry",
    "section": "Industrial Activities",
    "text": "Industrial Activities\n\nCo-founder of Abyss Processing, start-up developing AI solutions for ophthalmology\nConsulting in the finance and health-care industries"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Google-Scholar\n  \n  \n    \n     Arxiv\n  \n  \n    \n     twitter\n  \n\n      \nAssociate Professor\nDepartment of Statistics & Data Science\nNational University of Singapore\n\n\n\nComputational Statistics & Machine Learning\nMonte-Carlo methods\nUncertainty Quantification\nInverse problems\n\n\n\n\n\nTwo Research Fellow positions: Data Assimilation\n\n\n\n\nOffice: Building S16, Office #06-113\nEmail: a.h.thiery [at] nus.edu.sg"
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Computational Statistics & Machine Learning\nMonte-Carlo methods\nUncertainty Quantification\nInverse problems"
  },
  {
    "objectID": "index.html#available-positions",
    "href": "index.html#available-positions",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Two Research Fellow positions: Data Assimilation"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Office: Building S16, Office #06-113\nEmail: a.h.thiery [at] nus.edu.sg"
  },
  {
    "objectID": "notes/DDPM_deterministic/DDPM_deterministic.html",
    "href": "notes/DDPM_deterministic/DDPM_deterministic.html",
    "title": "From Denoising Diffusion to ODEs",
    "section": "",
    "text": "Consider an empirical data distribution \\(\\pi_{\\mathrm{data}}\\). In order to simulate approximate samples from \\(\\pi_{\\mathrm{data}}\\), Denoising Diffusion Probabilistic Models (DDPM) simulate a forward diffusion process \\(\\{X_t\\}_{[0,T]}\\) on an interval \\([0,T]\\). The diffusion is initialized at the data distribution, i.e. \\(X_0 \\sim \\pi_{\\mathrm{data}}\\), and is chosen so that that the distribution of \\(X_T\\) is very close to a known and tractable reference distribution \\(\\pi_{\\mathrm{ref}}\\), e.g. a Gaussian distribution. Denote by \\(p_t(dx)\\) the marginal distribution at time \\(0 \\leq t \\leq T\\), i.e. \\(\\mathop{\\mathrm{P}}(X_t \\in dx) = p_t(dx)\\). By choosing the forward distribution with simple and tractable transition probabilities, e.g. an Ornstein-Uhlenbeck, it is relatively easy to estimate \\(\\nabla \\log p_t(x)\\) from simulated data: this can be formulated as a simple regression problem. This allows one to simulate the diffusion backward in time and generate approximate samples from \\(\\pi_{\\mathrm{data}}\\). Why this is useful is another question…\nThe fact that the mapping from data-samples at time \\(t=0\\) to (approximate) Gaussian samples at time \\(t=T\\) is stochastic and described by diffusion processes is cumbersome. This would be much more convenient to build a deterministic mapping between the data-distribution \\(\\pi_{\\mathrm{data}}\\) and the Gaussian reference distribution \\(\\pi_{\\mathrm{ref}}\\): this would allows one to associate a likelihood to data samples and to easily “encode”/“decode” data-samples. To so this, one can try to replace diffusion by Ordinary Differential Equations."
  },
  {
    "objectID": "notes/DDPM_deterministic/DDPM_deterministic.html#likelihood-computation",
    "href": "notes/DDPM_deterministic/DDPM_deterministic.html#likelihood-computation",
    "title": "From Denoising Diffusion to ODEs",
    "section": "Likelihood computation",
    "text": "Likelihood computation\nWith the diffusion-ODE trick, we have just seen that it is possible to build a vector fields \\(F[0,T] \\times \\mathbb{R}^d \\to \\mathbb{R}^d\\) such that the forward ODE\n\\[\n\\frac{d}{dt} \\overrightarrow{Y_t} =\nF(t,\\overrightarrow{Y_t})\n\\qquad \\textrm{initialized at} \\qquad\n\\overrightarrow{Y}_0 \\sim \\pi_{\\mathrm{data}}\n\\tag{3}\\]\nand the backward ODE defined as\n\\[\n\\frac{d}{ds} \\overleftarrow{Y_s} =\n-F(T-s,\\overleftarrow{Y_s})\n\\qquad \\textrm{initialized at} \\qquad\n\\overleftarrow{Y}_0 \\sim \\pi_{\\mathrm{ref}}\n\\]\nare such that \\(\\overrightarrow{Y}_T \\approx \\pi_{\\mathrm{ref}}\\) and \\(\\overleftarrow{Y}_T \\approx \\pi_{\\mathrm{data}}\\).\nIn general, consider a vector field \\(F(t,x)\\) and a bunch of particles distributed according to a distribution \\(p_t\\) at time \\(t\\). If each particle follows the vector field for an amount of time \\(\\delta \\ll 1\\), the particles that were in the vicinity of some \\(x \\in \\mathbb{R}^d\\) at time \\(t\\) end up in the vicinity of \\(x + F(x,t) \\, \\delta\\) at time \\(t+\\delta\\). At the same time, a volume element \\(dx\\) around \\(x \\in \\mathbb{R}^d\\) gets stretch by a factor \\(1+\\delta \\, \\mathop{\\mathrm{Tr}}[\\mathop{\\mathrm{\\mathrm{Jac}}}F(x,t)] = 1 + \\delta \\mathop{\\mathrm{div}}F(x,t)\\) while following the vector field \\(F\\), which means that the density of particles at time \\(t+\\delta\\) and around \\(x + F(x,t) \\, \\delta\\) equals \\(p_t(x) / [1 + \\delta \\mathop{\\mathrm{div}}F(x,t)]\\). In other words \\(\\log p_{t+\\delta}(x + F(x,t) \\, \\delta) \\approx \\log p_t(x) - \\delta \\, \\mathop{\\mathrm{div}}F(x,t)\\). This means that if we follows a trajectory of \\(\\tfrac{d}{dt} X_t = F(t,X_t)\\) one gets\n\\[\n\\log p_T(X_T) = \\log p_0(X_0) - \\int_{t=0}^{T} \\mathop{\\mathrm{div}}F(X_t,t) \\, dt.\n\\]\nThat is the Lagrangian description of the density \\(p_t\\) of particles. Indeed, one could directly get this identity by differentiating \\(p_t(X_t)\\) with respect to time while using the continuity Equation 1. When applied to the DDPM, this gives a way to assign likelihood the data samples, namely\n\\[\n\\log \\pi_{\\mathrm{data}}(x) = \\log \\pi_{\\mathrm{ref}}(\\overrightarrow{Y_T}) + \\int_{t=0}^{T} \\mathop{\\mathrm{div}}F(t, \\overrightarrow{Y_t})\\, dt\n\\]\nwhere \\(\\overrightarrow{Y_t}\\) is trajectory of the forward ODE Equation 3 initialized as \\(\\overrightarrow{Y_0} = x\\). Note that in high-dimensional setting, it may be computationally expensive to compute the divergence term \\(\\mathop{\\mathrm{div}}F(t, \\overrightarrow{Y_t})\\) since it typically is \\(d\\) times slower that a gradient computation; for this reason, it is often advocated to use the Hutchinson trace estimator to get an unbiased estimate of it at a much lower computational cost."
  },
  {
    "objectID": "notes/reverse_and_tweedie/reverse_and_tweedie.html",
    "href": "notes/reverse_and_tweedie/reverse_and_tweedie.html",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "",
    "text": "Video\n\n\nImagine a scalar diffusion process \\(\\{X_t\\}_{t=0}^T\\) defined on the interval \\([0,T]\\),\n\\[dX = \\mu(X) \\, dt + \\sigma \\, dW\\]\nwhere \\(\\mu(X)\\) is the drift term, \\(\\sigma\\) is the diffusion coefficient, and \\(dW\\) is a Wiener process. Denote the distribution of this process at time \\(t\\) (\\(0 \\leq t \\leq T\\)) as \\(p_t(dx)\\) with an initial distribution of \\(p_0(dx)\\). Now, what happens if we reverse time and examine the process backward? In other words, consider the time-reversed process \\(\\overleftarrow{X}\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIntuitively, the process \\(\\overleftarrow{X}\\) is also a diffusion on the interval \\([0, T]\\), but with an initial distribution \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). To gain intuition, consider an Euler discretization of the forward process:\n\\[X_{t+\\delta} = X_{t} + \\mu(X_t)\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, \\mathbf{n} \\tag{1}\\]\nwhere \\(\\mathbf{n}\\sim \\mathcal{N}(0,1)\\) represents a noise term independent from \\(X_{t}\\), and \\(\\delta \\ll 1\\) is a time increment. Re-arranging terms and making the approximation \\(\\mu(X_t) \\approx \\mu(X_{t+\\delta})\\) gives that\n\\[X_{t} \\approx X_{t+\\delta} - \\mu(X_{t+\\delta})\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, (-\\mathbf{n}). \\tag{2}\\]\nThis seems to suggest that the time-reversed process follows the dynamics \\(d\\overleftarrow{X} = -\\mu(\\overleftarrow{X}) \\, dt + \\sigma \\, dW\\) started from \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). However, this conclusion is incorrect because this would suggest that the time-reversed of a standard Brownian motion (where \\(\\mu(x) \\equiv 0\\)) starting at zero is also a Brownian motion starting at \\(p_T(dx) = \\mathcal{N}(0,T)\\), which is clearly not the case. The flaw in this argument lies in assuming that the noise term \\(Z\\) is independent of \\(X_{t+\\delta}\\), which is not true, rendering the Euler discretization argument invalid.\nDeriving the dynamics of the backward process in a rigorous manner is not straightforward (Anderson 1982) (Haussmann and Pardoux 1986). What follows is a heuristic derivation that proceeds by estimating the mean and variance of \\(X_{t}\\) given \\(X_{t+\\delta} = x_{t+\\delta}\\), assuming \\(\\delta \\ll 1\\). Here, \\(x_{t+\\delta}\\) is treated as a fixed and constant value, and we are only interested in the conditional distribution of \\(X_t\\) given \\(x_{t+\\delta}\\). Bayes’ law gives\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta}) \\propto p_t(x) \\, \\exp\\left\\{ -\\frac{\\left( x_{t+\\delta} - [x + \\mu(x) \\, \\delta] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\},\\]\nwhere the exponential term corresponds to the transition of the forward diffusion for \\(\\delta \\ll 1\\). Using the 1st order approximation\n\\[p_t(x) \\approx p_t(x_{t+\\delta}) \\,\\exp\\left( \\langle \\nabla \\log p_t(x_{t+\\delta}), (x-x_{t+\\delta})\\rangle\\right),\\]\neliminating multiplicative constants and higher-order error terms, we obtain:\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta})\n\\propto\n\\exp\\left\\{ -\\frac{\\left( x - [x_{t+\\delta} - \\mu(x_{t+\\delta}) \\delta\n+ {\\color{red} \\sigma^2 \\, \\nabla \\log p_t(x_{t+\\delta}) \\, \\delta} ] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\}.\\]\nFor \\(\\delta \\ll 1\\), this is transition of the reverse diffusion\n\\[\nd\\overleftarrow{X}_t = -\\mu(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\]\nThe notation \\(B\\) is used to emphasize that this Brownian motion is distinct from the one used in the forward diffusion. The additional drift term, denoted as \\({\\color{red} \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X})}\\), is intuitive: it pushes the reverse diffusion toward regions where the forward diffusion spent a significant amount of time, i.e., where \\(p_t(dx)\\) is large. The popular “denoising diffusion models” (Ho, Jain, and Abbeel 2020) can be seen as discretizations of this backward process, employing various techniques to estimate the additional drift term from data."
  },
  {
    "objectID": "notes/reverse_and_tweedie/reverse_and_tweedie.html#reversing-a-diffusion",
    "href": "notes/reverse_and_tweedie/reverse_and_tweedie.html#reversing-a-diffusion",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "",
    "text": "Video\n\n\nImagine a scalar diffusion process \\(\\{X_t\\}_{t=0}^T\\) defined on the interval \\([0,T]\\),\n\\[dX = \\mu(X) \\, dt + \\sigma \\, dW\\]\nwhere \\(\\mu(X)\\) is the drift term, \\(\\sigma\\) is the diffusion coefficient, and \\(dW\\) is a Wiener process. Denote the distribution of this process at time \\(t\\) (\\(0 \\leq t \\leq T\\)) as \\(p_t(dx)\\) with an initial distribution of \\(p_0(dx)\\). Now, what happens if we reverse time and examine the process backward? In other words, consider the time-reversed process \\(\\overleftarrow{X}\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIntuitively, the process \\(\\overleftarrow{X}\\) is also a diffusion on the interval \\([0, T]\\), but with an initial distribution \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). To gain intuition, consider an Euler discretization of the forward process:\n\\[X_{t+\\delta} = X_{t} + \\mu(X_t)\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, \\mathbf{n} \\tag{1}\\]\nwhere \\(\\mathbf{n}\\sim \\mathcal{N}(0,1)\\) represents a noise term independent from \\(X_{t}\\), and \\(\\delta \\ll 1\\) is a time increment. Re-arranging terms and making the approximation \\(\\mu(X_t) \\approx \\mu(X_{t+\\delta})\\) gives that\n\\[X_{t} \\approx X_{t+\\delta} - \\mu(X_{t+\\delta})\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, (-\\mathbf{n}). \\tag{2}\\]\nThis seems to suggest that the time-reversed process follows the dynamics \\(d\\overleftarrow{X} = -\\mu(\\overleftarrow{X}) \\, dt + \\sigma \\, dW\\) started from \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). However, this conclusion is incorrect because this would suggest that the time-reversed of a standard Brownian motion (where \\(\\mu(x) \\equiv 0\\)) starting at zero is also a Brownian motion starting at \\(p_T(dx) = \\mathcal{N}(0,T)\\), which is clearly not the case. The flaw in this argument lies in assuming that the noise term \\(Z\\) is independent of \\(X_{t+\\delta}\\), which is not true, rendering the Euler discretization argument invalid.\nDeriving the dynamics of the backward process in a rigorous manner is not straightforward (Anderson 1982) (Haussmann and Pardoux 1986). What follows is a heuristic derivation that proceeds by estimating the mean and variance of \\(X_{t}\\) given \\(X_{t+\\delta} = x_{t+\\delta}\\), assuming \\(\\delta \\ll 1\\). Here, \\(x_{t+\\delta}\\) is treated as a fixed and constant value, and we are only interested in the conditional distribution of \\(X_t\\) given \\(x_{t+\\delta}\\). Bayes’ law gives\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta}) \\propto p_t(x) \\, \\exp\\left\\{ -\\frac{\\left( x_{t+\\delta} - [x + \\mu(x) \\, \\delta] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\},\\]\nwhere the exponential term corresponds to the transition of the forward diffusion for \\(\\delta \\ll 1\\). Using the 1st order approximation\n\\[p_t(x) \\approx p_t(x_{t+\\delta}) \\,\\exp\\left( \\langle \\nabla \\log p_t(x_{t+\\delta}), (x-x_{t+\\delta})\\rangle\\right),\\]\neliminating multiplicative constants and higher-order error terms, we obtain:\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta})\n\\propto\n\\exp\\left\\{ -\\frac{\\left( x - [x_{t+\\delta} - \\mu(x_{t+\\delta}) \\delta\n+ {\\color{red} \\sigma^2 \\, \\nabla \\log p_t(x_{t+\\delta}) \\, \\delta} ] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\}.\\]\nFor \\(\\delta \\ll 1\\), this is transition of the reverse diffusion\n\\[\nd\\overleftarrow{X}_t = -\\mu(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\]\nThe notation \\(B\\) is used to emphasize that this Brownian motion is distinct from the one used in the forward diffusion. The additional drift term, denoted as \\({\\color{red} \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X})}\\), is intuitive: it pushes the reverse diffusion toward regions where the forward diffusion spent a significant amount of time, i.e., where \\(p_t(dx)\\) is large. The popular “denoising diffusion models” (Ho, Jain, and Abbeel 2020) can be seen as discretizations of this backward process, employing various techniques to estimate the additional drift term from data."
  },
  {
    "objectID": "notes/reverse_and_tweedie/reverse_and_tweedie.html#denoising-score-matching-tweedie-formula",
    "href": "notes/reverse_and_tweedie/reverse_and_tweedie.html#denoising-score-matching-tweedie-formula",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "Denoising Score Matching & Tweedie formula",
    "text": "Denoising Score Matching & Tweedie formula\n\n\n\n\nMaurice Tweedie (1919–1996)\n\n\n\nThe previous section shows that a quantity often referred to as the “score” in machine learning (ML) literature, i.e. \\(\\mathcal{S}_t(x) \\equiv \\nabla_x \\log p_t(x)\\), naturally arises when a diffusion process runs backward in time. Interestingly, it is worth noting that since the beginning of times statisticians have been using the term “score” to refer to the other derivative, i.e. the derivative with respect to a model’s parameter, which is a completely different object!\nConsider a Brownian diffusion process \\(dX = \\sigma \\, dW\\) initiated from a distribution \\(\\mu(dx) = p_0(dx)\\). If this process is ran forward for a duration \\(\\delta&gt;0\\), we have:\n\\[X_{\\delta} = X_0 + \\mathcal{N}(0, \\sigma^2 \\, \\delta)\\]\nwhere \\(X_0 \\sim \\mu(dx)\\). Now, focusing on a specific sample \\(y = X_{\\delta}\\), the backward dynamics \\(d\\overleftarrow{X} = \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X}) , dt + \\sigma \\, dB\\) suggests that for sufficiently small \\(\\delta\\), the following approximation holds:\n\\[\n\\mathbb{E}[X_0 \\, | X_{\\delta} = y] \\; { \\color{\\red} \\approx} \\; y + \\nabla \\log p_{\\delta}(y) \\, \\sigma^2 \\delta.\n\\tag{3}\\]\nMaybe surprisingly, in the case of Brownian dynamics (no drift), this relationship holds exactly, even for arbitrarily large time increments \\(\\delta&gt;0\\). This observation attributed in (Efron 2011) to Maurice Tweedie (1919–1996) has a straightforward proof. Specifically, if \\(X \\sim \\mu(dx)\\), then \\(Y = X + \\mathcal{N}(0, \\Gamma^2)\\) has a density given by:\n\\[\np_Y(dy) = \\int \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx\n\\]\nwhere \\(\\rho_{\\Gamma}(z) \\propto \\exp[-z^2/(2 \\Gamma^2)]\\) is a centred Gaussian with variance \\(\\Gamma^2\\). It follows that\n\\[\n\\frac{ \\mathbb{E}[X \\, | Y = y]-y}{\\Gamma^2} = \\frac{ \\int \\left( \\frac {x-y }{\\Gamma^2} \\right)\\, \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx }{\\int \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx}\n=\n\\nabla_y \\log \\left\\{ \\int \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx \\right\\}\n\\]\nsince \\(\\nabla_y \\rho_{\\Gamma}(y-x) = (x-y)/\\Gamma^2\\). This leads to Tweedie’s formula:\n\\[\n\\mathbb{E}[X \\, | Y = y] \\; {\\color{red} =} \\; y + \\Gamma^2 \\, \\nabla \\log p_Y(y),\n\\]\nwhich is exactly the same as Equation 3 but with an equality sign: no approximation needed! Interestingly, this is what Machine-Learners often refer to as “denoising score matching” (Vincent 2011): if we have access to a large number of samples \\((X_i,Y_i)\\), where \\(X_i \\sim \\mu(dx)\\) and \\(Y_i = X_i + \\mathcal{N}(0, \\Gamma^2)\\), and fit a regression model \\(F_{\\theta}\\) by minimizing the mean-squared error \\(\\theta \\mapsto \\mathbb{E} \\|X - F_{\\theta}(Y)\\|^2\\), then Tweedie formula shows that in the limit of infinite samples and with sufficient flexibility in the regression model \\(F_{\\theta_\\star}(y) = y + \\Gamma^2 \\nabla \\log p_Y(y)\\). This allows one to estimate \\(\\nabla \\log p_Y\\) from data, which is often a good approximation of \\(\\nabla \\log \\mu\\) if the variance \\(\\Gamma^2\\) of the added noise is not too large. Indeed, things can go bad if \\(\\Gamma^2\\) is very small and the number of training data is not large, no free lunch!"
  },
  {
    "objectID": "notes/index_notes.html",
    "href": "notes/index_notes.html",
    "title": "Notes",
    "section": "",
    "text": "Notes for students, half-baked write-ups for myself, and probably other random maths/stats/ML texts. Certainly full of typos, comments welcome!\n\n\nNotes indexed by categories.\n\n\nDenoising Diffusion Probabilistic Models\n\nNoising and Reverse Ornstein-Uhlenbeck\nFrom Denoising Diffusion to ODEs\nReverse diffusions, Score & Tweedie\n\n\n\nInformation Theory\n\nReferences & Readings\nEntropy and Basic Definitions\nShannon Source Coding Theorem\nFano’s inequality\nShearer’s Lemma\n\n\n\nProbability Misc\n\nAuxiliary variable trick\nSanov’s Theorem\nWasserstein Gradients & Langevin Diffusions"
  },
  {
    "objectID": "notes/information_theory_shannon_coding/information_theory_shannon_coding.html",
    "href": "notes/information_theory_shannon_coding/information_theory_shannon_coding.html",
    "title": "Shannon Source Coding Theorem",
    "section": "",
    "text": "Claude Shannon (1916 – 2001)\n\n\n\n\nTransmission through a noisy channel\nConsider a scenario involving a “noisy channel,” where a message \\((x_1,x_2, \\ldots)\\) expressed in an alphabet \\(\\mathcal{X}\\) is transmitted before being received as a potentially different and corrupted message \\((y_1, y_2,\\ldots)\\) expressed using a potentially different alphabet \\(\\mathcal{Y}\\). One can assume that letter \\(x \\in \\mathcal{X}\\) is transformed into \\(y \\in \\mathcal{Y}\\) with probability \\(p(x \\to y)\\) so that the matrix \\(M_{x,y} = [p(x \\to y)]_{(x,y) \\in \\mathcal{X}\\times \\mathcal{Y}}\\) has rows summing-up to one, and that the “letters” of the message \\((x_1 x_2 \\ldots)\\) are transmitted one by one and independently from each other (ie. there is no memory effect in the channel).\nNow, imagine I have a text that needs to be transmitted through this channel. Assume that the text is represented using \\(N\\) bits. My goal is to encode it in a way that introduces redundancy, utilizing the alphabet \\(\\mathcal{X}\\), so that it can be transmitted through the noisy channel. Eventually, I want it to be successfully decoded, allowing the original message to be recovered with minimal errors.\n\n\n\n\nA Mathematical Theory of Communication\n\n\n\nIf transmitting each letter from the alphabet \\(\\mathcal{X}\\) takes \\(1\\) unit of time, I need to estimate the overall time it will take to transmit the entire text of \\(N\\) bits. In cases where the channel does not completely destroy the information, one can use about any non-idiotic encoding and transmit the same text multiple times to increase the chances of accurately recovering the original message. This can be achieved by employing techniques such as majority voting to decode the received messages, which are all distorted versions of the original text.\nThe transmission rate represents the inverse of the time required to transfer a single bit of information:\n\\[\n\\textrm{R = (Transmission Rate)} = \\frac{1}{\\textrm{(average time it takes to transfer one bit)}}.\n\\]\nIn other words, it takes about \\(N \\times R\\) unit of times to transfer a text of \\(N\\) bits. Moreover, the error rate refers to the percentage of errors in the decoded message, indicating the fraction of erroneous bits among the \\(N\\) decoded bits. Naturally, there exists a tradeoff, and it is evident that one can reach a vanishing error rate if one is willing to allow an arbitrarily slow transmission rate (eg. majority voting after transmitting a very large number of times the same text). For example, if \\(\\mathcal{X}= \\mathcal{Y}= \\{0,1\\}\\) and bits are flipped with probability \\(p_{\\text{flip}} \\ll 1\\), transmitting the text \\((2K+1)\\) times would lead to a transmission rate of \\(R = 1/(2K+1)\\) and an error rate approximately equal to \\(p_{\\text{flip}}^{K+1}\\).\nThe groundbreaking discovery made by Shannon is that it is possible to achieve a vanishing error rate even when transmitting at a finite transmission rate. He also managed to identify this optimal transmission rate. Shannon’s paper (Shannon 1948) is beautifully written and surprisingly readable for a text written more than 50 years ago.\n\n\nVanishing error rate: Shannon Codebooks\nLet’s imagine that we have a piece of information encoded in a variable, \\(X\\). We send \\(X\\) through a noisy channel, and at the other end we receive a somewhat distorted message, \\(Y\\). So, how much of our original information actually was transmitted? To reconstruct our original message, \\(X\\), using our received message, \\(Y\\), we require an average of \\(H(X|Y)\\) additional bits of information. On average, \\(X\\) contains \\(H(X)\\) bits of information. So, if we encode \\(H(X)\\) bits of useful information in \\(X\\), the variable \\(Y\\) that is correlated with \\(X\\) still holds \\(I(X;Y) = H(X) - H(X \\, | Y)\\) bits of that original information. The quantity \\(I(X;Y)\\) is the mutual information between the random variables \\(X\\) and \\(Y\\). In a noisy channel that transmits one “letter” at a time, the conditional probabilities \\(p(x \\rightarrow y)\\) are fixed. However, we can optimize the distribution of incoming messages. For instance, we can choose to transmit letters that are less likely to be corrupted. This discussion suggests that on average, transmitting \\(N\\) symbols through the channel can provide up to \\(N \\times C\\) bits of information, where \\(C = \\max I(X;Y)\\), the maximization being over the distribution of \\(X\\) while keeping the conditional probabilities \\(p(x \\rightarrow y)\\) fixed. It may seem that this implies a noisy channel cannot transmit information at a rate higher than \\(C\\). This hypothesis was precisely proven by Claude Shannon, who further established that this transmission rate can indeed be reached.\nTo prove that this transmission rate is achievable, Shannon’s idea was to simultaneously encode blocks of letters. To put it simply, consider the \\(2^N\\) feasible blocks \\(\\{ t^{[1]}, \\ldots, t^{[2^N]} \\}\\) of \\(N\\) binary letters. Each block \\(t^{[i]} \\in \\{0,1\\}^N\\) has \\(N\\) binary letters, \\(t^{[i]} = (t_1^{[i]}, \\ldots, t_N^{[i]})\\). Associate to each of block \\(t^{[i]} \\in \\{0,1\\}^N\\) a codeword \\(x^{[i]} \\in \\mathcal{X}^K\\) of size \\(K\\) in the alphabet \\(\\mathcal{X}\\). The set of these \\(2^N\\) codewords is usually called the codebook,\n\\[\n\\mathcal{C}= \\left\\{ x^{[1]}, x^{[2]}, \\ldots, x^{[2^N]} \\right\\} \\; \\subset \\mathcal{X}^{K}\n\\tag{1}\\]\nTo transmit a block of \\(N\\) letters from the original text, this block is first transformed into its associated codeword \\(x=(x_1, \\ldots, x_K) \\in \\mathcal{X}^K\\). This codeword is then sent through the noisy channel, resulting in a received message \\((y_1, \\ldots, y_K) \\in \\mathcal{Y}^K\\). The objective is to design a codebook with enough redundancy so that one can reconstruct the original codeword from the received message \\((y_1, \\ldots, y_K)\\): the higher the ratio \\(K/N\\), the larger the redundancy and the easier it should be to achieve this goal. The transmission rate is defined as \\(R = \\frac{N}{K}\\) since transmitting a binary text of length \\(N\\) with vanishing errors takes \\(K\\) units of time.\nFor generating the codebook in Equation 1, Shannon adopted a simple approach consisting in generating each \\(x^{[i]}_k\\) for \\(1 \\leq i \\leq 2^N\\) and \\(1 \\leq k \\leq K\\) independently at random from some (encoding) distribution \\(p_{\\text{code}}(dx)\\). The choice of this encoding distribution can be optimized at a later stage.\nConsider the codeword \\(x^{[0]} = (x^{[0]}_1, \\ldots, x^{[0]}_K)\\). After being transmitted through the noisy channel, this gives rise to a message \\(y_{\\star}\\). The codeword \\(x^{[0]}\\) can be easily recovered if \\((x^{[0]}, y_\\star)\\) is typical while all the other pairs \\((x^{[i]}, y_\\star)\\) for \\(2 \\leq i \\leq 2^N\\) are atypical. Since there are about \\(2^{K \\, H(X | Y)}\\) elements \\(x \\in \\mathcal{X}^K\\) such that \\((x, y_\\star)\\) is typical, and each codeword was chosen approximately uniformly at random within its typical set of size \\(2^{K \\, H(X)}\\), the probability for a random codeword to be atypical is about\n\\[1-2^{-K \\, [H(X) - H(X|Y)]} = 1 - 2^{-K \\, I(X;Y)}\\]\nConsequently, the probability that all the other pairs \\((x^{[i]}, y_\\star)\\) for \\(2 \\leq i \\leq 2^N\\) are atypical is\n\\[\np_{\\text{success}} = (1 - 2^{-K \\, I(X;Y)})^{2^N-1} \\approx (1 - 2^{-K \\, I(X;Y)})^{2^{KR}}.\n\\]\nThe probability \\(p_{\\text{success}} \\to 1\\) as soon as \\(R &lt; I(X;Y)\\) as \\(N \\to \\infty\\). Furthermore, remembering that one were free to optimize the encoding distribution \\(p_{\\text{code}}(dx)\\), a vanishing error rate is possible as soon as the transmission \\(R\\) rate is lower than\n\\[\n\\text{(Channel Capacity)} = C \\equiv \\max_{p_{\\text{code}}} \\; I(X;Y).\n\\]\nTo sum-up, consider \\(p_{\\mathcal{C}, \\text{success}}\\) the success rate of the codebook \\(\\mathcal{C}\\), ie. the probability that a random codeword of \\(\\mathcal{C}\\) is successfully decoded when passing through the noisy channel. The reasoning above shows that the averaged success rate \\(p_{\\text{success}} = \\left&lt; p_{\\mathcal{C}, \\text{success}} \\right&gt;\\), i.e. averaging \\(p_{\\mathcal{C}, \\text{success}}\\) over all possible codebooks \\(\\mathcal{C}\\), converges to one as long as the transmission rate is below the channel capacity \\(C\\). This means that one can find at least one codebook that works well! This reasoning is an example of the “probabilistic method”… Indeed, one also expect most random codebook to work well!\n\n\nNo vanishing error below the channel capacity\nTo demonstrate that transmission at vanishing error-rate is impossible when the transmission rate exceeds the channel capacity, \\(C\\), we can utilize Fano’s inequality.\nImagine selecting a message \\(M\\) uniformly at random within \\(\\{0,1\\}^N\\) and encode this message into the sequence \\(X=(X_1, ..., X_K) \\in \\mathcal{X}^K\\). We send \\(X\\) through a channel with capacity \\(C\\) and receive a corresponding, though somewhat distorted, signal \\(Y=(Y_1, ..., Y_K)\\). Finally, we decode this received message into \\(\\widehat{M}\\), an estimate of our original message:\n\\[\nM \\rightarrow X \\rightarrow Y \\rightarrow \\widehat{M}.\n\\]\nFano’s inequality points out that the error probability, \\(p_E = \\mathop{\\mathrm{P}}(\\widehat{M} \\neq M)\\) is such that\n\\[\n\\begin{align}\nH(M | \\widehat{M})\n&\\leq 1 + p_E \\, \\log_2(\\# \\textrm{possible values of } M)\\\\\n&= 1 + p_E \\, N\n\\end{align}\n\\]\nApplying the data-processing inequality to \\(M \\rightarrow X \\rightarrow Y \\rightarrow \\widehat{M}\\) proves:\n\\[\n\\begin{align}\nN &= H(M) = H(M | \\widehat{M}) + I(M; \\widehat{M}) \\\\\n& \\leq H(M | \\widehat{M}) + I(X; Y)\\\\\n& \\leq 1 + N \\, p_E + I(X; Y).\n\\end{align}\n\\]\nTo wrap up, recall that each received letter \\(Y_i\\) in the message (Y_1, , Y_K)$ depends solely on the corresponding letter \\(X_i\\) in the message sent through the channel. This implies that \\(I(X; Y) \\leq \\sum_{i=1}^K I(X_i; Y_i) \\leq K \\, C\\).This yields:\n\\[\nN \\leq 1 + N \\, p_E + K \\, C.\n\\]\nThis reveals that for the probability of error to go to zero, i.e. \\(p_E \\rightarrow 0\\) as \\(N \\rightarrow \\infty\\), the transmission rate \\(N/K\\) must be lower than \\(C\\).\n\n\nExperiment\nConsider the Binary Symmetric Channel (BSC) that randomly flips \\(0 \\mapsto 1\\) and \\(1 \\mapsto 0\\) with equal probability \\(0&lt;q&lt;1\\). The capacity of this channel is easily computed and equals \\(C = 1 - h_2(q)\\) where \\(h_2(q) = -[q \\, \\log_2(q) + (1-q) \\, \\log_2(1-q)]\\) is the binary entropy function: the optimal encoding distribution is\n\\[p_{\\text{code}}(0) = p_{\\text{code}}(2) = 1/2.\\]\nFor a flipping rate of \\(q=0.1\\) the channel capacity equals \\(C=0.53\\). To estimate the performance of the random Shannon codebook strategy, I chose \\(N=13\\) and several values of \\(K \\geq N\\). This means generating a random codebook \\(\\mathcal{C}= \\{x^{[1]}, \\ldots, x^{[2^N]}\\}\\) of size \\(2^{13} = 8192\\) consisting of random binary vectors of size \\(K\\). For a randomly chosen codeword \\(x^{[i]}\\), a received message \\(y_\\star\\) is generated by flipping each of the \\(K\\) coordinates of \\(x^{[i]}\\) independently with probability \\(q\\). In the BSC setting, it is easily seen that the codeword of \\(\\mathcal{C}\\) that was the most likely to have originated \\(y_{\\star}\\) is\n\\[\nx_\\star \\; = \\; \\mathop{\\mathrm{argmin}}_{x \\in \\mathcal{C}} \\; \\|x - y_\\star\\|_{L^2}.\n\\]\nThe nearest neighbor \\(x_\\star\\) can be relatively efficiently computed with a nearest-neighbor routine (eg. FAISS). The figure below reports the probability of error (i.e. “Block Error Rate”),\n\\[\n\\text{(Block Error Rate)} \\; = \\; \\mathop{\\mathrm{P}}(x_\\star \\neq x^{[i]})\n\\]\nwhen the codeword \\(x^{[i]}\\) is chosen uniformly at random within the codebook.\n\n\n\n\n\n\n\nIt can be seen that, although the error rate does go to zero for low transmission rate, the choice of \\(K = N / C\\) where \\(C\\) is the channel capacity still yields a relatively large block error rate. This indicates that the block size \\(N=13\\) is still far too low for the “law of large number” arguments presented in the previous section to kick-in. I did try for \\(N=20\\) and a codebook of \\(2^{20} \\approx 10^6\\) and the performace was still not impressive. This shows that even though the Shannon codebook approaches is an elegant construction, it is far from being practically useful. It requires a very large codebook of size \\(2^N\\) and decoding requires doing a nearest-neighbors search that can become slow as \\(N\\) increases.\n\n\n\n\n\nReferences\n\nShannon, Claude Elwood. 1948. “A Mathematical Theory of Communication.” The Bell System Technical Journal 27 (3). Nokia Bell Labs: 379–423."
  },
  {
    "objectID": "notes/information_theory_basics/information_theory_entropy.html",
    "href": "notes/information_theory_basics/information_theory_entropy.html",
    "title": "Information Theory: Entropy and Basic Definitions",
    "section": "",
    "text": "Claude Shannon (1916 – 2001)\n\n\n\n\nShannon Entropy & Compression\nIf Alice chooses a number \\(X\\) uniformly at random from the set \\(\\{1,2, \\ldots, N\\}\\), Bob can use a simple “dichotomy” strategy to ask Alice \\(\\log_2(N)\\) binary Yes/No questions and correctly identify the number. This result can be generalized to non-uniform distributions (cf. Huffman codes, and also Kraft-McMillan inequality). If Alice chooses a number \\(X\\) from \\(\\{1,2, \\ldots, N\\}\\) with probabilities \\(\\mathop{\\mathrm{P}}(X=k) = p_k\\), Bob can design a deterministic strategy to find the answer using, on average, about\n\\[\nH(X) = - \\sum_{k=1}^N p_k \\, \\log_2(p_k)\n\\tag{1}\\]\nbinary questions, ie. bits. To be more precise, there are strategies that require at most \\(H(X) + 1\\) questions on average, and none that can require less than \\(H(X)\\). Note that applying this remark to an iid sequence \\(X_{1:T} = (X_1, \\ldots, X_T)\\) and using the the fact that \\(H(X_1, \\ldots, X_T) = T \\, H(X)\\), this shows that one can exactly determining the sequence \\(X_{1:T}\\) with at most \\(T \\, H(X) + 1\\) binary questions on average. The quantity \\(H(X)\\) defined in Equation 1, known as the Shannon Entropy of the distribution \\((p_1, \\ldots, p_N)\\), also implies that there are strategies that can encode each integer \\(1 \\leq x \\leq N\\) as a binary string of length \\(L(x)\\) (i.e. with \\(L(x)\\) bits), with the expected length \\(\\mathop{\\mathrm{E}}[L(X)]\\) approximately equal to \\(H(X)\\). It is because a sequence of binary questions can be thought of as a binary tree, etc…\nThis remark can be used for compression. Imagine a very long sequence \\((X_1, \\ldots, X_T)\\) of iid samples from \\(X\\). Encoding each \\(X_i\\) with \\(L(X_i)\\) bits, one should be able to encode the resulting sequence with\n\\[\nL(X_1) + \\ldots + L(X_T) \\approx T \\, \\mathop{\\mathrm{E}}[L(X)] \\approx T \\cdot H(X)\n\\]\nbits. Can the usual zip compression algorithm do this? To test this, choose a probability distribution on \\(\\{1, \\ldots, N\\}\\), generate an iid sequence of length \\(T \\gg 1\\), compress this using the \\(\\texttt{gzip}(\\ldots)\\) command, and finally look at the size of the resulting file (in bits). I have done that a few times, with a few different values of \\(16 \\leq N \\leq 256\\) and a few random distributions on \\(\\{1, \\ldots, N\\}\\), and with \\(T = 10^6\\). The plot of size of the compressed files versus the Shannon entropy \\(H\\) looks as below:\n\n\n\n\n\n\n\nSeems like the zip-algorithm works almost optimally for compressing iid sequences.\n\n\nSequence of random variables\nNow consider a pair of discrete random variables \\((X,Y)\\). If Alice draws samples from this pair of rvs, one can ask \\(H(X,Y)\\) binary questions on average to exactly find out these values. To do that, one can ask \\(H(X)\\) questions to estimate \\(X\\), and once \\(X=x\\) is estimated, one can then ask about \\(H(Y|X=x) = -\\sum_y \\mathop{\\mathrm{P}}(Y=y|X=x) \\, \\log_2(\\mathop{\\mathrm{P}}(Y=y|X=x))\\) to estimate \\(Y\\). This strategy requires on average \\(H(X) + \\sum_x \\mathop{\\mathrm{P}}(X=x) \\, H(Y|X=x)\\) binary questions and is actually optimal, showing that\n\\[\nH(X,Y) = H(X) + H(Y | X)\n\\tag{2}\\]\nwhere we have defined \\(H(Y | X) = \\sum_x \\mathop{\\mathrm{P}}(X=x) \\, H(Y|X=x)\\).\nIndeed, one can generalize these concepts to more than two random variables. Iterating Equation 2 shows that the trajectory \\(X_{1:T} \\equiv (X_1, \\ldots, X_N)\\) of a stationary ergodic Markov chain can be estimated on average with \\(H(X_{1:T})\\) binary questions where\n\\[\n\\begin{align}\nH(X_{1:T})\n&= H(X_1) + H(X_2|X_1) + \\ldots + H(X_{T} | X_{t-1})\\\\\n&\\approx T \\, H(X_{k+1} | X_k)\\\\\n&= - T \\, \\sum_x \\pi(x) \\, \\sum_{y} p(x \\to y) \\, \\log_2[p(x \\to y)].\n\\end{align}\n\\]\nHere, \\(\\pi(dx)\\) is the equilibrium distribution of the Markov chain and \\(p(x \\to y)\\) are the transition probabilities.\nCan \\(\\texttt{gzip}(\\ldots)\\) compress Markovian trajectories and roughly achieve that level of compression? Indeed, one can test this by generating random Markov transition matrices (that are, with very high probability, ergodic with respect to an equilibrium distribution \\(\\pi\\)). Doing this with trajectories of length \\(10^4\\) (ie. quite short because it is quite slow to) on \\(\\{1, \\ldots, N\\}\\) with \\(2 \\leq N \\leq 64\\), one get the following results:\n\n\n\n\n\n\n\nIn red is the entropy estimated without using the Markovian structure and assuming that the \\(X_i\\) are iid samples. One can clearly see that the dependencies between successive samples are exploited for better compression. Nevertheless, the optimal compression rate given by the Shannon entropy is not reached. Indeed, \\(\\texttt{gzip}(\\ldots)\\) is not an optimal algorithm – it cannot even compress well enough the sequence \\((1,2,3,1,2,3,1,2,3,\\ldots)\\)!\n\n\nAsymptotic Equipartition Property (AEP)\nThe AEP is simple remark that gives a convenient way of reasoning about long sequences random variables \\(X_{1:T} = (X_1, \\ldots, X_T)\\) with \\(T \\gg 1\\). For example, assuming that the random variables \\(X_i\\) are independent and identically distributed as the random variable \\(X\\), the law of large numbers (LLN) gives that\n\\[\n-\\frac{1}{T} \\, \\log_2(X_{1:T}) = -\\frac{1}{T} \\, \\sum \\log_2 p(X_i) \\approx H(X).\n\\]\nThis means that any “typical” sequence has a probability about \\(2^{-T \\, H(X)}\\) of occurring, which also means that there are about \\(2^{T \\, H(X)}\\) such “typical” sequences. Indeed, one could use the LLN for Markov chains and obtain a similar statement for Markovian sequences. All this can be made rigorous with large deviation theory, for example. This also establishes the link with the “statistical physics” definition of entropy as the logarithm of the number of configurations… Set of the type\n\\[\nA_{\\varepsilon} = \\left\\{ x_{1:T} \\; : \\; \\left| -\\frac{1}{T} \\, \\log_2(x_{1:T}) - H(X)\\right| &lt; \\varepsilon\\right\\}\n\\]\nare usually called typical set: for any \\(\\varepsilon&gt; 0\\), the probability of \\(X_{1:T}\\) to belongs to \\(A_{\\varepsilon}\\) goes to one as \\(T \\to \\infty\\). For these reasons, it is often a good heuristic to think of a draw of \\((X_1, \\ldots, X_T)\\) as a uniformly distributed on the associated typical set. For example, if \\((X_1, \\ldots, X_N)\\) are \\(N\\) iid draws from a Bernoulli distribution with \\(\\mathop{\\mathrm{P}}(X=1) = 1-\\mathop{\\mathrm{P}}(X=0) =p\\), the set \\(A \\subset \\{0,1\\}^N\\) of sequences such that \\(x_1 + \\ldots + x_N = Np\\) has \\(\\binom{N}{Np} \\approx 2^{N \\, h_2(q)}\\) elements where \\(h_2(q) = -[q \\, \\log_2(q) + (1-q) \\, \\log_2(q)]\\) is the entropy of a \\(\\text{Bern}(q)\\) random variable.\n\n\nMutual information\nConsider a pair of random variables \\((X,Y)\\). Assuming that \\(X\\) stores (on average) \\(H(X)\\) bits of useful information, how much of it is still contained in \\(Y\\)? If \\(Y\\) is independent from \\(X\\), everything has been lost and, on the contrary, if \\(X=Y\\), nothing has been lost. If one knows \\(Y\\), one needs on average \\(H(X|Y)\\) binary questions (ie. bits of information) in order to determine \\(X\\) certainly and recovered all the information contained in \\(X\\). This means that the knowledge of \\(Y\\) still contains \\(H(X) - H(X|Y)\\) useful bits of information! This quantity is called the mutual information of the two random variable \\(X\\) and \\(Y\\), and it has the good taste of being symmetric:\n\\[\n\\begin{align}\nI(X;Y) &= H(X) - H(X|Y) \\\\\n&= H(X) + H(Y) - H(X,Y) \\\\\n&= H(Y;X).\n\\end{align}\n\\]\nNaturally, one can define conditional version of it by setting \\(I(X;Y \\, |Z) = \\sum_{z} \\mathop{\\mathrm{P}}(Z=z) \\, I(X_z | Y_z)\\) where \\((X_z, Y_z)\\) has the law of \\((X,Z)\\) conditioned on \\(Z=z\\). Since \\(I(X;Y \\,|Z)\\) is the reduction in uncertainty of \\(X\\) due to \\(Y\\) when \\(Z\\) is given, there are indeed situations when \\(I(X;Y \\, | Z)\\) is larger than \\(I(X;Y)\\) – it is to be contrasted to the intuitive inequality \\(H(X|Z) \\leq H(X)\\), which is indeed true. A standard such examples is when \\(X\\) and \\(Y\\) are independent \\(\\text{Bern}(1/2)\\) random variables and \\(Z = X+Y\\): a short computation gives that \\(I(X;Y \\, | Z) = 1/2\\) while, indeed, \\(I(X;Y) = 0\\). This definition of conditional mutual information leads to a chain-rule property,\n\\[\nI(X; (Y_1,Y_2)) = I(X;Y_1) + I(X;Y_2 | Y_3),\n\\]\nwhich can indeed be generalized to any number of variables. Furthermore, if the \\(Y_i\\) are conditionally independent given \\(X\\) (eg. if \\(X=(X_1, \\ldots, X_T)\\) and \\(Y_i\\) only depend on \\(X_i\\)), then the sub-additivity of the entropy readily gives that\n\\[\nI(X; (Y_1, \\ldots, Y_N)) \\leq \\sum_{i=1}^N I(X; Y_i).\n\\]\nImportantly, algebra shows that \\(I(X;Y)\\) can also be expressed as the Kullback-Leibler divergence between the joint distribution \\(\\mathop{\\mathrm{P}}_{(X,Y)}\\) and the product of the marginals \\(\\mathop{\\mathrm{P}}_X \\otimes \\mathop{\\mathrm{P}}_Y\\),\n\\[\nI(X;Y) \\; = \\;\n\\mathop{\\mathrm{D_{\\text{KL}}}} {\\left(  (X,Y) \\, \\| \\, X \\otimes Y \\right)} .\n\\]\nThis diagram from (MacKay 2003) nicely illustrate the different fundamental quantities \\(H(X)\\) and \\(H(X,Y)\\) and \\(H(Y|X)\\) and \\(I(X;Y)\\) and \\(H(X,Y)\\):\n\n\n\n\nFrom: Information Theory, Inference, and Learning Algorithms\n\n\n\nNaturally, if one considers three random variables \\(X \\mapsto Y \\mapsto Z\\) forming a “Markov chain”, we have the so-called data-processing inequality,\n\\[\nI(X;Z) \\leq I(X;Y)\n\\qquad \\text{and} \\qquad\nI(X;Z) \\leq I(Y;Z).\n\\]\nThe first inequality is clear since all the useful information contained in \\(Z\\) must be coming from \\(Y\\), and \\(Y\\) only contains \\(I(X;Y)\\) bits about \\(X\\). For the second inequality, note that if \\(Z\\) contains \\(I(Y;Z)\\) bits about \\(Y\\), and \\(Y\\) contains \\(H(Y;X)\\) bits about \\(X\\), then \\(Z\\) cannot contain more than \\(I(Y;Z)\\) bits of \\(X\\):\n\n\n\n\nData Processing Inequality\n\n\n\n\n\n\n\n\nReferences\n\nMacKay, David JC. 2003. Information Theory, Inference and Learning Algorithms. Cambridge university press."
  },
  {
    "objectID": "notes/index_notes_as_list.html",
    "href": "notes/index_notes_as_list.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n16-10-2023\n\n\nWasserstein Gradients & Langevin Diffusions\n\n\ndiffusion\n\n\n\n\n09-10-2023\n\n\nSanov’s Theorem\n\n\nLargeDeviation\n\n\n\n\n03-10-2023\n\n\nAuxiliary variable trick\n\n\nauxiliary-variable\n\n\n\n\n02-10-2023\n\n\nShearer’s lemma\n\n\ninfoTheory\n\n\n\n\n30-09-2023\n\n\nInformation Theory: References and Readings\n\n\ninfoTheory\n\n\n\n\n26-09-2023\n\n\nShannon Source Coding Theorem\n\n\ninfoTheory\n\n\n\n\n23-09-2023\n\n\nInformation Theory: Fano’s inequality\n\n\ninfoTheory\n\n\n\n\n23-09-2023\n\n\nInformation Theory: Entropy and Basic Definitions\n\n\ninfoTheory\n\n\n\n\n02-07-2023\n\n\nFrom Denoising Diffusion to ODEs\n\n\nDDPM,score\n\n\n\n\n02-07-2023\n\n\nDenoising Diffusion Probabilistic Models (DDPM)\n\n\nDDPM,score\n\n\n\n\n12-06-2023\n\n\nReverse diffusions, Score & Tweedie\n\n\nSDE,score\n\n\n\n\n01-01-2023\n\n\nNotes\n\n\nindex\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/DDPM/DDPM.html",
    "href": "notes/DDPM/DDPM.html",
    "title": "Denoising Diffusion Probabilistic Models (DDPM)",
    "section": "",
    "text": "Setting & Goals\nConsider \\(N\\) samples \\(\\mathcal{D}\\equiv \\{x_i\\}_{i=1}^N\\) in \\(\\mathbb{R}^D\\) from an unknown data distribution \\(\\pi_{\\mathrm{data}}(dx)\\). We would like to build an mechanism that can generate other samples from this data distribution. Implicitly, this means that we will be fitting a statistical model to this finite set of samples and have an algorithmic procedure to generate samples from the fitted probabilistic model. This type of models that directly define a stochastic procedure that generates data are called implicit probabilistic models in the ML literature.\n\n\nOrnstein–Uhlenbeck Noising process\nDDPMs work as follows. Consider a diffusion process \\(\\{ X_t \\}_{t=0}^T\\) that starts from the data distribution \\(p_0(dx) \\equiv \\pi_{\\mathrm{data}}(dx)\\) at time \\(t=0\\). The notation \\(p_t(dx)\\) refers to the marginal distribution of the diffusion at time \\(0 \\leq t \\leq T\\). Assume furthermore that at time \\(t=T\\), the marginal distribution is (very close to) a reference distribution \\(p_T(dx) = \\pi_{\\mathrm{ref}}(dx)\\) that is straightforward to sample from. Typically, \\(\\pi_{\\mathrm{ref}}(dx)\\) is an isotropic Gaussian distribution. This diffusion process is often called the noising process since it transform the data distribution into a reference measure that can be thought of as “pure noise”. It is often chosen as an Ornstein–Uhlenbeck (OU) diffusion,\n\\[\ndX = - \\frac12 X \\, dt + dW.\n\\tag{1}\\]\nThis diffusion is reversible with respect to, and quickly converges to, the reference distribution \\(\\pi_{\\mathrm{ref}} = \\mathcal{N}(0, I)\\) and has the good taste of having simple transition densities: the law of \\(X_{t+s}\\) given that \\(X_t = x_t\\) is the same as \\(e^{-s/2} x_t + \\sqrt{1-e^{-s}} \\, \\mathbf{n}\\), which we write as\n\\[\n\\alpha_s x + \\sigma_s \\, \\mathbf{n}\\qquad \\text{with} \\qquad\n\\left\\{\n\\begin{aligned}\n\\alpha_s &= \\sqrt{1-\\sigma_s^2}\\\\\n\\sigma^2_s &= 1-e^{-s}\n\\end{aligned}\n\\right.\n\\]\nfor isotropic Gaussian noise term \\(\\mathbf{n}\\sim \\pi_{\\mathrm{ref}} = \\mathcal{N}(0,I)\\). We have:\n\\[\nF(s,x,y) \\equiv \\mathop{\\mathrm{P}}(X_{t+s} \\in dy \\, | \\, X_t = x )\n\\; \\propto \\;\n\\exp {\\left\\{ -\\frac{(y - \\alpha_s \\, x)^2}{2 \\, \\sigma^2_s} \\right\\}} .\n\\tag{2}\\]\nwhere the notation \\(F(s,x,y)\\) designates the forward transition from \\(x\\) to \\(y\\) in “\\(s\\)” amount of time. This also means that one can directly generate samples from \\(p_t(dx)\\) by first choosing a data samples \\(x_i\\) from the data distribution \\(p_{\\mathrm{data}} \\equiv p_0\\) and blend it with noise by setting \\(x_i^{(t)} = \\alpha_t \\, x_i + \\sigma_t \\, \\mathbf{n}\\).\n\n\nThe reverse diffusion\nIn order to generate samples from the data distribution, the DDPM strategy consists in sampling from the Gaussian reference measure \\(\\pi_{\\mathrm{ref}}\\) at time \\(t=T\\) and simulate the OU process backward in time. In other words, one would like to simulate from the reverse process \\(\\overleftarrow{X}_t\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIn other words, the reverse process is distributed as \\(\\overleftarrow{X}_0 \\sim \\pi_{\\mathrm{ref}}\\) at time \\(t=0\\) and, crucially, we have that \\(\\overleftarrow{X}_T \\sim \\pi_{\\mathrm{data}}\\). Furthermore, and as explained in this note, the reverse diffusion follows the dynamics\n\\[\nd\\overleftarrow{X}_t = {\\color{red} + }\\frac12 \\overleftarrow{X}_t \\, dt\n\\; {\\color{red} + \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\;\n+ dB\n\\tag{3}\\]\nwhere \\(B\\) is another Wiener process. I have used the notation \\(B\\) to emphasize that there is no link between this Wiener process and the one used to simulate the forward process. Note that if the initial data distribution \\(p_0 \\equiv \\pi_{\\mathrm{data}}\\) were equal to the reference measure \\(\\pi_{\\mathrm{ref}}\\), i.e. \\(p_0 = p_T = \\pi_{\\mathrm{ref}}\\) then it is easy to see that the reverse diffusion would have exactly teh same dynamics as the forward diffusion. In order to simulate the reverse diffusion, one needs to be able to estimate the new term \\({\\color{red}\\nabla \\log p_{T-t}(x)}\\) called the score. If one can estimate the score, it is straightforward to simulate the reverse process \\(\\overleftarrow{X}_t\\) all the way to \\(t=T\\) and obtain samples from the data distribution.\n\n\nDenoising to estimating the score\nIn practice, the score is unknown and one has to build an approximation of it\n\\[\\mathcal{S}(t,x) \\; \\approx \\; \\nabla_x \\log p_t(x).\\]\nThe approximate score \\(\\mathcal{S}(t,x)\\) is often parametrized by a neural network. Since the forward transitions are available and\n\\[\n\\log p_t(x) \\; = \\; \\log \\int \\; F(t, x_0, x)\\; \\pi_{\\mathrm{data}}(d x_0)\n\\]\nthe analytical expression of \\(F(t, x_0, x)\\) given in Equation 2 readily gives that\n\\[\n\\nabla_x \\log p_t(x) \\; = \\; -\\frac{x - \\alpha_t \\, \\widehat{x}_0(x,t)}{\\sigma_t^2}\n\\tag{4}\\]\nwhere \\(\\widehat{x}_0(x,t)\\) is a “denoising” estimate of the initial position \\(x_0\\) given a noisy estimate \\(X_t=x\\) at time \\(t\\),\n\\[\n\\widehat{x}_0(x,t) \\; = \\; \\mathop{\\mathrm{E}}[X_0  \\; \\mid \\; X_t = x].\n\\]\nFor simplifying notation, I will often write \\(\\widehat{x}_0(x_t, t)\\) as \\(\\widehat{x}_0(x_t)\\) when it is clear that \\(x_t\\) is a sample obtained at time \\(0 \\leq t \\leq T\\). Equation 4 means that to estimate the score, one only needs to train a denoising function\n\\[\n\\widehat{x}_0(\\cdots): [0,T] \\times \\mathbb{R}^d \\to \\mathbb{R}^d.\n\\]\nIt is a simple regression problem: take a bunch of pairs \\((X_0, X_t)\\) that can be generated as\n\\[\nX_0 \\sim \\pi_{\\mathrm{data}}\n\\qquad \\textrm{and} \\qquad\nX_t = \\alpha_t X_0 + \\sigma_t \\, \\mathbf{n}\n\\]\nwith \\(\\mathbf{n}\\sim \\mathcal{N}(0,I)\\) and minimize the Mean Squared Error (MSE) loss, i.e.\n\\[\n\\mathop{\\mathrm{E}}\\|X_0 - \\widehat{x}_0(t, X_t)\\|^2,\n\\]\nwith stochastic gradient descent or any other stochastic optimization procedure. The score is then defined as\n\\[\n\\mathcal{S}(t,x) \\; = \\; -\\frac{x - \\alpha_t \\, \\widehat{x}_0(t,x)}{\\sigma^2_t}.\n\\]\n\n\nDenoiser: practical parametrization and training\nIn practice, it may not be efficient, or stable, to try to directly parametrize the denoiser \\(\\widehat{x}_0(\\cdots)\\) with a neural network and simply descend the loss\n\\[\n\\mathop{\\mathrm{E}}\\|X_0 - \\widehat{x}_0(t, X_t)\\|^2.\n\\]\nFor example, for \\(t \\approx 0\\), we have that \\(\\widehat{x}_0(t,X_t) \\approx X_t \\approx X_0\\) so that it is very easy to reconstruct \\(X_0\\) from \\(X_t\\). On the contrary, for large \\(t\\), there is almost no information contained within \\(X_t\\) to reconstruct \\(X_0\\). This means that the typical value of the loss depends widely on \\(t\\), which makes it difficult to optimize: with this parametrization, since large values of the loss will be typically concentrated to large values of \\(t\\), the denoiser will not be accurate for \\(t \\approx 0\\), leading to sub-optimal results. Since \\(X_t = \\alpha_t \\, X_0 + \\sigma_t \\, \\mathbf{n}\\), one can defined the Signal-to-Noise-Ratio as\n\\[\\mathrm{SNR}(t) = \\frac{\\alpha_t}{\\sigma_t}\\]\nand, in order to normalize by the reconstruction difficulty, it makes more sense to train a denoiser by minimizing the loss:\n\\[\n\\mathop{\\mathrm{E}} {\\left[  \\mathrm{SNR}^2(t) \\times \\|X_0 - \\widehat{x}_0(t, X_t)\\|^2  \\right]} .\n\\]\nIt turns out that it is entirely equivalent to minimizing the loss\n\\[\n\\mathop{\\mathrm{E}} {\\left[  \\| \\mathbf{n}- \\widehat{\\mathbf{n}}(t, X_t)\\|^2  \\right]} .\n\\]\nwhere \\(X_t = \\alpha_t \\, X_0 + \\sigma_t \\, \\mathbf{n}\\) while the denoiser \\(\\widehat{x}_0(\\ldots)\\) and noise estimator \\(\\widehat{\\mathbf{n}}(\\ldots)\\) are parametrized so that\n\\[\nX_t = \\alpha_t \\, \\widehat{x}_0(t, X_t) + \\sigma_t \\, \\widehat{\\mathbf{n}}(t,X_t).\n\\]\nThat is one of the reasons why most of the papers on DDPM are parametrizing the denoiser \\(\\widehat{x}_0(\\ldots)\\) by building instead a “noise estimator” \\(\\widehat{\\mathbf{n}}(\\ldots)\\) with a neural network and setting\n\\[\n\\widehat{x}_0(t,X_t) = \\frac{X_t - \\sigma_t \\, \\widehat{\\mathbf{n}}(t,X_T)}{\\alpha_t}.\n\\]\nSince \\(\\alpha_t \\to 1\\) and \\(\\sigma_t \\to 0\\) for \\(t \\ll 1\\), this also implicitly ensures that \\(\\widehat{x}_0(t,X_t) \\approx X_t\\) for \\(t \\ll 1\\), as required.\n\n\n\nThe “denoising” diffusion\nOnce the denoiser \\(\\widehat{x}_0(\\ldots)\\) has been trained, the reverse diffusion defined \\(\\overleftarrow{X}_s = X_{T-s}\\) as to be simulated. Plugging Equation 4 back in the expression of the dynamics of the reverse diffusion shows that\n\\[\nd\\overleftarrow{X}_s =\n-\\frac12 \\, \\frac{1}{\\tanh((T-s)/2)} \\,\n{\\left(  \\overleftarrow{X}_s - \\frac{\\widehat{x}_0(\\overleftarrow{X}_s)}{\\cosh((T-s)/2)}  \\right)}\n\\; + \\;\ndB\n\\tag{5}\\]\nThis dynamics is intuitive: as \\(s \\to T\\) we have \\(\\cosh((T-s)/2) \\to 1\\) and \\(\\varepsilon^2 \\equiv \\tanh((T-s)/2) \\sim (T-s)/2 \\to 0\\) so that the dynamics is similar to\n\\[\nd\\overleftarrow{X}_s =\n-\\frac12 \\, \\frac{1}{\\varepsilon^2} \\,\n{\\left(  \\overleftarrow{X}_s - \\widehat{x}_0 \\right)}\n\\; + \\;\ndB\n\\]\nwhich is OU process that converges quickly, i.e. on time-scale of order \\(\\mathcal{O}(\\varepsilon^2)\\), towards a Gaussian distribution with mean \\(\\widehat{x}_0\\) (i.e. the denoised estimate) and variance \\(\\varepsilon^2\\).\nTo discretize the reverse dynamics Equation 5 on a small interval \\([\\overline{s}, \\overline{s}+\\delta]\\), one can for example consider the slightly simplified (linear) dynamics\n\\[\nd\\overleftarrow{X}_s =\n-\\frac12 \\, \\frac{1}{\\tanh((T-s)/2)} \\,\n{\\left(  \\overleftarrow{X}_s - \\mu \\right)} .\n\\; + \\;\ndB\n\\]\nHere, \\(\\mu = \\widehat{x}_0(\\overleftarrow{X}_{\\overline{s}}) / \\cosh((T-\\overline{s})/2)\\) with \\(\\overleftarrow{X}_{\\overline{s}} = \\overleftarrow{x}_{\\overline{s}}\\). Algebra gives that, conditioned upon \\(\\overleftarrow{X}_{\\overline{s}} = \\overleftarrow{x}_{\\overline{s}}\\), we have\n\\[\n\\left\\{\n\\begin{aligned}\n\\mathop{\\mathrm{E}}[ \\overleftarrow{X}_{\\overline{s} + \\delta} ]\n\\quad &= \\quad \\mu + \\lambda \\, (\\overleftarrow{x}_{\\overline{s}} - \\mu)\\\\\n\\mathop{\\mathrm{Var}}[ \\overleftarrow{X}_{\\overline{s} + \\delta} ]\n\\quad &= \\quad\n\\tanh {\\left( \\frac{T-\\overline{s}-\\delta}{2} \\right)}  \\, (1-\\lambda^2)\n\\end{aligned}\n\\right.\n\\]\nwhere the coefficient \\(0&lt;\\lambda&lt;1\\) is given by\n\\[\n\\lambda = \\frac{\\sinh(T-\\overline{s}-\\delta)}{\\sinh(T-\\overline{s})}.\n\\]\nThis discretization is more stable and efficient than a naive Euler-Maruyama approach since the drift term can get large as \\(s \\to T\\). WIth the above discretization, one can easily simulate the reverse diffusion on \\([0,T]\\) and generate approximate samples from \\(\\pi_{\\mathrm{data}}\\).\nIn the animation below, the method described in these notes was used. This very small example trains in less than a minute on an old laptop without a GPU. The noise estimator was parametrized with an MLP with \\(\\mathrm{elu}(\\ldots)\\) non-linearity and two hidden-layers with size \\(H=128\\). It was very useful to add a few Fourier features as input of the network, I could not make this work properly without.\n\n\nVideo\n\n\nThe literature on DDPM is enormous and still growing!"
  },
  {
    "objectID": "teaching/teaching.html",
    "href": "teaching/teaching.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "DSA4212: Large Scale optimization, NUS (2017 – 2022)\nYSC4236: Optimization for Large Scale Inference, Yale-NUS, (2022)\nYSC3252: Computational Statistics, Yale-NUS, (2022)\nDSA5843: Learning from Data: Neural Networks, NUS (2020–2021)\nST3233: Applied Times Series, NUS, (2016, 2019, 2021)\nST4240: Data Mining, NUS, (2014–2017)\nST4231: Computationally Intensive Statistics, NUS (2014–2015)\nMultivariable Calculus II, Chennai (CMI), (2018)"
  },
  {
    "objectID": "teaching/teaching.html#semester-long-courses",
    "href": "teaching/teaching.html#semester-long-courses",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "DSA4212: Large Scale optimization, NUS (2017 – 2022)\nYSC4236: Optimization for Large Scale Inference, Yale-NUS, (2022)\nYSC3252: Computational Statistics, Yale-NUS, (2022)\nDSA5843: Learning from Data: Neural Networks, NUS (2020–2021)\nST3233: Applied Times Series, NUS, (2016, 2019, 2021)\nST4240: Data Mining, NUS, (2014–2017)\nST4231: Computationally Intensive Statistics, NUS (2014–2015)\nMultivariable Calculus II, Chennai (CMI), (2018)"
  },
  {
    "objectID": "teaching/teaching.html#lecture-series",
    "href": "teaching/teaching.html#lecture-series",
    "title": "Alexandre Thiéry",
    "section": "Lecture Series:",
    "text": "Lecture Series:\n\nSummer school on Bayesian statistics (5h Lecture), VIASM July 2023"
  },
  {
    "objectID": "people/index_people.html",
    "href": "people/index_people.html",
    "title": "Research team",
    "section": "",
    "text": "Woon Hao Xuan PhD (2023–Now): Uncertainty Quantification for High-Dimensional dynamical systems."
  },
  {
    "objectID": "people/index_people.html#phdpostdoc-co-supervision",
    "href": "people/index_people.html#phdpostdoc-co-supervision",
    "title": "Research team",
    "section": "",
    "text": "Woon Hao Xuan PhD (2023–Now): Uncertainty Quantification for High-Dimensional dynamical systems."
  },
  {
    "objectID": "people/index_people.html#alumni",
    "href": "people/index_people.html#alumni",
    "title": "Research team",
    "section": "Alumni",
    "text": "Alumni\n\nChristopher Hendra PhD (2019-2022): Genomics, Nanopore sequencing – co-supervised by Jonathan Göke (GIS). Chris is now a Senior Scientist at MSD.\nKhai Xiang Au PhD (2019-2023): PDE constrained Bayesian inverse problems, uncertainty quanification, variational inference. Khai is now working as a data scientist at American Express.\nRahul Rahaman PhD(2018-2022): Bayesian inference, Uncertainty Quantification, Deep-Learning. Rahul is now an Applied Research Scientist at Amazon.\nAtin Ghosh PhD (2017-2021): Deep Learning for Glaucoma Understanding, representation learning, generative models, semi-supervised learning. Atin is now an Applied Research Scientist at Amazon.\nSe-In Jang Research Fellow (2019-2021): Computer vision and application to ophthalmology. Se-In is now a research fellow in the Center for Advanced Medical Computing and Analysis and the Gordon Center for Medical Imaging, Massachusetts General Hospital (MGH) and Harvard Medical School.\nAxel Finke Research Fellow (2017-2020): Sequential Monte Carlo, MCMC, algorithms for high-dimensional problems; applications in finance, economics, ecology and molecular biology. Axel is now an assistant professor at Loughborough University (UK).\nZuozhu Liu Research Fellow (2019-2020): Bayesian inference, deep generative models, 3D vision and medical applications. Zuozhu is now Assistant Professor at the Zhejiang University-University of Illinois at Urbana-Champaign Institute.\nMatt Graham Research Fellow (2017-2020): Approximate inference methods, MCMC, approximate Bayesian computation, numerical simulation. Matt is now a research data scientist in the Advanced Research Computing Centre at University College London.\nWillem van den Boom Research Fellow (2018-2019): Willem is now a Senior Research Fellow in the Division of Biomedical Data Science at the Yong Loo Lin School of Medicine of the National University of Singapore.\nKhai Sing Chin Research Associate (2017-2018): Khai Sing is now working in the finance industry.\nDeborshee Sen PhD (NUS, 2014-2017). Winner of the 2017 DSAP NUS best researcher award. After a postdoc at Duke University and a position as an assistant Professor at Bath University, Deborshee now works as a Research Scientist at Amazon.\nDaniel Paulin Research Fellow (NUS, 2014-2015). Daniel is now an Assistant Professor at the School of Mathematics at the University of Edinburgh.\nEge Muzaffer PhD (NUS, 2016): Bayesian inverse problems and Sequential Monte Carlo. Ege is now a Machine Learning EngineerMachine Learning Engineer Ubisoft RedLynx"
  },
  {
    "objectID": "people/index_people.html#msc",
    "href": "people/index_people.html#msc",
    "title": "Research team",
    "section": "MSc",
    "text": "MSc\n\nQuang Huy Nguyen MSc (2018-2019): representation learning, robust models for image segmentation.\nAugustin Hoff (NUS, 2016-2017). Deep Neural Networks and Features Extraction. Augustin is now a Senior Data Scientist at MAIF.\nMajdi Rabia (NUS, 2016-2017). Numerical Method for Backward-Stochastic-Differential-Equations. Majdi is Co-founder and CTO @Fairphonic.\nBenjamin Scellier (NUS, 2015). Deep Learning. After his MSc at NUS, Benjamin joined Yoshua Bengio’s Group as a PhD. He is now a principal research scientist at Rain"
  },
  {
    "objectID": "jobs/2024_koopman/2024_koopman.html",
    "href": "jobs/2024_koopman/2024_koopman.html",
    "title": "Research Fellow positions: Data-Assimilation",
    "section": "",
    "text": "Two Postdoctoral Research Fellow positions are currently open in the Department of Statistics & Data Science at the National University of Singapore (NUS). These are two-year contracts, with the possibility of renewal upon review.\nWhile the research themes are flexible, our primary interest lies in the design and analysis of statistical methods for data-assimilation of high-dimensional time-series data. We anticipate applying tools from reinforcement learning, Koopman operators, and unsupervised representation learning, supplementing the more conventional techniques in state-space modeling (particle filters, EnKF, variational approaches, etc…)\n\nDetails:\n\nStart Date: Immediate\nDuration: 2 years (Renewable for up to an additional year)\nSalary Range: S$ 70k-100k / Year, plus benefits (Health Insurance / etc…)\nTravel: Generous budget for attending conferences\n\n\n\nRequirements:\nIdeal candidates should have:\n\na Ph.D. in Statistics, Computer Science, Physics, or a closely related field.\n\nstrong experience in Python programming.\n\nexperience with at least one deep-learning framework\na demonstrated record of academic publications\n\nresearch experience that aligns with the core focus of the research programme.\n\n\n\nContact:\nQueries about this post should be addressed to Alex Thiery at a.h.thiery@nus.edu.sg. We highly encourage prospective applicants to get in touch informally to discuss further details about the position first. This project is in collaboration with Jeremy Heng.\nThe positions will remain open until we find the right candidates."
  },
  {
    "objectID": "notes/information_theory_fano/information_theory_fano.html",
    "href": "notes/information_theory_fano/information_theory_fano.html",
    "title": "Information Theory: Fano’s inequality",
    "section": "",
    "text": "Robert Fano (1917 – 2016)\n\n\n\n\nFano’s inequality\nConsider a three random variables forming a Markov chain,\n\\[\nX \\mapsto Y \\mapsto \\widehat{X}\n\\tag{1}\\]\nin the sense that \\(Y = \\textrm{function}(X, \\text{noise})\\) and \\(Z = \\textrm{function}(Y, \\text{noise})\\). Typical situations include:\n\nWe select a parameter \\(\\theta\\) for a probabilistic model \\(\\mathop{\\mathrm{P}}_{\\theta}\\). Afterward, we collect data \\(X\\) from this model, and our goal is to estimate the parameter \\(\\theta\\) solely from the data \\(X\\).\nWe generate data \\(X\\), compress this data into \\(X_{\\text{zip}}\\), and then attempt to recover the original data \\(X\\) as closely as we can.\n\nSince each step in Equation 1 destroys some information (eg. data processing), it is important to measure how accurately \\(\\widehat{X}\\) estimates the initial input, \\(X\\). In other words, we want to know how much more information (expressed as ‘bits’) we need to reconstruct \\(X\\) using knowledge of \\(\\widehat{X}\\) alone, i.e. we would like to upper-bound \\(H(X \\, | \\widehat{X})\\). For this purpose, imagine an “error” variable \\(E\\) that indicates whether \\(\\widehat{X}\\) perfectly matches \\(X\\),\n\\[\nE = \\mathbf{1} {\\left( \\widehat{X} \\neq X \\right)} .\n\\]\nThe probability of error is \\(p_E = \\mathop{\\mathrm{P}}(\\widehat{X} \\neq X)\\) and \\(E = \\text{Bern}(p_E)\\). To estimate \\(X\\) from \\(\\widehat{X}\\), we can start by learning if \\(\\widehat{X}\\) equals \\(X\\), which costs us \\(H(E | \\widehat{X}) \\leq H(E) = h_2(p_E)\\) ‘bits’ of information. If it turns out that \\(E=0\\), we are done asking. If we find that \\(E=1\\), however, we need to ask additional \\(H(X | \\widehat{X}, E)\\) questions. Crucially, \\(H(X | \\widehat{X}, E) \\leq H(X)\\), but also \\(H(X | \\widehat{X}, E) \\leq \\log_2(|\\mathcal{X}|-1)\\) since \\(X\\) can take any value in \\(\\mathcal{X}\\) except \\(\\widehat{X}\\) when \\(E=1\\). Writing this reasoning quantitatively gives Fano’s inequality:\n\\[\n\\begin{align}\nH(X | \\widehat{X})\n&\\leq h_2(p_E) + p_E \\, \\log_2(|\\mathcal{X}|-1).\n\\end{align}\n\\tag{2}\\]\nApparently, this inequality was first derived by Robert Fano in the 50s while teaching a Ph.D. seminar at MIT. In words: a large \\(H(X | \\widehat{X})\\) means that \\(\\widehat{X}\\) offers insufficient information about \\(X\\), and as a result, the probability of error \\(p_E\\) must be high.\n\n\nApplications:\n\nConverse of Shannon’s coding theorem"
  },
  {
    "objectID": "notes/information_theory_references/information_theory_entropy.html",
    "href": "notes/information_theory_references/information_theory_entropy.html",
    "title": "Information Theory: Entropy and Basic Definitions",
    "section": "",
    "text": "Claude Shannon (1916 – 2001)\n\n\n\n\nShannon Entropy & Compression\nIf Alice chooses a number \\(X\\) uniformly at random from the set \\(\\{1,2, \\ldots, N\\}\\), Bob can use a simple “dichotomy” strategy to ask Alice \\(\\log_2(N)\\) binary Yes/No questions and correctly identify the number. This result can be generalized to non-uniform distributions (eg. using the Kraft-McMillan inequality). If Alice chooses a number \\(X\\) from \\(\\{1,2, \\ldots, N\\}\\) with probabilities \\(\\mathop{\\mathrm{P}}(X=k) = p_k\\), Bob can design a deterministic strategy to find the answer using, on average, approximately\n\\[\nH(X) = - \\sum_{k=1}^N p_k \\, \\log_2(p_k)\n\\tag{1}\\]\nbinary questions, ie. bits. He cannot achieve this with fewer questions. The quantity \\(H(X)\\) defined in Equation 1, known as the Shannon Entropy of the distribution \\((p_1, \\ldots, p_N)\\), also implies that the strategy can encode each integer \\(1 \\leq x \\leq N\\) as a binary string of length \\(L(x)\\) (i.e. with \\(L(x)\\) bits), with the expected length \\(\\mathop{\\mathrm{E}}[L(X)]\\) approximately equal to \\(H(X)\\).\nThis remark can be used for compression. Imagine a very long sequence \\((X_1, \\ldots, X_T)\\) of iid samples from \\(X\\). Encoding each \\(X_i\\) with \\(L(X_i)\\), one should be able to encode the resulting sequence with\n\\[\nL(X_1) + \\ldots + L(X_T) \\approx T \\, \\mathop{\\mathrm{E}}[L(X)] \\approx T \\cdot H(X)\n\\]\nbits. Can the usual zip compression algorithm do this? To test this, choose a probability distribution on \\(\\{1, \\ldots, N\\}\\), generate an iid sequence of length \\(T \\gg 1\\), compress this using the \\(\\texttt{gzip}(\\ldots)\\) command, and finally look at the size of the resulting file (in bits). I have done that a few times, with a few different values of \\(16 \\leq N \\leq 256\\) and a few random distributions on \\(\\{1, \\ldots, N\\}\\), and with \\(T = 10^6\\). The plot of size of the compressed files versus the Shannon entropy \\(H\\) looks as below:\n\n\n\n\n\n\n\nSeems like the zip-algorithm works almost optimally for compressing iid sequences.\n\n\nSequence of random variables\nNow consider a pair of discrete random variables \\((X,Y)\\). If Alice draws samples from this pair of rvs, one can ask \\(H(X,Y)\\) binary questions on average to exactly find out these values. To do that, one can ask \\(H(X)\\) questions to estimate \\(X\\), and once \\(X=x\\) is estimated, one can then ask about \\(H(Y|X=x) = -\\sum_y \\mathop{\\mathrm{P}}(Y=y|X=x) \\, \\log_2(\\mathop{\\mathrm{P}}(Y=y|X=x))\\) to estimate \\(Y\\). This strategy requires on average \\(H(X) + \\sum_x \\mathop{\\mathrm{P}}(X=x) \\, H(Y|X=x)\\) binary questions and is actually optimal, showing that\n\\[\nH(X,Y) = H(X) + H(Y | X)\n\\tag{2}\\]\nwhere we have defined \\(H(Y | X) = \\sum_x \\mathop{\\mathrm{P}}(X=x) \\, H(Y|X=x)\\).\nIndeed, one can generalize these concepts to more than two random variables. Iterating Equation 2 shows that the trajectory \\(X_{1:T} \\equiv (X_1, \\ldots, X_N)\\) of a stationary ergodic Markov chain can be estimated on average with \\(H(X_{1:T})\\) binary questions where\n\\[\n\\begin{align}\nH(X_{1:T})\n&= H(X_1) + H(X_2|X_1) + \\ldots + H(X_{T} | X_{t-1})\\\\\n&\\approx T \\, H(X_{k+1} | X_k)\\\\\n&= - T \\, \\sum_x \\pi(x) \\, \\sum_{y} p(x \\to y) \\, \\log_2[p(x \\to y)].\n\\end{align}\n\\]\nHere, \\(\\pi(dx)\\) is the equilibrium distribution of the Markov chain and \\(p(x \\to y)\\) are the transition probabilities.\nCan \\(\\texttt{gzip}(\\ldots)\\) compress Markovian trajectories and roughly achieve that level of compression? Indeed, one can test this by generating random Markov transition matrices (that are, with very high probability, ergodic with respect to an equilibrium distribution \\(\\pi\\)). Doing this with trajectories of length \\(10^4\\) (ie. quite short because it is quite slow to) on \\(\\{1, \\ldots, N\\}\\) with \\(2 \\leq N \\leq 64\\), one get the following results:\n\n\n\n\n\n\n\nIn red is the entropy estimated without using the Markovian structure and assuming that the \\(X_i\\) are iid samples. One can clearly see that the dependencies between successive samples are exploited for better compression. Nevertheless, the optimal compression rate given by the Shannon entropy is not reached. Indeed, \\(\\texttt{gzip}(\\ldots)\\) is not an optimal algorithm – it cannot even compress well enough the sequence \\((1,2,3,1,2,3,1,2,3,\\ldots)\\)!\n\n\nAsymptotic Equipartition Property (AEP)\nThe AEP is simple remark that gives a convenient way of reasoning about long sequences random variables \\(X_{1:T} = (X_1, \\ldots, X_T)\\) with \\(T \\gg 1\\). For example, assuming that the random variables \\(X_i\\) are independent and identically distributed as the random variable \\(X\\), the law of large numbers (LLN) gives that\n\\[\n-\\frac{1}{T} \\, \\log_2(X_{1:T}) = -\\frac{1}{T} \\, \\sum \\log_2 p(X_i) \\approx H(X).\n\\]\nThis means that any “typical” sequence has a probability about \\(2^{-T \\, H(X)}\\) of occurring, which also means that there are about \\(2^{T \\, H(X)}\\) such “typical” sequences. Indeed, one could use the LLN for Markov chains and obtain a similar statement for Markovian sequences. All this can be made much more rigorous with large deviation theory for example. This also establishes the link with the “statistical physics” definition of entropy as the logarithm of the number of configurations… Set of the type\n\\[\nA_{\\varepsilon} = \\left\\{ x_{1:T} \\; : \\; \\left| -\\frac{1}{T} \\, \\log_2(x_{1:T}) - H(X)\\right| &lt; \\varepsilon\\right\\}\n\\]\nare usually called “typical set”: for any \\(\\varepsilon&gt; 0\\), the probability of \\(X_{1:T}\\) to belongs to \\(A_{\\varepsilon}\\) goes to one as \\(T \\to \\infty\\).\n\n\nMutual information\nAnother important question is: assuming that \\(X\\) stores (on average) \\(H(X)\\) bits of useful information, how much useful information is still contained in \\(Y\\)? If \\(Y\\) is independent from \\(X\\), everything has been lost and if \\(X=Y\\), nothing has been lost. If one knows \\(Y\\), we need on average \\(H(X|Y)\\) binary questions (ie. bits of information) in order to determine \\(X\\) and recovered all the information contained in \\(X\\). This means that the knowledge of \\(Y\\) still contains \\(H(X) - H(X|Y)\\) useful bits of information! This quantity is called the mutual information of the two random variable \\(X\\) and \\(Y\\), and it has the good taste of being symmetric:\n\\[\n\\begin{align}\nI(X;Y) &= H(X) - H(X|Y) \\\\\n&= H(X) + H(Y) - H(X,Y) \\\\\n&= H(Y;X).\n\\end{align}\n\\]\nThis diagram from (MacKay 2003) nicely illustrate the different fundamental quantities \\(H(X)\\) and \\(H(Y|X)\\) and \\(I(X;Y)\\) and \\(H(X,Y)\\):\n\n\n\n\nFrom: Information Theory, Inference, and Learning Algorithms\n\n\n\nNaturally, if one consider three random variables \\(X \\mapsto Y \\mapsto Z\\) forming a “Markov chain”, we have the so-called data-processing inequality,\n\\[\nI(X;Z) \\leq I(X;Y)\n\\qquad \\text{and} \\qquad\nI(X;Z) \\leq I(Y;Z).\n\\]\nThe first inequality is quite intuitive since all the useful information contained in \\(Z\\) must be coming from \\(Y\\). The second inequality, note that if \\(Z\\) contains \\(I(Y;Z)\\) bits of \\(Y\\), and \\(Y\\) contains \\(H(Y;X)\\) bits of \\(X\\), then \\(Z\\) cannot contain more than \\(I(Y;Z)\\) bits of \\(X\\):\n\n\n\n\nData Processing Inequality\n\n\n\n\n\n\n\n\nReferences\n\nMacKay, David JC. 2003. Information Theory, Inference and Learning Algorithms. Cambridge university press."
  },
  {
    "objectID": "notes/information_theory_fano/information_theory_references.html",
    "href": "notes/information_theory_fano/information_theory_references.html",
    "title": "Information Theory References and Readings",
    "section": "",
    "text": "Books\n\n“Elements of information theory” by T. M. Cover and J. A. Thomas – perfect intro book to the topic.\n“Information Theory, Inference, and Learning Algorithms” by David J.C. MacKay – **\n“Information Theory From Coding to Learning” by Yury Polyanskiy and Yihong Wu\n\n\n\nLecture Notes & Articles\n\n“A Mathematical Theory of Communication” by C. Shannon (2948) – entertaining and readable, even 70+ years later!\n“Lecture Notes on Statistics and Information Theory” by John Duchi\n“Information-theoretic methods for high-dimensional statistics” by Yihong Wu"
  },
  {
    "objectID": "notes/information_theory_references/information_theory_references.html",
    "href": "notes/information_theory_references/information_theory_references.html",
    "title": "Information Theory: References and Readings",
    "section": "",
    "text": "Books\n\n“Elements of information theory” by T. M. Cover and J. A. Thomas – perfect intro book to the topic.\n“Information Theory, Inference, and Learning Algorithms” by David J.C. MacKay\n“Information Theory From Coding to Learning” by Yury Polyanskiy and Yihong Wu\n\n\n\nLecture Notes & Articles\n\n“A Mathematical Theory of Communication” by C. Shannon (2948) – entertaining and readable, even 70+ years later!\n“Lecture Notes on Statistics and Information Theory” by John Duchi\n“Information-theoretic methods for high-dimensional statistics” by Yihong Wu"
  },
  {
    "objectID": "notes/information_theory_basics copy/information_theory_fano.html",
    "href": "notes/information_theory_basics copy/information_theory_fano.html",
    "title": "Information Theory: Entropy and Basic Definitions",
    "section": "",
    "text": "Robert Fano (1917 – 2016)\n\n\n\n\nFano’s inequality\nConsider a three random variables that forms a Markov chain,\n\\[X \\mapsto Y \\mapsto \\widehat{X},\\]\nin the sense that \\(Y = \\textrm{function}(X, \\text{noise})\\) and \\(Z = \\textrm{function}(Y, \\text{noise})\\). Typical situations include:\n\nSome parameter \\(\\theta\\) of a probabilistic model \\(\\mathop{\\mathrm{P}}_{\\theta}\\) is chosen, some data is generated from that parameter, and one finally would like to estimate \\(\\theta\\) from the data alone.\nSome data \\(X\\) is generated, some “compressed” version of it \\(Y\\) is generated, and one finally tries to recover the original data as accurately as possible.\n\nSince each arrow destroys some information (eg. data processing inequality it is important quantify how much of \\(X\\) can be recovered from \\(\\widehat{X}\\)."
  },
  {
    "objectID": "notes/information_theory_shearer_lemma/shearer_lemma.html",
    "href": "notes/information_theory_shearer_lemma/shearer_lemma.html",
    "title": "Shearer’s lemma",
    "section": "",
    "text": "The Shearer’s lemma (Chung et al. 1986) is concerned with a generalization of the sub-additivity of the Shannon Entropy,\n\\[\nH(X_1, \\ldots, X_N) \\; \\leq \\; H(X_1) + \\ldots + H(X_N).\n\\]\nInstead, consider an integer \\(t \\geq 1\\) and a family \\(S_1, \\ldots, S_K\\) of subsets of \\(\\{1, \\ldots, N\\}\\) such that any index \\(1 \\leq n \\leq N\\) appears in at least \\(t\\) of these subsets. Note that for a subset \\(S_i = \\{ \\alpha_1, \\ldots, \\alpha_{r_i}\\}\\) with \\(\\alpha_1 &lt; \\ldots &lt; \\alpha_{r_i}\\) we have\n\\[\n\\begin{align}\nH(X_{S_i}) &\\equiv H(X_{\\alpha_1}, \\ldots, X_{\\alpha_{r_i}})\\\\\n&= H(X_{\\alpha_1}) + H(X_{\\alpha_2} | X_{\\alpha_1}) + \\ldots + H(X_{\\alpha_{r_i}} | X_{\\alpha_{r_i-1}}, \\ldots, X_{\\alpha_1} ) \\\\\n&\\geq H(X_{\\alpha_1}) + H(X_{\\alpha_2} | X_{1:(\\alpha_2-1)}) + \\ldots + H(X_{\\alpha_{r_i}} | X_{1:\\alpha_{r_i}-1}).\n\\end{align}\n\\tag{1}\\]\nSince each index appears in at least \\(t\\) of the subsets, summing Equation 1 over all the subset \\(S_i\\) yields\n\\[\n\\sum_{i=1}^K H(X_{S_k}) \\geq t \\, \\sum_{i=1}^N H(X_i \\, | X_{1:(i-1)}) = t \\, H(X).\n\\]\nThis means that the following inequality holds,\n\\[\nH(X) \\leq \\frac{1}{t} \\, \\sum_{k=1}^K H(X_{S_k})\n\\]\nIndeed, the standard sub-additivity property of the entropy corresponds to the set \\(S_k = [k]\\) for \\(1 \\leq k \\leq N\\) and \\(t=1\\).\n\nApplication: projection on hyperplanes\nConsider a measurable set \\(A \\subset \\mathbb{R}^n\\) and call \\(A_k\\) the projection of \\(A\\) on the hyperplane \\(\\{x=(x_1, \\ldots, x_n) \\in \\mathbb{R}^n \\, : \\, x_k=0\\}\\). A Theorem of Loomis and Whitney (Loomis and Whitney 1949) states that the lebesgue measure \\(|A|\\) of the set \\(A\\) satisfies\n\\[\n|A| \\; \\leq \\; \\prod_{k=1}^n |A_k|^{1/(n-1)}.\n\\]\nIn other words, if all the projections \\(A_k\\) of the set \\(A\\) are small then, necessarily, the set \\(A\\) itself is small. To proceed, one can approximate this set \\(A\\) with the union \\(A_{\\varepsilon}\\) of small cubes of side \\(\\varepsilon\\) centred on \\(\\varepsilon\\, \\mathbb{Z}^n\\). If one can prove the statement for \\(A^{[\\varepsilon]}\\), the results follows from a standard approximation argument (ie. outer measure). Now, each cube can be indexed with a \\(n\\)-uple of integers \\((x_1, \\ldots, x_n) \\in \\mathbb{Z}^n\\), and one can consider the random variable \\(X=(X_1, \\ldots, X_n)\\) that is uniformly distributed on the set of cubes coordinates. Because \\(2^{H(X)} = |A^{[\\varepsilon]}| / \\varepsilon^n\\) and \\(2^{H(X_2, \\ldots, X_n)} = |A_1^{[\\varepsilon]}| / \\varepsilon^n\\) etc…, choosing the subsets \\(S_i=[1:n] \\setminus \\{i\\}\\) and \\(t = (n-1)\\) in Shearer’s Lemma immediately gives the conclusion.\n\n\n\n\n\nReferences\n\nChung, Fan RK, Ronald L Graham, Peter Frankl, and James B Shearer. 1986. “Some Intersection Theorems for Ordered Sets and Graphs.” Journal of Combinatorial Theory, Series A 43 (1). Academic Press: 23–37.\n\n\nLoomis, Lynn H, and Hassler Whitney. 1949. “An Inequality Related to the Isoperimetric Inequality.”"
  },
  {
    "objectID": "notes/auxiliary_variable_trick/auxiliary_variable_trick.html",
    "href": "notes/auxiliary_variable_trick/auxiliary_variable_trick.html",
    "title": "Auxiliary variable trick",
    "section": "",
    "text": "Consider a complicated distribution on the state space \\(x \\in \\mathcal{X}\\) given by\n\\[\n\\pi(x) = \\frac{1}{\\mathcal{Z}} \\, e^{C(x) + B(x)}\n\\]\nfor a “complicated” functions \\(C(x)\\) and a simpler one \\(B(x)\\). In some situations, it is possible to introduce an auxiliary random variable \\(a \\in \\mathcal{A}\\) and an extended probability distribution \\(\\pi(x,a)\\) on the extended space \\(\\mathcal{X}\\times \\mathcal{A}\\),\n\\[\n\\pi(x,a) = \\pi(x) \\,  \\textcolor{red}{\\pi(a | x)} = \\frac{1}{\\mathcal{Z}} e^{C(x) + B(x)} \\,  \\textcolor{red}{e^{-C(x) + D(x, a)}},\n\\]\nwith a tractable conditional probability \\(\\pi(a | x)\\). This extended target distribution \\(\\pi(x,a) = (1/\\mathcal{Z}) \\, \\exp[B(x) + D(x,a)]\\) can be often be easier to explore, for example when \\(a\\) is continuous while \\(x\\) is discrete, or to analyze, since the “complicated” term \\(C(x)\\) has disappeared. Furthermore, there are a number of scenarios when the variable \\(x\\) can be averaged out of the extended distribution, i.e. the distribution\n\\[\n\\pi(a) = \\frac{1}{\\mathcal{Z}} \\, \\int_{x \\in \\mathcal{X}} e^{B(x) + D(x,a)}\n\\]\ncan be evaluated exactly.\n\nSwendsen–Wang algorithm\nConsider a set of edges \\(\\mathcal{E}\\) on a graph with vertices \\(\\{1, \\ldots, N\\}\\). The Ising model is defined as \\[\n\\pi(x) \\propto \\exp  {\\left\\{  \\sum_{(i,j) \\in \\mathcal{E}} \\beta x_i x_j  \\right\\}}\n\\]\nfor spin configurations \\(x=(x_1, \\ldots, x_N) \\in \\{-1,1\\}^N\\). The term \\(\\exp[\\beta x_i x_j]\\) couples the two spins \\(x_i\\) and \\(x_j\\) for each edge \\((i,j) \\in \\mathcal{E}\\). The idea of the Swendsen–Wang_algorithm is to introduce an auxiliary variable \\(u_{i,j}\\) for each edge \\((i,j) \\in \\mathcal{E}\\) that is uniformly distributed on the interval \\([0, \\exp(\\beta x_i x_j)]\\), i.e.\n\\[\n\\pi(u_{i,j} | x) \\; = \\; \\frac{ \\mathbf{1}  {\\left\\{  0 &lt; u_{i,j} &lt; \\exp[\\beta x_i x_j]  \\right\\}}  }{\\exp[\\beta x_i x_j] }\n\\]\nIt follows that the extended distribution on \\(\\{-1,1\\}^N \\times (0,\\infty)^{|\\mathcal{E}|}\\) reads\n\\[\n\\pi(x,u) = \\frac{1}{Z} \\prod_{(i,j) \\in \\mathcal{E}} \\; \\mathbf{1}  {\\left\\{  0 &lt; u_{i,j} &lt; \\exp[\\beta x_i x_j]  \\right\\}}\n\\]\nfor \\(x=(x_1, \\ldots, x_N)\\) and \\(u = (u_{i,j})_{(i,j) \\in \\mathcal{E}}\\): the coupling term \\(\\exp[\\beta x_i x_j]\\) has disappeared. Furthermore, it is straightforward to sample from the conditional distribution \\(\\pi(u | x)\\) and, perhaps surprisingly, it is also relatively straightforward to sample from the other conditional distribution \\(\\pi(x | u)\\) – this boils down to finding the connect components of the graph on \\(\\{1, \\ldots, N\\}\\) with an edge \\(i \\sim j\\) present if \\(u_{i,j} &gt; e^{-\\beta}\\) and flipping a fair coin for setting each connected component to \\(\\pm 1\\). This leads to an efficient Gibbs sampling scheme to sample from the extended distribution.\n\n\n\n\nSwendsen-Wang MCMC algorithm at critical temperature\n\n\n\n\n\nGaussian Integral trick: Curie-Weiss model\nFor an inverse temperature \\(\\beta &gt; 0\\), consider the distribution on \\(x \\in \\{-1,1\\}^N\\)\n\\[\n\\pi(x) = \\frac{1}{\\mathcal{Z}(\\beta)} \\, e^{\\beta \\, N \\, m^2}\n\\]\nwhere the magnetization of the system of spins \\(x=(x_1, \\ldots, x_N)\\) is defined as\n\\[\nm = \\frac{x_1 + \\ldots + x_N}{N}.\n\\]\nThe distribution \\(\\pi(x)\\) for \\(\\beta \\gg 1\\) favours configurations with a magnetization close to \\(+1\\) or \\(-1\\). The normalization constant (i.e. partition function) \\(\\mathcal{Z}(\\beta)\\) is a sum of \\(2^N\\) terms,\n\\[\n\\mathcal{Z}(\\beta) = \\sum_{s_1 \\in \\{ \\pm 1\\} } \\ldots \\sum_{s_N \\in \\{ \\pm 1\\} } \\exp  {\\left\\{  \\frac{\\beta}{N}  {\\left(  \\sum_{i=1}^N x_i  \\right)} ^2 \\right\\}} .\n\\]\nIt is not difficult to estimate \\(\\log \\mathcal{Z}(\\beta)\\) as \\(N \\to \\infty\\) with combinatorial arguments. Nevertheless, another way to proceed is as follows. One can introduce a Gaussian auxiliary random variable \\(\\pi(a | x) = \\mathcal{N}(\\alpha \\,  {\\left( \\sum_i x_i \\right)}  , \\sigma^2)\\) with mean \\(\\mu = \\alpha \\,  {\\left( \\sum_i x_i \\right)} \\) and variance \\(\\sigma^2\\): the parameters \\(\\alpha\\) and \\(\\sigma^2 &gt; 0\\) can then be judiciously chosen to cancel the bothering term \\(\\exp[\\frac{\\beta}{N} \\, m^2]\\). This approach is often called the a Hubbard-Stratonovich transformation. The bothering “coupling” term disappears when when choosing \\(\\frac{\\alpha^2}{2 \\sigma^2} = \\frac{\\beta}{N}\\). With such a choice, it follows that\n\\[\n\\pi(x, a) = \\frac{1}{\\mathcal{Z}(\\beta)} \\, \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp {\\left\\{  - \\frac{a^2}{2 \\sigma^2} + \\frac{\\alpha}{\\sigma^2} a \\,  {\\left( \\sum_i x_i \\right)}  \\right\\}} .\n\\]\nAveraging out the \\(x_i \\in \\{-1, +1\\}\\) gives that the partition function reads\n\\[\n\\mathcal{Z}(\\beta) \\; = \\;\n\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\,\n\\int_{a = -\\infty}^{a=\\infty} \\,\n\\exp {\\left\\{ -\\frac{a^2}{2 \\sigma^2} +  \\textcolor{red}{N} \\, \\log[ 2 \\, \\cosh(\\alpha a / \\sigma^2)] \\right\\}} .\n\\]\nIn order to use the method of steepest descent, it would be useful to have an integrand of the type \\(\\exp[N \\times (\\ldots)]\\). One can choose \\(1/(2 \\, \\sigma^2) = \\beta \\, N\\) and \\(\\alpha = 1/N\\). This gives\n\\[\n\\mathcal{Z}(\\beta) \\; = \\;\n\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\,\n\\int_{a = -\\infty}^{a=\\infty} \\,\n\\exp  {\\left\\{   \\textcolor{red}{N}  {\\left[  -\\beta \\, a^2 + \\log[ 2 \\, \\cosh(2 \\beta a )] \\right]}  \\right\\}}  \\, da\n\\]\nfrom which one directly obtains that:\n\\[\n\\lim_{N \\to \\infty} \\; -\\frac{\\log \\mathcal{Z}(\\beta)}{N}\n\\; = \\;\n\\min_{a \\in \\mathbb{R}} \\; \\Big\\{ \\beta a^2 - \\log[2 \\, \\cosh(2 \\beta a)] \\Big\\}.\n\\]\n\n\nSherrington–Kirkpatrick model\nConsider the distribution on \\(x \\in \\{-1,1\\}^N\\)\n\\[\n\\pi(x) = \\frac{1}{\\mathcal{Z}(\\beta)} \\,\n\\exp {\\left\\{ \\frac 12 \\, \\sum_{i,j} W_{ij} x_i x_j \\right\\}}\n=\n\\frac{1}{\\mathcal{Z}(\\beta)} \\,\n\\exp {\\left\\{ \\frac 12 \\, \\left&lt; x, W x \\right&gt; \\right\\}}\n\\]\nwhere the \\(w_{ij}\\) are some fixed weights with \\(w_{ij} = w_{ji}\\). We assume that the matrix \\(W = [W_{ij}]_{ij}\\) is positive definite: this can be achieved by adding \\(\\lambda \\, I_N\\) to it if necessary, which does not change the distribution \\(\\pi\\). As described in (Zhang et al. 2012), although I would not be surprised if similar ideas appeared in the physics literature much earlier, one can introduce a Gaussian auxiliary random variable \\(a = (a_1, \\ldots, a_N)\\) so that \\(\\pi(a | x)\\) has mean \\(Fx\\) and covariance \\(\\Gamma\\). In other words,\n\\[\n\\pi(a | x) = \\frac{1}{(2 \\pi)^{d/2} \\, |\\Gamma|^{1/2}}\n\\, \\exp {\\left\\{ -\\frac 12 \\left&lt; (a - Fx), \\Gamma^{-1} (a - Fx) \\right&gt; \\right\\}} .\n\\]\nIn order to cancel-out the \\(\\left&lt; x, W, x \\right&gt;\\) it suffices to make sure that \\(F^\\top \\, \\Gamma^{-1} \\, F = W\\). There are a number of possibilities, the simplest approaches being perhaps\n\\[\n(F,\\Gamma) = (W^{1/2}, I_N)\n\\quad \\text{or} \\quad\n(F,\\Gamma) = (I, W^{-1})\n\\quad \\text{or} \\quad\n(F,\\Gamma) = (W, W).\n\\]\nIn any case, the joint distribution reads\n\\[\n\\pi(x,a) \\; = \\; \\frac{1}{\\mathcal{Z}} \\, \\frac{1}{(2 \\pi)^{d/2} \\, |\\Gamma|^{1/2}} \\,\n\\exp {\\left\\{ -\\frac{1}{2} \\left&lt; a, \\Gamma^{-1} a \\right&gt; + \\left&lt; x, F^\\top \\Gamma^{-1} \\, a \\right&gt; \\right\\}} .\n\\]\nIndeed, one can try to implement some Gibbs-style update in order to explain this joint distribution since both \\(\\pi(x|a)\\) and \\(\\pi(a|x)\\) are straightforward to sample from: it is indeed related Restricted Boltzmann Machine models. One can also average-out the spins \\(x_i \\in \\{-1,1\\}\\) and obtain that\n\\[\n\\mathcal{Z}= \\int_{\\mathbb{R}^N}\n\\exp {\\left\\{  \\sum_{i=1}^N \\log {\\left( 2 \\, \\cosh([F^\\top \\Gamma^{-1} \\, a]_i) \\right)}  \\right\\}}  \\, \\mathcal{D}_{\\Gamma}(da)\n\\]\nwhere \\(\\mathcal{D}_{\\Gamma}\\) is the density of a centred Gaussian distribution with covariance \\(\\Gamma\\). [TODO: add SMC experiments to estimate \\(\\mathcal{Z}\\)].\n\n\n\n\n\nReferences\n\nZhang, Yichuan, Zoubin Ghahramani, Amos J Storkey, and Charles Sutton. 2012. “Continuous Relaxations for Discrete Hamiltonian Monte Carlo.” Advances in Neural Information Processing Systems 25."
  },
  {
    "objectID": "notes/legendre_transform_entropy/legendre_binary_entropy.html",
    "href": "notes/legendre_transform_entropy/legendre_binary_entropy.html",
    "title": "Legendre transform of the binary entropy",
    "section": "",
    "text": "When analyzing the Curie-Weiss model of spins \\(x \\in \\{-1,1\\}^N\\),\n\\[\n\\pi(x) = \\frac{1}{\\mathcal{Z}(\\beta)} \\, \\exp  {\\left\\{ \\beta \\, N \\,  {\\left( \\frac{x_1+\\ldots+x_N}{N} \\right)} ^2 \\right\\}}\n\\]\nand combinatorial / large-deviation argument quickly shows that\n\\[\n\\lim_{N \\to \\infty} \\; -\\frac{\\log \\mathcal{Z}(\\beta)}{N}\n\\; = \\; \\min_{m \\in [-1,1]} \\; \\Big\\{ -\\beta \\, m^2 - H(m) \\Big\\}\n\\tag{1}\\]\nwhere \\(H(m)\\) is the binary entropy function expressed in terms of the magnetization \\(m\\),\n\\[\nH(m) \\; = \\; -  {\\left\\{   {\\left( \\frac{1+m}{2} \\right)}  \\, \\log  {\\left( \\frac{1+m}{2} \\right)}  +  {\\left( \\frac{1-m}{2} \\right)}  \\, \\log  {\\left( \\frac{1-m}{2} \\right)}   \\right\\}} .\n\\]\nFurthermore, another approach to computing Equation 1 is to use a Hubbard-Stratonovich transformation. Doing this, one finds that\n\\[\n\\lim_{N \\to \\infty} \\; -\\frac{\\log \\mathcal{Z}(\\beta)}{N}\n\\; = \\;\n\\min_{a \\in \\mathbb{R}} \\; \\Big\\{ \\beta \\, a^2 - \\log  {\\left(  e^{2 \\beta a} + e^{-2 \\beta a}  \\right)}  \\Big\\}.\n\\]\nIndeed, these are these two expressions are the two faces of the same coin. This boils down to the remark that the Legendre Transform of the negative entropy function \\(m \\mapsto -H(m)\\) is given by the function \\(\\Phi(x) = \\log  {\\left( e^x + e^{-x} \\right)} \\),\n\\[\n\\left\\{\n\\begin{align}\n-H(m) &= \\max_{x \\in \\mathbb{R}} \\;  {\\left\\{  m \\, x \\, - \\, \\Phi(x) \\right\\}}  \\\\\n\\Phi(x) &= \\max_{-1 &lt; m &lt; 1} \\;  {\\left\\{  m \\, x \\, + \\, H(m)  \\right\\}} .\n\\end{align}\n\\right.\n\\]"
  },
  {
    "objectID": "notes/sanov/sanov.html",
    "href": "notes/sanov/sanov.html",
    "title": "Sanov’s Theorem",
    "section": "",
    "text": "Sanov’s Theorem\nConsider a random variable \\(X\\) on the finite alphabet \\(\\{a_1, \\ldots, a_K\\}\\) with \\(\\mathop{\\mathrm{P}}(X=a_k) = p_k\\). For \\(N \\gg 1\\), consider a sequence \\((x_1, \\ldots, x_N)\\) obtained by sampling \\(N\\) times independently from \\(X\\) and set\n\\[\n\\widehat{p}_k = \\frac{1}{N} \\, \\sum_{i=1}^N \\, \\mathbf{1} {\\left( x_i = a_k \\right)}\n\\]\nthe proportion of \\(a_k\\) within this sequence. In other words, the empirical distribution obtained from the samples \\((x_1, \\ldots, x_N)\\) reads\n\\[\n\\widehat{p} = \\sum_{k=1}^K \\, \\widehat{p}_k \\, \\delta_{a_k}.\n\\]\nIndeed, the LLN indicates that \\(\\widehat{p}_k \\to p_k\\) as \\(N \\to \\infty\\), and it is important to estimate the probability that \\(\\widehat{p}_k\\) significantly deviates from \\(p_k\\). To this end, note that for another probability vector \\(q=(q_1, \\ldots, q_K)\\) the probability that\n\\[\n(\\widehat{p}_1, \\ldots, \\widehat{p}_K) \\; = \\; (q_1, \\ldots, q_K)\n\\]\nis straightforward to compute and reads\n\\[\n\\mathop{\\mathrm{P}}(\\widehat{p} = q) \\; = \\; \\binom{N}{N q_1, \\ldots, N q_K} \\, p_1^{N q_1} \\ldots p_R^{N q_K}.\n\\]\nStirling’s approximation \\(m! \\asymp m \\, \\ln(m)\\) then gives that\n\\[\n\\mathop{\\mathrm{P}}(\\widehat{p} = q) \\; \\asymp \\;\n\\exp {\\left( -N \\cdot \\mathop{\\mathrm{D_{\\text{KL}}}}(q,p) \\right)}\n\\]\nwhere \\(\\mathop{\\mathrm{D_{\\text{KL}}}}(q,p) = \\sum_{k=1}^K q_k \\, \\log[q_k / p_k]\\) is the Kullback–Leibler divergence of \\(q\\) from \\(p\\). In other words, as soon as \\(q \\neq p\\), the probability of observing \\(\\widehat{p} \\approx q\\) falls exponentially quickly to zero. With the language of Large Deviations, one can make this statement slightly more precise, rigorous and general, but it is essentially the content of Sanov’s Theorem.\n\n\nRare events happen in the least unlikely manner\nGiven a list of mutually exclusive events \\(E_1, \\ldots, E_R\\) and the knowledge that at least one of these events has taken place, the probability that the event \\(E_k\\) was the one that happened is \\(\\mathop{\\mathrm{P}}(E_k) / [\\mathop{\\mathrm{P}}(E_1) + \\ldots + \\mathop{\\mathrm{P}}(E_R)]\\). The implication is that if all the events are rare, that is \\(p_k \\approx e^{-N \\, I_k} \\ll 1\\), and it is known that one event has indeed occurred, there is a high probability that the event with the smallest \\(I_k\\) value was the one that happened: the rare event took place in the least unlikely manner.\nConsider an iid sequence \\((X_1, \\ldots, X_N)\\) of \\(N \\gg 1\\) discrete real-valued random variables with \\(\\mathop{\\mathrm{P}}(X = a_k) = p_k\\) and mean \\(\\mathop{\\mathrm{E}}(X) \\in \\mathbb{R}\\). Suppose one observes the rare event\n\\[\n\\frac{1}{N} \\sum_{i=1}^N x_i  \\geq \\mu\n\\tag{1}\\]\nfor some level \\(\\mu\\) significantly above \\(\\mathop{\\mathrm{E}}(X)\\). Naturally, the least unlikely way for this to happen is if \\((x_1 + \\ldots + x_N) / N \\, \\approx \\, \\mu\\). Furthermore, one may be interested in the empirical distribution \\(\\widehat{p}\\) associated to the sequence \\((x_1, \\ldots, x_N)\\) when the rare event Equation 1 does happen. The least unlikely empirical distribution is the one that minimizes \\(\\mathop{\\mathrm{D_{\\text{KL}}}}(\\widehat{p}, p)\\) under the constraint that\n\\[\n\\sum_{i=1}^K a_k \\, \\widehat{p}_k = \\mu.\n\\tag{2}\\]\nThe function \\(\\widehat{p} \\mapsto \\mathop{\\mathrm{D_{\\text{KL}}}}(\\widehat{p}, p)\\) is convex and the introduction of Lagrange multipliers shows that the solution to this constraint minimization problem is given by the Boltzmann distribution \\(p_{\\beta_\\mu}\\) defined as\n\\[\np_{\\beta_\\mu}(a_k) = \\frac{ p_k \\, e^{-\\beta_{\\mu} \\, a_k} }{Z(\\beta_{\\mu})}.\n\\tag{3}\\]\nThe parameter \\(\\beta_{\\mu} \\in \\mathbb{R}\\) is chosen so that the constraint Equation 2 be satisfied and the minus sign is to follow the “physics” convention. Note in passing that, in fact, the joint function \\((\\widehat{p},p) \\mapsto \\mathop{\\mathrm{D_{\\text{KL}}}}(\\widehat{p}, p)\\) is convex! As usual, if one defines the log-partition function as \\(\\Phi(\\beta) = -\\log Z(\\beta)\\), with\n\\[\nZ(\\beta) \\; = \\; \\sum_{k=1}^K \\, p_k \\, e^{-\\beta \\, a_k},\n\\]\none obtains that the constraint is equivalent to requiring \\(\\mu = \\frac{d}{d \\beta} \\Phi(\\beta) \\mid_{\\beta = \\beta_\\mu}\\). Furthermore, since \\(\\Phi\\) is smooth and strictly concave, so is the function \\(\\beta \\mapsto \\left&lt; \\mu, \\beta \\right&gt; - \\Phi(\\beta)\\), so that the condition \\(\\mu = \\frac{d}{d \\beta} \\Phi(\\beta) \\mid_{\\beta = \\beta_\\mu}\\) is equivalent to setting\n\\[\n\\beta_{\\mu} \\; = \\; \\mathop{\\mathrm{argmin}}_{\\beta \\in \\mathbb{R}} \\; \\left&lt; \\mu, \\beta \\right&gt; - \\Phi(\\beta).\n\\]\nNaturally, one can now also estimate the probability of the event \\(\\mathop{\\mathrm{P}}[(X_1 + \\ldots + X_N)/N \\approx \\alpha]\\) happening since one now knows that it is equivalent (on a log scale) to \\(\\exp[-N \\, \\mathop{\\mathrm{D_{\\text{KL}}}}(p_{\\beta_\\mu}, p)]\\). Algebra gives\n\\[\n\\begin{align}\n\\mathop{\\mathrm{D_{\\text{KL}}}}(p_{\\beta_\\mu}, p)\n&= \\Phi(\\beta_\\mu) - \\left&lt; \\mu, \\beta_\\mu \\right&gt;\\\\\n&= \\max_{\\beta \\in \\mathbb{R}} \\; \\Phi(\\beta) - \\left&lt; \\mu, \\beta \\right&gt;.\n\\end{align}\n\\]\nAs a sanity check, note that since \\(\\Phi(0)=0\\), we have that \\(\\mathop{\\mathrm{D_{\\text{KL}}}}(p_{\\beta_\\mu}, p) \\geq 0\\), as required. The statement that\n\\[\n\\frac{1}{N} \\, \\log \\mathop{\\mathrm{P}} {\\left\\{ \\frac{X_1 + \\ldots + X_N}{N} \\; \\approx \\; \\mu \\right\\}}  \\; = \\; - I(\\mu)\n\\]\nwith a (Large Deviation) rate function given by\n\\[\nI(\\mu) \\; = \\; \\max_{\\beta \\in \\mathbb{R}} \\; \\Phi(\\beta) - \\left&lt; \\mu, \\beta \\right&gt;\n\\]\nis more or less the content of Cramer’s Theorem. The rate function \\(I(\\mu)\\) and the function \\(\\log Z( \\textcolor{red}{-}\\beta)\\) are related by a Legendre transform.\n\n\nExample: averaging uniforms…\nNow, to illustrate the above discussion, consider \\(N=10\\) iid uniform random variables on the interval \\([0,1]\\). It is straightforward to simulate these \\(N=10\\) uniforms conditioned on the event that their mean exceeds the level \\(\\mu = 0.7\\), which is a relatively rare event. When this is done repetitively and the empirical distribution is plotted, the resulting distribution is as follows:\n\n\n\n\nMean of \\(10\\) uniforms conditioned on being larger than \\(\\mu = 0.7\\)\n\n\n\nIndeed, the distribution in blue is (very close to) the Boltzmann distribution with density \\(\\mathcal{D}_{\\beta}(x) = e^{-\\beta \\, x} / Z(\\beta)\\) with \\(\\beta \\in \\mathbb{R}\\) chosen so that \\(\\int_{0}^{1} x \\, \\mathcal{D}_{\\beta}(dx) = \\mu\\)."
  },
  {
    "objectID": "notes/wasserstein_langevin/wasserstein_langevin.html",
    "href": "notes/wasserstein_langevin/wasserstein_langevin.html",
    "title": "Wasserstein Gradients & Langevin Diffusions",
    "section": "",
    "text": "Overdamped Langevin Diffusion as a gradient flow on the space of probability distributions endowed with the \\(2\\)-Wasserstein metric\n\n\n\nConsider a target probability density \\(\\pi(x) = \\overline{\\pi}(x)/ \\mathcal{Z}\\) on \\(\\mathbb{R}^D\\) known up to an intractable normalization constant \\(\\mathcal{Z}&gt; 0\\). Now, starting from another probability density \\(p_0(x)\\), we would like to continuously “update” it so that it converges to the target distribution. Slightly more formally, we would like to do some kind of gradient flow on the space of probability distributions to minimize the functional\n\\[\n\\mathcal{F}(p) \\; = \\; \\mathop{\\mathrm{D_{\\text{KL}}}} {\\left( p, \\pi \\right)}  \\; = \\; \\int p(x) \\, \\log  {\\left\\{ \\frac{p(x)}{\\overline{\\pi}(x)} \\right\\}}  \\, dx \\, + \\, \\textrm{(constant)}.\n\\]\nThis approach can be discretized by considering \\(N \\gg 1\\) particles \\(X_0^1, \\ldots, X_0^N \\in \\mathbb{R}^D\\) whose empirical distribution approximate \\(p_0(dx)\\),\n\\[\np_0(dx) \\; \\approx \\; \\frac{1}{N} \\sum_{i=1}^N \\, \\delta_{X_0^i}(dx),\n\\]\nand set \\(X_{\\delta}^i = X_0^i + \\delta_t \\, \\mu(X_0^i)\\) for a time discretization parameter \\(\\delta_t \\ll 1\\) and a drift function \\(\\mu:\\mathbb{R}^D \\to \\mathbb{R}^D\\). The main question is: what “drift function” should one choose? The Fokker-Planck equation shows that the resulting empirical distribution\n\\[\np_{\\delta_t}(dx) \\; \\approx \\; \\frac{1}{N} \\sum_{i=1}^N \\, \\delta_{X_{\\delta_t}^i}(dx)\n\\]\napproximates \\(p_{\\delta_t}(x)\\) given by\n\\[\n\\frac{p_{\\delta_t}(x)- p_0(x)}{\\delta_t} \\; = \\; -\\nabla \\cdot  {\\left[ \\mu(x) \\, p_0(x) \\right]} .\n\\tag{1}\\]\nHow should the drift function \\(\\mu: \\mathbb{R}^D \\to \\mathbb{R}^D\\) be chosen so that \\(p_{\\delta_t}\\) approximates \\(\\pi\\) best? A standard approach consists in choosing \\(\\mu:\\mathbb{R}^D \\to \\mathbb{R}^D\\) so that the quantity \\(\\mathcal{F}(p_{\\delta_t})\\) is minimized under the constraint that \\(p_{\\delta_t}\\) is not “too far” from \\(p_0\\). For example, one can constrain the \\(L^2\\) Wasserstein distance and require that\n\\[\n\\mathop{\\mathrm{D_{\\text{Wass}}}}(p_{0}, p_{\\delta_t}) \\approx \\int p_0(x) \\, \\| \\delta_t \\, \\mu(x) \\|^2 \\, dx \\leq \\varepsilon.\n\\tag{2}\\]\nMore pragmatically, it is generally easier to consider the joint objective\n\\[\n\\mathcal{F}(p_{\\delta_t}) + \\frac{1}{2 \\varepsilon} \\, \\mathop{\\mathrm{D_{\\text{Wass}}}}(p_{0}, p_{\\delta_t}).\n\\tag{3}\\]\nUsing Equation 1 and Equation 2, a first order expansion gives that the joint objective Equation 3 can be approximated by\n\\[\n-\\int \\nabla \\cdot \\Big\\{  \\textcolor{red}{[\\delta_t \\mu]}(x) \\, p_0(x) \\Big\\} \\, \\log  {\\left\\{  \\frac{p_0(x)}{\\pi(x)}  \\right\\}} \\, dx \\, + \\, \\frac{1}{2 \\varepsilon} \\, \\int p_0(x) \\, \\|  \\textcolor{red}{[\\delta_t \\, \\mu]}(x) \\|^2 \\, dx,\n\\tag{4}\\]\na simple quadratic function of the drift function \\(\\mu: \\mathbb{R}^D \\to \\mathbb{R}^D\\). The optimal drift function, ie. the minimizer of Equation 4, reads\n\\[\n\\mu(x) \\; = \\; - {\\left(  \\frac{\\varepsilon}{\\delta_t}  \\right)}  \\, \\nabla \\log  {\\left\\{  \\frac{p_0}{\\pi}  \\right\\}} .\n\\]\nIn other words, the drift function should be chosen proportional to \\(-\\nabla \\log[p_0(x) / \\pi(x)]\\). Iterating this scheme means sampling \\(N \\gg 1\\) particles \\(X_0^i \\sim p_0(dx)\\) and evolving each one of these particles through the differential equation\n\\[\n\\frac{d}{dt} X_t^i \\; = \\; - \\nabla \\log  {\\left\\{  \\frac{p_t(X_t^i) }{ \\pi(X_t^i) }  \\right\\}}\n\\]\nwhere \\(p_t\\) is the density of particle at time \\(t\\). It is the usual diffusion-ODE trick for describing the density evolution of an overdamped Langevin diffusion,\n\\[\ndX \\; = \\; -\\nabla \\log \\pi(X) \\, dt \\; + \\; \\sqrt{2} \\, dW,\n\\]\nas writing the associated Fokker-Planck equation immediately shows it. This (very) informal discussion shows that minimizing \\(\\mathop{\\mathrm{D_{\\text{KL}}}}(p, \\pi)\\) by implementing a gradient flow on the space of probability distributions endowed with the Wasserstein metric leads to a standard overdamped Langevin diffusion. Indeed, transforming this heuristic discussion into a formal statement is not trivial: the constructive proof in (Jordan, Kinderlehrer, and Otto 1998) is now usually referred to as the JKO scheme.\nThe above derivation shows that the Wasserstein distance plays particularly well with minimizing functionals of the space of probability distributions. Indeed, the same argument shows that minimizing a functional of the type\n\\[\n\\mathcal{F}(p) \\; = \\; \\int \\Phi[p(x)] \\, \\mu(dx)\n\\]\nfor some function \\(\\Phi: (0, \\infty) \\to \\mathbb{R}\\) leads to choosing a drift function \\(\\mu\\) minimizing\n\\[\n\\int -\\nabla \\cdot \\Big\\{  \\textcolor{red}{[\\delta_t \\, \\mu]}(x) \\, p(x) \\Big\\} \\Phi'[p(x)] \\, \\mu(dx)\n\\, + \\, \\frac{1}{2 \\varepsilon} \\, \\int p_0(x) \\, \\|  \\textcolor{red}{[\\delta_t \\, \\mu]}(x) \\|^2 \\, dx.\n\\]\nThis can be approached identically to what as been done in the case of minimizing \\(\\mathop{\\mathrm{D_{\\text{KL}}}}(p, \\pi)\\).\n\n\n\n\nReferences\n\nJordan, Richard, David Kinderlehrer, and Felix Otto. 1998. “The Variational Formulation of the Fokker–Planck Equation.” SIAM Journal on Mathematical Analysis 29 (1). SIAM: 1–17."
  }
]