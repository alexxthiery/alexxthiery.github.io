[
  {
    "objectID": "jobs/2024_koopman/2024_koopman.html",
    "href": "jobs/2024_koopman/2024_koopman.html",
    "title": "Research Fellow positions: Data-Assimilation",
    "section": "",
    "text": "Two Postdoctoral Research Fellow positions are currently open in the Department of Statistics & Data Science at the National University of Singapore (NUS). These are two-year contracts, with the possibility of renewal upon review.\nWhile the research themes are flexible, our primary interest lies in the design and analysis of statistical methods for data-assimilation of high-dimensional time-series data. We anticipate applying tools from reinforcement learning, Koopman operators, and unsupervised representation learning, supplementing the more conventional techniques in state-space modeling (particle filters, EnKF, variational approaches, etc…)\n\nDetails:\n\nStart Date: Immediate\nDuration: 2 years (Renewable for up to an additional year)\nSalary Range: S$ 70k-100k / Year, plus benefits (Health Insurance / etc…)\nTravel: Generous budget for attending conferences\n\n\n\nRequirements:\nIdeal candidates should have:\n\na Ph.D. in Statistics, Computer Science, Physics, or a closely related field.\n\nstrong experience in Python programming.\n\nexperience with at least one deep-learning framework\na demonstrated record of academic publications\n\nresearch experience that aligns with the core focus of the research programme.\n\n\n\nContact:\nQueries about this post should be addressed to Alex Thiery at a.h.thiery@nus.edu.sg. We highly encourage prospective applicants to get in touch informally to discuss further details about the position first. This project is in collaboration with Jeremy Heng.\nThe positions will remain open until we find the right candidates. [Positions have been filled]"
  },
  {
    "objectID": "teaching/teaching.html",
    "href": "teaching/teaching.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "YSC4216: Machine Learning, Yale-NUS (2024)\nDSA4212: Large Scale optimization, NUS (2017 – 2024)\nYSC4236: Optimization for Large Scale Inference, Yale-NUS, (2022)\nYSC3252: Computational Statistics, Yale-NUS, (2022)\nDSA5843: Learning from Data: Neural Networks, NUS (2020–2021)\nST3233: Applied Times Series, NUS, (2016, 2019, 2021)\nST4240: Data Mining, NUS, (2014–2017)\nST4231: Computationally Intensive Statistics, NUS (2014–2015)\nMultivariable Calculus II, Chennai (CMI), (2018)"
  },
  {
    "objectID": "teaching/teaching.html#semester-long-courses",
    "href": "teaching/teaching.html#semester-long-courses",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "YSC4216: Machine Learning, Yale-NUS (2024)\nDSA4212: Large Scale optimization, NUS (2017 – 2024)\nYSC4236: Optimization for Large Scale Inference, Yale-NUS, (2022)\nYSC3252: Computational Statistics, Yale-NUS, (2022)\nDSA5843: Learning from Data: Neural Networks, NUS (2020–2021)\nST3233: Applied Times Series, NUS, (2016, 2019, 2021)\nST4240: Data Mining, NUS, (2014–2017)\nST4231: Computationally Intensive Statistics, NUS (2014–2015)\nMultivariable Calculus II, Chennai (CMI), (2018)"
  },
  {
    "objectID": "teaching/teaching.html#lecture-series",
    "href": "teaching/teaching.html#lecture-series",
    "title": "Alexandre Thiéry",
    "section": "Lecture Series:",
    "text": "Lecture Series:\n\nSummer school on Bayesian statistics (5h Lecture), VIASM July 2023"
  },
  {
    "objectID": "notes/information_theory_references/information_theory_references.html",
    "href": "notes/information_theory_references/information_theory_references.html",
    "title": "Information Theory: References and Readings",
    "section": "",
    "text": "Books\n\n“Elements of information theory” by T. M. Cover and J. A. Thomas – perfect intro book to the topic.\n“Information Theory, Inference, and Learning Algorithms” by David J.C. MacKay\n“Information Theory From Coding to Learning” by Yury Polyanskiy and Yihong Wu\n\n\n\nLecture Notes & Articles\n\n“A Mathematical Theory of Communication” by C. Shannon (2948) – entertaining and readable, even 70+ years later!\n“Lecture Notes on Statistics and Information Theory” by John Duchi\n“Information-theoretic methods for high-dimensional statistics” by Yihong Wu"
  },
  {
    "objectID": "notes/DDPM/DDPM.html",
    "href": "notes/DDPM/DDPM.html",
    "title": "Denoising Diffusion Probabilistic Models (DDPM)",
    "section": "",
    "text": "Setting & Goals\nConsider \\(N\\) samples \\(\\mathcal{D}\\equiv \\{x_i\\}_{i=1}^N\\) in \\(\\mathbb{R}^D\\) from an unknown data distribution \\(\\pi_{\\mathrm{data}}(dx)\\). We would like to build an mechanism that can generate other samples from this data distribution. Implicitly, this means that we will be fitting a statistical model to this finite set of samples and have an algorithmic procedure to generate samples from the fitted probabilistic model. This type of models that directly define a stochastic procedure that generates data are called implicit probabilistic models in the ML literature.\n\n\nOrnstein–Uhlenbeck Noising process\nDDPMs work as follows. Consider a diffusion process \\(\\{ X_t \\}_{t=0}^T\\) that starts from the data distribution \\(p_0(dx) \\equiv \\pi_{\\mathrm{data}}(dx)\\) at time \\(t=0\\). The notation \\(p_t(dx)\\) refers to the marginal distribution of the diffusion at time \\(0 \\leq t \\leq T\\). Assume furthermore that at time \\(t=T\\), the marginal distribution is (very close to) a reference distribution \\(p_T(dx) = \\pi_{\\mathrm{ref}}(dx)\\) that is straightforward to sample from. Typically, \\(\\pi_{\\mathrm{ref}}(dx)\\) is an isotropic Gaussian distribution. This diffusion process is often called the noising process since it transform the data distribution into a reference measure that can be thought of as “pure noise”. It is often chosen as an Ornstein–Uhlenbeck (OU) diffusion,\n\\[\ndX = - \\frac12 X \\, dt + dW.\n\\tag{1}\\]\nThis diffusion is reversible with respect to, and quickly converges to, the reference distribution \\(\\pi_{\\mathrm{ref}} = \\mathcal{N}(0, I)\\) and has the good taste of having simple transition densities: the law of \\(X_{t+s}\\) given that \\(X_t = x_t\\) is the same as \\(e^{-s/2} x_t + \\sqrt{1-e^{-s}} \\, \\mathbf{n}\\), which we write as\n\\[\n\\alpha_s x + \\sigma_s \\, \\mathbf{n}\\qquad \\text{with} \\qquad\n\\left\\{\n\\begin{aligned}\n\\alpha_s &= \\sqrt{1-\\sigma_s^2}\\\\\n\\sigma^2_s &= 1-e^{-s}\n\\end{aligned}\n\\right.\n\\]\nfor isotropic Gaussian noise term \\(\\mathbf{n}\\sim \\pi_{\\mathrm{ref}} = \\mathcal{N}(0,I)\\). We have:\n\\[\nF(s,x,y) \\equiv \\mathop{\\mathrm{P}}(X_{t+s} \\in dy \\, | \\, X_t = x )\n\\; \\propto \\;\n\\exp {\\left\\{ -\\frac{(y - \\alpha_s \\, x)^2}{2 \\, \\sigma^2_s} \\right\\}} .\n\\tag{2}\\]\nwhere the notation \\(F(s,x,y)\\) designates the forward transition from \\(x\\) to \\(y\\) in “\\(s\\)” amount of time. This also means that one can directly generate samples from \\(p_t(dx)\\) by first choosing a data samples \\(x_i\\) from the data distribution \\(p_{\\mathrm{data}} \\equiv p_0\\) and blend it with noise by setting \\(x_i^{(t)} = \\alpha_t \\, x_i + \\sigma_t \\, \\mathbf{n}\\).\n\n\nThe reverse diffusion\nIn order to generate samples from the data distribution, the DDPM strategy consists in sampling from the Gaussian reference measure \\(\\pi_{\\mathrm{ref}}\\) at time \\(t=T\\) and simulate the OU process backward in time. In other words, one would like to simulate from the reverse process \\(\\overleftarrow{X}_t\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIn other words, the reverse process is distributed as \\(\\overleftarrow{X}_0 \\sim \\pi_{\\mathrm{ref}}\\) at time \\(t=0\\) and, crucially, we have that \\(\\overleftarrow{X}_T \\sim \\pi_{\\mathrm{data}}\\). Furthermore, and as explained in this note, the reverse diffusion follows the dynamics\n\\[\nd\\overleftarrow{X}_t = {\\color{red} + }\\frac12 \\overleftarrow{X}_t \\, dt\n\\; {\\color{red} + \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\;\n+ dB\n\\tag{3}\\]\nwhere \\(B\\) is another Wiener process. I have used the notation \\(B\\) to emphasize that there is no link between this Wiener process and the one used to simulate the forward process. Note that if the initial data distribution \\(p_0 \\equiv \\pi_{\\mathrm{data}}\\) were equal to the reference measure \\(\\pi_{\\mathrm{ref}}\\), i.e. \\(p_0 = p_T = \\pi_{\\mathrm{ref}}\\) then it is easy to see that the reverse diffusion would have exactly teh same dynamics as the forward diffusion. In order to simulate the reverse diffusion, one needs to be able to estimate the new term \\({\\color{red}\\nabla \\log p_{T-t}(x)}\\) called the score. If one can estimate the score, it is straightforward to simulate the reverse process \\(\\overleftarrow{X}_t\\) all the way to \\(t=T\\) and obtain samples from the data distribution.\n\n\nDenoising to estimating the score\nIn practice, the score is unknown and one has to build an approximation of it\n\\[\\mathcal{S}(t,x) \\; \\approx \\; \\nabla_x \\log p_t(x).\\]\nThe approximate score \\(\\mathcal{S}(t,x)\\) is often parametrized by a neural network. Since the forward transitions are available and\n\\[\n\\log p_t(x) \\; = \\; \\log \\int \\; F(t, x_0, x)\\; \\pi_{\\mathrm{data}}(d x_0)\n\\]\nthe analytical expression of \\(F(t, x_0, x)\\) given in Equation 2 readily gives that\n\\[\n\\nabla_x \\log p_t(x) \\; = \\; -\\frac{x - \\alpha_t \\, \\widehat{x}_0(x,t)}{\\sigma_t^2}\n\\tag{4}\\]\nwhere \\(\\widehat{x}_0(x,t)\\) is a “denoising” estimate of the initial position \\(x_0\\) given a noisy estimate \\(X_t=x\\) at time \\(t\\),\n\\[\n\\widehat{x}_0(x,t) \\; = \\; \\mathop{\\mathrm{E}}[X_0  \\; \\mid \\; X_t = x].\n\\]\nFor simplifying notation, I will often write \\(\\widehat{x}_0(x_t, t)\\) as \\(\\widehat{x}_0(x_t)\\) when it is clear that \\(x_t\\) is a sample obtained at time \\(0 \\leq t \\leq T\\). Equation 4 means that to estimate the score, one only needs to train a denoising function\n\\[\n\\widehat{x}_0(\\cdots): [0,T] \\times \\mathbb{R}^d \\to \\mathbb{R}^d.\n\\]\nIt is a simple regression problem: take a bunch of pairs \\((X_0, X_t)\\) that can be generated as\n\\[\nX_0 \\sim \\pi_{\\mathrm{data}}\n\\qquad \\textrm{and} \\qquad\nX_t = \\alpha_t X_0 + \\sigma_t \\, \\mathbf{n}\n\\]\nwith \\(\\mathbf{n}\\sim \\mathcal{N}(0,I)\\) and minimize the Mean Squared Error (MSE) loss, i.e.\n\\[\n\\mathop{\\mathrm{E}}\\|X_0 - \\widehat{x}_0(t, X_t)\\|^2,\n\\]\nwith stochastic gradient descent or any other stochastic optimization procedure. The score is then defined as\n\\[\n\\mathcal{S}(t,x) \\; = \\; -\\frac{x - \\alpha_t \\, \\widehat{x}_0(t,x)}{\\sigma^2_t}.\n\\]\n\n\nDenoiser: practical parametrization and training\nIn practice, it may not be efficient, or stable, to try to directly parametrize the denoiser \\(\\widehat{x}_0(\\cdots)\\) with a neural network and simply descend the loss\n\\[\n\\mathop{\\mathrm{E}}\\|X_0 - \\widehat{x}_0(t, X_t)\\|^2.\n\\]\nFor example, for \\(t \\approx 0\\), we have that \\(\\widehat{x}_0(t,X_t) \\approx X_t \\approx X_0\\) so that it is very easy to reconstruct \\(X_0\\) from \\(X_t\\). On the contrary, for large \\(t\\), there is almost no information contained within \\(X_t\\) to reconstruct \\(X_0\\). This means that the typical value of the loss depends widely on \\(t\\), which makes it difficult to optimize: with this parametrization, since large values of the loss will be typically concentrated to large values of \\(t\\), the denoiser will not be accurate for \\(t \\approx 0\\), leading to sub-optimal results. Since \\(X_t = \\alpha_t \\, X_0 + \\sigma_t \\, \\mathbf{n}\\), one can defined the Signal-to-Noise-Ratio as\n\\[\\mathrm{SNR}(t) = \\frac{\\alpha_t}{\\sigma_t}\\]\nand, in order to normalize by the reconstruction difficulty, it makes more sense to train a denoiser by minimizing the loss:\n\\[\n\\mathop{\\mathrm{E}} {\\left[  \\mathrm{SNR}^2(t) \\times \\|X_0 - \\widehat{x}_0(t, X_t)\\|^2  \\right]} .\n\\]\nIt turns out that it is entirely equivalent to minimizing the loss\n\\[\n\\mathop{\\mathrm{E}} {\\left[  \\| \\mathbf{n}- \\widehat{\\mathbf{n}}(t, X_t)\\|^2  \\right]} .\n\\]\nwhere \\(X_t = \\alpha_t \\, X_0 + \\sigma_t \\, \\mathbf{n}\\) while the denoiser \\(\\widehat{x}_0(\\ldots)\\) and noise estimator \\(\\widehat{\\mathbf{n}}(\\ldots)\\) are parametrized so that\n\\[\nX_t = \\alpha_t \\, \\widehat{x}_0(t, X_t) + \\sigma_t \\, \\widehat{\\mathbf{n}}(t,X_t).\n\\]\nThat is one of the reasons why most of the papers on DDPM are parametrizing the denoiser \\(\\widehat{x}_0(\\ldots)\\) by building instead a “noise estimator” \\(\\widehat{\\mathbf{n}}(\\ldots)\\) with a neural network and setting\n\\[\n\\widehat{x}_0(t,X_t) = \\frac{X_t - \\sigma_t \\, \\widehat{\\mathbf{n}}(t,X_T)}{\\alpha_t}.\n\\]\nSince \\(\\alpha_t \\to 1\\) and \\(\\sigma_t \\to 0\\) for \\(t \\ll 1\\), this also implicitly ensures that \\(\\widehat{x}_0(t,X_t) \\approx X_t\\) for \\(t \\ll 1\\), as required.\n\n\n\nThe “denoising” diffusion\nOnce the denoiser \\(\\widehat{x}_0(\\ldots)\\) has been trained, the reverse diffusion defined \\(\\overleftarrow{X}_s = X_{T-s}\\) as to be simulated. Plugging Equation 4 back in the expression of the dynamics of the reverse diffusion shows that\n\\[\nd\\overleftarrow{X}_s =\n-\\frac12 \\, \\frac{1}{\\tanh((T-s)/2)} \\,\n{\\left(  \\overleftarrow{X}_s - \\frac{\\widehat{x}_0(\\overleftarrow{X}_s)}{\\cosh((T-s)/2)}  \\right)}\n\\; + \\;\ndB\n\\tag{5}\\]\nThis dynamics is intuitive: as \\(s \\to T\\) we have \\(\\cosh((T-s)/2) \\to 1\\) and \\(\\varepsilon^2 \\equiv \\tanh((T-s)/2) \\sim (T-s)/2 \\to 0\\) so that the dynamics is similar to\n\\[\nd\\overleftarrow{X}_s =\n-\\frac12 \\, \\frac{1}{\\varepsilon^2} \\,\n{\\left(  \\overleftarrow{X}_s - \\widehat{x}_0 \\right)}\n\\; + \\;\ndB\n\\]\nwhich is OU process that converges quickly, i.e. on time-scale of order \\(\\mathcal{O}(\\varepsilon^2)\\), towards a Gaussian distribution with mean \\(\\widehat{x}_0\\) (i.e. the denoised estimate) and variance \\(\\varepsilon^2\\).\nTo discretize the reverse dynamics Equation 5 on a small interval \\([\\overline{s}, \\overline{s}+\\delta]\\), one can for example consider the slightly simplified (linear) dynamics\n\\[\nd\\overleftarrow{X}_s =\n-\\frac12 \\, \\frac{1}{\\tanh((T-s)/2)} \\,\n{\\left(  \\overleftarrow{X}_s - \\mu \\right)} .\n\\; + \\;\ndB\n\\]\nHere, \\(\\mu = \\widehat{x}_0(\\overleftarrow{X}_{\\overline{s}}) / \\cosh((T-\\overline{s})/2)\\) with \\(\\overleftarrow{X}_{\\overline{s}} = \\overleftarrow{x}_{\\overline{s}}\\). Algebra gives that, conditioned upon \\(\\overleftarrow{X}_{\\overline{s}} = \\overleftarrow{x}_{\\overline{s}}\\), we have\n\\[\n\\left\\{\n\\begin{aligned}\n\\mathop{\\mathrm{E}}[ \\overleftarrow{X}_{\\overline{s} + \\delta} ]\n\\quad &= \\quad \\mu + \\lambda \\, (\\overleftarrow{x}_{\\overline{s}} - \\mu)\\\\\n\\mathop{\\mathrm{Var}}[ \\overleftarrow{X}_{\\overline{s} + \\delta} ]\n\\quad &= \\quad\n\\tanh {\\left( \\frac{T-\\overline{s}-\\delta}{2} \\right)}  \\, (1-\\lambda^2)\n\\end{aligned}\n\\right.\n\\]\nwhere the coefficient \\(0&lt;\\lambda&lt;1\\) is given by\n\\[\n\\lambda = \\frac{\\sinh(T-\\overline{s}-\\delta)}{\\sinh(T-\\overline{s})}.\n\\]\nThis discretization is more stable and efficient than a naive Euler-Maruyama approach since the drift term can get large as \\(s \\to T\\). WIth the above discretization, one can easily simulate the reverse diffusion on \\([0,T]\\) and generate approximate samples from \\(\\pi_{\\mathrm{data}}\\).\nIn the animation below, the method described in these notes was used. This very small example trains in less than a minute on an old laptop without a GPU. The noise estimator was parametrized with an MLP with \\(\\mathrm{elu}(\\ldots)\\) non-linearity and two hidden-layers with size \\(H=128\\). It was very useful to add a few Fourier features as input of the network, I could not make this work properly without.\n\n\nVideo\n\n\nThe literature on DDPM is enormous and still growing!"
  },
  {
    "objectID": "notes/information_theory_basics/information_theory_entropy.html",
    "href": "notes/information_theory_basics/information_theory_entropy.html",
    "title": "Information Theory: Entropy and Basic Definitions",
    "section": "",
    "text": "Claude Shannon (1916 – 2001)\n\n\n\n\nShannon Entropy & Compression\nIf Alice chooses a number \\(X\\) uniformly at random from the set \\(\\{1,2, \\ldots, N\\}\\), Bob can use a simple “dichotomy” strategy to ask Alice \\(\\log_2(N)\\) binary Yes/No questions and correctly identify the number. This result can be generalized to non-uniform distributions (cf. Huffman codes, and also Kraft-McMillan inequality). If Alice chooses a number \\(X\\) from \\(\\{1,2, \\ldots, N\\}\\) with probabilities \\(\\mathop{\\mathrm{P}}(X=k) = p_k\\), Bob can design a deterministic strategy to find the answer using, on average, about\n\\[\nH(X) = - \\sum_{k=1}^N p_k \\, \\log_2(p_k)\n\\tag{1}\\]\nbinary questions, ie. bits. To be more precise, there are strategies that require at most \\(H(X) + 1\\) questions on average, and none that can require less than \\(H(X)\\). Note that applying this remark to an iid sequence \\(X_{1:T} = (X_1, \\ldots, X_T)\\) and using the the fact that \\(H(X_1, \\ldots, X_T) = T \\, H(X)\\), this shows that one can exactly determining the sequence \\(X_{1:T}\\) with at most \\(T \\, H(X) + 1\\) binary questions on average. The quantity \\(H(X)\\) defined in Equation 1, known as the Shannon Entropy of the distribution \\((p_1, \\ldots, p_N)\\), also implies that there are strategies that can encode each integer \\(1 \\leq x \\leq N\\) as a binary string of length \\(L(x)\\) (i.e. with \\(L(x)\\) bits), with the expected length \\(\\mathop{\\mathrm{E}}[L(X)]\\) approximately equal to \\(H(X)\\). It is because a sequence of binary questions can be thought of as a binary tree, etc…\nThis remark can be used for compression. Imagine a very long sequence \\((X_1, \\ldots, X_T)\\) of iid samples from \\(X\\). Encoding each \\(X_i\\) with \\(L(X_i)\\) bits, one should be able to encode the resulting sequence with\n\\[\nL(X_1) + \\ldots + L(X_T) \\approx T \\, \\mathop{\\mathrm{E}}[L(X)] \\approx T \\cdot H(X)\n\\]\nbits. Can the usual zip compression algorithm do this? To test this, choose a probability distribution on \\(\\{1, \\ldots, N\\}\\), generate an iid sequence of length \\(T \\gg 1\\), compress this using the \\(\\texttt{gzip}(\\ldots)\\) command, and finally look at the size of the resulting file (in bits). I have done that a few times, with a few different values of \\(16 \\leq N \\leq 256\\) and a few random distributions on \\(\\{1, \\ldots, N\\}\\), and with \\(T = 10^6\\). The plot of size of the compressed files versus the Shannon entropy \\(H\\) looks as below:\n\n\n\n\n\n\n\nSeems like the zip-algorithm works almost optimally for compressing iid sequences.\n\n\nSequence of random variables\nNow consider a pair of discrete random variables \\((X,Y)\\). If Alice draws samples from this pair of rvs, one can ask \\(H(X,Y)\\) binary questions on average to exactly find out these values. To do that, one can ask \\(H(X)\\) questions to estimate \\(X\\), and once \\(X=x\\) is estimated, one can then ask about \\(H(Y|X=x) = -\\sum_y \\mathop{\\mathrm{P}}(Y=y|X=x) \\, \\log_2(\\mathop{\\mathrm{P}}(Y=y|X=x))\\) to estimate \\(Y\\). This strategy requires on average \\(H(X) + \\sum_x \\mathop{\\mathrm{P}}(X=x) \\, H(Y|X=x)\\) binary questions and is actually optimal, showing that\n\\[\nH(X,Y) = H(X) + H(Y | X)\n\\tag{2}\\]\nwhere we have defined \\(H(Y | X) = \\sum_x \\mathop{\\mathrm{P}}(X=x) \\, H(Y|X=x)\\).\nIndeed, one can generalize these concepts to more than two random variables. Iterating Equation 2 shows that the trajectory \\(X_{1:T} \\equiv (X_1, \\ldots, X_N)\\) of a stationary ergodic Markov chain can be estimated on average with \\(H(X_{1:T})\\) binary questions where\n\\[\n\\begin{align}\nH(X_{1:T})\n&= H(X_1) + H(X_2|X_1) + \\ldots + H(X_{T} | X_{t-1})\\\\\n&\\approx T \\, H(X_{k+1} | X_k)\\\\\n&= - T \\, \\sum_x \\pi(x) \\, \\sum_{y} p(x \\to y) \\, \\log_2[p(x \\to y)].\n\\end{align}\n\\]\nHere, \\(\\pi(dx)\\) is the equilibrium distribution of the Markov chain and \\(p(x \\to y)\\) are the transition probabilities.\nCan \\(\\texttt{gzip}(\\ldots)\\) compress Markovian trajectories and roughly achieve that level of compression? Indeed, one can test this by generating random Markov transition matrices (that are, with very high probability, ergodic with respect to an equilibrium distribution \\(\\pi\\)). Doing this with trajectories of length \\(10^4\\) (ie. quite short because it is quite slow to) on \\(\\{1, \\ldots, N\\}\\) with \\(2 \\leq N \\leq 64\\), one get the following results:\n\n\n\n\n\n\n\nIn red is the entropy estimated without using the Markovian structure and assuming that the \\(X_i\\) are iid samples. One can clearly see that the dependencies between successive samples are exploited for better compression. Nevertheless, the optimal compression rate given by the Shannon entropy is not reached. Indeed, \\(\\texttt{gzip}(\\ldots)\\) is not an optimal algorithm – it cannot even compress well enough the sequence \\((1,2,3,1,2,3,1,2,3,\\ldots)\\)!\n\n\nAsymptotic Equipartition Property (AEP)\nThe AEP is simple remark that gives a convenient way of reasoning about long sequences random variables \\(X_{1:T} = (X_1, \\ldots, X_T)\\) with \\(T \\gg 1\\). For example, assuming that the random variables \\(X_i\\) are independent and identically distributed as the random variable \\(X\\), the law of large numbers (LLN) gives that\n\\[\n-\\frac{1}{T} \\, \\log_2(X_{1:T}) = -\\frac{1}{T} \\, \\sum \\log_2 p(X_i) \\approx H(X).\n\\]\nThis means that any “typical” sequence has a probability about \\(2^{-T \\, H(X)}\\) of occurring, which also means that there are about \\(2^{T \\, H(X)}\\) such “typical” sequences. Indeed, one could use the LLN for Markov chains and obtain a similar statement for Markovian sequences. All this can be made rigorous with large deviation theory, for example. This also establishes the link with the “statistical physics” definition of entropy as the logarithm of the number of configurations… Set of the type\n\\[\nA_{\\varepsilon} = \\left\\{ x_{1:T} \\; : \\; \\left| -\\frac{1}{T} \\, \\log_2(x_{1:T}) - H(X)\\right| &lt; \\varepsilon\\right\\}\n\\]\nare usually called typical set: for any \\(\\varepsilon&gt; 0\\), the probability of \\(X_{1:T}\\) to belongs to \\(A_{\\varepsilon}\\) goes to one as \\(T \\to \\infty\\). For these reasons, it is often a good heuristic to think of a draw of \\((X_1, \\ldots, X_T)\\) as a uniformly distributed on the associated typical set. For example, if \\((X_1, \\ldots, X_N)\\) are \\(N\\) iid draws from a Bernoulli distribution with \\(\\mathop{\\mathrm{P}}(X=1) = 1-\\mathop{\\mathrm{P}}(X=0) =p\\), the set \\(A \\subset \\{0,1\\}^N\\) of sequences such that \\(x_1 + \\ldots + x_N = Np\\) has \\(\\binom{N}{Np} \\approx 2^{N \\, h_2(q)}\\) elements where \\(h_2(q) = -[q \\, \\log_2(q) + (1-q) \\, \\log_2(q)]\\) is the entropy of a \\(\\text{Bern}(q)\\) random variable.\n\n\nMutual information\nConsider a pair of random variables \\((X,Y)\\). Assuming that \\(X\\) stores (on average) \\(H(X)\\) bits of useful information, how much of it is still contained in \\(Y\\)? If \\(Y\\) is independent from \\(X\\), everything has been lost and, on the contrary, if \\(X=Y\\), nothing has been lost. If one knows \\(Y\\), one needs on average \\(H(X|Y)\\) binary questions (ie. bits of information) in order to determine \\(X\\) certainly and recovered all the information contained in \\(X\\). This means that the knowledge of \\(Y\\) still contains \\(H(X) - H(X|Y)\\) useful bits of information! This quantity is called the mutual information of the two random variable \\(X\\) and \\(Y\\), and it has the good taste of being symmetric:\n\\[\n\\begin{align}\nI(X;Y) &= H(X) - H(X|Y) \\\\\n&= H(X) + H(Y) - H(X,Y) \\\\\n&= H(Y;X).\n\\end{align}\n\\]\nNaturally, one can define conditional version of it by setting \\(I(X;Y \\, |Z) = \\sum_{z} \\mathop{\\mathrm{P}}(Z=z) \\, I(X_z | Y_z)\\) where \\((X_z, Y_z)\\) has the law of \\((X,Z)\\) conditioned on \\(Z=z\\). Since \\(I(X;Y \\,|Z)\\) is the reduction in uncertainty of \\(X\\) due to \\(Y\\) when \\(Z\\) is given, there are indeed situations when \\(I(X;Y \\, | Z)\\) is larger than \\(I(X;Y)\\) – it is to be contrasted to the intuitive inequality \\(H(X|Z) \\leq H(X)\\), which is indeed true. A standard such examples is when \\(X\\) and \\(Y\\) are independent \\(\\text{Bern}(1/2)\\) random variables and \\(Z = X+Y\\): a short computation gives that \\(I(X;Y \\, | Z) = 1/2\\) while, indeed, \\(I(X;Y) = 0\\). This definition of conditional mutual information leads to a chain-rule property,\n\\[\nI(X; (Y_1,Y_2)) = I(X;Y_1) + I(X;Y_2 | Y_3),\n\\]\nwhich can indeed be generalized to any number of variables. Furthermore, if the \\(Y_i\\) are conditionally independent given \\(X\\) (eg. if \\(X=(X_1, \\ldots, X_T)\\) and \\(Y_i\\) only depend on \\(X_i\\)), then the sub-additivity of the entropy readily gives that\n\\[\nI(X; (Y_1, \\ldots, Y_N)) \\leq \\sum_{i=1}^N I(X; Y_i).\n\\]\nImportantly, algebra shows that \\(I(X;Y)\\) can also be expressed as the Kullback-Leibler divergence between the joint distribution \\(\\mathop{\\mathrm{P}}_{(X,Y)}\\) and the product of the marginals \\(\\mathop{\\mathrm{P}}_X \\otimes \\mathop{\\mathrm{P}}_Y\\),\n\\[\nI(X;Y) \\; = \\;\n\\mathop{\\mathrm{D_{\\text{KL}}}} {\\left(  (X,Y) \\, \\| \\, X \\otimes Y \\right)} .\n\\]\nThis diagram from (MacKay 2003) nicely illustrate the different fundamental quantities \\(H(X)\\) and \\(H(X,Y)\\) and \\(H(Y|X)\\) and \\(I(X;Y)\\) and \\(H(X,Y)\\):\n\n\n\n\nFrom: Information Theory, Inference, and Learning Algorithms\n\n\n\nNaturally, if one considers three random variables \\(X \\mapsto Y \\mapsto Z\\) forming a “Markov chain”, we have the so-called data-processing inequality,\n\\[\nI(X;Z) \\leq I(X;Y)\n\\qquad \\text{and} \\qquad\nI(X;Z) \\leq I(Y;Z).\n\\]\nThe first inequality is clear since all the useful information contained in \\(Z\\) must be coming from \\(Y\\), and \\(Y\\) only contains \\(I(X;Y)\\) bits about \\(X\\). For the second inequality, note that if \\(Z\\) contains \\(I(Y;Z)\\) bits about \\(Y\\), and \\(Y\\) contains \\(H(Y;X)\\) bits about \\(X\\), then \\(Z\\) cannot contain more than \\(I(Y;Z)\\) bits of \\(X\\):\n\n\n\n\nData Processing Inequality\n\n\n\n\n\n\n\n\nReferences\n\nMacKay, David JC. 2003. Information Theory, Inference and Learning Algorithms. Cambridge university press."
  },
  {
    "objectID": "notes/information_theory_shearer_lemma/shearer_lemma.html",
    "href": "notes/information_theory_shearer_lemma/shearer_lemma.html",
    "title": "Shearer’s lemma",
    "section": "",
    "text": "The Shearer’s lemma (Chung et al. 1986) is concerned with a generalization of the sub-additivity of the Shannon Entropy,\n\\[\nH(X_1, \\ldots, X_N) \\; \\leq \\; H(X_1) + \\ldots + H(X_N).\n\\]\nInstead, consider an integer \\(t \\geq 1\\) and a family \\(S_1, \\ldots, S_K\\) of subsets of \\(\\{1, \\ldots, N\\}\\) such that any index \\(1 \\leq n \\leq N\\) appears in at least \\(t\\) of these subsets. Note that for a subset \\(S_i = \\{ \\alpha_1, \\ldots, \\alpha_{r_i}\\}\\) with \\(\\alpha_1 &lt; \\ldots &lt; \\alpha_{r_i}\\) we have\n\\[\n\\begin{align}\nH(X_{S_i}) &\\equiv H(X_{\\alpha_1}, \\ldots, X_{\\alpha_{r_i}})\\\\\n&= H(X_{\\alpha_1}) + H(X_{\\alpha_2} | X_{\\alpha_1}) + \\ldots + H(X_{\\alpha_{r_i}} | X_{\\alpha_{r_i-1}}, \\ldots, X_{\\alpha_1} ) \\\\\n&\\geq H(X_{\\alpha_1}) + H(X_{\\alpha_2} | X_{1:(\\alpha_2-1)}) + \\ldots + H(X_{\\alpha_{r_i}} | X_{1:\\alpha_{r_i}-1}).\n\\end{align}\n\\tag{1}\\]\nSince each index appears in at least \\(t\\) of the subsets, summing Equation 1 over all the subset \\(S_i\\) yields\n\\[\n\\sum_{i=1}^K H(X_{S_k}) \\geq t \\, \\sum_{i=1}^N H(X_i \\, | X_{1:(i-1)}) = t \\, H(X).\n\\]\nThis means that the following inequality holds,\n\\[\nH(X) \\leq \\frac{1}{t} \\, \\sum_{k=1}^K H(X_{S_k})\n\\]\nIndeed, the standard sub-additivity property of the entropy corresponds to the set \\(S_k = [k]\\) for \\(1 \\leq k \\leq N\\) and \\(t=1\\).\n\nApplication: projection on hyperplanes\nConsider a measurable set \\(A \\subset \\mathbb{R}^n\\) and call \\(A_k\\) the projection of \\(A\\) on the hyperplane \\(\\{x=(x_1, \\ldots, x_n) \\in \\mathbb{R}^n \\, : \\, x_k=0\\}\\). A Theorem of Loomis and Whitney (Loomis and Whitney 1949) states that the lebesgue measure \\(|A|\\) of the set \\(A\\) satisfies\n\\[\n|A| \\; \\leq \\; \\prod_{k=1}^n |A_k|^{1/(n-1)}.\n\\]\nIn other words, if all the projections \\(A_k\\) of the set \\(A\\) are small then, necessarily, the set \\(A\\) itself is small. To proceed, one can approximate this set \\(A\\) with the union \\(A_{\\varepsilon}\\) of small cubes of side \\(\\varepsilon\\) centred on \\(\\varepsilon\\, \\mathbb{Z}^n\\). If one can prove the statement for \\(A^{[\\varepsilon]}\\), the results follows from a standard approximation argument (ie. outer measure). Now, each cube can be indexed with a \\(n\\)-uple of integers \\((x_1, \\ldots, x_n) \\in \\mathbb{Z}^n\\), and one can consider the random variable \\(X=(X_1, \\ldots, X_n)\\) that is uniformly distributed on the set of cubes coordinates. Because \\(2^{H(X)} = |A^{[\\varepsilon]}| / \\varepsilon^n\\) and \\(2^{H(X_2, \\ldots, X_n)} = |A_1^{[\\varepsilon]}| / \\varepsilon^n\\) etc…, choosing the subsets \\(S_i=[1:n] \\setminus \\{i\\}\\) and \\(t = (n-1)\\) in Shearer’s Lemma immediately gives the conclusion.\n\n\n\n\n\nReferences\n\nChung, Fan RK, Ronald L Graham, Peter Frankl, and James B Shearer. 1986. “Some Intersection Theorems for Ordered Sets and Graphs.” Journal of Combinatorial Theory, Series A 43 (1). Academic Press: 23–37.\n\n\nLoomis, Lynn H, and Hassler Whitney. 1949. “An Inequality Related to the Isoperimetric Inequality.”"
  },
  {
    "objectID": "notes/information_theory_fano/information_theory_fano.html",
    "href": "notes/information_theory_fano/information_theory_fano.html",
    "title": "Information Theory: Fano’s inequality",
    "section": "",
    "text": "Robert Fano (1917 – 2016)\n\n\n\n\nFano’s inequality\nConsider a three random variables forming a Markov chain,\n\\[\nX \\mapsto Y \\mapsto \\widehat{X}\n\\tag{1}\\]\nin the sense that \\(Y = \\textrm{function}(X, \\text{noise})\\) and \\(Z = \\textrm{function}(Y, \\text{noise})\\). Typical situations include:\n\nWe select a parameter \\(\\theta\\) for a probabilistic model \\(\\mathop{\\mathrm{P}}_{\\theta}\\). Afterward, we collect data \\(X\\) from this model, and our goal is to estimate the parameter \\(\\theta\\) solely from the data \\(X\\).\nWe generate data \\(X\\), compress this data into \\(X_{\\text{zip}}\\), and then attempt to recover the original data \\(X\\) as closely as we can.\n\nSince each step in Equation 1 destroys some information (eg. data processing), it is important to measure how accurately \\(\\widehat{X}\\) estimates the initial input, \\(X\\). In other words, we want to know how much more information (expressed as ‘bits’) we need to reconstruct \\(X\\) using knowledge of \\(\\widehat{X}\\) alone, i.e. we would like to upper-bound \\(H(X \\, | \\widehat{X})\\). For this purpose, imagine an “error” variable \\(E\\) that indicates whether \\(\\widehat{X}\\) perfectly matches \\(X\\),\n\\[\nE = \\mathbf{1} {\\left( \\widehat{X} \\neq X \\right)} .\n\\]\nThe probability of error is \\(p_E = \\mathop{\\mathrm{P}}(\\widehat{X} \\neq X)\\) and \\(E = \\text{Bern}(p_E)\\). To estimate \\(X\\) from \\(\\widehat{X}\\), we can start by learning if \\(\\widehat{X}\\) equals \\(X\\), which costs us \\(H(E | \\widehat{X}) \\leq H(E) = h_2(p_E)\\) ‘bits’ of information. If it turns out that \\(E=0\\), we are done asking. If we find that \\(E=1\\), however, we need to ask additional \\(H(X | \\widehat{X}, E)\\) questions. Crucially, \\(H(X | \\widehat{X}, E) \\leq H(X)\\), but also \\(H(X | \\widehat{X}, E) \\leq \\log_2(|\\mathcal{X}|-1)\\) since \\(X\\) can take any value in \\(\\mathcal{X}\\) except \\(\\widehat{X}\\) when \\(E=1\\). Writing this reasoning quantitatively gives Fano’s inequality:\n\\[\n\\begin{align}\nH(X | \\widehat{X})\n&\\leq h_2(p_E) + p_E \\, \\log_2(|\\mathcal{X}|-1).\n\\end{align}\n\\tag{2}\\]\nApparently, this inequality was first derived by Robert Fano in the 50s while teaching a Ph.D. seminar at MIT. In words: a large \\(H(X | \\widehat{X})\\) means that \\(\\widehat{X}\\) offers insufficient information about \\(X\\), and as a result, the probability of error \\(p_E\\) must be high.\n\n\nApplications:\n\nConverse of Shannon’s coding theorem"
  },
  {
    "objectID": "notes/wasserstein_langevin/wasserstein_langevin.html",
    "href": "notes/wasserstein_langevin/wasserstein_langevin.html",
    "title": "Wasserstein Gradients & Langevin Diffusions",
    "section": "",
    "text": "Overdamped Langevin Diffusion as a gradient flow on the space of probability distributions endowed with the \\(2\\)-Wasserstein metric\n\n\n\nConsider a target probability density \\(\\pi(x) = \\frac{\\overline{\\pi}(x)}{\\mathcal{Z}}\\) on \\(\\mathbb{R}^D\\) that is known up to a normalizing constant \\(\\mathcal{Z}&gt; 0\\). We also have a different probability density \\(p_0(x)\\). The goal is to gradually tweak \\(p_0(x)\\) so that it eventually matches \\(\\pi(x)\\). More concretely, we aim to perform a gradient descent on the space of probability distributions to reduce the functional\n\\[\n\\mathcal{F}(p) \\; = \\; \\mathop{\\mathrm{D_{\\text{KL}}}} {\\left( p, \\pi \\right)}  \\; = \\; \\int p(x) \\, \\log  {\\left\\{ \\frac{p(x)}{\\overline{\\pi}(x)} \\right\\}}  \\, dx \\, + \\, \\textrm{(constant)}.\n\\]\nThis approach can be discretized: assume \\(N \\gg 1\\) particles \\(X_0^1, \\ldots, X_0^N \\in \\mathbb{R}^D\\) forming an empirical distribution that approximates \\(p_0(dx)\\),\n\\[\np_0(dx) \\; \\approx \\; \\frac{1}{N} \\sum_{i=1}^N \\, \\delta_{X_0^i}(dx).\n\\]\nDefine \\(X_{\\delta}^i = X_0^i + \\delta_t \\, \\mu(X_0^i)\\) where \\(\\delta_t \\ll 1\\) denotes a time discretization parameter and \\(\\mu:\\mathbb{R}^D \\to \\mathbb{R}^D\\) is a “drift” function. Finding a suitable ‘drift function’ is the main problem. According to the Fokker-Planck equation, the computed empirical distribution\n\\[\np_{\\delta_t}(dx) \\; \\approx \\; \\frac{1}{N} \\sum_{i=1}^N \\, \\delta_{X_{\\delta_t}^i}(dx)\n\\]\napproximates \\(p_{\\delta_t}(x)\\) given by\n\\[\n\\frac{p_{\\delta_t}(x)- p_0(x)}{\\delta_t} \\; = \\; -\\nabla \\cdot  {\\left[ \\mu(x) \\, p_0(x) \\right]} .\n\\tag{1}\\]\nWhat is the optimal drift function \\(\\mu: \\mathbb{R}^D \\to \\mathbb{R}^D\\) that ensures that \\(p_{\\delta_t}\\) comes as close as possible to \\(\\pi\\)? Typically, we select \\(\\mu:\\mathbb{R}^D \\to \\mathbb{R}^D\\) such that the quantity \\(\\mathcal{F}(p_{\\delta_t})\\) is minimized, provided that \\(p_{\\delta_t}\\) is not drastically different from \\(p_0\\). One method is to use the \\(L^2\\) Wasserstein distance and assume the constraint\n\\[\n\\mathop{\\mathrm{D_{\\text{Wass}}}}(p_{0}, p_{\\delta_t}) \\approx \\int p_0(x) \\, \\| \\delta_t \\, \\mu(x) \\|^2 \\, dx \\leq \\varepsilon\n\\tag{2}\\]\nfor a parameter \\(\\varepsilon\\ll 1\\). More pragmatically, it is generally easier (eg. proximal methods) to minimize the joint objective\n\\[\n\\mathcal{F}(p_{\\delta_t}) + \\frac{1}{2 \\varepsilon} \\, \\mathop{\\mathrm{D_{\\text{Wass}}}}(p_{0}, p_{\\delta_t}).\n\\tag{3}\\]\nBased on equations Equation 1 and Equation 2, a first-order expansion shows that the joint objective Equation 3 can be approximated by\n\\[\n\\begin{align}\n-\\int &\\nabla \\cdot \\Big\\{  \\textcolor{red}{[\\delta_t \\mu]}(x) \\, p_0(x) \\Big\\} \\, \\log  {\\left\\{  \\frac{p_0(x)}{\\overline{\\pi}(x)}  \\right\\}} \\, dx \\, \\\\ &\\qquad + \\qquad \\, \\frac{1}{2 \\varepsilon} \\, \\int p_0(x) \\, \\|  \\textcolor{red}{[\\delta_t \\, \\mu]}(x) \\|^2 \\, dx,\n\\end{align}\n\\tag{4}\\]\na relatively straightforward quadratic function of the drift function \\(\\mu: \\mathbb{R}^D \\to \\mathbb{R}^D\\). The optimal drift function, ie. the minimizer of Equation 4, is given by \\[\n\\mu(x) \\; = \\; - {\\left(  \\frac{\\varepsilon}{\\delta_t}  \\right)}  \\, \\nabla \\log  {\\left\\{  \\frac{p_0(x)}{\\overline{\\pi}(x) }  \\right\\}} .\n\\]\nPut simply, this suggests that we should select the drift function proportional to \\(-\\nabla \\log[p_0(x) / \\overline{\\pi}(x)]\\). To implement this scheme, we begin by sampling \\(N \\gg 1\\) particles \\(X_0^i \\sim p_0(dx)\\) and let evolve each particle according to the following differential equation\n\\[\n\\frac{d}{dt} X_t^i \\; = \\; - \\nabla \\log  {\\left\\{  \\frac{p_t(X_t^i) }{ \\overline{\\pi}(X_t^i) }  \\right\\}}\n\\]\nwhere \\(p_t\\) is the density of the set of particles at time \\(t\\). It is the usual diffusion-ODE trick for describing the evolution of the density of an overdamped Langevin diffusion,\n\\[\ndX_t \\; = \\; -\\nabla \\log \\overline{\\pi}(X_t) \\, dt \\; + \\; \\sqrt{2} \\, dW_t.\n\\]\nThis can be shown by writing down the associated Fokker-Planck equation. This heuristic discussion shows that minimizing \\(\\mathop{\\mathrm{D_{\\text{KL}}}}(p, \\pi)\\) by introducing a gradient flow in the space of probability distributions with the Wasserstein metric essentially produces a standard overdamped Langevin diffusion. Indeed, transforming this heuristic discussion into a formal statement is not trivial: the constructive proof in (Jordan, Kinderlehrer, and Otto 1998) is now usually referred to as the JKO scheme.\nThe above derivation shows that the Wasserstein distance plays particularly well with minimizing functionals of the space of probability distributions. The same heuristic discussion shows that minimizing a functional of the type\n\\[\n\\mathcal{F}(p) \\; = \\; \\int \\Phi[p(x)] \\, \\nu(dx)\n\\]\nfor some cost function \\(\\Phi: (0, \\infty) \\to \\mathbb{R}\\) and distribution \\(\\nu(dx)\\) leads to choosing a drift function \\(\\mu:\\mathbb{R}\\to \\mathbb{R}\\) minimizing\n\\[\n\\int -\\nabla \\cdot \\Big\\{  \\textcolor{red}{[\\delta_t \\, \\mu]}(x) \\, p(x) \\Big\\} \\Phi'[p(x)] \\, \\nu(dx)\n\\, + \\, \\frac{1}{2 \\varepsilon} \\, \\int p_0(x) \\, \\|  \\textcolor{red}{[\\delta_t \\, \\mu]}(x) \\|^2 \\, dx.\n\\]\nThis can be approached identically to what as been done in the case of minimizing \\(\\mathop{\\mathrm{D_{\\text{KL}}}}(p, \\pi)\\).\n\n\n\n\nReferences\n\nJordan, Richard, David Kinderlehrer, and Felix Otto. 1998. “The Variational Formulation of the Fokker–Planck Equation.” SIAM Journal on Mathematical Analysis 29 (1). SIAM: 1–17."
  },
  {
    "objectID": "notes/sanov/sanov.html",
    "href": "notes/sanov/sanov.html",
    "title": "Sanov’s Theorem",
    "section": "",
    "text": "Sanov’s Theorem\nConsider a random variable \\(X\\) on the finite alphabet \\(\\{a_1, \\ldots, a_K\\}\\) with \\(\\mathop{\\mathrm{P}}(X=a_k) = p_k\\). For \\(N \\gg 1\\), consider a sequence \\((x_1, \\ldots, x_N)\\) obtained by sampling \\(N\\) times independently from \\(X\\) and set\n\\[\n\\widehat{p}_k = \\frac{1}{N} \\, \\sum_{i=1}^N \\, \\mathbf{1} {\\left( x_i = a_k \\right)}\n\\]\nthe proportion of \\(a_k\\) within this sequence. In other words, the empirical distribution obtained from the samples \\((x_1, \\ldots, x_N)\\) reads\n\\[\n\\widehat{p} = \\sum_{k=1}^K \\, \\widehat{p}_k \\, \\delta_{a_k}.\n\\]\nIndeed, the LLN indicates that \\(\\widehat{p}_k \\to p_k\\) as \\(N \\to \\infty\\), and it is important to estimate the probability that \\(\\widehat{p}_k\\) significantly deviates from \\(p_k\\). To this end, note that for another probability vector \\(q=(q_1, \\ldots, q_K)\\) the probability that\n\\[\n(\\widehat{p}_1, \\ldots, \\widehat{p}_K) \\; = \\; (q_1, \\ldots, q_K)\n\\]\nis straightforward to compute and reads\n\\[\n\\mathop{\\mathrm{P}}(\\widehat{p} = q) \\; = \\; \\binom{N}{N q_1, \\ldots, N q_K} \\, p_1^{N q_1} \\ldots p_R^{N q_K}.\n\\]\nStirling’s approximation \\(m! \\asymp m \\, \\ln(m)\\) then gives that\n\\[\n\\mathop{\\mathrm{P}}(\\widehat{p} = q) \\; \\asymp \\;\n\\exp {\\left( -N \\cdot \\mathop{\\mathrm{D_{\\text{KL}}}}(q,p) \\right)}\n\\]\nwhere \\(\\mathop{\\mathrm{D_{\\text{KL}}}}(q,p) = \\sum_{k=1}^K q_k \\, \\log[q_k / p_k]\\) is the Kullback–Leibler divergence of \\(q\\) from \\(p\\). In other words, as soon as \\(q \\neq p\\), the probability of observing \\(\\widehat{p} \\approx q\\) falls exponentially quickly to zero. With the language of Large Deviations, one can make this statement slightly more precise, rigorous and general, but it is essentially the content of Sanov’s Theorem.\n\n\nRare events happen in the least unlikely manner\nGiven a list of mutually exclusive events \\(E_1, \\ldots, E_R\\) and the knowledge that at least one of these events has taken place, the probability that the event \\(E_k\\) was the one that happened is \\(\\mathop{\\mathrm{P}}(E_k) / [\\mathop{\\mathrm{P}}(E_1) + \\ldots + \\mathop{\\mathrm{P}}(E_R)]\\). The implication is that if all the events are rare, that is \\(p_k \\approx e^{-N \\, I_k} \\ll 1\\), and it is known that one event has indeed occurred, there is a high probability that the event with the smallest \\(I_k\\) value was the one that happened: the rare event took place in the least unlikely manner.\nConsider an iid sequence \\((X_1, \\ldots, X_N)\\) of \\(N \\gg 1\\) discrete real-valued random variables with \\(\\mathop{\\mathrm{P}}(X = a_k) = p_k\\) and mean \\(\\mathop{\\mathrm{E}}(X) \\in \\mathbb{R}\\). Suppose one observes the rare event\n\\[\n\\frac{1}{N} \\sum_{i=1}^N x_i  \\geq \\mu\n\\tag{1}\\]\nfor some level \\(\\mu\\) significantly above \\(\\mathop{\\mathrm{E}}(X)\\). Naturally, the least unlikely way for this to happen is if \\((x_1 + \\ldots + x_N) / N \\, \\approx \\, \\mu\\). Furthermore, one may be interested in the empirical distribution \\(\\widehat{p}\\) associated to the sequence \\((x_1, \\ldots, x_N)\\) when the rare event Equation 1 does happen. The least unlikely empirical distribution is the one that minimizes \\(\\mathop{\\mathrm{D_{\\text{KL}}}}(\\widehat{p}, p)\\) under the constraint that\n\\[\n\\sum_{i=1}^K a_k \\, \\widehat{p}_k = \\mu.\n\\tag{2}\\]\nThe function \\(\\widehat{p} \\mapsto \\mathop{\\mathrm{D_{\\text{KL}}}}(\\widehat{p}, p)\\) is convex and the introduction of Lagrange multipliers shows that the solution to this constraint minimization problem is given by the Boltzmann distribution \\(p_{\\beta_\\mu}\\) defined as\n\\[\np_{\\beta_\\mu}(a_k) = \\frac{ p_k \\, e^{-\\beta_{\\mu} \\, a_k} }{Z(\\beta_{\\mu})}.\n\\tag{3}\\]\nThe parameter \\(\\beta_{\\mu} \\in \\mathbb{R}\\) is chosen so that the constraint Equation 2 be satisfied and the minus sign is to follow the “physics” convention. Note in passing that, in fact, the joint function \\((\\widehat{p},p) \\mapsto \\mathop{\\mathrm{D_{\\text{KL}}}}(\\widehat{p}, p)\\) is convex! As usual, if one defines the log-partition function as \\(\\Phi(\\beta) = -\\log Z(\\beta)\\), with\n\\[\nZ(\\beta) \\; = \\; \\sum_{k=1}^K \\, p_k \\, e^{-\\beta \\, a_k},\n\\]\none obtains that the constraint is equivalent to requiring \\(\\mu = \\frac{d}{d \\beta} \\Phi(\\beta) \\mid_{\\beta = \\beta_\\mu}\\). Furthermore, since \\(\\Phi\\) is smooth and strictly concave, so is the function \\(\\beta \\mapsto \\left&lt; \\mu, \\beta \\right&gt; - \\Phi(\\beta)\\), so that the condition \\(\\mu = \\frac{d}{d \\beta} \\Phi(\\beta) \\mid_{\\beta = \\beta_\\mu}\\) is equivalent to setting\n\\[\n\\beta_{\\mu} \\; = \\; \\mathop{\\mathrm{argmin}}_{\\beta \\in \\mathbb{R}} \\; \\left&lt; \\mu, \\beta \\right&gt; - \\Phi(\\beta).\n\\]\nNaturally, one can now also estimate the probability of the event \\(\\mathop{\\mathrm{P}}[(X_1 + \\ldots + X_N)/N \\approx \\alpha]\\) happening since one now knows that it is equivalent (on a log scale) to \\(\\exp[-N \\, \\mathop{\\mathrm{D_{\\text{KL}}}}(p_{\\beta_\\mu}, p)]\\). Algebra gives\n\\[\n\\begin{align}\n\\mathop{\\mathrm{D_{\\text{KL}}}}(p_{\\beta_\\mu}, p)\n&= \\Phi(\\beta_\\mu) - \\left&lt; \\mu, \\beta_\\mu \\right&gt;\\\\\n&= \\max_{\\beta \\in \\mathbb{R}} \\; \\Phi(\\beta) - \\left&lt; \\mu, \\beta \\right&gt;.\n\\end{align}\n\\]\nAs a sanity check, note that since \\(\\Phi(0)=0\\), we have that \\(\\mathop{\\mathrm{D_{\\text{KL}}}}(p_{\\beta_\\mu}, p) \\geq 0\\), as required. The statement that\n\\[\n\\frac{1}{N} \\, \\log \\mathop{\\mathrm{P}} {\\left\\{ \\frac{X_1 + \\ldots + X_N}{N} \\; \\approx \\; \\mu \\right\\}}  \\; = \\; - I(\\mu)\n\\]\nwith a (Large Deviation) rate function given by\n\\[\nI(\\mu) \\; = \\; \\max_{\\beta \\in \\mathbb{R}} \\; \\Phi(\\beta) - \\left&lt; \\mu, \\beta \\right&gt;\n\\]\nis more or less the content of Cramer’s Theorem. The rate function \\(I(\\mu)\\) and the function \\(\\log Z( \\textcolor{red}{-}\\beta)\\) are related by a Legendre transform.\n\n\nExample: averaging uniforms…\nNow, to illustrate the above discussion, consider \\(N=10\\) iid uniform random variables on the interval \\([0,1]\\). It is straightforward to simulate these \\(N=10\\) uniforms conditioned on the event that their mean exceeds the level \\(\\mu = 0.7\\), which is a relatively rare event. When this is done repetitively and the empirical distribution is plotted, the resulting distribution is as follows:\n\n\n\n\nMean of \\(10\\) uniforms conditioned on being larger than \\(\\mu = 0.7\\)\n\n\n\nIndeed, the distribution in blue is (very close to) the Boltzmann distribution with density \\(\\mathcal{D}_{\\beta}(x) = e^{-\\beta \\, x} / Z(\\beta)\\) with \\(\\beta \\in \\mathbb{R}\\) chosen so that \\(\\int_{0}^{1} x \\, \\mathcal{D}_{\\beta}(dx) = \\mu\\)."
  },
  {
    "objectID": "notes/DDPM_deterministic/DDPM_deterministic.html",
    "href": "notes/DDPM_deterministic/DDPM_deterministic.html",
    "title": "From Denoising Diffusion to ODEs",
    "section": "",
    "text": "Consider an empirical data distribution \\(\\pi_{\\mathrm{data}}\\). In order to simulate approximate samples from \\(\\pi_{\\mathrm{data}}\\), Denoising Diffusion Probabilistic Models (DDPM) simulate a forward diffusion process \\(\\{X_t\\}_{[0,T]}\\) on an interval \\([0,T]\\). The diffusion is initialized at the data distribution, i.e. \\(X_0 \\sim \\pi_{\\mathrm{data}}\\), and is chosen so that that the distribution of \\(X_T\\) is very close to a known and tractable reference distribution \\(\\pi_{\\mathrm{ref}}\\), e.g. a Gaussian distribution. Denote by \\(p_t(dx)\\) the marginal distribution at time \\(0 \\leq t \\leq T\\), i.e. \\(\\mathop{\\mathrm{P}}(X_t \\in dx) = p_t(dx)\\). By choosing the forward distribution with simple and tractable transition probabilities, e.g. an Ornstein-Uhlenbeck, it is relatively easy to estimate \\(\\nabla \\log p_t(x)\\) from simulated data: this can be formulated as a simple regression problem. This allows one to simulate the diffusion backward in time and generate approximate samples from \\(\\pi_{\\mathrm{data}}\\). Why this is useful is another question…\nThe fact that the mapping from data-samples at time \\(t=0\\) to (approximate) Gaussian samples at time \\(t=T\\) is stochastic and described by diffusion processes is cumbersome. This would be much more convenient to build a deterministic mapping between the data-distribution \\(\\pi_{\\mathrm{data}}\\) and the Gaussian reference distribution \\(\\pi_{\\mathrm{ref}}\\): this would allows one to associate a likelihood to data samples and to easily “encode”/“decode” data-samples. To so this, one can try to replace diffusion by Ordinary Differential Equations."
  },
  {
    "objectID": "notes/DDPM_deterministic/DDPM_deterministic.html#likelihood-computation",
    "href": "notes/DDPM_deterministic/DDPM_deterministic.html#likelihood-computation",
    "title": "From Denoising Diffusion to ODEs",
    "section": "Likelihood computation",
    "text": "Likelihood computation\nWith the diffusion-ODE trick, we have just seen that it is possible to build a vector fields \\(F[0,T] \\times \\mathbb{R}^d \\to \\mathbb{R}^d\\) such that the forward ODE\n\\[\n\\frac{d}{dt} \\overrightarrow{Y_t} =\nF(t,\\overrightarrow{Y_t})\n\\qquad \\textrm{initialized at} \\qquad\n\\overrightarrow{Y}_0 \\sim \\pi_{\\mathrm{data}}\n\\tag{3}\\]\nand the backward ODE defined as\n\\[\n\\frac{d}{ds} \\overleftarrow{Y_s} =\n-F(T-s,\\overleftarrow{Y_s})\n\\qquad \\textrm{initialized at} \\qquad\n\\overleftarrow{Y}_0 \\sim \\pi_{\\mathrm{ref}}\n\\]\nare such that \\(\\overrightarrow{Y}_T \\approx \\pi_{\\mathrm{ref}}\\) and \\(\\overleftarrow{Y}_T \\approx \\pi_{\\mathrm{data}}\\).\nIn general, consider a vector field \\(F(t,x)\\) and a bunch of particles distributed according to a distribution \\(p_t\\) at time \\(t\\). If each particle follows the vector field for an amount of time \\(\\delta \\ll 1\\), the particles that were in the vicinity of some \\(x \\in \\mathbb{R}^d\\) at time \\(t\\) end up in the vicinity of \\(x + F(x,t) \\, \\delta\\) at time \\(t+\\delta\\). At the same time, a volume element \\(dx\\) around \\(x \\in \\mathbb{R}^d\\) gets stretch by a factor \\(1+\\delta \\, \\mathop{\\mathrm{Tr}}[\\mathop{\\mathrm{\\mathrm{Jac}}}F(x,t)] = 1 + \\delta \\mathop{\\mathrm{div}}F(x,t)\\) while following the vector field \\(F\\), which means that the density of particles at time \\(t+\\delta\\) and around \\(x + F(x,t) \\, \\delta\\) equals \\(p_t(x) / [1 + \\delta \\mathop{\\mathrm{div}}F(x,t)]\\). In other words \\(\\log p_{t+\\delta}(x + F(x,t) \\, \\delta) \\approx \\log p_t(x) - \\delta \\, \\mathop{\\mathrm{div}}F(x,t)\\). This means that if we follows a trajectory of \\(\\tfrac{d}{dt} X_t = F(t,X_t)\\) one gets\n\\[\n\\log p_T(X_T) = \\log p_0(X_0) - \\int_{t=0}^{T} \\mathop{\\mathrm{div}}F(X_t,t) \\, dt.\n\\]\nThat is the Lagrangian description of the density \\(p_t\\) of particles. Indeed, one could directly get this identity by differentiating \\(p_t(X_t)\\) with respect to time while using the continuity Equation 1. When applied to the DDPM, this gives a way to assign likelihood the data samples, namely\n\\[\n\\log \\pi_{\\mathrm{data}}(x) = \\log \\pi_{\\mathrm{ref}}(\\overrightarrow{Y_T}) + \\int_{t=0}^{T} \\mathop{\\mathrm{div}}F(t, \\overrightarrow{Y_t})\\, dt\n\\]\nwhere \\(\\overrightarrow{Y_t}\\) is trajectory of the forward ODE Equation 3 initialized as \\(\\overrightarrow{Y_0} = x\\). Note that in high-dimensional setting, it may be computationally expensive to compute the divergence term \\(\\mathop{\\mathrm{div}}F(t, \\overrightarrow{Y_t})\\) since it typically is \\(d\\) times slower that a gradient computation; for this reason, it is often advocated to use the Hutchinson trace estimator to get an unbiased estimate of it at a much lower computational cost."
  },
  {
    "objectID": "about/about.html",
    "href": "about/about.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Email: a.h.thiery [at] nus.edu.sg\nOffice: Building S16, Office #06-113"
  },
  {
    "objectID": "about/about.html#address",
    "href": "about/about.html#address",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Email: a.h.thiery [at] nus.edu.sg\nOffice: Building S16, Office #06-113"
  },
  {
    "objectID": "about/about.html#qualifications",
    "href": "about/about.html#qualifications",
    "title": "Alexandre Thiéry",
    "section": "Qualifications",
    "text": "Qualifications\n\nPh.D., Probability & Statistics, Warwick University, 2009-2013.\nEcole Normale Superieure of Paris, Mathematics, 2005-2009\n\nResearch Assistant, Statslab (Cambridge, UK)\nMSc (Probability & Finance), University of Paris VI\nMSc (Partial Differential Equations & Modeling), University of Paris VI"
  },
  {
    "objectID": "about/about.html#employment-history",
    "href": "about/about.html#employment-history",
    "title": "Alexandre Thiéry",
    "section": "Employment history",
    "text": "Employment history\n\nAssociate Professor, Department of Statistics & Data Sciences, NUS, 2020–present.\nAffiliate, NUS Centre for Data Science and Machine Learning, 2021–present\nAffiliate, NUS Institute of Data Science, 2028–present\nAffiliate, NUS Graduate School for Integrative Sciences and Engineering, 2017–present\nAssistant Professor, Department of Statistics & Probability, NUS, 2014–2019.\nResearch Fellow, Department of Statistics & Probability, NUS, 2013"
  },
  {
    "objectID": "about/about.html#leadership",
    "href": "about/about.html#leadership",
    "title": "Alexandre Thiéry",
    "section": "Leadership",
    "text": "Leadership\n\nDeputy Director of Institute for Mathematical Sciences, 7/2020 – 12/2023"
  },
  {
    "objectID": "about/about.html#service",
    "href": "about/about.html#service",
    "title": "Alexandre Thiéry",
    "section": "Service",
    "text": "Service\n\nArea Chair for AISTAT (2023), ACML (2023), NeurIPS (2023), ICLR (2024)\nAssociate Editor for Statistics & Computing (2020–2022)"
  },
  {
    "objectID": "about/about.html#awards-and-honours",
    "href": "about/about.html#awards-and-honours",
    "title": "Alexandre Thiéry",
    "section": "Awards and honours",
    "text": "Awards and honours\n\nNUS Faculty of Sciences Dean’s Chair Associate Professor, 2022–2025\nNUS Faculty Teaching Excellence Award, 2022\nNUS Faculty Teaching Excellence Award, 2019\nNUS Faculty Teaching Excellence Award, 2018\nNUS Young Scientist Award. (Faculty of Science: 1 awardee per year), 2017\nNUS Young Investigator Award. (Faculty of Science: 3 awardees per year), 2016\nJohn Copas prize for the best PhD dissertation, 2013"
  },
  {
    "objectID": "about/about.html#industrial-activities",
    "href": "about/about.html#industrial-activities",
    "title": "Alexandre Thiéry",
    "section": "Industrial Activities",
    "text": "Industrial Activities\n\nCo-founder of Abyss Processing, start-up developing AI solutions for ophthalmology\nConsulting in the finance and health-care industries"
  },
  {
    "objectID": "notes_DRAFT/SSM/SSM.html",
    "href": "notes_DRAFT/SSM/SSM.html",
    "title": "State Space Models",
    "section": "",
    "text": "Consider a Markov chain \\((X_t, Y_t)\\) with \\(X_t \\in \\mathcal{X}\\) and \\(Y_t \\in \\mathcal{Y}\\) with initial distribution \\(X_0 \\in dx_0 = \\mu_0(dx_0)\\) and described by the dynamics\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\n(X_{t+1} \\in dx_{t+1}) \\mid (X_t = x_t) \\; &= \\; f(x_t, x_{t+1}) \\, dx_{t+1}\\\\\n(Y_{t} \\in dy_{t}) \\mid (X_{t} = x_{t}) \\; &= \\; g(x_{t}, y_{t}) \\, dy_{t}\n\\end{aligned}\n\\right.\n\\end{align}\n\\]\nfor some transition “density” \\(f(\\cdot, \\cdot)\\) and observation density \\(g(\\cdot, \\cdot)\\). These functions could be time-dependent but we will assume not for lightening notations."
  },
  {
    "objectID": "posts/hello_world/index.html",
    "href": "posts/hello_world/index.html",
    "title": "Hello World",
    "section": "",
    "text": "Just testing whether Latex is working correctly: \\[\\frac{1}{2}\\left( \\int \\exp\\left\\{ -\\frac{x^2}{2}\\right\\} \\, dx \\right)^2 \\approx 22/7\\]\nHere is a reference (MacKay 2003) and below is an animation:\n\n\nVideo\n\n\n\n\n\n\nReferences\n\nMacKay, David JC. 2003. Information Theory, Inference and Learning Algorithms. Cambridge university press."
  },
  {
    "objectID": "posts/reverse_and_tweedie/reverse_and_tweedie.html",
    "href": "posts/reverse_and_tweedie/reverse_and_tweedie.html",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "",
    "text": "Video\n\n\nImagine a scalar diffusion process \\(\\{X_t\\}_{t=0}^T\\) defined on the interval \\([0,T]\\),\n\\[dX = \\mu(X) \\, dt + \\sigma \\, dW\\]\nwhere \\(\\mu(X)\\) is the drift term, \\(\\sigma\\) is the diffusion coefficient, and \\(dW\\) is a Wiener process. Denote the distribution of this process at time \\(t\\) (\\(0 \\leq t \\leq T\\)) as \\(p_t(dx)\\) with an initial distribution of \\(p_0(dx)\\). Now, what happens if we reverse time and examine the process backward? In other words, consider the time-reversed process \\(\\overleftarrow{X}\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIntuitively, the process \\(\\overleftarrow{X}\\) is also a diffusion on the interval \\([0, T]\\), but with an initial distribution \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). To gain intuition, consider an Euler discretization of the forward process:\n\\[X_{t+\\delta} = X_{t} + \\mu(X_t)\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, \\mathbf{n} \\tag{1}\\]\nwhere \\(\\mathbf{n}\\sim \\mathcal{N}(0,1)\\) represents a noise term independent from \\(X_{t}\\), and \\(\\delta \\ll 1\\) is a time increment. Re-arranging terms and making the approximation \\(\\mu(X_t) \\approx \\mu(X_{t+\\delta})\\) gives that\n\\[X_{t} \\approx X_{t+\\delta} - \\mu(X_{t+\\delta})\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, (-\\mathbf{n}). \\tag{2}\\]\nThis seems to suggest that the time-reversed process follows the dynamics \\(d\\overleftarrow{X} = -\\mu(\\overleftarrow{X}) \\, dt + \\sigma \\, dW\\) started from \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). However, this conclusion is incorrect because this would suggest that the time-reversed of a standard Brownian motion (where \\(\\mu(x) \\equiv 0\\)) starting at zero is also a Brownian motion starting at \\(p_T(dx) = \\mathcal{N}(0,T)\\), which is clearly not the case. The flaw in this argument lies in assuming that the noise term \\(Z\\) is independent of \\(X_{t+\\delta}\\), which is not true, rendering the Euler discretization argument invalid.\nDeriving the dynamics of the backward process in a rigorous manner is not straightforward (Anderson 1982) (Haussmann and Pardoux 1986). What follows is a heuristic derivation that proceeds by estimating the mean and variance of \\(X_{t}\\) given \\(X_{t+\\delta} = x_{t+\\delta}\\), assuming \\(\\delta \\ll 1\\). Here, \\(x_{t+\\delta}\\) is treated as a fixed and constant value, and we are only interested in the conditional distribution of \\(X_t\\) given \\(x_{t+\\delta}\\). Bayes’ law gives\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta}) \\propto p_t(x) \\, \\exp\\left\\{ -\\frac{\\left( x_{t+\\delta} - [x + \\mu(x) \\, \\delta] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\},\\]\nwhere the exponential term corresponds to the transition of the forward diffusion for \\(\\delta \\ll 1\\). Using the 1st order approximation\n\\[p_t(x) \\approx p_t(x_{t+\\delta}) \\,\\exp\\left( \\langle \\nabla \\log p_t(x_{t+\\delta}), (x-x_{t+\\delta})\\rangle\\right),\\]\neliminating multiplicative constants and higher-order error terms, we obtain:\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta})\n\\propto\n\\exp\\left\\{ -\\frac{\\left( x - [x_{t+\\delta} - \\mu(x_{t+\\delta}) \\delta\n+ {\\color{red} \\sigma^2 \\, \\nabla \\log p_t(x_{t+\\delta}) \\, \\delta} ] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\}.\\]\nFor \\(\\delta \\ll 1\\), this is transition of the reverse diffusion\n\\[\nd\\overleftarrow{X}_t = -\\mu(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\]\nThe notation \\(B\\) is used to emphasize that this Brownian motion is distinct from the one used in the forward diffusion. The additional drift term, denoted as \\({\\color{red} \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X})}\\), is intuitive: it pushes the reverse diffusion toward regions where the forward diffusion spent a significant amount of time, i.e., where \\(p_t(dx)\\) is large. The popular “denoising diffusion models” (Ho, Jain, and Abbeel 2020) can be seen as discretizations of this backward process, employing various techniques to estimate the additional drift term from data."
  },
  {
    "objectID": "posts/reverse_and_tweedie/reverse_and_tweedie.html#reversing-a-diffusion",
    "href": "posts/reverse_and_tweedie/reverse_and_tweedie.html#reversing-a-diffusion",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "",
    "text": "Video\n\n\nImagine a scalar diffusion process \\(\\{X_t\\}_{t=0}^T\\) defined on the interval \\([0,T]\\),\n\\[dX = \\mu(X) \\, dt + \\sigma \\, dW\\]\nwhere \\(\\mu(X)\\) is the drift term, \\(\\sigma\\) is the diffusion coefficient, and \\(dW\\) is a Wiener process. Denote the distribution of this process at time \\(t\\) (\\(0 \\leq t \\leq T\\)) as \\(p_t(dx)\\) with an initial distribution of \\(p_0(dx)\\). Now, what happens if we reverse time and examine the process backward? In other words, consider the time-reversed process \\(\\overleftarrow{X}\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIntuitively, the process \\(\\overleftarrow{X}\\) is also a diffusion on the interval \\([0, T]\\), but with an initial distribution \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). To gain intuition, consider an Euler discretization of the forward process:\n\\[X_{t+\\delta} = X_{t} + \\mu(X_t)\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, \\mathbf{n} \\tag{1}\\]\nwhere \\(\\mathbf{n}\\sim \\mathcal{N}(0,1)\\) represents a noise term independent from \\(X_{t}\\), and \\(\\delta \\ll 1\\) is a time increment. Re-arranging terms and making the approximation \\(\\mu(X_t) \\approx \\mu(X_{t+\\delta})\\) gives that\n\\[X_{t} \\approx X_{t+\\delta} - \\mu(X_{t+\\delta})\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, (-\\mathbf{n}). \\tag{2}\\]\nThis seems to suggest that the time-reversed process follows the dynamics \\(d\\overleftarrow{X} = -\\mu(\\overleftarrow{X}) \\, dt + \\sigma \\, dW\\) started from \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). However, this conclusion is incorrect because this would suggest that the time-reversed of a standard Brownian motion (where \\(\\mu(x) \\equiv 0\\)) starting at zero is also a Brownian motion starting at \\(p_T(dx) = \\mathcal{N}(0,T)\\), which is clearly not the case. The flaw in this argument lies in assuming that the noise term \\(Z\\) is independent of \\(X_{t+\\delta}\\), which is not true, rendering the Euler discretization argument invalid.\nDeriving the dynamics of the backward process in a rigorous manner is not straightforward (Anderson 1982) (Haussmann and Pardoux 1986). What follows is a heuristic derivation that proceeds by estimating the mean and variance of \\(X_{t}\\) given \\(X_{t+\\delta} = x_{t+\\delta}\\), assuming \\(\\delta \\ll 1\\). Here, \\(x_{t+\\delta}\\) is treated as a fixed and constant value, and we are only interested in the conditional distribution of \\(X_t\\) given \\(x_{t+\\delta}\\). Bayes’ law gives\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta}) \\propto p_t(x) \\, \\exp\\left\\{ -\\frac{\\left( x_{t+\\delta} - [x + \\mu(x) \\, \\delta] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\},\\]\nwhere the exponential term corresponds to the transition of the forward diffusion for \\(\\delta \\ll 1\\). Using the 1st order approximation\n\\[p_t(x) \\approx p_t(x_{t+\\delta}) \\,\\exp\\left( \\langle \\nabla \\log p_t(x_{t+\\delta}), (x-x_{t+\\delta})\\rangle\\right),\\]\neliminating multiplicative constants and higher-order error terms, we obtain:\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta})\n\\propto\n\\exp\\left\\{ -\\frac{\\left( x - [x_{t+\\delta} - \\mu(x_{t+\\delta}) \\delta\n+ {\\color{red} \\sigma^2 \\, \\nabla \\log p_t(x_{t+\\delta}) \\, \\delta} ] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\}.\\]\nFor \\(\\delta \\ll 1\\), this is transition of the reverse diffusion\n\\[\nd\\overleftarrow{X}_t = -\\mu(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\]\nThe notation \\(B\\) is used to emphasize that this Brownian motion is distinct from the one used in the forward diffusion. The additional drift term, denoted as \\({\\color{red} \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X})}\\), is intuitive: it pushes the reverse diffusion toward regions where the forward diffusion spent a significant amount of time, i.e., where \\(p_t(dx)\\) is large. The popular “denoising diffusion models” (Ho, Jain, and Abbeel 2020) can be seen as discretizations of this backward process, employing various techniques to estimate the additional drift term from data."
  },
  {
    "objectID": "posts/reverse_and_tweedie/reverse_and_tweedie.html#denoising-score-matching-tweedie-formula",
    "href": "posts/reverse_and_tweedie/reverse_and_tweedie.html#denoising-score-matching-tweedie-formula",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "Denoising Score Matching & Tweedie formula",
    "text": "Denoising Score Matching & Tweedie formula\n\n\n\n\nMaurice Tweedie (1919–1996)\n\n\n\nThe previous section shows that a quantity often referred to as the “score” in machine learning (ML) literature, i.e. \\(\\mathcal{S}_t(x) \\equiv \\nabla_x \\log p_t(x)\\), naturally arises when a diffusion process runs backward in time. Interestingly, it is worth noting that since the beginning of times statisticians have been using the term “score” to refer to the other derivative, i.e. the derivative with respect to a model’s parameter, which is a completely different object!\nConsider a Brownian diffusion process \\(dX = \\sigma \\, dW\\) initiated from a distribution \\(\\mu(dx) = p_0(dx)\\). If this process is ran forward for a duration \\(\\delta&gt;0\\), we have:\n\\[X_{\\delta} = X_0 + \\mathcal{N}(0, \\sigma^2 \\, \\delta)\\]\nwhere \\(X_0 \\sim \\mu(dx)\\). Now, focusing on a specific sample \\(y = X_{\\delta}\\), the backward dynamics \\(d\\overleftarrow{X} = \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X}) , dt + \\sigma \\, dB\\) suggests that for sufficiently small \\(\\delta\\), the following approximation holds:\n\\[\n\\mathbb{E}[X_0 \\, | X_{\\delta} = y] \\; { \\color{\\red} \\approx} \\; y + \\nabla \\log p_{\\delta}(y) \\, \\sigma^2 \\delta.\n\\tag{3}\\]\nMaybe surprisingly, in the case of Brownian dynamics (no drift), this relationship holds exactly, even for arbitrarily large time increments \\(\\delta&gt;0\\). This observation attributed in (Efron 2011) to Maurice Tweedie (1919–1996) has a straightforward proof. Specifically, if \\(X \\sim \\mu(dx)\\), then \\(Y = X + \\mathcal{N}(0, \\Gamma^2)\\) has a density given by:\n\\[\np_Y(dy) = \\int \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx\n\\]\nwhere \\(\\rho_{\\Gamma}(z) \\propto \\exp[-z^2/(2 \\Gamma^2)]\\) is a centred Gaussian with variance \\(\\Gamma^2\\). It follows that\n\\[\n\\frac{ \\mathbb{E}[X \\, | Y = y]-y}{\\Gamma^2} = \\frac{ \\int \\left( \\frac {x-y }{\\Gamma^2} \\right)\\, \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx }{\\int \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx}\n=\n\\nabla_y \\log \\left\\{ \\int \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx \\right\\}\n\\]\nsince \\(\\nabla_y \\rho_{\\Gamma}(y-x) = (x-y)/\\Gamma^2\\). This leads to Tweedie’s formula:\n\\[\n\\mathbb{E}[X \\, | Y = y] \\; {\\color{red} =} \\; y + \\Gamma^2 \\, \\nabla \\log p_Y(y),\n\\]\nwhich is exactly the same as Equation 3 but with an equality sign: no approximation needed! Interestingly, this is what Machine-Learners often refer to as “denoising score matching” (Vincent 2011): if we have access to a large number of samples \\((X_i,Y_i)\\), where \\(X_i \\sim \\mu(dx)\\) and \\(Y_i = X_i + \\mathcal{N}(0, \\Gamma^2)\\), and fit a regression model \\(F_{\\theta}\\) by minimizing the mean-squared error \\(\\theta \\mapsto \\mathbb{E} \\|X - F_{\\theta}(Y)\\|^2\\), then Tweedie formula shows that in the limit of infinite samples and with sufficient flexibility in the regression model \\(F_{\\theta_\\star}(y) = y + \\Gamma^2 \\nabla \\log p_Y(y)\\). This allows one to estimate \\(\\nabla \\log p_Y\\) from data, which is often a good approximation of \\(\\nabla \\log \\mu\\) if the variance \\(\\Gamma^2\\) of the added noise is not too large. Indeed, things can go bad if \\(\\Gamma^2\\) is very small and the number of training data is not large, no free lunch!"
  },
  {
    "objectID": "posts/index_blog.html",
    "href": "posts/index_blog.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Blogposts (vaguely) related to research, notes for students, announcements, things I would like to write down to understand better and/or not forget, etc… Comments, corrections, suggestions are welcome!\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n12-06-2023\n\n\nReverse diffusions, Score & Tweedie\n\n\nSDE,Generative-Model\n\n\n\n\n06-06-2023\n\n\nHello World\n\n\ntesting\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/index_pubs.html#theory-methods",
    "href": "publications/index_pubs.html#theory-methods",
    "title": "Alexandre Thiéry",
    "section": "THEORY & METHODS:",
    "text": "THEORY & METHODS:\n\n\n\n GIT-Net: Generalized Integral Transform for Operator Learning   Chao Wang and Alexandre H. Thiery, (2023)   Transactions on Machine Learning Research, 2023    (Arxiv)   (Journal)  \n\n\n Computational Doob’s H-transforms for Online Filtering of Discretely Observed Diffusions   Nicolas Chopin, Andras Fulop, Jeremy Heng, Alexandre H. Thiery, (2023)   International Conference on Machine Learning (ICML) 2023    (Arxiv) \n\n\n Conditional sequential Monte Carlo in high dimensions   Axel Finke and Alexandre H. Thiery, (2023)   Annals of Statistics 2023, In Press    (Arxiv)   (Journal)  \n\n\n Pretrained equivariant features improve unsupervised landmark discovery   Rahul Rahaman, Atin Ghosh and Alexandre H. Thiery, (2022)   International Conference of Pattern Recognition (ICPR) 2022    (Arxiv) \n\n\n Manifold lifting: scaling MCMC to the vanishing noise regime   Khai Xiang Au, Matthew Graham, Alexandre H. Thiery, (2022)   JRSSB 2022    (Arxiv)   (Journal)  \n\n\n A discrete Bouncy Particle Sampler   Chris Sherlock and Alexandre H. Thiery, (2022)   Biometrika, Volume 109, Issue 2 (2022)    (Arxiv)   (Journal)  \n\n\n A Generalized & Robust Framework For Timestamp Supervision in Temporal Action Segmentation   Rahul Rahaman, Dipika Singhania, Alexandre H. Thiery, Angela Yao, (2022)   European Conference on Computer Vision (ECCV) 2022    (Journal)  \n\n\n Manifold Markov chain Monte Carlo methods for Bayesian inference in a wide class of diffusion models   Matthew Graham, Alexandre H. Thiery, Alex Beskos, (2022)   JRSSB, Volume 84 (4), 2022    (Arxiv)   (Journal)  \n\n\n Sequential Ensemble Transform for Bayesian Inverse Problems   Aaron Myers, Alexandre H. Thiery, Kainan Wang and Tan Bui-Tanh, (2021)   Journal of Computational Physics, Volume 427 (2021)    (Arxiv)   (Journal)  \n\n\n On Data-Augmentation and Consistency-Based Semi-Supervised Learning   Atin Ghosh and Alexandre H. Thiery, (2021)   ICLR 2021    (Arxiv) \n\n\n Uncertainty Quantification and Deep Ensembles   Rahul Rahaman and Alexandre H. Thiery, (2021)   NeuRIPS 2021    (Arxiv) \n\n\n On importance-weighted autoencoders   Axel Finke and Alexandre H. Thiery, (2019)   Tech Report    (Arxiv) \n\n\n A scalable optimal-transport based local particle filter   Matthew Graham and Alexandre H. Thiery, (2019)   Tech Report    (Arxiv) \n\n\n Particle Filter efficiency under limited communication   Deborshee Sen and Alexandre H. Thiery, (2019)   Tech Report    (Arxiv) \n\n\n Error Bounds for Sequential Monte Carlo Samplers for Multimodal Distributions   Daniel Paulin, Ajay Jasra and Alexandre H. Thiery, (2019)   Bernoulli, Volume 25, Number 1 (2019)    (Arxiv)   (Journal)  \n\n\n Asymptotic Analysis of the Random-Walk Metropolis Algorithm on Ridged Densities   Alex Beskos, Gareth Roberts, Alexandre H. Thiery and Natesh Pillai, (2018)   Annals of Applied Probability, Volume 28, Number 5 (2018)    (Arxiv)   (Journal)  \n\n\n On Coupling Particle Filter Trajectories   Deborshee Sen, Alexandre H. Thiery, Ajay Jasra, (2018)   Statistics and Computing, Volume 28, Number 2 (2018)    (Arxiv)   (Journal)  \n\n\n Levy statistics of interacting Rydberg gases   Thibault Vogt, Jingshan Han, Alexandre H. Thiery, Wenhui Li, (2017)   Physical Review A, Volume 95, Number 5 (2017)    (Arxiv)   (Journal)  \n\n\n Pseudo-marginal Metropolis–Hastings using averages of unbiased estimators   Chris Sherlock, Alexandre H. Thiery and Anthony Lee, (2017)   Biometrika, Volume 104, Number 3 (2017)    (Arxiv)   (Journal)  \n\n\n On the Convergence of Adaptive Sequential Monte Carlo Methods   Alex Beskos, Ajay Jasra, Nikolas Kantas, Alexandre H. Thiery, (2016)   Annals of Applied Probability, Volume 26, Number 2 (2016)    (Arxiv)   (Journal)  \n\n\n Consistency and fluctuations for stochastic gradient Langevin dynamics   Yee Whye Teh, Alexandre H. Thiery, Sebastian Vollmer, (2016)   Journal of Machine Learning Research, Volume 17 (2016)    (Arxiv)   (Journal)  \n\n\n On the efficiency of pseudo-marginal random walk Metropolis algorithms   Chris Sherlock, Alexandre H. Thiery, Gareth Roberts, Jeff Rosenthal, (2015)   Annals of Statistics, Volume 43, Number 1 (2015)    (Arxiv)   (Journal)  \n\n\n On non-negative unbiased estimators   Pierre Jacob, Alexandre H. Thiery, (2015)   Annals of Statistics, Volume 43, Number 2 (2015)    (Arxiv)   (Journal)  \n\n\n Efficiency of delayed-acceptance random walk Metropolis algorithms   Chris Sherlock, Alexandre H. Thiery and Andrew Golightly, (2015)   Annals of Statistics (In Press)    (Arxiv)   (Journal)  \n\n\n Noisy gradient flow from a random walk in Hilbert space   Natesh Pillai, Andrew Stuart, Alexandre H. Thiery, (2014)   Stochastic Partial Differential Equations: Analysis and Computations, Volume 2, Number 2 (2014)    (Arxiv)   (Journal)  \n\n\n Optimal Scaling and Diffusion Limits for the Langevin Algorithm in High Dimensions   Natesh Pillai, Andrew Stuart, Alexandre H. Thiery, (2012)   Annals of Applied Probability, Volume 22, Number 6 (2012)    (Arxiv)   (Journal)  \n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/index_pubs.html#applications",
    "href": "publications/index_pubs.html#applications",
    "title": "Alexandre Thiéry",
    "section": "APPLICATIONS",
    "text": "APPLICATIONS\n\n\n\n Three-Dimensional Structural Phenotype of the Optic Nerve Head as a Function of Glaucoma Severity   Braeu, F.A., Chuangsuwanich, T., Tun, T.A., Perera, S.A., Husain, R., Kadziauskienė, A., Schmetterer, L., Thiéry, A.H., Barbastathis, G., Aung, T. and Girard, M.J.A., (2023)   JAMA Ophthalmology, 2023    (Arxiv)   (Journal)  \n\n\n Towards Label-Free 3D Segmentation of Optical Coherence Tomography Images of the Optic Nerve Head Using Deep Learning   Devalla,S.K., Pham, T.H., Panda, S.K, Zhang,L., Subramanian,G., Swaminathan,A., Chin,Z.Y, Rajan,M., Mohan,S., Krishnadas,R., Senthil,V., Leon,J.M, Tun,T.A., Cheng,C.Y., Schmetterer, L., Perera,S., Aung,T., Thiery,A.H., Girard,M.J.A., (2023)   Biomedical Optics Express, Vol. 11, Issue 11    (Arxiv)   (Journal)  \n\n\n Geometric Deep Learning to Identify the Critical 3D Structural Features of the Optic Nerve Head for Glaucoma Diagnosis   Braeu, F., Thiery, A.H, Tun, T.A., Kadziauskiene, A., Barbastathis, G., Aung, T., and Girard. M.J.A., (2023)   American Journal of Ophthalmology, 2023    (Arxiv)   (Journal)  \n\n\n Medical Application of Geometric Deep Learning for the Diagnosis of Glaucoma   Thiery, A.H, Braeu, F., Tun, T.A., Aung, T., Girard. M.J.A., (2023)   Translational Vision Science and Technology, 2023    (Arxiv)   (Journal)  \n\n\n Detection of m6A from direct RNA sequencing using a Multiple Instance Learning framework   Hendra C, Pratanwanich PN, Wan YK, Goh WS, Thiery A, Göke J+., (2022)   Nature Methods (2022)    (Arxiv)   (Journal)  \n\n\n Describing the Structural Phenotype of the Glaucomatous Optic Nerve Head Using Artificial Intelligence   Panda, S.K., Cheong, H., Tun,T.A., Devella, S.K., Krishnadas, R., Buist, M.L., Perera, S., Cheng, C-Y., Aung, T., Thiery A.H., Girard, M.J.A., (2022)   American Journal of Ophthalmology, 2022    (Arxiv)   (Journal)  \n\n\n The Three-Dimensional Structural Configuration of the Central Retinal Vessel Trunk and Branches as a Glaucoma Biomarker   Satish K Panda, Haris Cheong, Tin A Tun, Thanadet Chuangsuwanich, Aiste Kadziauskiene, Vijayalakshmi Senthil, Ramaswami Krishnadas, Martin L Buist, Shamira Perera, Ching-Yu Cheng, Tin Aung, Alexandre H Thiery, Michaël JA Girard, (2022)   American Journal of Ophthalmology, 2022    (Journal)  \n\n\n Deep Learning Algorithms to Isolate and Quantify the Structures of the Anterior Segment in Optical Coherence Tomography Images   Pham, T.H, Devalla,S.K., Ang, A., Da,S.Z., Thiery,A.H., Boote,C., Cheng,C.Y., Koh,V., Girard, M.J.A., (2021)   British Journal of Ophthalmology    (Arxiv)   (Journal)  \n\n\n Identification of differential RNA modifications from nanopore direct RNA sequencing with xPore   Ploy N. Pratanwanich, Fei Yao, Ying Chen, Casslynn W.Q. Koh, Christopher Hendra, Polly Poon, Yeek Teck Goh, Phoebe M. L. Yap, Choi Jing Yuan, Wee Joo Chng, Sarah Ng, Alexandre Thiery, W.S. Sho Goh, Jonathan Goeke, (2021)   Nature Biotechnology, 2021    (Arxiv)   (Journal)  \n\n\n Beyond quadratic error: Case-study of a multiple criteria approach to the performance assessment of numerical forecasts of solar irradiance in the tropics   Verbois, H., Blanc, P., Huva, R., Saint-Drenan, Y-M, Rusydi, A.. Thiery, A., (2020)   Renewable and Sustainable Energy Reviews, Volume 117, (2020)    (Journal)  \n\n\n NanoVar: Accurate Characterization of Patients Genomic Structural Variants Using Low-Depth Nanopore Sequencing   Tham, C.Y, Tirado-Magallanes, R., Goh, Y., Fullwood, M. J., Koh, B.T.H. , Wang, W., Ng, C.H, Chng, W.J., Thiery, A.H., Tenen, D.G, Benoukraf, (2020)   Genome Biology (2020)    (Arxiv)   (Journal)  \n\n\n DeshadowGAN: A Deep Learning Approach to Remove Shadows from Optical Coherence Tomography Images   Cheong, H., Devalla, S.K., Pham, T.H., Liang, Z., Tun, T.A., Wang, X., Perera, S., Schmetterer, L., Tin, A., Boote, C., Thiery, A.H., Girard, M.J.A., (2020)   Translational Vision Science & Technology    (Arxiv)   (Journal)  \n\n\n Deep Learning Algorithms to Isolate and Quantify the Structures of the Anterior Segment in Optical Coherence Tomography Images   Pham, T.H, Devalla,S.K., Ang, A., Da, S.Z., Thiery, A.H., Boote,C., Cheng,C.Y., Koh,V., Girard, M.J.A, (2020)   British Journal of Ophthalmology, 2020    (Arxiv)   (Journal)  \n\n\n A Deep Learning Approach to Denoise Optical Coherence Tomography Images of the Optic Nerve Head   Devalla SK, Subramanian G, Pham TH, Wang X, Perera S, Tun TA, Aung T, Schmetterer L, Thiery A.H., Girard MJA, (2019)   Scientific Reports (2019)    (Arxiv)   (Journal)  \n\n\n Glaucoma management in the era of artificial intelligence   Devalla S.K., Liang Z., Pham T.H., Boote, C., Strouthidis, N.G., Thiery A.H., Girard M.J.A., (2019)   British Journal of Ophthalmology (2019)    (Journal)  \n\n\n DRUNET: A Dilated-Residual U-Net Deep Learning Network to Digitally Stain Optic Nerve Head Tissues in Optical Coherence Tomography Images   Devalla SK, Renukanand PK, Sreedhar BK, Perera SA, Mari JM, Chin KS, Tun TA, Strouthidis N, Aung T, Thiery A.H., Girard MJA, (2018)   Biomedical Optics Express, Vol. 9, Issue 7 (2018)    (Arxiv)   (Journal)  \n\n\n Probabilistic forecasting of day-ahead solar irradiance using quantile gradient boosting   Verbois, H., Rusydi, A., Thiery, A.H., (2018)   Solar Energy 173, 313-327 (2018)    (Journal)  \n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Google-Scholar\n  \n  \n    \n     Arxiv\n  \n  \n    \n     twitter\n  \n\n      \nAssociate Professor\nDepartment of Statistics & Data Science\nNational University of Singapore\n\n\n\nComputational Statistics & Machine Learning\nMonte-Carlo methods\nUncertainty Quantification\nInverse problems\n\n\n\n\n\nOffice: Building S16, Office #06-113\nEmail: a.h.thiery [at] nus.edu.sg"
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Computational Statistics & Machine Learning\nMonte-Carlo methods\nUncertainty Quantification\nInverse problems"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Office: Building S16, Office #06-113\nEmail: a.h.thiery [at] nus.edu.sg"
  },
  {
    "objectID": "notes/reverse_and_tweedie/reverse_and_tweedie.html",
    "href": "notes/reverse_and_tweedie/reverse_and_tweedie.html",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "",
    "text": "Imagine a scalar diffusion process \\(\\{X_t\\}_{t=0}^T\\) defined on the interval \\([0,T]\\),\n\\[dX = \\mu(X) \\, dt + \\sigma \\, dW\\]\nwhere \\(\\mu(X)\\) is the drift term, \\(\\sigma\\) is the diffusion coefficient, and \\(dW\\) is a Wiener process. Denote the distribution of this process at time \\(t\\) (\\(0 \\leq t \\leq T\\)) as \\(p_t(dx)\\) with an initial distribution of \\(p_0(dx)\\). Now, what happens if we reverse time and examine the process backward? In other words, consider the time-reversed process \\(\\overleftarrow{X}\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIntuitively, the process \\(\\overleftarrow{X}\\) is also a diffusion on the interval \\([0, T]\\), but with an initial distribution \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). To gain intuition, consider an Euler discretization of the forward process:\n\\[X_{t+\\delta} = X_{t} + \\mu(X_t)\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, \\mathbf{n} \\tag{1}\\]\nwhere \\(\\mathbf{n}\\sim \\mathcal{N}(0,1)\\) represents a noise term independent from \\(X_{t}\\), and \\(\\delta \\ll 1\\) is a time increment. Re-arranging terms and making the approximation \\(\\mu(X_t) \\approx \\mu(X_{t+\\delta})\\) gives that\n\\[X_{t} \\approx X_{t+\\delta} - \\mu(X_{t+\\delta})\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, (-\\mathbf{n}). \\tag{2}\\]\nThis seems to suggest that the time-reversed process follows the dynamics \\(d\\overleftarrow{X} = -\\mu(\\overleftarrow{X}) \\, dt + \\sigma \\, dW\\) started from \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). However, this conclusion is incorrect because this would suggest that the time-reversed of a standard Brownian motion (where \\(\\mu(x) \\equiv 0\\)) starting at zero is also a Brownian motion starting at \\(p_T(dx) = \\mathcal{N}(0,T)\\), which is clearly not the case. The flaw in this argument lies in assuming that the noise term \\(Z\\) is independent of \\(X_{t+\\delta}\\), which is not true, rendering the Euler discretization argument invalid.\nDeriving the dynamics of the backward process in a rigorous manner is not straightforward (Anderson 1982) (Haussmann and Pardoux 1986). What follows is a heuristic derivation that proceeds by estimating the mean and variance of \\(X_{t}\\) given \\(X_{t+\\delta} = x_{t+\\delta}\\), assuming \\(\\delta \\ll 1\\). Here, \\(x_{t+\\delta}\\) is treated as a fixed and constant value, and we are only interested in the conditional distribution of \\(X_t\\) given \\(x_{t+\\delta}\\). Bayes’ law gives\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta}) \\propto p_t(x) \\, \\exp\\left\\{ -\\frac{\\left( x_{t+\\delta} - [x + \\mu(x) \\, \\delta] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\},\\]\nwhere the exponential term corresponds to the transition of the forward diffusion for \\(\\delta \\ll 1\\). Using the 1st order approximation\n\\[p_t(x) \\approx p_t(x_{t+\\delta}) \\,\\exp\\left( \\langle \\nabla \\log p_t(x_{t+\\delta}), (x-x_{t+\\delta})\\rangle\\right),\\]\neliminating multiplicative constants and higher-order error terms, we obtain:\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta})\n\\propto\n\\exp\\left\\{ -\\frac{\\left( x - [x_{t+\\delta} - \\mu(x_{t+\\delta}) \\delta\n+ {\\color{red} \\sigma^2 \\, \\nabla \\log p_t(x_{t+\\delta}) \\, \\delta} ] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\}.\\]\nFor \\(\\delta \\ll 1\\), this is transition of the reverse diffusion\n\\[\nd\\overleftarrow{X}_t = -\\mu(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\]\nThe notation \\(B\\) is used to emphasize that this Brownian motion is distinct from the one used in the forward diffusion. The additional drift term, denoted as \\({\\color{red} \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X})}\\), is intuitive: it pushes the reverse diffusion toward regions where the forward diffusion spent a significant amount of time, i.e., where \\(p_t(dx)\\) is large. The popular “denoising diffusion models” (Ho, Jain, and Abbeel 2020) can be seen as discretizations of this backward process, employing various techniques to estimate the additional drift term from data."
  },
  {
    "objectID": "notes/reverse_and_tweedie/reverse_and_tweedie.html#reversing-a-diffusion",
    "href": "notes/reverse_and_tweedie/reverse_and_tweedie.html#reversing-a-diffusion",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "",
    "text": "Imagine a scalar diffusion process \\(\\{X_t\\}_{t=0}^T\\) defined on the interval \\([0,T]\\),\n\\[dX = \\mu(X) \\, dt + \\sigma \\, dW\\]\nwhere \\(\\mu(X)\\) is the drift term, \\(\\sigma\\) is the diffusion coefficient, and \\(dW\\) is a Wiener process. Denote the distribution of this process at time \\(t\\) (\\(0 \\leq t \\leq T\\)) as \\(p_t(dx)\\) with an initial distribution of \\(p_0(dx)\\). Now, what happens if we reverse time and examine the process backward? In other words, consider the time-reversed process \\(\\overleftarrow{X}\\) defined as\n\\[\\overleftarrow{X}_s = X_{T-s}.\\]\nIntuitively, the process \\(\\overleftarrow{X}\\) is also a diffusion on the interval \\([0, T]\\), but with an initial distribution \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). To gain intuition, consider an Euler discretization of the forward process:\n\\[X_{t+\\delta} = X_{t} + \\mu(X_t)\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, \\mathbf{n} \\tag{1}\\]\nwhere \\(\\mathbf{n}\\sim \\mathcal{N}(0,1)\\) represents a noise term independent from \\(X_{t}\\), and \\(\\delta \\ll 1\\) is a time increment. Re-arranging terms and making the approximation \\(\\mu(X_t) \\approx \\mu(X_{t+\\delta})\\) gives that\n\\[X_{t} \\approx X_{t+\\delta} - \\mu(X_{t+\\delta})\\, \\delta + \\sigma \\, \\sqrt{\\delta} \\, (-\\mathbf{n}). \\tag{2}\\]\nThis seems to suggest that the time-reversed process follows the dynamics \\(d\\overleftarrow{X} = -\\mu(\\overleftarrow{X}) \\, dt + \\sigma \\, dW\\) started from \\(\\overleftarrow{X}_0 \\sim p_T(dx)\\). However, this conclusion is incorrect because this would suggest that the time-reversed of a standard Brownian motion (where \\(\\mu(x) \\equiv 0\\)) starting at zero is also a Brownian motion starting at \\(p_T(dx) = \\mathcal{N}(0,T)\\), which is clearly not the case. The flaw in this argument lies in assuming that the noise term \\(Z\\) is independent of \\(X_{t+\\delta}\\), which is not true, rendering the Euler discretization argument invalid.\nDeriving the dynamics of the backward process in a rigorous manner is not straightforward (Anderson 1982) (Haussmann and Pardoux 1986). What follows is a heuristic derivation that proceeds by estimating the mean and variance of \\(X_{t}\\) given \\(X_{t+\\delta} = x_{t+\\delta}\\), assuming \\(\\delta \\ll 1\\). Here, \\(x_{t+\\delta}\\) is treated as a fixed and constant value, and we are only interested in the conditional distribution of \\(X_t\\) given \\(x_{t+\\delta}\\). Bayes’ law gives\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta}) \\propto p_t(x) \\, \\exp\\left\\{ -\\frac{\\left( x_{t+\\delta} - [x + \\mu(x) \\, \\delta] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\},\\]\nwhere the exponential term corresponds to the transition of the forward diffusion for \\(\\delta \\ll 1\\). Using the 1st order approximation\n\\[p_t(x) \\approx p_t(x_{t+\\delta}) \\,\\exp\\left( \\langle \\nabla \\log p_t(x_{t+\\delta}), (x-x_{t+\\delta})\\rangle\\right),\\]\neliminating multiplicative constants and higher-order error terms, we obtain:\n\\[\\mathbb{P}(X_t \\in dx | x_{t+\\delta})\n\\propto\n\\exp\\left\\{ -\\frac{\\left( x - [x_{t+\\delta} - \\mu(x_{t+\\delta}) \\delta\n+ {\\color{red} \\sigma^2 \\, \\nabla \\log p_t(x_{t+\\delta}) \\, \\delta} ] \\right)^2}{2 \\sigma^2 \\, \\delta} \\right\\}.\\]\nFor \\(\\delta \\ll 1\\), this is transition of the reverse diffusion\n\\[\nd\\overleftarrow{X}_t = -\\mu(\\overleftarrow{X}_t) \\, dt \\; + {\\color{red} \\sigma^2 \\, \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\, dt} \\; + \\; \\sigma \\, dB.\n\\]\nThe notation \\(B\\) is used to emphasize that this Brownian motion is distinct from the one used in the forward diffusion. The additional drift term, denoted as \\({\\color{red} \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X})}\\), is intuitive: it pushes the reverse diffusion toward regions where the forward diffusion spent a significant amount of time, i.e., where \\(p_t(dx)\\) is large. The popular “denoising diffusion models” (Ho, Jain, and Abbeel 2020) can be seen as discretizations of this backward process, employing various techniques to estimate the additional drift term from data."
  },
  {
    "objectID": "notes/reverse_and_tweedie/reverse_and_tweedie.html#denoising-score-matching-tweedie-formula",
    "href": "notes/reverse_and_tweedie/reverse_and_tweedie.html#denoising-score-matching-tweedie-formula",
    "title": "Reverse diffusions, Score & Tweedie",
    "section": "Denoising Score Matching & Tweedie formula",
    "text": "Denoising Score Matching & Tweedie formula\n\n\n\n\nMaurice Tweedie (1919–1996)\n\n\n\nThe previous section shows that a quantity often referred to as the “score” in machine learning (ML) literature, i.e. \\(\\mathcal{S}_t(x) \\equiv \\nabla_x \\log p_t(x)\\), naturally arises when a diffusion process runs backward in time. Interestingly, it is worth noting that since the beginning of times statisticians have been using the term “score” to refer to the other derivative, i.e. the derivative with respect to a model’s parameter, which is a completely different object!\nConsider a Brownian diffusion process \\(dX = \\sigma \\, dW\\) initiated from a distribution \\(\\mu(dx) = p_0(dx)\\). If this process is ran forward for a duration \\(\\delta&gt;0\\), we have:\n\\[X_{\\delta} = X_0 + \\mathcal{N}(0, \\sigma^2 \\, \\delta)\\]\nwhere \\(X_0 \\sim \\mu(dx)\\). Now, focusing on a specific sample \\(y = X_{\\delta}\\), the backward dynamics \\(d\\overleftarrow{X} = \\sigma^2 \\, \\nabla \\log p_t(\\overleftarrow{X}) , dt + \\sigma \\, dB\\) suggests that for sufficiently small \\(\\delta\\), the following approximation holds:\n\\[\n\\mathbb{E}[X_0 \\, | X_{\\delta} = y] \\; { \\color{\\red} \\approx} \\; y + \\nabla \\log p_{\\delta}(y) \\, \\sigma^2 \\delta.\n\\tag{3}\\]\nMaybe surprisingly, in the case of Brownian dynamics (no drift), this relationship holds exactly, even for arbitrarily large time increments \\(\\delta&gt;0\\). This observation attributed in (Efron 2011) to Maurice Tweedie (1919–1996) has a straightforward proof. Specifically, if \\(X \\sim \\mu(dx)\\), then \\(Y = X + \\mathcal{N}(0, \\Gamma^2)\\) has a density given by:\n\\[\np_Y(dy) = \\int \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx\n\\]\nwhere \\(\\rho_{\\Gamma}(z) \\propto \\exp[-z^2/(2 \\Gamma^2)]\\) is a centred Gaussian with variance \\(\\Gamma^2\\). It follows that\n\\[\n\\frac{ \\mathbb{E}[X \\, | Y = y]-y}{\\Gamma^2} = \\frac{ \\int \\left( \\frac {x-y }{\\Gamma^2} \\right)\\, \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx }{\\int \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx}\n=\n\\nabla_y \\log \\left\\{ \\int \\mu(x) \\, \\rho_{\\Gamma}(y-x) \\, dx \\right\\}\n\\]\nsince \\(\\nabla_y \\rho_{\\Gamma}(y-x) = (x-y)/\\Gamma^2\\). This leads to Tweedie’s formula:\n\\[\n\\mathbb{E}[X \\, | Y = y] \\; {\\color{red} =} \\; y + \\Gamma^2 \\, \\nabla \\log p_Y(y),\n\\]\nwhich is exactly the same as Equation 3 but with an equality sign: no approximation needed! Interestingly, this is what Machine-Learners often refer to as “denoising score matching” (Vincent 2011): if we have access to a large number of samples \\((X_i,Y_i)\\), where \\(X_i \\sim \\mu(dx)\\) and \\(Y_i = X_i + \\mathcal{N}(0, \\Gamma^2)\\), and fit a regression model \\(F_{\\theta}\\) by minimizing the mean-squared error \\(\\theta \\mapsto \\mathbb{E} \\|X - F_{\\theta}(Y)\\|^2\\), then Tweedie formula shows that in the limit of infinite samples and with sufficient flexibility in the regression model \\(F_{\\theta_\\star}(y) = y + \\Gamma^2 \\nabla \\log p_Y(y)\\). This allows one to estimate \\(\\nabla \\log p_Y\\) from data, which is often a good approximation of \\(\\nabla \\log \\mu\\) if the variance \\(\\Gamma^2\\) of the added noise is not too large. Indeed, things can go bad if \\(\\Gamma^2\\) is very small and the number of training data is not large, no free lunch!"
  },
  {
    "objectID": "notes/Gaussian_Assimilation/gaussian_assimilation.html",
    "href": "notes/Gaussian_Assimilation/gaussian_assimilation.html",
    "title": "Gaussian Assimilation & the EnKF",
    "section": "",
    "text": "Assume a prior Gaussian prior distribution \\(\\pi_0 \\equiv \\mathcal{N}(m_0,P_0)\\) and a noisy observation \\(y_\\star \\in \\mathbb{R}^{D_y}\\) with\n\\[\ny_\\star = H x + \\xi\n\\qquad \\text{with} \\qquad\n\\xi \\sim \\mathcal{N}(0,R)\n\\]\nwhere \\(x \\in \\mathbb{R}^{D_x}\\) is an unknown quantity of interest. The posterior distribution \\(\\pi \\equiv \\mathcal{N}(m,P)\\) is Gaussian and is given by\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &= m_0 + P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}(y_\\star - H m)\\\\\nP &= P_0 - P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1} \\, H P_0,\n\\end{aligned}\n\\right.\n\\end{align}\n\\]\nas standard Gaussian conditioning shows it. This can also be written as\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &= m_0 + K \\, (y_\\star - H m_0)\\\\\nP &= (I - K \\, H) P_0,\n\\end{aligned}\n\\right.\n\\end{align}\n\\]\nfor Kalman Gain Matrix \\(K \\in \\mathbb{R}^{D_x,D_y}\\) defined as\n\\[\nK \\; = \\; P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}.\n\\tag{1}\\]\nwhich can also be expressed as\n\\[\nK \\; = \\; \\mathop{\\mathrm{Cov}}(X, HX) \\, \\mathop{\\mathrm{Var}}(Y)^{-1}.\n\\]\nfor \\(X \\sim \\pi_0\\) and \\(Y = HX + \\mathcal{N}(0,R)\\); this point of view can be a useful for establishing generalization to non-linear scenarios. The important remark is that the posterior covariance matrix \\(P \\in \\mathbb{R}^{D_x,D_x}\\) and the posterior mean \\(m \\in \\mathbb{R}^{D_x}\\) can also be expressed as\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &=  \\textcolor{red}{(I - K \\, H)} \\, m_0 +  \\textcolor{blue}{K} \\, y_\\star\\\\\nP &= \\;  \\textcolor{red}{(I - K \\, H)} \\, P_0 \\,  \\textcolor{red}{(I - K \\, H)^\\top} +  \\textcolor{blue}{K} R  \\textcolor{blue}{K^\\top}.\n\\end{aligned}\n\\right.\n\\end{align}\n\\tag{2}\\]\nThis shows that \\(P\\) is indeed positive semi-definite. More importantly, this gives a mechanism for transforming samples from the prior distributions into samples from the posterior distributions. Indeed, consider \\(N\\) iid samples from the prior distribution, \\(x_1, \\ldots, x_N \\sim \\pi_0(dx)\\), and set\n\\[\n\\widetilde{x}_i\n=\n\\textcolor{red}{(I - K \\, H)} \\, x_i +  \\textcolor{blue}{K}(y_\\star + \\xi_i)\n\\]\nfor iid noise terms \\(\\xi_i \\sim \\mathcal{N}(0,R)\\). From Equation 2 it is clear that \\(\\widetilde{x}_1, \\ldots, \\widetilde{x}_N\\) are iid samples from the Gaussian posterior distribution \\(\\pi = \\mathcal{N}(m,P)\\). It is more intuitive to write this as\n\\[\n\\widetilde{x}_i\n=\nx_i + K \\, (  \\textcolor{green}{ \\widetilde{y}_{i,\\star} } - H \\, x_i)\n\\]\nwhere \\(\\widetilde{y}_{i,\\star} \\in \\mathbb{R}^{D_y}\\) are fake observations that are obtained by perturbing the actual observation \\(y_\\star\\) with additive Gaussian noise terms with covariance \\(R\\),\n\\[\n\\textcolor{green}{\\widetilde{y}_{i,\\star} \\, = \\, y_\\star + \\xi_i}.\n\\]\n\n\nSuppose that we would like to estimate \\(x \\in \\mathbb{R}^{D_x}\\) from the noisy observation\n\\[\ny_\\star = \\mathcal{H}(x) + \\xi\n\\qquad \\text{with} \\qquad\n\\xi \\sim \\mathcal{N}(0,R)\n\\]\nand possibly-nonlinear observation operator \\(\\mathcal{H}: \\mathbb{R}^{D_x} \\to \\mathbb{R}^{D_y}\\). Assume that we also have \\(N\\) samples \\(x_1, \\ldots, x_N\\) generated from some (unkown) prior distribution. For example, these samples could come from another numerical procedure. In order to obtain \\(N\\) approximate samples from the posterior distribution, one can set\n\\[\n\\widetilde{x}_i\n=\nx_i + \\widehat{K} \\, [  \\textcolor{green}{ \\widetilde{y}_{i,\\star} } - \\mathcal{H}(x_i)]\n\\]\nfor fake observations \\( \\textcolor{green}{\\widetilde{y}_{i,\\star} } = y_\\star + \\xi_i\\). The approximate Kalman gain matrix \\(\\widehat{K}\\) is obtained by noting that in Equation 1 giving\n\\[\nK \\; = \\; P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}\n\\]\nwe have \\(P_0 H^\\top = \\mathop{\\mathrm{Cov}}(X,HX)\\) and \\(H P_0 H^\\top = \\mathop{\\mathrm{Var}}(HX)\\) for \\(X \\sim \\pi_0\\). This means that an approximate Kalman matrix can be obtained using empirical estimates of these covariance matrices:\n\\[\n\\widehat{K} \\; = \\; \\widehat{\\mathop{\\mathrm{Cov}}}([x_i]_i, [\\mathcal{H}(x_i)]_i) \\,  {\\left( \\widehat{\\mathop{\\mathrm{Var}}}([\\mathcal{H}(x_i)]_i) + R \\right)} ^{-1}.\n\\]\nThese updates form the basis of the Ensemble Kalman filter (EnKF), and very successful and scalable approach to data-assimilation of high-dimensional dynamical systems. This method is operationally employed across various weather forecasting centers across the globe.\n\n\n\n\nEnKF Bible by Geir Evensen\n\n\n\n\n\n\nInterestingly enough, the remarks above can be design in a relatively principled manner a derivative free optimizer (Huang et al. 2022). For example, assume one would like to minimize a functional of the type\n\\[\n\\Psi(x) \\; = \\; \\|y_\\star - \\mathcal{H}(x)\\|^2_{R^{-1}}.\n\\]\nOne can start with a cloud of particles \\(x_1, \\ldots, x_N\\) and keep updating them by assuming that one assimilates the noisy observation \\(y_\\star\\) generated from a postulated observation process\n\\[\ny_\\star = \\mathcal{H}(x) + \\varepsilon^{-1} \\, \\xi\n\\]\nfor \\(\\xi \\sim \\mathcal{N}(0,R)\\) and \\(\\varepsilon\\ll 1\\) a “step-size”. Each assimilation step steers the cloud of points towards the rights direction. Careful choice of the step-size \\(\\varepsilon\\) is often crucial, as in any optimization procedure. It is indeed related to Information-Geometric Optimization Algorithms (IGO): the article (Ollivier et al. 2017) is beautiful!"
  },
  {
    "objectID": "notes/Gaussian_Assimilation/gaussian_assimilation.html#ensemble-kalman-updates",
    "href": "notes/Gaussian_Assimilation/gaussian_assimilation.html#ensemble-kalman-updates",
    "title": "Gaussian Assimilation & the EnKF",
    "section": "",
    "text": "Assume a prior Gaussian prior distribution \\(\\pi_0 \\equiv \\mathcal{N}(m_0,P_0)\\) and a noisy observation \\(y_\\star \\in \\mathbb{R}^{D_y}\\) with\n\\[\ny_\\star = H x + \\xi\n\\qquad \\text{with} \\qquad\n\\xi \\sim \\mathcal{N}(0,R)\n\\]\nwhere \\(x \\in \\mathbb{R}^{D_x}\\) is an unknown quantity of interest. The posterior distribution \\(\\pi \\equiv \\mathcal{N}(m,P)\\) is Gaussian and is given by\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &= m_0 + P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}(y_\\star - H m)\\\\\nP &= P_0 - P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1} \\, H P_0,\n\\end{aligned}\n\\right.\n\\end{align}\n\\]\nas standard Gaussian conditioning shows it. This can also be written as\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &= m_0 + K \\, (y_\\star - H m_0)\\\\\nP &= (I - K \\, H) P_0,\n\\end{aligned}\n\\right.\n\\end{align}\n\\]\nfor Kalman Gain Matrix \\(K \\in \\mathbb{R}^{D_x,D_y}\\) defined as\n\\[\nK \\; = \\; P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}.\n\\tag{1}\\]\nwhich can also be expressed as\n\\[\nK \\; = \\; \\mathop{\\mathrm{Cov}}(X, HX) \\, \\mathop{\\mathrm{Var}}(Y)^{-1}.\n\\]\nfor \\(X \\sim \\pi_0\\) and \\(Y = HX + \\mathcal{N}(0,R)\\); this point of view can be a useful for establishing generalization to non-linear scenarios. The important remark is that the posterior covariance matrix \\(P \\in \\mathbb{R}^{D_x,D_x}\\) and the posterior mean \\(m \\in \\mathbb{R}^{D_x}\\) can also be expressed as\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &=  \\textcolor{red}{(I - K \\, H)} \\, m_0 +  \\textcolor{blue}{K} \\, y_\\star\\\\\nP &= \\;  \\textcolor{red}{(I - K \\, H)} \\, P_0 \\,  \\textcolor{red}{(I - K \\, H)^\\top} +  \\textcolor{blue}{K} R  \\textcolor{blue}{K^\\top}.\n\\end{aligned}\n\\right.\n\\end{align}\n\\tag{2}\\]\nThis shows that \\(P\\) is indeed positive semi-definite. More importantly, this gives a mechanism for transforming samples from the prior distributions into samples from the posterior distributions. Indeed, consider \\(N\\) iid samples from the prior distribution, \\(x_1, \\ldots, x_N \\sim \\pi_0(dx)\\), and set\n\\[\n\\widetilde{x}_i\n=\n\\textcolor{red}{(I - K \\, H)} \\, x_i +  \\textcolor{blue}{K}(y_\\star + \\xi_i)\n\\]\nfor iid noise terms \\(\\xi_i \\sim \\mathcal{N}(0,R)\\). From Equation 2 it is clear that \\(\\widetilde{x}_1, \\ldots, \\widetilde{x}_N\\) are iid samples from the Gaussian posterior distribution \\(\\pi = \\mathcal{N}(m,P)\\). It is more intuitive to write this as\n\\[\n\\widetilde{x}_i\n=\nx_i + K \\, (  \\textcolor{green}{ \\widetilde{y}_{i,\\star} } - H \\, x_i)\n\\]\nwhere \\(\\widetilde{y}_{i,\\star} \\in \\mathbb{R}^{D_y}\\) are fake observations that are obtained by perturbing the actual observation \\(y_\\star\\) with additive Gaussian noise terms with covariance \\(R\\),\n\\[\n\\textcolor{green}{\\widetilde{y}_{i,\\star} \\, = \\, y_\\star + \\xi_i}.\n\\]\n\n\nSuppose that we would like to estimate \\(x \\in \\mathbb{R}^{D_x}\\) from the noisy observation\n\\[\ny_\\star = \\mathcal{H}(x) + \\xi\n\\qquad \\text{with} \\qquad\n\\xi \\sim \\mathcal{N}(0,R)\n\\]\nand possibly-nonlinear observation operator \\(\\mathcal{H}: \\mathbb{R}^{D_x} \\to \\mathbb{R}^{D_y}\\). Assume that we also have \\(N\\) samples \\(x_1, \\ldots, x_N\\) generated from some (unkown) prior distribution. For example, these samples could come from another numerical procedure. In order to obtain \\(N\\) approximate samples from the posterior distribution, one can set\n\\[\n\\widetilde{x}_i\n=\nx_i + \\widehat{K} \\, [  \\textcolor{green}{ \\widetilde{y}_{i,\\star} } - \\mathcal{H}(x_i)]\n\\]\nfor fake observations \\( \\textcolor{green}{\\widetilde{y}_{i,\\star} } = y_\\star + \\xi_i\\). The approximate Kalman gain matrix \\(\\widehat{K}\\) is obtained by noting that in Equation 1 giving\n\\[\nK \\; = \\; P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}\n\\]\nwe have \\(P_0 H^\\top = \\mathop{\\mathrm{Cov}}(X,HX)\\) and \\(H P_0 H^\\top = \\mathop{\\mathrm{Var}}(HX)\\) for \\(X \\sim \\pi_0\\). This means that an approximate Kalman matrix can be obtained using empirical estimates of these covariance matrices:\n\\[\n\\widehat{K} \\; = \\; \\widehat{\\mathop{\\mathrm{Cov}}}([x_i]_i, [\\mathcal{H}(x_i)]_i) \\,  {\\left( \\widehat{\\mathop{\\mathrm{Var}}}([\\mathcal{H}(x_i)]_i) + R \\right)} ^{-1}.\n\\]\nThese updates form the basis of the Ensemble Kalman filter (EnKF), and very successful and scalable approach to data-assimilation of high-dimensional dynamical systems. This method is operationally employed across various weather forecasting centers across the globe.\n\n\n\n\nEnKF Bible by Geir Evensen\n\n\n\n\n\n\nInterestingly enough, the remarks above can be design in a relatively principled manner a derivative free optimizer (Huang et al. 2022). For example, assume one would like to minimize a functional of the type\n\\[\n\\Psi(x) \\; = \\; \\|y_\\star - \\mathcal{H}(x)\\|^2_{R^{-1}}.\n\\]\nOne can start with a cloud of particles \\(x_1, \\ldots, x_N\\) and keep updating them by assuming that one assimilates the noisy observation \\(y_\\star\\) generated from a postulated observation process\n\\[\ny_\\star = \\mathcal{H}(x) + \\varepsilon^{-1} \\, \\xi\n\\]\nfor \\(\\xi \\sim \\mathcal{N}(0,R)\\) and \\(\\varepsilon\\ll 1\\) a “step-size”. Each assimilation step steers the cloud of points towards the rights direction. Careful choice of the step-size \\(\\varepsilon\\) is often crucial, as in any optimization procedure. It is indeed related to Information-Geometric Optimization Algorithms (IGO): the article (Ollivier et al. 2017) is beautiful!"
  },
  {
    "objectID": "notes/index_notes.html",
    "href": "notes/index_notes.html",
    "title": "Notes",
    "section": "",
    "text": "Notes for students, half-baked write-ups for myself, and probably other random maths/stats/ML texts. Certainly full of typos, comments welcome!\n\n\nNotes indexed by categories.\n\n\nDenoising Diffusion Probabilistic Models\n\nNoising and Reverse Ornstein-Uhlenbeck\nFrom Denoising Diffusion to ODEs\nReverse diffusions, Score & Tweedie\n\n\n\nInformation Theory\n\nReferences & Readings\nEntropy and Basic Definitions\nShannon Source Coding Theorem\nFano’s inequality\nShearer’s Lemma\n\n\n\nMonte-Carlo methods\n\nDeriving the Langevin MCMC algorithm\nGaussian Assimilation & the EnKF\nEnsemble Kalman Smoothers\nMCMC with deterministic proposals\n\n\n\nProbability Misc\n\nAuxiliary variable trick\nSanov’s Theorem\nWasserstein Gradients & Langevin Diffusions\nPoisson Equation & Asymptotic Variance\nAveraging and Homogenization"
  },
  {
    "objectID": "notes/on_Langevin_MCMC/on_Langevin_MCMC.html",
    "href": "notes/on_Langevin_MCMC/on_Langevin_MCMC.html",
    "title": "Deriving Langevin MCMC",
    "section": "",
    "text": "Julian Besag (1945 – 2010)\n\n\n\nConsider a target density \\(\\pi(x)\\) in \\(\\mathbb{R}^D\\). Since the Langevin diffusion\n\\[\ndX_t = \\nabla \\log \\pi(X_t) \\, dt + \\sqrt{2} \\, dW\n\\tag{1}\\]\nis reversible with respect to \\(\\pi\\), it is natural to use a Euler-Maruyama discretization of Equation 1 to build MCMC proposals: in a MCMC simulation and for a time discretization parameter \\(\\varepsilon&gt; 0\\), if the current position is \\(x \\in \\mathbb{R}^D\\), a proposal \\(y \\in \\mathbb{R}^D\\) can be generated as\n\\[\ny = x + \\varepsilon\\, \\nabla \\log \\pi(X_t) + \\sqrt{2 \\varepsilon} \\, \\xi\n\\]\nwith \\(\\xi \\sim \\mathcal{N}(0,I)\\) before being accepted-or-reject according to the usual Metropolis-Hastings ratio. This MCMC method, first proposed by Julian Besag in 1994, is commonly referred to as the Metropolis-Adjusted-Langevin-Algorithm (MALA). But how can one come-up with this proposal mechanism without knowing before hand the existence of this reversible Langevin diffusion Equation 1? While it is intuitively clear that following the direction of \\(\\nabla \\log \\pi\\) is not such a bad idea, i.e. one would like to move towards areas of “high probability mass”, where does this \\(\\sqrt{2}\\) comes from? Naturally, one could look at proposals of the type \\(y = x + \\nabla \\log \\pi(X_t) \\, \\varepsilon+ \\lambda \\, \\xi\\) for some free parameter \\(\\lambda &gt; 0\\) and study the behavior of the Metropolis-Hastings ratio in the regime \\(\\varepsilon\\to 0\\): as simple as it sounds, it is not entirely straightforward and requires quite a bit of algebra (do it!). Instead, I very much like the type of approaches described in (Titsias and Papaspiliopoulos 2018). To summarize, we would like to generate a MCMC proposal \\(y \\in \\mathbb{R}^D\\) that stays in the vicinity of the current position \\(x \\in \\mathbb{R}^D\\) while exploiting the knowledge of \\(\\nabla \\log \\pi(x)\\). One cannot simply approximate the target distribution as \\(\\pi(x) \\approx \\pi(x_k) e^{\\left&lt; \\nabla \\log \\pi(x_k), x-x_k \\right&gt;}\\) and sample from this approximation since it is typically does not define a probability distribution. Instead, consider the following extended target distribution\n\\[\n\\overline{\\pi}(x,z) \\, \\propto \\pi(x) \\, \\exp {\\left\\{ -\\frac{1}{2\\varepsilon}\\|z-x\\|^2 \\right\\}} .\n\\]\nIn other words, the Gaussian auxiliary variable \\(z \\in \\mathbb{R}^D\\) is centred at \\(x\\) and at distance about \\(\\sqrt{\\varepsilon}\\) of it. Now, given the current position \\(x_k\\), to generate a proposal \\(y_\\star\\) that stays in the vicinity of \\(x_k\\), one can proceed in two steps, in the spirit of a Gibbs-sampling approach:\n\nFirst, generate \\(z_\\star \\sim \\overline{\\pi}(dz | x_k) \\sim \\mathcal{N}(x_k, \\sqrt{\\varepsilon}I)\\)\nSecond, sample from \\(y_\\star \\sim \\overline{\\pi}(dx | z_\\star)\\).\n\nUnfortunately, the second step is typically not tractable. Nevertheless, the conditional density \\(\\overline{\\pi}(dx | z_\\star)\\) is concentrated in a \\(\\sqrt{\\varepsilon}\\)-neighborhood of \\(z_\\star\\) and a simple Gaussian approximation around \\((x_k, z_\\star)\\) should be enough for our purpose. We have:\n\\[\n\\begin{align}\n\\log \\overline{\\pi}(dx | z_\\star)\n&=\n\\log \\pi(x) - \\frac{1}{2 \\varepsilon} \\|x - z_\\star\\|^2 + \\textrm{(Cst)}\\\\\n&\\approx\n\\left&lt;  \\nabla \\log \\pi(x_k), x-x_k  \\right&gt; - \\frac{1}{2 \\varepsilon} \\|x - z_\\star\\|^2 + \\textrm{(Cst)}\\\\\n&=\n- \\frac{1}{2 \\varepsilon} \\|x - [z_\\star + \\varepsilon\\, \\nabla \\log \\pi(x_k)]\\|^2 + \\textrm{(Cst)}.\n\\end{align}\n\\]\nThis shows that the conditional \\(\\overline{\\pi}(dx | z_\\star)\\) can be approximated by a Gaussian distribution centred at \\([z_\\star + \\nabla \\log \\pi(x_k)]\\) and variance \\(\\varepsilon\\, I\\). This means that the final proposal \\(y \\in \\mathbb{R}^D\\) can be generated as \\(y \\sim z_\\star + \\nabla \\log \\pi(x_k) + \\xi\\) where \\(\\xi \\sim \\mathcal{N}(0,\\varepsilon)\\). But that is equivalent to setting\n\\[\ny \\sim x + \\varepsilon\\, \\nabla \\log \\pi(x_k) + \\sqrt{2 \\varepsilon} \\, \\xi\n\\]\nwith \\(\\xi \\sim \\mathcal{N}(0,I)\\) since \\(z_\\star \\sim \\mathcal{N}(x, \\sqrt{\\varepsilon} I)\\). It is exactly the MALA proposal. Naturally, one can also try to be slightly more clever and use an extended distribution\n\\[\n\\overline{\\pi}(x,z) \\, \\propto \\, \\pi(x) \\, \\exp {\\left\\{  -\\frac{1}{2\\varepsilon} \\left&lt; (z-x), M^{-1} \\, (z-x) \\right&gt;  \\right\\}}\n\\]\nfor some appropriate positive-definite “mass” matrix \\(M \\in \\mathbb{R}^{D,D}\\). Indeed, this immediately leads to preconditioned MALA methods. I really like this approach since it can be adapted and generalized to quite a few other situations!\n\n\n\n\nReferences\n\nTitsias, Michalis K, and Omiros Papaspiliopoulos. 2018. “Auxiliary Gradient-Based Sampling Algorithms.” Journal of the Royal Statistical Society Series B: Statistical Methodology 80 (4). Oxford University Press: 749–67."
  },
  {
    "objectID": "notes/information_theory_shannon_coding/information_theory_shannon_coding.html",
    "href": "notes/information_theory_shannon_coding/information_theory_shannon_coding.html",
    "title": "Shannon Source Coding Theorem",
    "section": "",
    "text": "Claude Shannon (1916 – 2001)\n\n\n\n\nTransmission through a noisy channel\nConsider a scenario involving a “noisy channel,” where a message \\((x_1,x_2, \\ldots)\\) expressed in an alphabet \\(\\mathcal{X}\\) is transmitted before being received as a potentially different and corrupted message \\((y_1, y_2,\\ldots)\\) expressed using a potentially different alphabet \\(\\mathcal{Y}\\). One can assume that letter \\(x \\in \\mathcal{X}\\) is transformed into \\(y \\in \\mathcal{Y}\\) with probability \\(p(x \\to y)\\) so that the matrix \\(M_{x,y} = [p(x \\to y)]_{(x,y) \\in \\mathcal{X}\\times \\mathcal{Y}}\\) has rows summing-up to one, and that the “letters” of the message \\((x_1 x_2 \\ldots)\\) are transmitted one by one and independently from each other (ie. there is no memory effect in the channel).\nNow, imagine I have a text that needs to be transmitted through this channel. Assume that the text is represented using \\(N\\) bits. My goal is to encode it in a way that introduces redundancy, utilizing the alphabet \\(\\mathcal{X}\\), so that it can be transmitted through the noisy channel. Eventually, I want it to be successfully decoded, allowing the original message to be recovered with minimal errors.\n\n\n\n\nA Mathematical Theory of Communication\n\n\n\nIf transmitting each letter from the alphabet \\(\\mathcal{X}\\) takes \\(1\\) unit of time, I need to estimate the overall time it will take to transmit the entire text of \\(N\\) bits. In cases where the channel does not completely destroy the information, one can use about any non-idiotic encoding and transmit the same text multiple times to increase the chances of accurately recovering the original message. This can be achieved by employing techniques such as majority voting to decode the received messages, which are all distorted versions of the original text.\nThe transmission rate represents the inverse of the time required to transfer a single bit of information:\n\\[\n\\textrm{R = (Transmission Rate)} = \\frac{1}{\\textrm{(average time it takes to transfer one bit)}}.\n\\]\nIn other words, it takes about \\(N \\times R\\) unit of times to transfer a text of \\(N\\) bits. Moreover, the error rate refers to the percentage of errors in the decoded message, indicating the fraction of erroneous bits among the \\(N\\) decoded bits. Naturally, there exists a tradeoff, and it is evident that one can reach a vanishing error rate if one is willing to allow an arbitrarily slow transmission rate (eg. majority voting after transmitting a very large number of times the same text). For example, if \\(\\mathcal{X}= \\mathcal{Y}= \\{0,1\\}\\) and bits are flipped with probability \\(p_{\\text{flip}} \\ll 1\\), transmitting the text \\((2K+1)\\) times would lead to a transmission rate of \\(R = 1/(2K+1)\\) and an error rate approximately equal to \\(p_{\\text{flip}}^{K+1}\\).\nThe groundbreaking discovery made by Shannon is that it is possible to achieve a vanishing error rate even when transmitting at a finite transmission rate. He also managed to identify this optimal transmission rate. Shannon’s paper (Shannon 1948) is beautifully written and surprisingly readable for a text written more than 50 years ago.\n\n\nVanishing error rate: Shannon Codebooks\nLet’s imagine that we have a piece of information encoded in a variable, \\(X\\). We send \\(X\\) through a noisy channel, and at the other end we receive a somewhat distorted message, \\(Y\\). So, how much of our original information actually was transmitted? To reconstruct our original message, \\(X\\), using our received message, \\(Y\\), we require an average of \\(H(X|Y)\\) additional bits of information. On average, \\(X\\) contains \\(H(X)\\) bits of information. So, if we encode \\(H(X)\\) bits of useful information in \\(X\\), the variable \\(Y\\) that is correlated with \\(X\\) still holds \\(I(X;Y) = H(X) - H(X \\, | Y)\\) bits of that original information. The quantity \\(I(X;Y)\\) is the mutual information between the random variables \\(X\\) and \\(Y\\). In a noisy channel that transmits one “letter” at a time, the conditional probabilities \\(p(x \\rightarrow y)\\) are fixed. However, we can optimize the distribution of incoming messages. For instance, we can choose to transmit letters that are less likely to be corrupted. This discussion suggests that on average, transmitting \\(N\\) symbols through the channel can provide up to \\(N \\times C\\) bits of information, where \\(C = \\max I(X;Y)\\), the maximization being over the distribution of \\(X\\) while keeping the conditional probabilities \\(p(x \\rightarrow y)\\) fixed. It may seem that this implies a noisy channel cannot transmit information at a rate higher than \\(C\\). This hypothesis was precisely proven by Claude Shannon, who further established that this transmission rate can indeed be reached.\nTo prove that this transmission rate is achievable, Shannon’s idea was to simultaneously encode blocks of letters. To put it simply, consider the \\(2^N\\) feasible blocks \\(\\{ t^{[1]}, \\ldots, t^{[2^N]} \\}\\) of \\(N\\) binary letters. Each block \\(t^{[i]} \\in \\{0,1\\}^N\\) has \\(N\\) binary letters, \\(t^{[i]} = (t_1^{[i]}, \\ldots, t_N^{[i]})\\). Associate to each of block \\(t^{[i]} \\in \\{0,1\\}^N\\) a codeword \\(x^{[i]} \\in \\mathcal{X}^K\\) of size \\(K\\) in the alphabet \\(\\mathcal{X}\\). The set of these \\(2^N\\) codewords is usually called the codebook,\n\\[\n\\mathcal{C}= \\left\\{ x^{[1]}, x^{[2]}, \\ldots, x^{[2^N]} \\right\\} \\; \\subset \\mathcal{X}^{K}\n\\tag{1}\\]\nTo transmit a block of \\(N\\) letters from the original text, this block is first transformed into its associated codeword \\(x=(x_1, \\ldots, x_K) \\in \\mathcal{X}^K\\). This codeword is then sent through the noisy channel, resulting in a received message \\((y_1, \\ldots, y_K) \\in \\mathcal{Y}^K\\). The objective is to design a codebook with enough redundancy so that one can reconstruct the original codeword from the received message \\((y_1, \\ldots, y_K)\\): the higher the ratio \\(K/N\\), the larger the redundancy and the easier it should be to achieve this goal. The transmission rate is defined as \\(R = \\frac{N}{K}\\) since transmitting a binary text of length \\(N\\) with vanishing errors takes \\(K\\) units of time.\nFor generating the codebook in Equation 1, Shannon adopted a simple approach consisting in generating each \\(x^{[i]}_k\\) for \\(1 \\leq i \\leq 2^N\\) and \\(1 \\leq k \\leq K\\) independently at random from some (encoding) distribution \\(p_{\\text{code}}(dx)\\). The choice of this encoding distribution can be optimized at a later stage.\nConsider the codeword \\(x^{[0]} = (x^{[0]}_1, \\ldots, x^{[0]}_K)\\). After being transmitted through the noisy channel, this gives rise to a message \\(y_{\\star}\\). The codeword \\(x^{[0]}\\) can be easily recovered if \\((x^{[0]}, y_\\star)\\) is typical while all the other pairs \\((x^{[i]}, y_\\star)\\) for \\(2 \\leq i \\leq 2^N\\) are atypical. Since there are about \\(2^{K \\, H(X | Y)}\\) elements \\(x \\in \\mathcal{X}^K\\) such that \\((x, y_\\star)\\) is typical, and each codeword was chosen approximately uniformly at random within its typical set of size \\(2^{K \\, H(X)}\\), the probability for a random codeword to be atypical is about\n\\[1-2^{-K \\, [H(X) - H(X|Y)]} = 1 - 2^{-K \\, I(X;Y)}\\]\nConsequently, the probability that all the other pairs \\((x^{[i]}, y_\\star)\\) for \\(2 \\leq i \\leq 2^N\\) are atypical is\n\\[\np_{\\text{success}} = (1 - 2^{-K \\, I(X;Y)})^{2^N-1} \\approx (1 - 2^{-K \\, I(X;Y)})^{2^{KR}}.\n\\]\nThe probability \\(p_{\\text{success}} \\to 1\\) as soon as \\(R &lt; I(X;Y)\\) as \\(N \\to \\infty\\). Furthermore, remembering that one were free to optimize the encoding distribution \\(p_{\\text{code}}(dx)\\), a vanishing error rate is possible as soon as the transmission \\(R\\) rate is lower than\n\\[\n\\text{(Channel Capacity)} = C \\equiv \\max_{p_{\\text{code}}} \\; I(X;Y).\n\\]\nTo sum-up, consider \\(p_{\\mathcal{C}, \\text{success}}\\) the success rate of the codebook \\(\\mathcal{C}\\), ie. the probability that a random codeword of \\(\\mathcal{C}\\) is successfully decoded when passing through the noisy channel. The reasoning above shows that the averaged success rate \\(p_{\\text{success}} = \\left&lt; p_{\\mathcal{C}, \\text{success}} \\right&gt;\\), i.e. averaging \\(p_{\\mathcal{C}, \\text{success}}\\) over all possible codebooks \\(\\mathcal{C}\\), converges to one as long as the transmission rate is below the channel capacity \\(C\\). This means that one can find at least one codebook that works well! This reasoning is an example of the “probabilistic method”… Indeed, one also expect most random codebook to work well!\n\n\nNo vanishing error below the channel capacity\nTo demonstrate that transmission at vanishing error-rate is impossible when the transmission rate exceeds the channel capacity, \\(C\\), we can utilize Fano’s inequality.\nImagine selecting a message \\(M\\) uniformly at random within \\(\\{0,1\\}^N\\) and encode this message into the sequence \\(X=(X_1, ..., X_K) \\in \\mathcal{X}^K\\). We send \\(X\\) through a channel with capacity \\(C\\) and receive a corresponding, though somewhat distorted, signal \\(Y=(Y_1, ..., Y_K)\\). Finally, we decode this received message into \\(\\widehat{M}\\), an estimate of our original message:\n\\[\nM \\rightarrow X \\rightarrow Y \\rightarrow \\widehat{M}.\n\\]\nFano’s inequality points out that the error probability, \\(p_E = \\mathop{\\mathrm{P}}(\\widehat{M} \\neq M)\\) is such that\n\\[\n\\begin{align}\nH(M | \\widehat{M})\n&\\leq 1 + p_E \\, \\log_2(\\# \\textrm{possible values of } M)\\\\\n&= 1 + p_E \\, N\n\\end{align}\n\\]\nApplying the data-processing inequality to \\(M \\rightarrow X \\rightarrow Y \\rightarrow \\widehat{M}\\) proves:\n\\[\n\\begin{align}\nN &= H(M) = H(M | \\widehat{M}) + I(M; \\widehat{M}) \\\\\n& \\leq H(M | \\widehat{M}) + I(X; Y)\\\\\n& \\leq 1 + N \\, p_E + I(X; Y).\n\\end{align}\n\\]\nTo wrap up, recall that each received letter \\(Y_i\\) in the message (Y_1, , Y_K)$ depends solely on the corresponding letter \\(X_i\\) in the message sent through the channel. This implies that \\(I(X; Y) \\leq \\sum_{i=1}^K I(X_i; Y_i) \\leq K \\, C\\).This yields:\n\\[\nN \\leq 1 + N \\, p_E + K \\, C.\n\\]\nThis reveals that for the probability of error to go to zero, i.e. \\(p_E \\rightarrow 0\\) as \\(N \\rightarrow \\infty\\), the transmission rate \\(N/K\\) must be lower than \\(C\\).\n\n\nExperiment\nConsider the Binary Symmetric Channel (BSC) that randomly flips \\(0 \\mapsto 1\\) and \\(1 \\mapsto 0\\) with equal probability \\(0&lt;q&lt;1\\). The capacity of this channel is easily computed and equals \\(C = 1 - h_2(q)\\) where \\(h_2(q) = -[q \\, \\log_2(q) + (1-q) \\, \\log_2(1-q)]\\) is the binary entropy function: the optimal encoding distribution is\n\\[p_{\\text{code}}(0) = p_{\\text{code}}(2) = 1/2.\\]\nFor a flipping rate of \\(q=0.1\\) the channel capacity equals \\(C=0.53\\). To estimate the performance of the random Shannon codebook strategy, I chose \\(N=13\\) and several values of \\(K \\geq N\\). This means generating a random codebook \\(\\mathcal{C}= \\{x^{[1]}, \\ldots, x^{[2^N]}\\}\\) of size \\(2^{13} = 8192\\) consisting of random binary vectors of size \\(K\\). For a randomly chosen codeword \\(x^{[i]}\\), a received message \\(y_\\star\\) is generated by flipping each of the \\(K\\) coordinates of \\(x^{[i]}\\) independently with probability \\(q\\). In the BSC setting, it is easily seen that the codeword of \\(\\mathcal{C}\\) that was the most likely to have originated \\(y_{\\star}\\) is\n\\[\nx_\\star \\; = \\; \\mathop{\\mathrm{argmin}}_{x \\in \\mathcal{C}} \\; \\|x - y_\\star\\|_{L^2}.\n\\]\nThe nearest neighbor \\(x_\\star\\) can be relatively efficiently computed with a nearest-neighbor routine (eg. FAISS). The figure below reports the probability of error (i.e. “Block Error Rate”),\n\\[\n\\text{(Block Error Rate)} \\; = \\; \\mathop{\\mathrm{P}}(x_\\star \\neq x^{[i]})\n\\]\nwhen the codeword \\(x^{[i]}\\) is chosen uniformly at random within the codebook.\n\n\n\n\n\n\n\nIt can be seen that, although the error rate does go to zero for low transmission rate, the choice of \\(K = N / C\\) where \\(C\\) is the channel capacity still yields a relatively large block error rate. This indicates that the block size \\(N=13\\) is still far too low for the “law of large number” arguments presented in the previous section to kick-in. I did try for \\(N=20\\) and a codebook of \\(2^{20} \\approx 10^6\\) and the performace was still not impressive. This shows that even though the Shannon codebook approaches is an elegant construction, it is far from being practically useful. It requires a very large codebook of size \\(2^N\\) and decoding requires doing a nearest-neighbors search that can become slow as \\(N\\) increases.\n\n\n\n\n\nReferences\n\nShannon, Claude Elwood. 1948. “A Mathematical Theory of Communication.” The Bell System Technical Journal 27 (3). Nokia Bell Labs: 379–423."
  },
  {
    "objectID": "notes/index_notes_as_list.html",
    "href": "notes/index_notes_as_list.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n18-12-2023\n\n\nMetropolis-Hastings ratio with deterministic proposals\n\n\nauxiliary-variable\n\n\n\n\n28-11-2023\n\n\nAveraging and homogenization\n\n\ndiffusion\n\n\n\n\n18-11-2023\n\n\nEnsemble Kalman Smoother (EnKS)\n\n\nenkf,data-assimilation\n\n\n\n\n11-11-2023\n\n\nAsymptotic variance & Poisson Equation\n\n\nmarkov\n\n\n\n\n23-10-2023\n\n\nGaussian Assimilation & the EnKF\n\n\nenkf,data-assimilation\n\n\n\n\n19-10-2023\n\n\nDeriving Langevin MCMC\n\n\nMCMC\n\n\n\n\n16-10-2023\n\n\nWasserstein Gradients & Langevin Diffusions\n\n\ndiffusion\n\n\n\n\n09-10-2023\n\n\nSanov’s Theorem\n\n\nLargeDeviation\n\n\n\n\n03-10-2023\n\n\nAuxiliary variable trick\n\n\nauxiliary-variable\n\n\n\n\n02-10-2023\n\n\nShearer’s lemma\n\n\ninfoTheory\n\n\n\n\n30-09-2023\n\n\nInformation Theory: References and Readings\n\n\ninfoTheory\n\n\n\n\n26-09-2023\n\n\nShannon Source Coding Theorem\n\n\ninfoTheory\n\n\n\n\n23-09-2023\n\n\nInformation Theory: Fano’s inequality\n\n\ninfoTheory\n\n\n\n\n23-09-2023\n\n\nInformation Theory: Entropy and Basic Definitions\n\n\ninfoTheory\n\n\n\n\n02-07-2023\n\n\nFrom Denoising Diffusion to ODEs\n\n\nDDPM,score\n\n\n\n\n02-07-2023\n\n\nDenoising Diffusion Probabilistic Models (DDPM)\n\n\nDDPM,score\n\n\n\n\n12-06-2023\n\n\nReverse diffusions, Score & Tweedie\n\n\nSDE,score\n\n\n\n\n01-01-2023\n\n\nNotes\n\n\nindex\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/Poisson_Eq_Asymp_Var/Poisson_Eq_Asymp_Var.html",
    "href": "notes/Poisson_Eq_Asymp_Var/Poisson_Eq_Asymp_Var.html",
    "title": "Asymptotic variance & Poisson Equation",
    "section": "",
    "text": "Consider a continuous time Markov process \\(X_t\\) on \\(\\mathbb{R}^D\\) that is ergodic with respect to the probability distribution \\(\\pi(dx)\\). A Langevin diffusion is a typical example. Call \\(\\mathcal{L}\\) the generator of this process so that for a test function \\(\\varphi: \\mathbb{R}^D \\to \\mathbb{R}\\) we have\n\\[\n\\varphi(X_t) = \\varphi(X_0) + \\int_{s=0}^t \\mathcal{L}\\varphi(X_s) \\, ds + \\textrm{($M_t \\equiv$ martingale)}.\n\\tag{1}\\]\nNow, assume further that \\(\\mathop{\\mathrm{E}}_{\\pi}[\\varphi(X)] = 0\\) and that a Central Limit Theorem holds,\n\\[\n\\frac{1}{\\sqrt{T}} \\int_{s=0}^T \\varphi(X_s) \\, ds \\; \\to \\; \\mathcal{N}(0, \\sigma^2).\n\\tag{2}\\]\nHow can one estimate the asymptotic variance \\(\\sigma^2\\)?\n\nApproach I: Integrated autocovariance\nOne can directly try to compute the second moment of Equation 2 and obtain that\n\\[\n\\sigma^2 \\; = \\; \\lim_{T \\to \\infty} \\;\n\\frac{1}{T} \\, \\iint_{0 \\leq s,t \\leq T} \\mathop{\\mathrm{E}}[\\varphi(X_s) \\varphi(X_t)] \\, ds \\, dt\n\\]\nSince \\(\\mathop{\\mathrm{E}}[\\varphi(X_s) \\varphi(X_t)]\\) falls quickly to zero as \\(|s-t| \\to 0\\) and defining the auto-covariance at lag \\(r &gt; 0\\) as\n\\[\nC(r) = \\mathop{\\mathrm{E}}[\\varphi(X_t) \\varphi(X_{t+r})],\n\\]\none obtains that an expression of the asymptotic as the integrated autocovariance function,\n\\[\n\\sigma^2 \\; = \\; 2 \\, \\int_{r=0}^\\infty C(r) \\, dr.\n\\tag{3}\\]\nIn the MCMC literature, this relation is often expressed as\n\\[\n\\sigma^2 \\; = \\; \\mathop{\\mathrm{Var}}_{\\pi}[\\varphi] \\, \\times \\, \\textrm{(IACT)}\n\\]\nwhere the integrated autocorrelation function is defined as\n\\[\n\\textrm{(IACT)} = 2 \\, \\int_{r=0}^\\infty \\rho(r) \\, dr.\n\\]\nfor autocorrelation at lag \\(r\\geq 0\\) defined as \\(\\rho(r) \\equiv \\mathop{\\mathrm{Corr}}[\\varphi(X_t), \\varphi(X_{t+r})]\\). The slower the autocorrelation function \\(\\rho(r)\\) falls to zero as \\(r \\to \\infty\\), the larger the asymptotic variance \\(\\sigma^2\\). Although Equation 3 is very intuitive, it can be difficult to estimate the autocorrelation function.\n\n\nApproach II: Poisson Equation\nUnder relatively general and mild conditions, since the expectation of \\(\\varphi\\) under the invariant distribution \\(\\pi\\) is zero and the Markov process is ergodic with respect to \\(\\pi\\), there exists a function \\(\\Phi: \\mathbb{R}^D \\to \\mathbb{R}\\) such that\n\\[\n\\mathcal{L}\\Phi \\; = \\; \\varphi.\n\\tag{4}\\]\nEquation 4 is called a Poisson Equation since \\(\\mathcal{L}\\) is often a Laplacian-like operator (eg. diffusion-type processes). Equation 1 gives that\n\\[\n\\frac{1}{\\sqrt{T}} \\int_{s=0}^T \\varphi(X_s) \\, ds\n\\; = \\;\n\\frac{M_T}{\\sqrt{T}} +  {\\left\\{  \\frac{\\Phi(X_T) - \\Phi(X_0)}{\\sqrt{T}}  \\right\\}}\n\\]\nwhere \\(M_T\\) is the martingale and \\([\\Phi(X_T) - \\Phi(X_0)]/\\sqrt{T}\\) typically vanishes as \\(T \\to \\infty\\) and can be neglected. For computing the asymptotic variance, it suffices to estimate \\(\\mathop{\\mathrm{E}}(M_T^2)\\). And using the martingale property, it equals \\(\\int_{s=0}^T \\mathop{\\mathrm{E}}(dM_t)^2\\). Also, since \\(M_t = \\varphi(X_t) - \\varphi(X_0) - \\int_{s=0}^t \\mathcal{L}\\varphi(X_s) \\, ds\\), algebra gives that\n\\[\n\\frac{1}{\\varepsilon} \\, \\mathop{\\mathrm{E}} {\\left[  (M_{t+\\varepsilon} - M_t)^2  \\right]}  \\approx 2 \\mathop{\\mathrm{E}} {\\left[  (\\Gamma \\Phi)(X_t)  \\right]}\n\\]\nwhere the so-called carré du champ \\((\\Gamma \\Phi)\\) is defined as\n\\[\n2 \\, (\\Gamma \\Phi)(X_t)\n\\; = \\;  {\\left(  \\mathcal{L}(\\Phi^2) - 2 \\Phi \\mathcal{L}\\Phi \\right)} (X_t)\n\\; = \\; \\lim_{\\varepsilon\\to 0} \\; \\frac{1}{\\varepsilon} \\mathop{\\mathrm{Var}}(\\Phi(X_{t+\\varepsilon}) \\, | \\, X_t).\n\\]\nThis shows that the asymptotic variance satisfies\n\\[\n\\sigma^2\n\\; = \\;\n\\lim_{T \\to \\infty} \\frac{2}{T} \\int_{s=0}^T \\Gamma \\Phi(X_s) \\, ds\n\\; = \\;\n2 \\, \\int_{x \\in \\mathbb{R}^D} \\Gamma \\Phi(x) \\, \\pi(dx).\n\\]\nFinally, since \\(\\int (\\mathcal{L}\\Phi^2)(x) \\, \\pi(dx) = 0\\), this can equivalently be written as\n\\[\n\\sigma^2\n\\; = \\;\n-2 \\, \\int_{x \\in \\mathbb{R}^D} \\Phi(x) \\, \\mathcal{L}\\Phi(x) \\, \\pi(dx)\n\\; = \\; 2 \\, \\mathcal{D}(\\Phi)\n\\tag{5}\\]\nwhere \\(\\mathcal{D}(\\Phi)\\) is the so-called Dirichlet form. In summary, we have just shown that the asymptotic variance of the additive functional \\(T^{-1/2} \\, \\int_0^T \\varphi(X_s) \\, ds\\) is given by two times the Dirichlet form \\(\\mathcal{D}(\\Psi)\\) where \\(\\Phi\\) is solution to the Poisson equation \\(\\mathcal{L}\\Phi = \\varphi\\). Note that this implies that the generator \\(\\Phi\\) is a negative operator in the sense that for a test function \\(\\Phi\\) we have that\n\\[\n\\left&lt; \\Phi, \\mathcal{L}\\Phi \\right&gt;_{\\pi} \\; \\leq \\; 0\n\\]\nwhere we have used the dot-product notation \\(\\left&lt; f,g \\right&gt;_{\\pi} = \\int f(x) g(x) \\pi(dx)\\).\n\n\nPoisson equation: Integral representation\nIt is often useful to think of the generator \\(\\mathcal{L}\\) as an infinite dimensional equivalent of a standard negative definite symmetric matrix/operator \\(M \\in \\mathbb{R}^{n,n}\\). And since \\(M^{-1} = -\\int_{t=0}^{\\infty} \\exp(tM) \\, dt\\), as can be seen by diagonalizing \\(M\\), one can expect the following equation to hold,\n\\[\n\\mathcal{L}^{-1} \\; = \\; -\\int_{t=0}^{\\infty} e^{t \\, \\mathcal{L}} \\, dt.\n\\]\nThat is just another way of writing that the solution \\(\\Phi\\) to the Poisson equation \\(\\mathcal{L}\\Phi = \\varphi\\), with the centering condition \\(\\mathop{\\mathrm{E}}_{\\pi}[\\Phi(X)]=0\\) for picking one solution out of the many possible solutions to the Poisson equation differing from each other by an additive constant, can be expressed as\n\\[\n\\Phi(x) \\, = \\, -\\int_{t=0}^{\\infty} \\mathop{\\mathrm{E}}[\\varphi(X_t)|X_0=x] \\, dt.\n\\tag{6}\\]\nEquation 6 is easily proved with Equation 1 by writing\n\\[\n\\Phi(x)-\\Phi(X_T) = -\\int_{t=0}^\\infty \\varphi(X_t) \\, dt + \\textrm{(martingale)}\n\\]\nand by taking expectation from both sides and noticing that \\(\\mathop{\\mathrm{E}}[\\Phi(X_T)] \\to 0\\) thanks to the assumed centering condition \\(\\mathop{\\mathrm{E}}_{\\pi}[\\Phi(X)]=0\\). Note that this remarks allows to give another derivation of Equation 5 starting from the integrated autocovariance formulation Equation 3. Indeed, note that\n\\[\n\\begin{align}\n\\sigma^2 &= 2 \\, \\int_{r=0}^{\\infty} C(r) \\, dr\\\\\n&=\n2 \\, \\int_{x \\in \\mathbb{R}^D} \\varphi(x)  {\\left\\{  \\int_{r=0}^{\\infty}  \\mathop{\\mathrm{E}}[\\varphi(X_t) | X_0=x] \\, dr  \\right\\}}  \\, \\pi(dx)\\\\\n&=\n-2 \\, \\int_{x \\in \\mathbb{R}^D} \\varphi(x) \\Phi(x) \\, \\pi(dx)\n=\n-2 \\left&lt; \\Phi, \\mathcal{L}\\Phi \\right&gt;_{\\pi}.\n\\end{align}\n\\]\n\n\nExample: OU process\nConsider a OU process that is ergodic with respect to the standard Gaussian density \\(\\pi(x) = e^{-x^2/2} / \\sqrt{2\\pi}\\),\n\\[\ndX\n\\; = \\;\n-\\varepsilon^{-1}X \\, dt + \\sqrt{2 \\, \\varepsilon^{-1}} \\, dW.\n\\]\nThat’s a standard OU process accelerated by a factor \\(\\varepsilon^{-1} &gt; 0\\). Its generator reads\n\\[\n\\mathcal{L}\\, = \\, \\varepsilon^{-1} [-x \\, \\partial_x + \\partial_{xx}].\n\\]\nThe function \\(\\varphi(x)=x\\) is such that \\(\\pi(\\varphi)=0\\) and a solution to the Poisson equation \\(\\mathcal{L}\\Phi = \\varphi\\) is \\(\\Phi(x) = -\\varepsilon\\, x\\). This shows that the asymptotic variance is\n\\[\n\\sigma^2\n\\; = \\;\n2 \\, \\int_{x \\in \\mathbb{R}^D} \\varepsilon x^2 \\, \\pi(dx) \\; = \\; 2 \\varepsilon.\n\\]\nAs expected, accelerating the OU process by a factor \\(\\varepsilon^{-1}\\) means reducing the variance by a factor \\(\\varepsilon\\)."
  },
  {
    "objectID": "notes/auxiliary_variable_trick/auxiliary_variable_trick.html",
    "href": "notes/auxiliary_variable_trick/auxiliary_variable_trick.html",
    "title": "Auxiliary variable trick",
    "section": "",
    "text": "Consider a complicated distribution on the state space \\(x \\in \\mathcal{X}\\) given by\n\\[\n\\pi(x) = \\frac{1}{\\mathcal{Z}} \\, e^{C(x) + B(x)}\n\\]\nfor a “complicated” functions \\(C(x)\\) and a simpler one \\(B(x)\\). In some situations, it is possible to introduce an auxiliary random variable \\(a \\in \\mathcal{A}\\) and an extended probability distribution \\(\\pi(x,a)\\) on the extended space \\(\\mathcal{X}\\times \\mathcal{A}\\),\n\\[\n\\pi(x,a) = \\pi(x) \\,  \\textcolor{red}{\\pi(a | x)} = \\frac{1}{\\mathcal{Z}} e^{C(x) + B(x)} \\,  \\textcolor{red}{e^{-C(x) + D(x, a)}},\n\\]\nwith a tractable conditional probability \\(\\pi(a | x)\\). This extended target distribution \\(\\pi(x,a) = (1/\\mathcal{Z}) \\, \\exp[B(x) + D(x,a)]\\) can be often be easier to explore, for example when \\(a\\) is continuous while \\(x\\) is discrete, or to analyze, since the “complicated” term \\(C(x)\\) has disappeared. Furthermore, there are a number of scenarios when the variable \\(x\\) can be averaged out of the extended distribution, i.e. the distribution\n\\[\n\\pi(a) = \\frac{1}{\\mathcal{Z}} \\, \\int_{x \\in \\mathcal{X}} e^{B(x) + D(x,a)}\n\\]\ncan be evaluated exactly.\n\nSwendsen–Wang algorithm\nConsider a set of edges \\(\\mathcal{E}\\) on a graph with vertices \\(\\{1, \\ldots, N\\}\\). The Ising model is defined as \\[\n\\pi(x) \\propto \\exp  {\\left\\{  \\sum_{(i,j) \\in \\mathcal{E}} \\beta x_i x_j  \\right\\}}\n\\]\nfor spin configurations \\(x=(x_1, \\ldots, x_N) \\in \\{-1,1\\}^N\\). The term \\(\\exp[\\beta x_i x_j]\\) couples the two spins \\(x_i\\) and \\(x_j\\) for each edge \\((i,j) \\in \\mathcal{E}\\). The idea of the Swendsen–Wang_algorithm is to introduce an auxiliary variable \\(u_{i,j}\\) for each edge \\((i,j) \\in \\mathcal{E}\\) that is uniformly distributed on the interval \\([0, \\exp(\\beta x_i x_j)]\\), i.e.\n\\[\n\\pi(u_{i,j} | x) \\; = \\; \\frac{ \\mathbf{1}  {\\left\\{  0 &lt; u_{i,j} &lt; \\exp[\\beta x_i x_j]  \\right\\}}  }{\\exp[\\beta x_i x_j] }\n\\]\nIt follows that the extended distribution on \\(\\{-1,1\\}^N \\times (0,\\infty)^{|\\mathcal{E}|}\\) reads\n\\[\n\\pi(x,u) = \\frac{1}{Z} \\prod_{(i,j) \\in \\mathcal{E}} \\; \\mathbf{1}  {\\left\\{  0 &lt; u_{i,j} &lt; \\exp[\\beta x_i x_j]  \\right\\}}\n\\]\nfor \\(x=(x_1, \\ldots, x_N)\\) and \\(u = (u_{i,j})_{(i,j) \\in \\mathcal{E}}\\): the coupling term \\(\\exp[\\beta x_i x_j]\\) has disappeared. Furthermore, it is straightforward to sample from the conditional distribution \\(\\pi(u | x)\\) and, perhaps surprisingly, it is also relatively straightforward to sample from the other conditional distribution \\(\\pi(x | u)\\) – this boils down to finding the connect components of the graph on \\(\\{1, \\ldots, N\\}\\) with an edge \\(i \\sim j\\) present if \\(u_{i,j} &gt; e^{-\\beta}\\) and flipping a fair coin for setting each connected component to \\(\\pm 1\\). This leads to an efficient Gibbs sampling scheme to sample from the extended distribution.\n\n\n\n\nSwendsen-Wang MCMC algorithm at critical temperature\n\n\n\n\n\nGaussian Integral trick: Curie-Weiss model\nFor an inverse temperature \\(\\beta &gt; 0\\), consider the distribution on \\(x \\in \\{-1,1\\}^N\\)\n\\[\n\\pi(x) = \\frac{1}{\\mathcal{Z}(\\beta)} \\, e^{\\beta \\, N \\, m^2}\n\\]\nwhere the magnetization of the system of spins \\(x=(x_1, \\ldots, x_N)\\) is defined as\n\\[\nm = \\frac{x_1 + \\ldots + x_N}{N}.\n\\]\nThe distribution \\(\\pi(x)\\) for \\(\\beta \\gg 1\\) favours configurations with a magnetization close to \\(+1\\) or \\(-1\\). The normalization constant (i.e. partition function) \\(\\mathcal{Z}(\\beta)\\) is a sum of \\(2^N\\) terms,\n\\[\n\\mathcal{Z}(\\beta) = \\sum_{s_1 \\in \\{ \\pm 1\\} } \\ldots \\sum_{s_N \\in \\{ \\pm 1\\} } \\exp  {\\left\\{  \\frac{\\beta}{N}  {\\left(  \\sum_{i=1}^N x_i  \\right)} ^2 \\right\\}} .\n\\]\nIt is not difficult to estimate \\(\\log \\mathcal{Z}(\\beta)\\) as \\(N \\to \\infty\\) with combinatorial arguments. Nevertheless, another way to proceed is as follows. One can introduce a Gaussian auxiliary random variable \\(\\pi(a | x) = \\mathcal{N}(\\alpha \\,  {\\left( \\sum_i x_i \\right)}  , \\sigma^2)\\) with mean \\(\\mu = \\alpha \\,  {\\left( \\sum_i x_i \\right)} \\) and variance \\(\\sigma^2\\): the parameters \\(\\alpha\\) and \\(\\sigma^2 &gt; 0\\) can then be judiciously chosen to cancel the bothering term \\(\\exp[\\frac{\\beta}{N} \\, m^2]\\). This approach is often called the a Hubbard-Stratonovich transformation. The bothering “coupling” term disappears when when choosing \\(\\frac{\\alpha^2}{2 \\sigma^2} = \\frac{\\beta}{N}\\). With such a choice, it follows that\n\\[\n\\pi(x, a) = \\frac{1}{\\mathcal{Z}(\\beta)} \\, \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp {\\left\\{  - \\frac{a^2}{2 \\sigma^2} + \\frac{\\alpha}{\\sigma^2} a \\,  {\\left( \\sum_i x_i \\right)}  \\right\\}} .\n\\]\nAveraging out the \\(x_i \\in \\{-1, +1\\}\\) gives that the partition function reads\n\\[\n\\mathcal{Z}(\\beta) \\; = \\;\n\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\,\n\\int_{a = -\\infty}^{a=\\infty} \\,\n\\exp {\\left\\{ -\\frac{a^2}{2 \\sigma^2} +  \\textcolor{red}{N} \\, \\log[ 2 \\, \\cosh(\\alpha a / \\sigma^2)] \\right\\}} .\n\\]\nIn order to use the method of steepest descent, it would be useful to have an integrand of the type \\(\\exp[N \\times (\\ldots)]\\). One can choose \\(1/(2 \\, \\sigma^2) = \\beta \\, N\\) and \\(\\alpha = 1/N\\). This gives\n\\[\n\\mathcal{Z}(\\beta) \\; = \\;\n\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\,\n\\int_{a = -\\infty}^{a=\\infty} \\,\n\\exp  {\\left\\{   \\textcolor{red}{N}  {\\left[  -\\beta \\, a^2 + \\log[ 2 \\, \\cosh(2 \\beta a )] \\right]}  \\right\\}}  \\, da\n\\]\nfrom which one directly obtains that:\n\\[\n\\lim_{N \\to \\infty} \\; -\\frac{\\log \\mathcal{Z}(\\beta)}{N}\n\\; = \\;\n\\min_{a \\in \\mathbb{R}} \\; \\Big\\{ \\beta a^2 - \\log[2 \\, \\cosh(2 \\beta a)] \\Big\\}.\n\\]\n\n\nSherrington–Kirkpatrick model\nConsider the distribution on \\(x \\in \\{-1,1\\}^N\\)\n\\[\n\\pi(x) = \\frac{1}{\\mathcal{Z}(\\beta)} \\,\n\\exp {\\left\\{ \\frac 12 \\, \\sum_{i,j} W_{ij} x_i x_j \\right\\}}\n=\n\\frac{1}{\\mathcal{Z}(\\beta)} \\,\n\\exp {\\left\\{ \\frac 12 \\, \\left&lt; x, W x \\right&gt; \\right\\}}\n\\]\nwhere the \\(w_{ij}\\) are some fixed weights with \\(w_{ij} = w_{ji}\\). We assume that the matrix \\(W = [W_{ij}]_{ij}\\) is positive definite: this can be achieved by adding \\(\\lambda \\, I_N\\) to it if necessary, which does not change the distribution \\(\\pi\\). As described in (Zhang et al. 2012), although I would not be surprised if similar ideas appeared in the physics literature much earlier, one can introduce a Gaussian auxiliary random variable \\(a = (a_1, \\ldots, a_N)\\) so that \\(\\pi(a | x)\\) has mean \\(Fx\\) and covariance \\(\\Gamma\\). In other words,\n\\[\n\\pi(a | x) = \\frac{1}{(2 \\pi)^{d/2} \\, |\\Gamma|^{1/2}}\n\\, \\exp {\\left\\{ -\\frac 12 \\left&lt; (a - Fx), \\Gamma^{-1} (a - Fx) \\right&gt; \\right\\}} .\n\\]\nIn order to cancel-out the \\(\\left&lt; x, W, x \\right&gt;\\) it suffices to make sure that \\(F^\\top \\, \\Gamma^{-1} \\, F = W\\). There are a number of possibilities, the simplest approaches being perhaps\n\\[\n(F,\\Gamma) = (W^{1/2}, I_N)\n\\quad \\text{or} \\quad\n(F,\\Gamma) = (I, W^{-1})\n\\quad \\text{or} \\quad\n(F,\\Gamma) = (W, W).\n\\]\nIn any case, the joint distribution reads\n\\[\n\\pi(x,a) \\; = \\; \\frac{1}{\\mathcal{Z}} \\, \\frac{1}{(2 \\pi)^{d/2} \\, |\\Gamma|^{1/2}} \\,\n\\exp {\\left\\{ -\\frac{1}{2} \\left&lt; a, \\Gamma^{-1} a \\right&gt; + \\left&lt; x, F^\\top \\Gamma^{-1} \\, a \\right&gt; \\right\\}} .\n\\]\nIndeed, one can try to implement some Gibbs-style update in order to explain this joint distribution since both \\(\\pi(x|a)\\) and \\(\\pi(a|x)\\) are straightforward to sample from: it is indeed related Restricted Boltzmann Machine models. One can also average-out the spins \\(x_i \\in \\{-1,1\\}\\) and obtain that\n\\[\n\\mathcal{Z}= \\int_{\\mathbb{R}^N}\n\\exp {\\left\\{  \\sum_{i=1}^N \\log {\\left( 2 \\, \\cosh([F^\\top \\Gamma^{-1} \\, a]_i) \\right)}  \\right\\}}  \\, \\mathcal{D}_{\\Gamma}(da)\n\\]\nwhere \\(\\mathcal{D}_{\\Gamma}\\) is the density of a centred Gaussian distribution with covariance \\(\\Gamma\\). [TODO: add SMC experiments to estimate \\(\\mathcal{Z}\\)].\n\n\n\n\n\nReferences\n\nZhang, Yichuan, Zoubin Ghahramani, Amos J Storkey, and Charles Sutton. 2012. “Continuous Relaxations for Discrete Hamiltonian Monte Carlo.” Advances in Neural Information Processing Systems 25."
  },
  {
    "objectID": "people/index_people.html",
    "href": "people/index_people.html",
    "title": "Research team",
    "section": "",
    "text": "Woon Hao Xuan PhD (2023–Now): Uncertainty Quantification for High-Dimensional dynamical systems."
  },
  {
    "objectID": "people/index_people.html#phdpostdoc-co-supervision",
    "href": "people/index_people.html#phdpostdoc-co-supervision",
    "title": "Research team",
    "section": "",
    "text": "Woon Hao Xuan PhD (2023–Now): Uncertainty Quantification for High-Dimensional dynamical systems."
  },
  {
    "objectID": "people/index_people.html#alumni",
    "href": "people/index_people.html#alumni",
    "title": "Research team",
    "section": "Alumni",
    "text": "Alumni\n\nWang Chao Research Fellow (2021-2023): Bayesian inverse problems, generative models, data-driven priors.\nChristopher Hendra PhD (2019-2022): Genomics, Nanopore sequencing – co-supervised by Jonathan Göke (GIS). Chris is now a Senior Scientist at MSD.\nKhai Xiang Au PhD (2019-2023): PDE constrained Bayesian inverse problems, uncertainty quanification, variational inference. Khai is now working as a data scientist at American Express.\nRahul Rahaman PhD(2018-2022): Bayesian inference, Uncertainty Quantification, Deep-Learning. Rahul is now an Applied Research Scientist at Amazon.\nAtin Ghosh PhD (2017-2021): Deep Learning for Glaucoma Understanding, representation learning, generative models, semi-supervised learning. Atin is now an Applied Research Scientist at Amazon.\nSe-In Jang Research Fellow (2019-2021): Computer vision and application to ophthalmology. Se-In is now a research fellow in the Center for Advanced Medical Computing and Analysis and the Gordon Center for Medical Imaging, Massachusetts General Hospital (MGH) and Harvard Medical School.\nAxel Finke Research Fellow (2017-2020): Sequential Monte Carlo, MCMC, algorithms for high-dimensional problems; applications in finance, economics, ecology and molecular biology. Axel is now an assistant professor at Loughborough University (UK).\nZuozhu Liu Research Fellow (2019-2020): Bayesian inference, deep generative models, 3D vision and medical applications. Zuozhu is now Assistant Professor at the Zhejiang University-University of Illinois at Urbana-Champaign Institute.\nMatt Graham Research Fellow (2017-2020): Approximate inference methods, MCMC, approximate Bayesian computation, numerical simulation. Matt is now a research data scientist in the Advanced Research Computing Centre at University College London.\nWillem van den Boom Research Fellow (2018-2019): Willem is now a Senior Research Fellow in the Division of Biomedical Data Science at the Yong Loo Lin School of Medicine of the National University of Singapore.\nKhai Sing Chin Research Associate (2017-2018): Khai Sing is now working in the finance industry.\nDeborshee Sen PhD (NUS, 2014-2017). Winner of the 2017 DSAP NUS best researcher award. After a postdoc at Duke University and a position as an assistant Professor at Bath University, Deborshee now works as a Research Scientist at Amazon.\nDaniel Paulin Research Fellow (NUS, 2014-2015). Daniel is now an Assistant Professor at the School of Mathematics at the University of Edinburgh.\nEge Muzaffer PhD (NUS, 2016): Bayesian inverse problems and Sequential Monte Carlo. Ege is now a Machine Learning EngineerMachine Learning Engineer Ubisoft RedLynx"
  },
  {
    "objectID": "people/index_people.html#msc",
    "href": "people/index_people.html#msc",
    "title": "Research team",
    "section": "MSc",
    "text": "MSc\n\nQuang Huy Nguyen MSc (2018-2019): representation learning, robust models for image segmentation.\nAugustin Hoff (NUS, 2016-2017). Deep Neural Networks and Features Extraction. Augustin is now a Senior Data Scientist at MAIF.\nMajdi Rabia (NUS, 2016-2017). Numerical Method for Backward-Stochastic-Differential-Equations. Majdi is Co-founder and CTO @Fairphonic.\nBenjamin Scellier (NUS, 2015). Deep Learning. After his MSc at NUS, Benjamin joined Yoshua Bengio’s Group as a PhD. He is now a principal research scientist at Rain"
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "Alexandre Thiéry",
    "section": "",
    "text": "import numpy as np\nimport pylab as plt\n\n\ndt = 0.1\nT = 1000\n\ndef gen(T,dt, eps=1.):\n    N = int(T//dt)\n    x = np.random.normal(0,1)\n    noise = np.random.normal(0,1,N)\n    traj = np.zeros(N)\n    S = 0.\n    for k in range(N):\n        x += -x*dt/eps + np.sqrt(2*dt/eps)*noise[k]\n        traj[k] = x\n    return np.arange(N)*dt, traj\n\n\ndt = 10**-5\ndts, traj = gen(T,dt, eps**2)\nnp.mean(1-traj**2)\n\n-0.027903924221273836\n\n\n\n\n\n0.07185292800119701\n\n\n\ndef ito(T,dt,eps):\n    dts, traj = gen(T,dt, eps**2)\n\n    # integrate\n    x = 0.5\n    x_traj = []\n    for y in traj:\n        x += x*(1-y**2)*dt/eps\n        x_traj.append(x)\n    return dts, x_traj\n\nT = 0.01\ndt = 10**-5\neps = 0.01\n\nfinal = []\nS = 3000\nfor _ in range(S):\n    dts, x_traj = ito(T,dt,eps)\n    plt.plot(dts, x_traj)\n    final.append(x_traj[-1])\n\n\n\n\n\nnp.mean(final), np.std(final)/np.sqrt(S)\n\n(0.47737275253700734, 0.0036375965945713213)\n\n\n\nplt.plot(dts, traj)\n\n\n\n\n\n\n\n\n\n\n\n_ = plt.hist(samples, bins=100)"
  },
  {
    "objectID": "notes/Gaussian_Assimilation/gaussian_assimilation_smoothing.html",
    "href": "notes/Gaussian_Assimilation/gaussian_assimilation_smoothing.html",
    "title": "Ensemble Kalman Smoother (EnKS)",
    "section": "",
    "text": "Consider a linear-Gaussian state space model with \\(\\mathbb{R}^{D_x}\\)-valued dynamics \\(X_{t+1} \\sim F \\, X_t + \\mathcal{N}(0,Q)\\) and \\(\\mathbb{R}^{D_y}\\)-valued observations \\(Y_t \\sim H X_t + \\mathcal{N}(0,R)\\). Assuming a Gaussian initial distribution, the filtering distributions \\(\\mathop{\\mathrm{P}}(X_t \\in dx \\, | Y_{1:t}) \\equiv \\mathcal{N}(\\mu_{t|t}, P_{t|t})\\) are Gaussian and can be sequentially computed with the Kalman Filter. Similarly, the predictive distributions \\(\\mathop{\\mathrm{P}}(X_{t+1} \\in dx \\, | Y_{1:t}) \\equiv \\mathcal{N}(\\mu_{t+1|t}, P_{t+1|t})\\) are straightforward to obtain from the filtering distributions: \\(\\mu_{t+1|t} = F \\, \\mu_{t|t}\\) and \\(P_{t+1|t} = F \\, P_{t|t} \\, F^\\top + Q\\). Given observations \\(y_{1:T} \\equiv (y_1, \\ldots, y_T)\\) and \\(1 \\leq t \\leq T\\), the smoothing distributions \\(\\mathop{\\mathrm{P}}(X_t \\in dx \\, | Y_{1:T}) \\equiv \\mathcal{N}(\\mu_{t|T}, P_{t|T})\\) can computed by performing a “backward pass”. Since everything is linear and Gaussian, it is just an exercise in Linear Algebra & Gaussian-conditioning, as described by the Rauch-Tung-Striebel (Rauch, Tung, and Striebel 1965) smoothing recursions. The backward recursion reads\n\\[\n\\left\\{\n\\begin{aligned}\n\\mu_{t|T}\n&= \\mu_{t|t} + B_t \\,  {\\left( \\mu_{t+1|T} - \\mu_{t+1|t} \\right)} \\\\\nP_{t|T}\n&=\nP_{t|t} + B_t  {\\left(  P_{t+1|T} - P_{t+1|t}  \\right)}  B^\\top_{t}\n\\end{aligned}\n\\right.\n\\tag{1}\\]\nand allows one to compute the smoothing means and covariances matrices \\((\\mu_{t|T}, P_{t|T})\\) for \\(1 \\leq t \\leq T\\) starting from the knowledge of \\((\\mu_{T|T}, P_{T|T})\\). In Equation 1, the smoothing gain matrix \\(B_t\\) is given by\n\\[\n\\begin{align}\nB_t &=\n\\mathop{\\mathrm{Cov}}(X_t, X_{t+1} \\, | y_{1:t}) \\, \\mathop{\\mathrm{Var}}(X_{t+1} \\, | y_{1:t})^{-1} \\\\\n&=\nP_{t|t} F^\\top \\,  {\\left( F \\, P_{t|t} \\, F^\\top + Q \\right)} ^{-1}.\n\\end{align}\n\\tag{2}\\]\nThe Ensemble Kalman Filter (EnKF) is a non-linear equivalent of the Kalman filter, and the purpose of these notes is to derive the equivalent “ensemble version” of the backward recursion Equation 1. For this purpose, it is important to understand slightly better the role of the smoothing gain matrix \\(B_t\\). Consider the pair of random variable \\((X^f_t, X^p_{t+1})\\) distributed according to the joint distribution between the filtering distribution at time \\(t\\) and the predictive distribution at time \\(t+1\\) in the sense that\n\\[\n(X^f_t, X^p_t) \\; \\underbrace{=}_{\\text{(law)}}\\; (X_t, X_{t+1} \\, \\mid \\, y_{1:t}).\n\\]\nThis means that \\(X^f_t \\sim \\mathcal{N}(\\mu_{t|t}, P_{t|t})\\) and \\(X^p_{t+1} \\sim \\mathcal{N}(\\mu_{t+1|t}, P_{t+1|t})\\) and \\(X^p_t = F \\, X^f_t + \\mathcal{N}(0, Q)\\). Furthermore, Equation 2 and the standard gaussian conditional probabilities formulas give that the conditional means and covariances are given by\n\\[\n\\left\\{\n\\begin{align}\n\\textrm{Mean} {\\left( X^f_t | (X^p_{t+1}=x_{t+1}) \\right)}  \n\\; &= \\;\n\\mu_{t|t} + B_t (x_{t+1} - \\mu_{t+1|t}) \\\\\n\\textrm{Cov} {\\left( X^f_t | (X^p_{t+1}=x_{t+1}) \\right)}  \n\\; &= \\;\nP_{t|t} - B_t \\, P_{t+1|t} \\, B_t^\\top.\n\\end{align}\n\\right.\n\\tag{3}\\]\nThe above expression for the conditional mean also shows that the matrix \\(B_t\\) is a minimizer of the loss\n\\[\nM \\; \\mapsto \\;\n\\mathop{\\mathrm{E}} {\\left(  \\left\\| (X^f_t - \\mu_{t|t}) - B (X^p_{t+1} - \\mu_{t+1|t}) \\right\\|^2  \\right)}\n\\]\nover all matrices \\(M \\in \\mathbb{R}^{D_x, D_x}\\). Heuristically, this shows that the smoothing gain matrix \\(B_t\\) can easily be computed by regressing \\(X^f_t\\) against \\(X^p_{t+1}\\). We can use this remark to build an ensemble version of the backward recursion Equation 1. Recall that when running a EnKF for filtering the observations \\(y_{1:T}\\), the final stage proceeds in two steps:\n\nObtain an ensemble of particles \\(X^{i,p}_{T} = F \\, X^{i,f}_{T-1} + \\mathcal{N}(0,Q)\\) that approximate the predictive distribution \\(\\mathop{\\mathrm{P}}(X_T | y_{1:T-1})\\).\n\nAssimilate the last observation \\(y_T\\) using the Kalman gain matrix \\(K_T\\) and the correction \\(\\Delta_T^i = K_T \\, (\\tilde{y}_{i,\\star} - H \\, X^{i,p}_T)\\) by setting \\[\nX^{i,s}_T = X^{i,p}_T + \\Delta_T^i.\n\\tag{4}\\] The particles \\(X^{i,s}_T\\) approximate the smoothing distribution \\(\\mathop{\\mathrm{P}}(X_T | y_{1:T})\\).\n\nFollowing our discussion of the smoothing gain matrix \\(B_{t}\\) and Equation 4, it seems sensible to set\n\\[\n\\begin{align}\nX^{i,s}_{T-1}\n&= X^{i,f}_{T-1} + B_{T-1} \\, \\Delta^i_T\\\\\n&= X^{i,f}_{T-1} + B_{T-1} \\, (X^{i,s}_{T} - X^{i,p}_{T})\n\\end{align}\n\\tag{5}\\]\nand hope that the ensemble of updated particles \\(X^{i,s}_{T-1}\\) approximate the smoothing distribution \\(\\mathop{\\mathrm{P}}(X_{T-1} | y_{1:T})\\). In words, the particle \\(X^{i,s}_{T-1}\\) is obtained by “pulling” the correction term \\(\\Delta^i_{T} = X^{i,s}_{T} - X^{i,p}_{T}\\) back to \\(X^{i,f}_{T-1}\\) through the “regression” smoothing gain matrix \\(B_{T-1}\\). To check that the particles \\(X^{i,s}_{T-1}\\) indeed approximate the smoothing distribution \\(\\mathop{\\mathrm{P}}(X_{T-1} \\,|y_{1:T})\\), it suffices to compute the mean/variance and verify that they are matching the one given by Equation 1. Recall that Equation 3 gives that the filtering/predictive distributions satisfy\n\\[\nX^f_{T-1} = \\mu_{T-1|T-1} + B_{T-1} \\, (X^p_{T} - \\mu_{T|T-1}) + \\varepsilon_t\n\\]\nwhere \\(\\varepsilon_t \\sim \\mathcal{N}(0, P_{T-1|T-1} - B_{T-1} \\, P_{T|T-1} \\, B_{T-1}^\\top)\\) is independent from all other sources of randomness. Plugging this into Equation 5 gives that\n\\[\nX^{i,s}_{T-1}\n=\n\\mu_{T-1|T-1} + B_{T-1} \\, (X^{i,s}_{T} - \\mu_{T|T-1}) + \\varepsilon_t.\n\\]\nSince the \\(X^{i,s}_{T}\\) are distributed according to the smoothing distribution, i.e. \\(X^{i,s}_{T} \\sim \\mathcal{N}(\\mu_{T|T}, P_{T|T})\\), this immediately shows that \\(X^{i,s}_{T-1}\\) is Gaussian with\n\\[\n\\left\\{\n\\begin{align}\n\\textrm{Mean} &= \\mu_{T-1|T} = \\mu_{T-1|T-1} + B_{T-1} \\,  {\\left( \\mu_{T|T} - \\mu_{T|T-1} \\right)} \\\\\n\\textrm{Covariance} &= P_{T-1|T} = P_{T-1|T-1} + B_{T-1}  {\\left(  P_{T|T} - P_{T|T-1}  \\right)}  B^\\top_{T-1},\n\\end{align}\n\\right.\n\\]\nas it should. One can then iterate this construction to obtain particle approximations of the smoothing distributions \\(\\mathop{\\mathrm{P}}(X_t | y_{1:T})\\) for \\(1 \\leq t \\leq T\\) by running a backward pass and recursively setting\n\\[\nX^{i,s}_t \\; = \\; X^{i,f}_t + B_t \\,  {\\left( X^{i,s}_{t+1} - X^{i,p}_{t+1} \\right)} .\n\\]\nThe ensemble of particles \\(X^{i,s}_t\\) approximates the smoothing distribution \\(\\mathop{\\mathrm{P}}(X_t | y_{1:T})\\). In a nonlinear setting, it suffices to approximate the smoothing gain matrices with\n\\[\n\\widehat{B}_t = \\mathop{\\mathrm{Cov}} {\\left(  x^f_{t,i}, x^p_{t+1,i}  \\right)}  \\, \\mathop{\\mathrm{Var}} {\\left(  x^p_{t+1,i}  \\right)} ^{-1}.\n\\]\n[Experiments: TODO]\n\n\n\n\nReferences\n\nRauch, Herbert E, F Tung, and Charlotte T Striebel. 1965. “Maximum Likelihood Estimates of Linear Dynamic Systems.” AIAA Journal 3 (8): 1445–50."
  },
  {
    "objectID": "notes/Gaussian_Assimilation/gaussian_assimilation_smoothing.html#ensemble-kalman-updates",
    "href": "notes/Gaussian_Assimilation/gaussian_assimilation_smoothing.html#ensemble-kalman-updates",
    "title": "Gaussian Assimilation & the EnKF",
    "section": "",
    "text": "Assume a prior Gaussian prior distribution \\(\\pi_0 \\equiv \\mathcal{N}(m_0,P_0)\\) and a noisy observation \\(y_\\star \\in \\mathbb{R}^{D_y}\\) with\n\\[\ny_\\star = H x + \\xi\n\\qquad \\text{with} \\qquad\n\\xi \\sim \\mathcal{N}(0,R)\n\\]\nwhere \\(x \\in \\mathbb{R}^{D_x}\\) is an unknown quantity of interest. The posterior distribution \\(\\pi \\equiv \\mathcal{N}(m,P)\\) is Gaussian and is given by\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &= m_0 + P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}(y_\\star - H m)\\\\\nP &= P_0 - P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1} \\, H P_0,\n\\end{aligned}\n\\right.\n\\end{align}\n\\]\nas standard Gaussian conditioning shows it. This can also be written as\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &= m_0 + K \\, (y_\\star - H m_0)\\\\\nP &= (I - K \\, H) P_0,\n\\end{aligned}\n\\right.\n\\end{align}\n\\]\nfor Kalman Gain Matrix \\(K \\in \\mathbb{R}^{D_x,D_y}\\) defined as\n\\[\nK \\; = \\; P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}.\n\\tag{1}\\]\nThe important remark is that the posterior covariance matrix \\(P \\in \\mathbb{R}^{D_x,D_x}\\) and the posterior mean \\(m \\in \\mathbb{R}^{D_x}\\) can also be expressed as\n\\[\n\\begin{align}\n\\left\\{\n\\begin{aligned}\nm &=  \\textcolor{red}{(I - K \\, H)} \\, m_0 +  \\textcolor{blue}{K} \\, y_\\star\\\\\nP &= \\;  \\textcolor{red}{(I - K \\, H)} \\, P_0 \\,  \\textcolor{red}{(I - K \\, H)^\\top} +  \\textcolor{blue}{K} R  \\textcolor{blue}{K^\\top}.\n\\end{aligned}\n\\right.\n\\end{align}\n\\tag{2}\\]\nThis shows that \\(P\\) is indeed positive semi-definite. More importantly, this gives a mechanism for transforming samples from the prior distributions into samples from the posterior distributions. Indeed, consider \\(N\\) iid samples from the prior distribution, \\(x_1, \\ldots, x_N \\sim \\pi_0(dx)\\), and set\n\\[\n\\widetilde{x}_i\n=\n\\textcolor{red}{(I - K \\, H)} \\, x_i +  \\textcolor{blue}{K}(y_\\star + \\xi_i)\n\\]\nfor iid noise terms \\(\\xi_i \\sim \\mathcal{N}(0,R)\\). From Equation 2 it is clear that \\(\\widetilde{x}_1, \\ldots, \\widetilde{x}_N\\) are iid samples from the Gaussian posterior distribution \\(\\pi = \\mathcal{N}(m,P)\\). It is more intuitive to write this as\n\\[\n\\widetilde{x}_i\n=\nx_i + K \\, (  \\textcolor{green}{ \\widetilde{y}_{i,\\star} } - H \\, x_i)\n\\]\nwhere \\(\\widetilde{y}_{i,\\star} \\in \\mathbb{R}^{D_y}\\) are fake observations that are obtained by perturbing the actual observation \\(y_\\star\\) with additive Gaussian noise terms with covariance \\(R\\),\n\\[\n\\textcolor{green}{\\widetilde{y}_{i,\\star} \\, = \\, y_\\star + \\xi_i}.\n\\]\n\n\nSuppose that we would like to estimate \\(x \\in \\mathbb{R}^{D_x}\\) from the noisy observation\n\\[\ny_\\star = \\mathcal{H}(x) + \\xi\n\\qquad \\text{with} \\qquad\n\\xi \\sim \\mathcal{N}(0,R)\n\\]\nand possibly-nonlinear observation operator \\(\\mathcal{H}: \\mathbb{R}^{D_x} \\to \\mathbb{R}^{D_y}\\). Assume that we also have \\(N\\) samples \\(x_1, \\ldots, x_N\\) generated from some (unkown) prior distribution. For example, these samples could come from another numerical procedure. In order to obtain \\(N\\) approximate samples from the posterior distribution, one can set\n\\[\n\\widetilde{x}_i\n=\nx_i + \\widehat{K} \\, [  \\textcolor{green}{ \\widetilde{y}_{i,\\star} } - \\mathcal{H}(x_i)]\n\\]\nfor fake observations \\( \\textcolor{green}{\\widetilde{y}_{i,\\star} } = y_\\star + \\xi_i\\). The approximate Kalman gain matrix \\(\\widehat{K}\\) is obtained by noting that in Equation 1 giving\n\\[\nK \\; = \\; P_0 H^\\top  {\\left( H P_0 H^\\top + R \\right)} ^{-1}\n\\]\nwe have \\(P_0 H^\\top = \\mathop{\\mathrm{Cov}}(X,HX)\\) and \\(H P_0 H^\\top = \\mathop{\\mathrm{Var}}(HX)\\) for \\(X \\sim \\pi_0\\). This means that an approximate Kalman matrix can be obtained using empirical estimates of these covariance matrices:\n\\[\n\\widehat{K} \\; = \\; \\widehat{\\mathop{\\mathrm{Cov}}}([x_i]_i, [\\mathcal{H}(x_i)]_i) \\,  {\\left( \\widehat{\\mathop{\\mathrm{Var}}}([\\mathcal{H}(x_i)]_i) + R \\right)} ^{-1}.\n\\]\nThese updates form the basis of the Ensemble Kalman filter (EnKF), and very successful and scalable approach to data-assimilation of high-dimensional dynamical systems. This method is operationally employed across various weather forecasting centers across the globe.\n\n\n\n\nEnKF Bible by Geir Evensen\n\n\n\n\n\n\nInterestingly enough, the remarks above can be design in a relatively principled manner a derivative free optimizer (Huang et al. 2022). For example, assume one would like to minimize a functional of the type\n\\[\n\\Psi(x) \\; = \\; \\|y_\\star - \\mathcal{H}(x)\\|^2_{R^{-1}}.\n\\]\nOne can start with a cloud of particles \\(x_1, \\ldots, x_N\\) and keep updating them by assuming that one assimilates the noisy observation \\(y_\\star\\) generated from a postulated observation process\n\\[\ny_\\star = \\mathcal{H}(x) + \\varepsilon^{-1} \\, \\xi\n\\]\nfor \\(\\xi \\sim \\mathcal{N}(0,R)\\) and \\(\\varepsilon\\ll 1\\) a “step-size”. Each assimilation step steers the cloud of points towards the rights direction. Careful choice of the step-size \\(\\varepsilon\\) is often crucial, as in any optimization procedure. It is indeed related to Information-Geometric Optimization Algorithms (IGO): the article (Ollivier et al. 2017) is beautiful!"
  },
  {
    "objectID": "notes/averaging_homogenization/averaging_homogenization.html",
    "href": "notes/averaging_homogenization/averaging_homogenization.html",
    "title": "Averaging and homogenization",
    "section": "",
    "text": "Averaging\nConsider a pair of (coupled) Markov processes \\(X_t \\in \\mathcal{X}\\) and \\(Y_t \\in \\mathcal{Y}\\) with dynamics that can informally be described as\n\\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon}/dt &= F(X^{\\varepsilon},Y^{\\varepsilon}, W^X)\\\\\ndY^{\\varepsilon}/dt &=  \\textcolor{red}{\\varepsilon^{-1}} \\, G(X^{\\varepsilon},Y^{\\varepsilon}, W^Y)\\\\\n\\end{align}\n\\right.\n\\]\nfor two independent “noise” terms \\(W^X\\) and \\(W^Y\\) and a time-scale parameter \\(\\varepsilon\\ll 1\\). We assume that \\(X\\) is a slow component that moves by \\(\\mathcal{O}(\\delta)\\) in on the time interval \\([t, t+\\delta]\\). The scaling \\(\\varepsilon^{-1}\\) in the dynamics of fast process \\(Y^{\\varepsilon}\\) indicates that we expect the process \\(Y\\) to evolve on a time scale of order \\(\\mathcal{O}(\\varepsilon)\\). We are interested in the limit \\(\\varepsilon\\to 0\\) and hope to “average out” the fast process \\(Y^{\\varepsilon}\\) and be able to describe the slow (and interesting) process \\(X^{\\varepsilon}\\) without referring to the fast process. Informally, we would like to describe the process \\(X^{\\varepsilon}\\), in the limit \\(\\varepsilon\\to 0\\), as following an effective Markovian dynamics\n\\[\ndX/dt = \\overline{F}(X, W^X).\n\\]\nFor describing the averaging phenomenon, we typically assume some ergodicity conditions on the fast process \\(Y\\). Here, we assume that for each fixed \\(x \\in \\mathcal{X}\\), the fast process process \\(Y^{[x]}\\) with fixed slow-component \\(x \\in \\mathcal{X}\\), i.e.\n\\[\ndY^{[x]}/dt = G(x,Y^{[x]}, W^Y)\n\\]\nis ergodic with respect to some probability distribution \\(\\rho_x(dy)\\). Although the averaging phenomenon is quite general, it is somewhat easier to illustrate it for diffusion processes. In this case, let us assume that the slow process is given by\n\\[\ndX^{\\varepsilon} = \\mu(X^{\\varepsilon}, Y^{\\varepsilon}) \\, dt + \\sigma(X^{\\varepsilon}, Y^{\\varepsilon}) \\, dW^x.\n\\]\nFor \\(X^{\\varepsilon}_{t} = x\\) and for a time increment \\(\\delta \\ll 1\\), since the process \\(X^{\\varepsilon}\\) can be considered constant we have\n\\[\n\\begin{align}\nX^{\\varepsilon}_{t+\\delta} - x\n&\\approx \\;\n{\\left(  \\frac{ \\int_{t}^{t+\\delta} \\mu(x, Y^{\\varepsilon}) \\, dt}{\\delta}  \\right)}  \\, \\delta + \\\n{\\left(  \\frac{ \\int_{t}^{t+\\delta} \\sigma^2(x, Y^{\\varepsilon}) }{\\delta} \\right)} ^{1/2} \\, \\mathcal{N}(0, \\delta).\n\\end{align}\n\\]\nThis can be regarded as a time-discretization of the averaged process\n\\[\ndX \\, = \\; \\overline{\\mu}(X) \\, dt + \\overline{\\sigma}(X) \\, dW\n\\]\nfor averaged drift and volatility functions give by\n\\[\n\\left\\{\n\\begin{align}\n\\overline{\\mu}(x)\n&=\n\\int \\mu(x,y) \\, \\rho_x(dy) \\\\\n\\overline{\\sigma}^2(x)\n&= \\int \\sigma^2(x,y) \\, \\rho_x(dy).\n\\end{align}\n\\right.\n\\tag{1}\\]\nOne standard approach for proving this type of results is to write the Kolmogorov equations\n\\[\\frac{d}{dt}\\varphi^{\\varepsilon}(x,y,t) = \\mathcal{L}^{\\varepsilon} \\varphi^{\\varepsilon}(x,y,t)\\] for \\(\\varphi^{\\varepsilon}(x,y,t) = \\mathop{\\mathrm{E}}[\\varphi(X^{\\varepsilon}_{t}, Y^{\\varepsilon}_{t}, t) | X^{\\varepsilon}_{0}=x, Y^{\\varepsilon}_{0}=y]\\) and perform a multiscale expansion (Hinch 1991) (Pavliotis and Stuart 2008) (Weinan 2011)\n\\[\n\\varphi^{\\varepsilon}(x,y,t)\n=\nA(x,t) + \\varepsilon B(x,y,t) + \\mathcal{O}(\\varepsilon^2).\n\\tag{2}\\]\nIndeed, the first order term \\(A(x,t)\\) is expected to not depend on the initial condition \\(y\\) since the process \\((X^{\\varepsilon}_t, Y^{\\varepsilon}_t)\\) forgets \\(Y^{\\varepsilon}_0 = y\\) on time scales of order \\(\\varepsilon\\) and we are interested in the regime \\(\\varepsilon\\to 0\\). From Equation 2 one can obtain the dynamics of the averaged process described by the function \\(A(x,t)\\). One finds that \\(A\\) is described by the averaged generator of the slow component, i.e. averaging \\(\\mathcal{L}^{X^{\\varepsilon}}\\) under \\(\\rho_x(dy)\\); this exactly gives Equation 1 in the case of diffusions. A typical example could be as follows:\n\\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon} &= \\mu(X^{\\varepsilon}, Y^{\\varepsilon}) dt + \\sigma(X^{\\varepsilon}, Y^{\\varepsilon}) \\, dW^X\\\\\ndY^{\\varepsilon} &= - \\textcolor{red}{\\varepsilon^{-1}} \\frac{ (Y^{\\varepsilon} - X^{\\varepsilon}) }{\\sigma^2} \\, dt + \\sqrt{2  \\textcolor{red}{\\varepsilon^{-1}} } \\, dW^Y.\\\\\n\\end{align}\n\\right.\n\\]\nThe fast process \\(Y^{\\varepsilon}_t\\) is a Ornstein-Uhlenbeck process sped-up by a factor \\(1/\\varepsilon\\) that will very rapidly oscillate around \\(X^{\\varepsilon}_t\\), with Gaussian fluctuations with variance \\(\\sigma^2&gt;0\\), ie:\n\\[\n\\rho_x(dy) \\; = \\; \\frac{ e^{-(y-x)^2/2} }{\\sqrt{2\\pi \\sigma^2}}\\, dy.\n\\]\nThis averaging phenomenon is relatively straightforward and not extremely surprising. More interesting is the homogenization phenomenon described in the next Section.\n\n\nHomogenization\nConsider the presence of an additional intermediate time scale \\(\\varepsilon^{-1/2}\\), \\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon}/dt &=  \\textcolor{blue}{\\varepsilon^{-1/2} H(X^{\\varepsilon},Y^{\\varepsilon})} \\, + \\,F(X^{\\varepsilon},Y^{\\varepsilon}, W^X)\\\\\ndY^{\\varepsilon}/dt &=  \\textcolor{red}{\\varepsilon^{-1}} \\, G(X^{\\varepsilon},Y^{\\varepsilon}, W^Y)\\\\\n\\end{align}\n\\right.\n\\] with the same assumption that for any fixed \\(x \\in \\mathcal{X}\\) the process \\(dY^{[x]}/dt = G(x,Y^{[x]}, W^Y)\\) is ergodic with respect to the probability distribution \\(\\rho_x(dy)\\). The same reasoning as in the averaging case shows that averaging the term \\(F(X^{\\varepsilon},Y^{\\varepsilon}, W^X)\\) is relatively straightforward and has the exact same expression: it suffices to average under \\(\\rho_x(dy)\\). This means that one can study instead\n\\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon}/dt &=  \\textcolor{blue}{\\varepsilon^{-1/2} H(X^{\\varepsilon},Y^{\\varepsilon})} \\, + \\, \\overline{F}(X^{\\varepsilon}, W^X)\\\\\ndY^{\\varepsilon}/dt &=  \\textcolor{red}{\\varepsilon^{-1}} \\, G(X^{\\varepsilon},Y^{\\varepsilon}, W^Y)\\\\\n\\end{align}\n\\right.\n\\]\nwith, informally, \\(\\overline{F}(x,w) = \\int F(x,y,w) \\, \\rho_x(dy)\\). The new interesting phenomenon is coming from the intermediate time scale \\(\\varepsilon^{-1/2}\\). Contrarily to the averaging phenomenon of the previous section that was only relying on a Law of Large Numbers, dealing with the intermediate time-scale requires exploiting a CLT and quantifying the rate of mixing of the fast process \\(Y^{[x]}\\) Note that since \\(\\varepsilon^{-1/2} \\gg 1\\), for the dynamics to not explode one needs the centering condition:\n\\[\n\\int_{\\mathcal{Y}} H(x,y) \\, \\rho_x(dy) = 0\n\\qquad \\textrm{for all } x \\in \\mathcal{X}.\n\\tag{3}\\]\nBecause of the centering condition*, the term \\( \\textcolor{blue}{\\varepsilon^{-1/2} H(X^{\\varepsilon},Y^{\\varepsilon})}\\) will contribute an additional noise term in the effective dynamics of the slow process. To describe this additional noise term, assume an ergodic central limit theorem (CLT) for the fast process \\(dY^{[x]}/dt = G(x,Y^{[x]}, W^Y)\\): for a test function \\(\\varphi: \\mathcal{Y}\\to \\mathbb{R}\\) with zero expectation under \\(\\rho_x(dy)\\) we have:\n\\[\n\\lim_{t \\to \\infty} \\; T^{-1/2}\n\\int_{t=0}^T \\, \\varphi(Y^{[x]}_t) \\, dt\n\\; = \\; \\mathcal{N}(0, V_x[\\varphi])\n\\tag{4}\\]\nfor asymptotic variance \\(V_x[\\varphi] \\geq 0\\). For a time increment \\(\\delta &gt; 0\\) and assuming \\(X^{\\varepsilon}_{t}=x\\) we have\n\\[\n\\begin{align}\nX^{\\varepsilon}_{t+\\delta} - x\n&\\approx  \\textcolor{blue}{ \\varepsilon^{-1/2} \\, \\int_{u=t}^{t+\\delta} H(X^{\\varepsilon}_u,Y^{\\varepsilon}_u)} \\, du \\, + \\, \\int_{u=t}^{t+\\delta} \\overline{F}(x, W^X_u) \\, du.\n\\end{align}\n\\tag{5}\\]\nThe second integral term is an averaging term that can be treated easily. Approximating the process \\(t \\mapsto Y^{\\varepsilon}_t\\) by \\(t \\mapsto Y^{[x]}_{t \\varepsilon^{-1}}\\), the first integral on the RHS of Equation 5 can be approximated as\n\\[\n\\begin{align}\n\\underbrace{\\varepsilon^{-1/2} \\int_{u=t}^{t+\\delta}  H(x,Y^{[x]}_{u \\, \\varepsilon^{-1}}) \\, du}_{\\textrm{CLT}} \\,\n+\n\\underbrace{\\int_{u=t}^{t+\\delta} \\varepsilon^{-1/2} \\partial_x H(x,Y^{[x]}_{u \\, \\varepsilon^{-1}}) \\, (X^{\\varepsilon}_u - x) \\, \\, du}_{\\textrm{(drift)}}.\n\\end{align}\n\\]\nAfter a time-rescaling, one can readily see that the first term is described by the CLT of Equation 4,\n\\[\n\\varepsilon^{-1/2} \\int_{u=t}^{t+\\delta}  H(x,Y^{[x]}_{u \\, \\varepsilon^{-1}}) \\, du\n\\approx V_x[H(x, \\cdot)]^{1/2} \\mathcal{N}(0, \\delta).\n\\]\nThe second term is further approximated as\n\\[\n\\begin{align}\n\\varepsilon^{-1} \\, &\\int_{u=t}^{t+\\delta}\\int_{v=t}^{t+\\delta}\n\\partial_x H(x,Y^{[x]}_{u \\, \\varepsilon^{-1}}) \\, H(x,Y^{[x]}_{v \\, \\varepsilon^{-1}}) \\, 1_{v&lt;u} \\, du \\, dv\\\\\n&=  {\\left(  \\frac{1}{\\delta \\varepsilon^{-1}} \\int_{u=t}^{t+\\delta \\varepsilon^{-1}}\\int_{v=t}^{t+\\delta \\varepsilon^{-1}} \\partial_x H(x,Y^{[x]}_{u}) \\, H(x,Y^{[x]}_{v}) \\, 1_{v&lt;u}  \\right)}  \\, \\delta,\n\\end{align}\n\\]\nthe second equality coming from the time-rescaling \\(t \\mapsto t \\varepsilon\\). The process \\(Y^{[x]}\\) mixes on scale \\(\\mathcal{O}(1)\\) so that the term inside bracket \\( {\\left( \\ldots \\right)} \\) converges to its expectation. Setting \\(T = \\delta \\, \\varepsilon^{-1} \\to \\infty\\), one obtains\n\\[\n\\begin{align}\nI(x) &=\n\\frac{1}{T} \\iint_{[0,T]^2}\n\\partial_x H(x,Y^{[x]}_{u}) \\, H(x,Y^{[x]}_{v}) \\, 1_{v&lt;u}  \\, du \\, dv \\\\\n&\\to\n\\int \\rho_x(dy) \\, H(x,y) \\,   {\\left\\{  \\int_{s=0}^{\\infty} \\mathop{\\mathrm{E}}[\\partial_x H(\\hat{x}, Y^{[x]}_s) \\, |  Y^{[x]}_0=y] \\, ds  \\right\\}} .\n\\end{align}\n\\]\nIn conclusion, the fast-slow system\n\\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon} &=  \\textcolor{blue}{\\varepsilon^{-1/2} \\, H(X^{\\varepsilon}, Y^{\\varepsilon})} \\, dt + \\mu(X^{\\varepsilon}, Y^{\\varepsilon}) \\, dt + \\sigma(X^{\\varepsilon}, Y^{\\varepsilon}) \\, dW^x\\\\\ndY^{\\varepsilon} &=  \\textcolor{red}{\\varepsilon^{-1}} \\, G(X^{\\varepsilon},Y^{\\varepsilon}, W^Y) \\, dt\n\\end{align}\n\\right.\n\\]\ncan be described in the regime \\(\\varepsilon\\to 0\\) by the effective dynamics\n\\[\ndX =  \\textcolor{blue}{I(X) \\, dt + \\Gamma^{1/2}(X) \\, dW^{H}}\n+\n\\overline{\\mu}(X) \\, dt + \\overline{\\sigma}(X) \\, dW^X.\n\\]\nfor two independent Brownian motions \\(W^X\\) and \\(W^H\\). The volatility terms \\( \\textcolor{blue}{\\Gamma(x)}\\) comes from the CLT and the drift term \\( \\textcolor{blue}{I(x)}\\) comes from the self-interaction term:\n\\[\n\\left\\{\n\\begin{align}\n\\Gamma(x)\n&= \\lim_{T \\to \\infty} \\; \\mathop{\\mathrm{Var}} {\\left\\{ T^{-1/2} \\int_{0}^T H(x, Y^{[x]}_t) \\, dt \\right\\}} \\\\\n%\nI(x)\n&= \\lim_{T \\to \\infty} \\; \\frac{1}{T} \\iint_{0&lt;u&lt;v&lt;T} H(x, Y^{[x]}_u) \\, \\partial_x H(x, Y^{[x]}_v) \\, du \\, dv.\n\\end{align}\n\\right.\n\\tag{6}\\]\nFor the drift function, the scaling \\(T^{-1} \\int_{0&lt;u&lt;v&lt;T} (\\ldots)\\) may look a bit surprising at first sight as one may expect \\(T^{-2} \\int_{0&lt;u&lt;v&lt;T} (\\ldots)\\) instead. Note that since the process \\(Y^{[x]}\\) mixes on a time scale \\(\\mathcal{O}(1)\\) and the centering condition \\(\\int H(x, y) \\rho_x(dy)=0\\) holds, the expectation \\(\\mathop{\\mathrm{E}}[H(x, Y^{[x]}_u) \\, \\partial_x H(x, Y^{[x]}_v)]\\) goes to zero as soon as \\(|u-v| \\gg 1\\). This means that only the subset \\(|u-v| = \\mathcal{O}(1)\\) of \\([0,T]^2\\) really matters in that double integral, hence the \\((1/T)\\) normalization factor.\n\n\nClosed form solution & Poisson equation:\nThe drift and volatility terms \\(\\Gamma(x)\\) and \\(I(x)\\) quantify the mixing properties of the fast process \\(Y^{[x]}\\). While formulas Equation 6 are intuitive, they can be difficult to deal with if one needs the exact expressions of the drift and volatility functions. Instead, they can also be expressed in terms of the solution to an appropriate Poisson equations.\n\\[\n\\begin{align}\nI(x) &=\n\\frac{1}{T} \\iint_{[0,T]^2}\nH(x,Y^{[x]}_{v}) \\, \\partial_x H(x,Y^{[x]}_{u}) \\, 1_{v&lt;u}  \\, du \\, dv \\\\\n&\\to\n\\int \\rho_x(dy) \\, H(x,y) \\, \\partial_{\\hat{x}}  {\\left\\{  \\int_{s=0}^{\\infty} \\mathop{\\mathrm{E}}[H(\\hat{x}, Y^{[x]}_s) \\, |Y^{[x]}_0=y] \\, ds  \\right\\}} \\\\\n&=\n-\\int \\rho_x(dy) \\, H(x,y) \\, \\partial_{x} \\Phi(x,y)\\\\\n&= -\\left&lt; H(x, \\cdot), \\partial_x \\Phi(x, \\cdot) \\right&gt;_{\\rho_x}\n\\end{align}\n\\tag{7}\\]\nwhere the function \\(\\Phi(x,y)\\) is solution to the Poisson equation\n\\[\n\\mathcal{L}^{Y^{[x]}} \\Phi(x, \\cdot) = H(x, \\cdot)\n\\]\nfor all \\(x \\in \\mathcal{X}\\) and \\(\\mathcal{L}^{Y^{[x]}}\\) is the generator of the fast process \\(dY^{[x]}/dt = G(x,Y^{[x]}, W^Y)\\). The last equality in Equation 7 follows from the integral representation of the Poisson equation. Similarly, and also as explained here, the asymptotic variance term can also be expressed in terms of the function \\(\\Phi\\),\n\\[\n\\begin{align}\n\\Gamma(x)\n&= \\lim_{T \\to \\infty} \\; \\mathop{\\mathrm{Var}} {\\left\\{ T^{-1/2} \\int_{0}^T H(x, Y^{[x]}_t) \\, dt \\right\\}} \\\\\n&= -2 \\int_{\\mathcal{Y}} \\Phi(x, y) \\, H(x, y) \\, \\rho_x(dy)\\\\\n&= -2 \\left&lt; \\Phi, \\mathcal{L}^{Y^{[x]}} \\Phi \\right&gt;_{\\rho_x}.\n\\end{align}\n\\]\n\n\nExample: integrated OU process\nConsider a slow process obtained by integrating an OU process,\n\\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon} &= \\varepsilon^{-1/2} \\, Y^{\\varepsilon} \\, dt\\\\\ndY^{\\varepsilon} &= -\\lambda \\varepsilon^{-1}\\, Y^{\\varepsilon} \\, dt + \\sqrt{2 \\lambda/\\varepsilon} \\, dW^Y,\n\\end{align}\n\\right.\n\\]\nwhere \\(\\lambda &gt; 0\\) is just a fixed time-scaling parameter. The fast OU process mixes on time scales of order \\(\\mathcal{O}(\\varepsilon)\\) and has a standard Gaussian distribution as invariant distribution. Homogenization gives that in the regime \\(\\varepsilon\\to 0\\), the slow process can be approximated as\n\\[\ndX = \\sqrt{2/\\lambda} \\, dW\n\\tag{8}\\]\nsince the asymptotic variance is\n\\[\n\\mathop{\\mathrm{Var}} {\\left\\{  T^{-1/2} \\int_{t=0}^{T} Y_t \\, dt \\right\\}}\n\\to\n2 \\, \\int_{0}^{\\infty} C(r) \\, dr = 2/\\lambda\n\\]\nwhere \\(C(r) = \\mathop{\\mathrm{E}}[Y_t Y_{t+r}] = \\exp[-\\lambda r]\\) is the autocorrelation function of the fast OU process, as explained here. The fact that the effective diffusion is (twice) the integrated autocorrelation of the fast process is an example of Green-Kubo relations.\n\n\nExample: Overdamped Langevin Dynamics\nThis example does not exactly fall within the homogenization result described in the previous section, but almost. Consider a potential \\(U\\) and the slow-fast dynamics:\n\\[\n\\left\\{\n\\begin{align}\ndX^{\\varepsilon} &= \\varepsilon^{-1/2} \\, Y^{\\varepsilon} \\, dt\\\\\ndY^{\\varepsilon} &= - \\varepsilon^{-1}\\, [Y^{\\varepsilon}+\\varepsilon^{1/2}\\nabla U(X^{\\varepsilon})] \\, dt + \\sqrt{2 /\\varepsilon} \\, dW^Y.\n\\end{align}\n\\right.\n\\]\nFor any fixed value of \\(x \\in \\mathcal{X}\\), the fast OU-dynamics\n\\[\ndY = -[Y^{\\varepsilon}+\\varepsilon^{1/2}\\nabla U(x)] \\, dt + \\sqrt{2} \\, dW^Y\n\\]\nconverges to a Gaussian distribution with mean \\(-\\nabla U(x)\\) and unit variance. The same arguments as the previous section immediately give that, starting from \\(X^{\\varepsilon}_0=x\\), we have\n\\[\n\\varepsilon^{-1/2} \\, \\int_{0}^{\\delta} Y^{\\varepsilon} \\, dt\n\\; \\to \\;\n-\\nabla U(x) \\, \\delta + \\sqrt{2 \\delta} \\, \\mathcal{N}(0,1).\n\\]\nThe \\(\\sqrt{2}\\) terms comes from the OU asymptotic variance. this shows that the slow process converges as \\(\\varepsilon\\to 0\\) to the overdamped Langevin dynamics\n\\[\ndX = -\\nabla U(X) \\; + \\; \\sqrt{2} \\, dW.\n\\]\n\n\nExample: Stratonovich Corrections\nConsider a function \\(f: \\mathbb{R}\\to \\mathbb{R}\\) and the slow-fast system\n\\[\ndX^{\\varepsilon} = \\varepsilon^{-1/2} \\, f(X^{\\varepsilon}) \\, Y^{\\varepsilon} \\, dt\n\\]\nwhere \\(dY^{\\varepsilon} = -(\\lambda/\\varepsilon) Y^{\\varepsilon} + \\sqrt{2 \\lambda / \\varepsilon}\\) is a fast OU process mixing on scales of order \\(\\mathcal{O}(\\varepsilon)\\) and with standard centred Gaussian invariant distribution \\(\\rho(dy)\\).The discussion leading to Equation 8 suggest that the term \\(\\varepsilon^{-1/2} \\, Y^{\\varepsilon} \\, dt\\) can be heuristically be thought of as \\((2/\\lambda)^{1/2} \\, dW\\), which would imply that the effective dynamics for the slow-process is\n\\[\ndX = \\sqrt{2/\\lambda} \\, f(X) \\, dW.\n\\]\nWe will see that this heuristic is wrong! In order to obtain the effective dynamics of the slow process as \\(\\varepsilon\\to 0\\), since the generator of the fast-OU reads \\(\\mathcal{L}\\varphi= \\lambda [ -y\\,\\varphi_y + \\varphi_{yy}]\\), one can solve the Poisson equation \\(\\mathcal{L}\\Psi(x,y) = f(x)y\\) to obtain that \\(\\Phi(x,y) = -f(x)y/\\lambda\\). One already knows that \\(\\mathop{\\mathrm{Var}}[T^{-1}\\int_{[0,T]} Y_t \\, dt] = 2/\\lambda\\). The drift term is given by\n\\[\n\\begin{align}\nI(x) &= \\int f(x) \\partial_x \\Phi(x,y) \\, \\rho(dy)\\\\\n&= \\lambda^{-1} \\int f(x) f'(x) y^2 \\, \\rho(dy)\\\\\n&= \\lambda^{-1} f(x) f'(x).\n\\end{align}\n\\]\nPutting everything together gives that the effective slow dynamics reads\n\\[\n\\begin{align}\ndX &=  \\textcolor{blue}{ \\lambda^{-1} f'(X) f(X) \\, dt } + \\sqrt{2/\\lambda} \\, f(X) \\, dW\\\\\n&= \\sqrt{2/\\lambda} \\, f(X)  \\textcolor{red}{\\circ} dW\n\\end{align}\n\\]\nwhere \\( \\textcolor{red}{\\circ}\\) denotes Stratonovich integration.\n\n\nReadings\nThe book (Pavliotis and Stuart 2008) is beautiful, and I quite like the section on multiscale expansion in (Weinan 2011). For proving this type of results with the “martingale problem” approach (Stroock and Varadhan 1997), the lectures (Papanicolaou 1977) are nicely done.\n\n\n\n\n\nReferences\n\nHinch, E. J. 1991. Perturbation Methods. Cambridge University Press.\n\n\nPapanicolaou, George. 1977. “Martingale Approach to Some Limit Theorems.” In Papers from the Duke Turbulence Conference, Duke Univ., Durham, NC, 1977.\n\n\nPavliotis, Grigoris, and Andrew Stuart. 2008. Multiscale Methods: Averaging and Homogenization. Springer Science & Business Media.\n\n\nStroock, Daniel W, and SR Srinivasa Varadhan. 1997. Multidimensional Diffusion Processes. Vol. 233. Springer Science & Business Media.\n\n\nWeinan, E. 2011. Principles of Multiscale Modeling. Cambridge University Press."
  },
  {
    "objectID": "notes/MCMC_deterministic_proposals/MCMC_deterministic.html",
    "href": "notes/MCMC_deterministic_proposals/MCMC_deterministic.html",
    "title": "Metropolis-Hastings ratio with deterministic proposals",
    "section": "",
    "text": "Consider a probability density \\(\\pi(x)\\) on \\(\\mathbb{R}^d\\) and a (deterministic) function \\(F: \\mathbb{R}^d \\to \\mathbb{R}^d\\). We assume further that \\(F\\) is an involution in the sense that\n\\[\nF(F(x)) = x\n\\]\nfor all \\(x \\in \\mathbb{R}^d\\). To keep simple since it is not really the point of this short note, suppose that \\(\\pi(x)&gt;0\\) everywhere and that \\(F\\) is smooth. This type of transformations can be used to define Markov Chain Monte Carlo algorithms, eg. the standard Hamiltonian Monte Carlo (HMC) algorithm. In order to use this involution \\(F\\) to design a MCMC scheme, one needs to answer the following basic question: suppose that \\(X \\sim \\pi(dx)\\) and the proposal \\(Y = F(X)\\) is constructed and accepted with probability \\(\\alpha(X)\\), how should the acceptance probability function \\(\\alpha: \\mathbb{R}^d \\to [0,1]\\) be chosen so that the resulting random variable \\(Z \\; = \\; Y \\, B + (1-B) \\, X\\) is also distributed according to \\(\\pi(dx)\\)? The Bernoulli random variable \\(B\\) is such that \\(\\mathop{\\mathrm{P}}(B=1|X=x)=\\alpha(x)\\). In other words, for any test function \\(\\varphi: \\mathbb{R}^d \\to \\mathbb{R}\\), we would like \\(\\mathop{\\mathrm{E}}[\\varphi(Z)] = \\mathop{\\mathrm{E}}[\\varphi(X)]\\), which means that\n\\[\n\\int  {\\left\\{  \\varphi(F(x)) \\, \\alpha(x) + \\varphi(x) \\, (1-\\alpha(x))  \\right\\}}  \\, \\pi(dx) = \\int \\varphi(x) \\, \\pi(dx).\n\\tag{1}\\]\nA change of variable to transform \\(\\varphi(F(x)) \\to \\varphi(x)\\) and requiring that Equation 1 holds for any test function \\(\\varphi\\) shows that this is equivalent to asking for the relation\n\\[\n\\alpha(x) \\, \\pi(x) \\; = \\; \\alpha(y) \\, \\pi(y) \\, |J_F(x)|\n\\]\nto hold for any \\(x \\in \\mathbb{R}^d\\) where \\(y=F(x)\\) and \\(J_F(x)\\) is the Jacobian of \\(F\\) at \\(x\\). To make things slightly more symmetrical, since \\(|J_F(y)| \\times |J_F(x)| = 1\\) because of the involution property, this also reads\n\\[\n\\alpha(x) \\, \\frac{\\pi(x) }{|J_F(x)|^{1/2}} \\; = \\;\n\\alpha(y) \\, \\frac{\\pi(y) }{|J_F(y)|^{1/2}}.\n\\]\nAt this point, an obvious solution for anybody who has looked at the correctness-proof of the usual Metropolis-Hastings algorithm is to set\n\\[\n\\alpha(x) \\; = \\; \\min {\\left\\{ 1, \\frac{\\pi(y) / |J_F(y)|^{1/2}}{\\pi(x) / |J_F(x)|^{1/2}} \\right\\}} .\n\\]\nSince \\(|J_F(y)| \\times |J_F(x)| = 1\\), that is the same asking\n\\[\n\\alpha(x) \\; = \\; \\min {\\left\\{ 1, \\frac{\\pi(y)}{\\pi(x)} \\, |J_F(x)| \\right\\}} .\n\\tag{2}\\]\nOne could reach a similar conclusion by looking at the Radon-Nikodym ratio \\([\\pi(dx) \\otimes q(x,dy)] / [\\pi(dy) \\otimes q(y,dx)]\\) where \\(q(x,dy)\\) describes the deterministic transformation (Green 1995), but I do not find this significantly simpler in this situation. Indeed, Equation 2 is often used in the simpler case when \\(F\\) is volume preserving, i.e. \\(|J_F(x)|=1\\), as is the case for the Hamiltonian Monte Carlo (HMC). The discussion above was prompted by a student implementing a variant of this but with the wrong acceptance ratio \\(\\alpha(x) = \\min {\\left\\{ 1, \\frac{\\pi(y)}{\\pi(x)} \\, \\frac{|J_F(x)|}{|J_F(y)|} \\right\\}} \\) and us taking quite a bit of time to find the bug… Note that there are situations when the function \\(F\\) is not exactly an involution in the sense that sometimes \\(F(F(x))=x\\), but sometimes not. This can happen for example when implementing MCMC on a manifold \\(\\mathcal{M} \\subset \\mathbb{R}^d\\) and the function \\(F\\) involves a “projection” on the manifold \\(\\mathcal{M}\\), as for example described in the really interesting article (Zappa, Holmes-Cerfon, and Goodman 2018). In that case, it suffices to add a “reversibility check”, i.e. make sure that when applying \\(F\\) starting from \\(y=F(x)\\), one gets back to \\(x\\). The acceptance probability in that case should be amended and expressed as\n\\[\n\\alpha(x) \\; = \\; \\min {\\left\\{ 1, \\frac{\\pi(y)}{\\pi(x)} \\, |J_F(x)| \\right\\}}  \\, \\mathbf{1} {\\left( F(y)=x \\right)} .\n\\]\nIn other words, if applying \\(F\\) to the proposal \\(y=F(x)\\) does not lead back to \\(x\\), the proposal is rejected.\n\nA mixture of deterministic transformations?\nA small riddle (whose answer I do not have), to conclude these notes. One can check that for any value of \\(c \\in \\mathbb{R}\\), the function \\(F_{c}(x) = c + 1/(x-c)\\) is an involution of the real line. This means that for any target density \\(\\pi(x)\\) on the real line, one can build the associated Markov kernel \\(M_c\\) defined as\n\\[\nM_c(x, dy) = \\alpha_c(x) \\, \\delta_{F_c(x)}(dy) + (1-\\alpha_c(x)) \\, \\delta_x(dy)\n\\]\nfor acceptance probability described above, namely\n\\[\n\\alpha_c(x) = \\min {\\left\\{ 1, \\frac{\\pi[F_c(x)]}{\\pi(x)} |F'_c(x)| \\right\\}} .\n\\]\nFor \\(N \\geq 2\\), choose a few values \\(x_1, \\ldots, c_N \\in \\mathbb{R}\\) and consider the mixture of Markov kernels\n\\[\nM(x,dy) \\; = \\; \\frac{1}{N} \\sum_{i=1}^N M_{c_i}(x, dy).\n\\]\nThe Markov kernel \\(M(x, dy)\\) lets the distribution \\(\\pi\\) invariant, but it is not clear at all (to me) under what conditions the associated MCMC algorithm does converge to \\(\\pi\\). One can empirically check that if \\(N\\) is very small, things can break down quite easily. On the other, for \\(N\\) not too small, things can start to work.\n\n\n\n\n\n\n\nFor \\(N=5\\) random values \\(c_1, \\ldots, c_5 \\in \\mathbb{R}\\), the illustration aboves shows the empirical distribution of the associated Markov chain ran for \\(T=10^6\\) iterations and targeting the standard Gaussian distribution \\(\\pi(dx) \\equiv \\mathcal{N}(0,1)\\): the fit seems almost perfect and I would not be surprised if \\(M(x,dy)\\) were ergodic with respect to \\(\\pi(dx)\\) although I cannot prove it.\n\n\n\n\n\nReferences\n\nGreen, Peter J. 1995. “Reversible Jump Markov Chain Monte Carlo Computation and Bayesian Model Determination.” Biometrika 82 (4). Oxford University Press: 711–32.\n\n\nZappa, Emilio, Miranda Holmes-Cerfon, and Jonathan Goodman. 2018. “Monte Carlo on Manifolds: Sampling Densities and Integrating Functions.” Communications on Pure and Applied Mathematics 71 (12). Wiley Online Library: 2609–47."
  }
]