<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Alexandre Thiéry</title>
<link>https://alexxthiery.github.io/posts/index_blog.html</link>
<atom:link href="https://alexxthiery.github.io/posts/index_blog.xml" rel="self" type="application/rss+xml"/>
<description>Alex Thiery Notes</description>
<generator>quarto-1.3.433</generator>
<lastBuildDate>Sun, 11 Jun 2023 17:00:00 GMT</lastBuildDate>
<item>
  <title>Reverse diffusions, Score &amp; Tweedie</title>
  <link>https://alexxthiery.github.io/posts/reverse_and_tweedie/reverse_and_tweedie.html</link>
  <description><![CDATA[ 




<section id="reversing-a-diffusion" class="level2">
<h2 class="anchored" data-anchor-id="reversing-a-diffusion">Reversing a diffusion</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="reverse_cropped.mp4" class="img-fluid" style="width:100.0%" controls=""><a href="reverse_cropped.mp4">Video</a></video></p>
</figure>
</div>
<p>Imagine a scalar diffusion process <img src="https://latex.codecogs.com/png.latex?%5C%7BX_t%5C%7D_%7Bt=0%7D%5ET"> defined on the interval <img src="https://latex.codecogs.com/png.latex?%5B0,T%5D">,</p>
<p><img src="https://latex.codecogs.com/png.latex?dX%20=%20%5Cmu(X)%20%5C,%20dt%20+%20%5Csigma%20%5C,%20dW"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmu(X)"> is the drift term, <img src="https://latex.codecogs.com/png.latex?%5Csigma"> is the diffusion coefficient, and <img src="https://latex.codecogs.com/png.latex?dW"> is a Wiener process. Denote the distribution of this process at time <img src="https://latex.codecogs.com/png.latex?t"> (<img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20t%20%5Cleq%20T">) as <img src="https://latex.codecogs.com/png.latex?p_t(dx)"> with an initial distribution of <img src="https://latex.codecogs.com/png.latex?p_0(dx)">. Now, what happens if we reverse time and examine the process backward? In other words, consider the time-reversed process <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D"> defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_s%20=%20X_%7BT-s%7D."></p>
<p>Intuitively, the process <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D"> is also a diffusion on the interval <img src="https://latex.codecogs.com/png.latex?%5B0,%20T%5D">, but with an initial distribution <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_0%20%5Csim%20p_T(dx)">. To gain intuition, consider an Euler discretization of the forward process:</p>
<p><span id="eq-rev-wrong"><img src="https://latex.codecogs.com/png.latex?X_%7Bt+%5Cdelta%7D%20=%20X_%7Bt%7D%20+%20%5Cmu(X_t)%5C,%20%5Cdelta%20+%20%5Csigma%20%5C,%20%5Csqrt%7B%5Cdelta%7D%20%5C,%20%5Cmathbf%7Bn%7D%20%5Ctag%7B1%7D"></span></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bn%7D%5Csim%20%5Cmathcal%7BN%7D(0,1)"> represents a noise term independent from <img src="https://latex.codecogs.com/png.latex?X_%7Bt%7D">, and <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Cll%201"> is a time increment. Re-arranging terms and making the approximation <img src="https://latex.codecogs.com/png.latex?%5Cmu(X_t)%20%5Capprox%20%5Cmu(X_%7Bt+%5Cdelta%7D)"> gives that</p>
<p><span id="eq-rev-euler-wrong"><img src="https://latex.codecogs.com/png.latex?X_%7Bt%7D%20%5Capprox%20X_%7Bt+%5Cdelta%7D%20-%20%5Cmu(X_%7Bt+%5Cdelta%7D)%5C,%20%5Cdelta%20+%20%5Csigma%20%5C,%20%5Csqrt%7B%5Cdelta%7D%20%5C,%20(-%5Cmathbf%7Bn%7D).%20%5Ctag%7B2%7D"></span></p>
<p>This seems to suggest that the time-reversed process follows the dynamics <img src="https://latex.codecogs.com/png.latex?d%5Coverleftarrow%7BX%7D%20=%20-%5Cmu(%5Coverleftarrow%7BX%7D)%20%5C,%20dt%20+%20%5Csigma%20%5C,%20dW"> started from <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_0%20%5Csim%20p_T(dx)">. However, this conclusion is incorrect because this would suggest that the time-reversed of a standard Brownian motion (where <img src="https://latex.codecogs.com/png.latex?%5Cmu(x)%20%5Cequiv%200">) starting at zero is also a Brownian motion starting at <img src="https://latex.codecogs.com/png.latex?p_T(dx)%20=%20%5Cmathcal%7BN%7D(0,T)">, which is clearly not the case. The flaw in this argument lies in assuming that the noise term <img src="https://latex.codecogs.com/png.latex?Z"> is independent of <img src="https://latex.codecogs.com/png.latex?X_%7Bt+%5Cdelta%7D">, which is not true, rendering the Euler discretization argument invalid.</p>
<p>Deriving the dynamics of the backward process in a rigorous manner is not straightforward <span class="citation" data-cites="anderson1982reverse">(Anderson 1982)</span> <span class="citation" data-cites="haussmann1986time">(Haussmann and Pardoux 1986)</span>. What follows is a heuristic derivation that proceeds by estimating the mean and variance of <img src="https://latex.codecogs.com/png.latex?X_%7Bt%7D"> given <img src="https://latex.codecogs.com/png.latex?X_%7Bt+%5Cdelta%7D%20=%20x_%7Bt+%5Cdelta%7D">, assuming <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Cll%201">. Here, <img src="https://latex.codecogs.com/png.latex?x_%7Bt+%5Cdelta%7D"> is treated as a fixed and constant value, and we are only interested in the conditional distribution of <img src="https://latex.codecogs.com/png.latex?X_t"> given <img src="https://latex.codecogs.com/png.latex?x_%7Bt+%5Cdelta%7D">. Bayes’ law gives</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(X_t%20%5Cin%20dx%20%7C%20x_%7Bt+%5Cdelta%7D)%20%5Cpropto%20p_t(x)%20%5C,%20%5Cexp%5Cleft%5C%7B%20-%5Cfrac%7B%5Cleft(%20x_%7Bt+%5Cdelta%7D%20-%20%5Bx%20+%20%5Cmu(x)%20%5C,%20%5Cdelta%5D%20%5Cright)%5E2%7D%7B2%20%5Csigma%5E2%20%5C,%20%5Cdelta%7D%20%5Cright%5C%7D,"></p>
<p>where the exponential term corresponds to the transition of the forward diffusion for <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Cll%201">. Using the 1st order approximation</p>
<p><img src="https://latex.codecogs.com/png.latex?p_t(x)%20%5Capprox%20p_t(x_%7Bt+%5Cdelta%7D)%20%5C,%5Cexp%5Cleft(%20%5Clangle%20%5Cnabla%20%5Clog%20p_t(x_%7Bt+%5Cdelta%7D),%20(x-x_%7Bt+%5Cdelta%7D)%5Crangle%5Cright),"></p>
<p>eliminating multiplicative constants and higher-order error terms, we obtain:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(X_t%20%5Cin%20dx%20%7C%20x_%7Bt+%5Cdelta%7D)%0A%5Cpropto%0A%5Cexp%5Cleft%5C%7B%20-%5Cfrac%7B%5Cleft(%20x%20-%20%5Bx_%7Bt+%5Cdelta%7D%20-%20%5Cmu(x_%7Bt+%5Cdelta%7D)%20%5Cdelta%0A+%20%7B%5Ccolor%7Bred%7D%20%5Csigma%5E2%20%5C,%20%5Cnabla%20%5Clog%20p_t(x_%7Bt+%5Cdelta%7D)%20%5C,%20%5Cdelta%7D%20%5D%20%5Cright)%5E2%7D%7B2%20%5Csigma%5E2%20%5C,%20%5Cdelta%7D%20%5Cright%5C%7D."></p>
<p>For <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Cll%201">, this is transition of the reverse diffusion</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ad%5Coverleftarrow%7BX%7D_t%20=%20-%5Cmu(%5Coverleftarrow%7BX%7D_t)%20%5C,%20dt%20%5C;%20+%20%7B%5Ccolor%7Bred%7D%20%5Csigma%5E2%20%5C,%20%5Cnabla%20%5Clog%20p_%7BT-t%7D(%5Coverleftarrow%7BX%7D_t)%20%5C,%20dt%7D%20%5C;%20+%20%5C;%20%5Csigma%20%5C,%20dB.%0A"></p>
<p>The notation <img src="https://latex.codecogs.com/png.latex?B"> is used to emphasize that this Brownian motion is distinct from the one used in the forward diffusion. The additional drift term, denoted as <img src="https://latex.codecogs.com/png.latex?%7B%5Ccolor%7Bred%7D%20%5Csigma%5E2%20%5C,%20%5Cnabla%20%5Clog%20p_t(%5Coverleftarrow%7BX%7D)%7D">, is intuitive: it pushes the reverse diffusion toward regions where the forward diffusion spent a significant amount of time, i.e., where <img src="https://latex.codecogs.com/png.latex?p_t(dx)"> is large. The popular “denoising diffusion models” <span class="citation" data-cites="ho2020denoising">(Ho, Jain, and Abbeel 2020)</span> can be seen as discretizations of this backward process, employing various techniques to estimate the additional drift term from data.</p>
</section>
<section id="denoising-score-matching-tweedie-formula" class="level2">
<h2 class="anchored" data-anchor-id="denoising-score-matching-tweedie-formula">Denoising Score Matching &amp; Tweedie formula</h2>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://en.wikipedia.org/wiki/Maurice_Tweedie"><img src="https://alexxthiery.github.io/posts/reverse_and_tweedie/maurice_tweedie.gif" class="img-fluid figure-img" style="width:35.0%"></a></p>
<figcaption class="figure-caption">Maurice Tweedie (1919–1996)</figcaption>
</figure>
</div>
</div>
<p>The previous section shows that a quantity often referred to as the “score” in machine learning (ML) literature, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BS%7D_t(x)%20%5Cequiv%20%5Cnabla_x%20%5Clog%20p_t(x)">, naturally arises when a diffusion process runs backward in time. Interestingly, it is worth noting that since the beginning of times statisticians have been using the term <a href="https://en.wikipedia.org/wiki/Score_(statistics)">“score”</a> to refer to the other derivative, i.e.&nbsp;the derivative with respect to a model’s parameter, which is a completely different object!</p>
<p>Consider a Brownian diffusion process <img src="https://latex.codecogs.com/png.latex?dX%20=%20%5Csigma%20%5C,%20dW"> initiated from a distribution <img src="https://latex.codecogs.com/png.latex?%5Cmu(dx)%20=%20p_0(dx)">. If this process is ran forward for a duration <img src="https://latex.codecogs.com/png.latex?%5Cdelta%3E0">, we have:</p>
<p><img src="https://latex.codecogs.com/png.latex?X_%7B%5Cdelta%7D%20=%20X_0%20+%20%5Cmathcal%7BN%7D(0,%20%5Csigma%5E2%20%5C,%20%5Cdelta)"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?X_0%20%5Csim%20%5Cmu(dx)">. Now, focusing on a specific sample <img src="https://latex.codecogs.com/png.latex?y%20=%20X_%7B%5Cdelta%7D">, the backward dynamics <img src="https://latex.codecogs.com/png.latex?d%5Coverleftarrow%7BX%7D%20=%20%5Csigma%5E2%20%5C,%20%5Cnabla%20%5Clog%20p_t(%5Coverleftarrow%7BX%7D)%20,%20dt%20+%20%5Csigma%20%5C,%20dB"> suggests that for sufficiently small <img src="https://latex.codecogs.com/png.latex?%5Cdelta">, the following approximation holds:</p>
<p><span id="eq-rev-tweedie"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5BX_0%20%5C,%20%7C%20X_%7B%5Cdelta%7D%20=%20y%5D%20%5C;%20%7B%20%5Ccolor%7B%5Cred%7D%20%5Capprox%7D%20%5C;%20y%20+%20%5Cnabla%20%5Clog%20p_%7B%5Cdelta%7D(y)%20%5C,%20%5Csigma%5E2%20%5Cdelta.%0A%5Ctag%7B3%7D"></span></p>
<p>Maybe surprisingly, in the case of Brownian dynamics (no drift), this relationship holds exactly, even for arbitrarily large time increments <img src="https://latex.codecogs.com/png.latex?%5Cdelta%3E0">. This observation attributed in <span class="citation" data-cites="efron2011tweedie">(Efron 2011)</span> to <a href="https://en.wikipedia.org/wiki/Maurice_Tweedie">Maurice Tweedie (1919–1996)</a> has a straightforward proof. Specifically, if <img src="https://latex.codecogs.com/png.latex?X%20%5Csim%20%5Cmu(dx)">, then <img src="https://latex.codecogs.com/png.latex?Y%20=%20X%20+%20%5Cmathcal%7BN%7D(0,%20%5CGamma%5E2)"> has a density given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap_Y(dy)%20=%20%5Cint%20%5Cmu(x)%20%5C,%20%5Crho_%7B%5CGamma%7D(y-x)%20%5C,%20dx%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Crho_%7B%5CGamma%7D(z)%20%5Cpropto%20%5Cexp%5B-z%5E2/(2%20%5CGamma%5E2)%5D"> is a centred Gaussian with variance <img src="https://latex.codecogs.com/png.latex?%5CGamma%5E2">. It follows that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%20%5Cmathbb%7BE%7D%5BX%20%5C,%20%7C%20Y%20=%20y%5D-y%7D%7B%5CGamma%5E2%7D%20=%20%5Cfrac%7B%20%5Cint%20%5Cleft(%20%5Cfrac%20%7Bx-y%20%7D%7B%5CGamma%5E2%7D%20%5Cright)%5C,%20%5Cmu(x)%20%5C,%20%5Crho_%7B%5CGamma%7D(y-x)%20%5C,%20dx%20%7D%7B%5Cint%20%5Cmu(x)%20%5C,%20%5Crho_%7B%5CGamma%7D(y-x)%20%5C,%20dx%7D%0A=%0A%5Cnabla_y%20%5Clog%20%5Cleft%5C%7B%20%5Cint%20%5Cmu(x)%20%5C,%20%5Crho_%7B%5CGamma%7D(y-x)%20%5C,%20dx%20%5Cright%5C%7D%0A"></p>
<p>since <img src="https://latex.codecogs.com/png.latex?%5Cnabla_y%20%5Crho_%7B%5CGamma%7D(y-x)%20=%20(x-y)/%5CGamma%5E2">. This leads to Tweedie’s formula:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5BX%20%5C,%20%7C%20Y%20=%20y%5D%20%5C;%20%7B%5Ccolor%7Bred%7D%20=%7D%20%5C;%20y%20+%20%5CGamma%5E2%20%5C,%20%5Cnabla%20%5Clog%20p_Y(y),%0A"></p>
<p>which is exactly the same as Equation&nbsp;3 but with an equality sign: no approximation needed! Interestingly, this is what Machine-Learners often refer to as “denoising score matching” <span class="citation" data-cites="vincent2011connection">(Vincent 2011)</span>: if we have access to a large number of samples <img src="https://latex.codecogs.com/png.latex?(X_i,Y_i)">, where <img src="https://latex.codecogs.com/png.latex?X_i%20%5Csim%20%5Cmu(dx)"> and <img src="https://latex.codecogs.com/png.latex?Y_i%20=%20X_i%20+%20%5Cmathcal%7BN%7D(0,%20%5CGamma%5E2)">, and fit a regression model <img src="https://latex.codecogs.com/png.latex?F_%7B%5Ctheta%7D"> by minimizing the mean-squared error <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20%5Cmapsto%20%5Cmathbb%7BE%7D%20%5C%7CX%20-%20F_%7B%5Ctheta%7D(Y)%5C%7C%5E2">, then Tweedie formula shows that in the limit of infinite samples and with sufficient flexibility in the regression model <img src="https://latex.codecogs.com/png.latex?F_%7B%5Ctheta_%5Cstar%7D(y)%20=%20y%20+%20%5CGamma%5E2%20%5Cnabla%20%5Clog%20p_Y(y)">. This allows one to estimate <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20%5Clog%20p_Y"> from data, which is often a good approximation of <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20%5Clog%20%5Cmu"> if the variance <img src="https://latex.codecogs.com/png.latex?%5CGamma%5E2"> of the added noise is not too large. Indeed, things can go bad if <img src="https://latex.codecogs.com/png.latex?%5CGamma%5E2"> is very small and the number of training data is not large, no free lunch!</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-anderson1982reverse" class="csl-entry">
Anderson, Brian DO. 1982. <span>“Reverse-Time Diffusion Equation Models.”</span> <em>Stochastic Processes and Their Applications</em> 12 (3): 313–26.
</div>
<div id="ref-efron2011tweedie" class="csl-entry">
Efron, Bradley. 2011. <span>“Tweedie’s Formula and Selection Bias.”</span> <em>Journal of the American Statistical Association</em> 106 (496): 1602–14.
</div>
<div id="ref-haussmann1986time" class="csl-entry">
Haussmann, Ulrich G, and Etienne Pardoux. 1986. <span>“Time Reversal of Diffusions.”</span> <em>The Annals of Probability</em>, 1188–1205.
</div>
<div id="ref-ho2020denoising" class="csl-entry">
Ho, Jonathan, Ajay Jain, and Pieter Abbeel. 2020. <span>“Denoising Diffusion Probabilistic Models.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 6840–51.
</div>
<div id="ref-vincent2011connection" class="csl-entry">
Vincent, Pascal. 2011. <span>“A Connection Between Score Matching and Denoising Autoencoders.”</span> <em>Neural Computation</em> 23 (7): 1661–74.
</div>
</div></section></div> ]]></description>
  <category>SDE</category>
  <category>Generative-Model</category>
  <guid>https://alexxthiery.github.io/posts/reverse_and_tweedie/reverse_and_tweedie.html</guid>
  <pubDate>Sun, 11 Jun 2023 17:00:00 GMT</pubDate>
</item>
<item>
  <title>Hello World</title>
  <link>https://alexxthiery.github.io/posts/hello_world/index.html</link>
  <description><![CDATA[ 




<p>Just testing whether Latex is working correctly: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D%5Cleft(%20%5Cint%20%5Cexp%5Cleft%5C%7B%20-%5Cfrac%7Bx%5E2%7D%7B2%7D%5Cright%5C%7D%20%5C,%20dx%20%5Cright)%5E2%20%5Capprox%2022/7"></p>
<p>Here is a reference <span class="citation" data-cites="mackay2003information">(MacKay 2003)</span> and below is an animation:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="particles_bouncing.mp4" class="img-fluid" style="width:80.0%" controls=""><a href="particles_bouncing.mp4">Video</a></video></p>
</figure>
</div>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-mackay2003information" class="csl-entry">
MacKay, David JC. 2003. <em>Information Theory, Inference and Learning Algorithms</em>. Cambridge university press.
</div>
</div></section></div> ]]></description>
  <category>testing</category>
  <guid>https://alexxthiery.github.io/posts/hello_world/index.html</guid>
  <pubDate>Mon, 05 Jun 2023 17:00:00 GMT</pubDate>
</item>
</channel>
</rss>
