<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Alexandre Thiéry</title>
<link>https://alexxthiery.github.io/notes/index_blog.html</link>
<atom:link href="https://alexxthiery.github.io/notes/index_blog.xml" rel="self" type="application/rss+xml"/>
<description>Alex Thiery Notes</description>
<generator>quarto-1.3.353</generator>
<lastBuildDate>Fri, 22 Sep 2023 16:00:00 GMT</lastBuildDate>
<item>
  <title>Information Theory: Entropy and Basic Definitions</title>
  <link>https://alexxthiery.github.io/notes/information_theory_basics/information_theory_entropy.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/information_theory_basics/shannon_destruction.gif" class="img-fluid figure-img" style="width:90.0%"></p>
</figure>
</div>
</div>
<section id="shannon-entropy-compression" class="level3">
<h3 class="anchored" data-anchor-id="shannon-entropy-compression">Shannon Entropy &amp; Compression</h3>
<p>If Alice chooses a number <img src="https://latex.codecogs.com/png.latex?X"> uniformly at random from the set <img src="https://latex.codecogs.com/png.latex?%5C%7B1,2,%20%5Cldots,%20N%5C%7D">, Bob can use a simple “dichotomy” strategy to ask Alice <img src="https://latex.codecogs.com/png.latex?%5Clog_2(N)"> binary Yes/No questions and correctly identify the number. This result can be generalized to non-uniform distributions (eg. using the <a href="https://en.wikipedia.org/wiki/Kraft–McMillan_inequality">Kraft-McMillan inequality</a>). If Alice chooses a number <img src="https://latex.codecogs.com/png.latex?X"> from <img src="https://latex.codecogs.com/png.latex?%5C%7B1,2,%20%5Cldots,%20N%5C%7D"> with probabilities <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BP%7D%7D(X=k)%20=%20p_k">, Bob can design a deterministic strategy to find the answer using, on average, approximately</p>
<p><span id="eq-OU"><img src="https://latex.codecogs.com/png.latex?%0AH(X)%20=%20-%20%5Csum_%7Bk=1%7D%5EN%20p_k%20%5C,%20%5Clog_2(p_k)%0A%5Ctag%7B1%7D"></span></p>
<p>binary questions, ie. bits. He cannot achieve this with fewer questions. The quantity <img src="https://latex.codecogs.com/png.latex?H(X)"> defined in Equation&nbsp;1, known as the <strong>Shannon Entropy</strong> of the distribution <img src="https://latex.codecogs.com/png.latex?(p_1,%20%5Cldots,%20p_N)">, also implies that the strategy can encode each integer <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20x%20%5Cleq%20N"> as a binary string of length <img src="https://latex.codecogs.com/png.latex?L(x)"> (i.e.&nbsp;with <img src="https://latex.codecogs.com/png.latex?L(x)"> bits), with the expected length <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BE%7D%7D%5BL(X)%5D"> approximately equal to <img src="https://latex.codecogs.com/png.latex?H(X)">.</p>
<p>This remark can be used for compression. Imagine a very long sequence <img src="https://latex.codecogs.com/png.latex?(X_1,%20%5Cldots,%20X_T)"> of iid samples from <img src="https://latex.codecogs.com/png.latex?X">. Encoding each <img src="https://latex.codecogs.com/png.latex?X_i"> with <img src="https://latex.codecogs.com/png.latex?L(X_i)">, one should be able to encode the resulting sequence with</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL(X_1)%20+%20%5Cldots%20+%20L(X_T)%20%5Capprox%20T%20%5C,%20%5Cmathop%7B%5Cmathrm%7BE%7D%7D%5BL(X)%5D%20%5Capprox%20T%20%5Ccdot%20H(X)%0A"></p>
<p>bits. Can the usual <strong>zip compression</strong> algorithm do this? To test this, choose a probability distribution on <img src="https://latex.codecogs.com/png.latex?%5C%7B1,%20%5Cldots,%20N%5C%7D">, generate an iid sequence of length <img src="https://latex.codecogs.com/png.latex?T%20%5Cgg%201">, compress this using the <img src="https://latex.codecogs.com/png.latex?%5Ctexttt%7Bgzip%7D(%5Cldots)"> command, and finally look at the size of the resulting file (in bits). I have done that a few times, with a few different values of <img src="https://latex.codecogs.com/png.latex?16%20%5Cleq%20N%20%5Cleq%20256"> and a few random distributions on <img src="https://latex.codecogs.com/png.latex?%5C%7B1,%20%5Cldots,%20N%5C%7D">, and with <img src="https://latex.codecogs.com/png.latex?T%20=%2010%5E6">. The plot of size of the compressed files versus the Shannon entropy <img src="https://latex.codecogs.com/png.latex?H"> looks as below:</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/information_theory_basics/shannon_vs_zip.png" class="img-fluid figure-img" style="width:90.0%"></p>
</figure>
</div>
</div>
<p>Seems like the zip-algorithm works almost optimally for compressing iid sequences.</p>
<p>Now consider a pair of discrete random variables <img src="https://latex.codecogs.com/png.latex?(X,Y)">. If Alice draws samples from this pair of rvs, one can ask <img src="https://latex.codecogs.com/png.latex?H(X,Y)"> binary questions on average to exactly find out these values. To do that, one can ask <img src="https://latex.codecogs.com/png.latex?H(X)"> questions to estimate <img src="https://latex.codecogs.com/png.latex?X">, and once <img src="https://latex.codecogs.com/png.latex?X=x"> is estimated, one can then ask about <img src="https://latex.codecogs.com/png.latex?H(Y%7CX=x)%20=%20-%5Csum_y%20%5Cmathop%7B%5Cmathrm%7BP%7D%7D(Y=y%7CX=x)%20%5C,%20%5Clog_2(%5Cmathop%7B%5Cmathrm%7BP%7D%7D(Y=y%7CX=x))"> to estimate <img src="https://latex.codecogs.com/png.latex?Y">. This strategy requires on average <img src="https://latex.codecogs.com/png.latex?H(X)%20+%20%5Csum_x%20%5Cmathop%7B%5Cmathrm%7BP%7D%7D(X=x)%20%5C,%20H(Y%7CX=x)"> binary questions and is actually optimal, showing that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AH(X,Y)%20=%20H(X)%20+%20H(Y%20%7C%20X)%0A"></p>
<p>where we have defined <img src="https://latex.codecogs.com/png.latex?H(Y%20%7C%20X)%20=%20%5Csum_x%20%5Cmathop%7B%5Cmathrm%7BP%7D%7D(X=x)%20%5C,%20H(Y%7CX=x)">. Iterating this argument shows the trajectory <img src="https://latex.codecogs.com/png.latex?X_%7B1:T%7D%20%5Cequiv%20(X_1,%20%5Cldots,%20X_N)"> of a stationary ergodic Markov chain can be estimated on average with <img src="https://latex.codecogs.com/png.latex?H(X_%7B1:T%7D)"> binary questions where</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AH(X_%7B1:T%7D)%0A&amp;=%20H(X_1)%20+%20H(X_2%7CX_1)%20+%20%5Cldots%20+%20H(X_%7BT%7D%20%7C%20X_%7Bt-1%7D)%5C%5C%0A&amp;%5Capprox%20T%20%5C,%20H(X_%7Bk+1%7D%20%7C%20X_k)%5C%5C%0A&amp;=%20-%20T%20%5C,%20%5Csum_x%20%5Cpi(x)%20%5C,%20%5Csum_%7By%7D%20p(x%20%5Cto%20y)%20%5C,%20%5Clog_2%5Bp(x%20%5Cto%20y)%5D.%0A%5Cend%7Balign%7D%0A"></p>
<p>Here, <img src="https://latex.codecogs.com/png.latex?%5Cpi(dx)"> is the equilibrium distribution of the Markov chain and <img src="https://latex.codecogs.com/png.latex?p(x%20%5Cto%20y)"> are the transition probabilities.</p>
<p>Can <img src="https://latex.codecogs.com/png.latex?%5Ctexttt%7Bgzip%7D(%5Cldots)"> compress Markovian trajectories and roughly achieve that level of compression? Indeed, one can test this by generating random Markov transition matrices (that are, with very high probability, ergodic with respect to an equilibrium distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi">). Doing this with trajectories of length <img src="https://latex.codecogs.com/png.latex?10%5E4"> (ie. quite short because it is quite slow to) on <img src="https://latex.codecogs.com/png.latex?%5C%7B1,%20%5Cldots,%20N%5C%7D"> with <img src="https://latex.codecogs.com/png.latex?2%20%5Cleq%20N%20%5Cleq%2064">, one get the following results:</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/information_theory_basics/shannon_vs_zip_markov.png" class="img-fluid figure-img" style="width:90.0%"></p>
</figure>
</div>
</div>
<p>In red is the entropy estimated without using the Markovian structure and assuming that the <img src="https://latex.codecogs.com/png.latex?X_i"> are iid samples. One can clearly see that the dependencies between successive samples are exploited for better compression. Nevertheless, the optimal compression rate given by the Shannon entropy is not reached. Indeed, <img src="https://latex.codecogs.com/png.latex?%5Ctexttt%7Bgzip%7D(%5Cldots)"> is not an optimal algorithm – it cannot even compress well enough the sequence <img src="https://latex.codecogs.com/png.latex?(1,2,3,1,2,3,1,2,3,%5Cldots)">!</p>
</section>
<section id="asymptotic-equipartition-property" class="level3">
<h3 class="anchored" data-anchor-id="asymptotic-equipartition-property">Asymptotic Equipartition Property</h3>
<p>TODO</p>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<p>TODO</p>


</section>

 ]]></description>
  <category>information_theory</category>
  <guid>https://alexxthiery.github.io/notes/information_theory_basics/information_theory_entropy.html</guid>
  <pubDate>Fri, 22 Sep 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>From Denoising Diffusion to ODEs</title>
  <link>https://alexxthiery.github.io/notes/DDPM_deterministic/DDPM_deterministic.html</link>
  <description><![CDATA[ 




<section id="setting-goals" class="level3">
<h3 class="anchored" data-anchor-id="setting-goals">Setting &amp; Goals</h3>
<p>Consider an empirical data distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D">. In order to simulate approximate samples from <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D">, <a href="../../notes/DDPM/DDPM.html">Denoising Diffusion Probabilistic Models</a> (DDPM) simulate a forward diffusion process <img src="https://latex.codecogs.com/png.latex?%5C%7BX_t%5C%7D_%7B%5B0,T%5D%7D"> on an interval <img src="https://latex.codecogs.com/png.latex?%5B0,T%5D">. The diffusion is initialized at the data distribution, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?X_0%20%5Csim%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D">, and is chosen so that that the distribution of <img src="https://latex.codecogs.com/png.latex?X_T"> is very close to a known and tractable reference distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bref%7D%7D">, e.g.&nbsp;a Gaussian distribution. Denote by <img src="https://latex.codecogs.com/png.latex?p_t(dx)"> the marginal distribution at time <img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20t%20%5Cleq%20T">, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BP%7D%7D(X_t%20%5Cin%20dx)%20=%20p_t(dx)">. By choosing the forward distribution with simple and tractable transition probabilities, e.g.&nbsp;an Ornstein-Uhlenbeck, it is relatively easy to estimate <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20%5Clog%20p_t(x)"> from simulated data: this can be formulated as a simple regression problem. This allows one to simulate the diffusion backward in time and generate approximate samples from <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D">. Why this is useful is another question…</p>
<p>The fact that the mapping from data-samples at time <img src="https://latex.codecogs.com/png.latex?t=0"> to (approximate) Gaussian samples at time <img src="https://latex.codecogs.com/png.latex?t=T"> is stochastic and described by diffusion processes is cumbersome. This would be much more convenient to build a deterministic mapping between the data-distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D"> and the Gaussian reference distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bref%7D%7D">: this would allows one to associate a likelihood to data samples and to easily “encode”/“decode” data-samples. To so this, one can try to replace diffusion by Ordinary Differential Equations.</p>
</section>
<section id="the-diffusion-ode-trick" class="level3">
<h3 class="anchored" data-anchor-id="the-diffusion-ode-trick">The diffusion-ODE trick</h3>
<p>Consider an arbitrary diffusion process <img src="https://latex.codecogs.com/png.latex?dX%20=%20%5Cmu(t,X)%20%5C,%20dt%20+%20dB"> with associated distribution <img src="https://latex.codecogs.com/png.latex?p_t(dx)"> at time <img src="https://latex.codecogs.com/png.latex?t">. The <a href="https://en.wikipedia.org/wiki/Fokker–Planck_equation">Fokker-Planck</a> equation that describes the evolution of <img src="https://latex.codecogs.com/png.latex?p_t"> reads</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpartial_t%20p_t%20=%20-%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7D(%20p_t%20%5C,%20%5Cmu)%20+%20%5Cfrac12%20%5C,%20%5CDelta%20p_t%0A"></p>
<p>If there were no diffusion term <img src="https://latex.codecogs.com/png.latex?dB"> and <img src="https://latex.codecogs.com/png.latex?X"> was describing instead the evolution of differential equation <img src="https://latex.codecogs.com/png.latex?dX/dt%20=%20F(t,X)">, the associated evolution of the density of <img src="https://latex.codecogs.com/png.latex?X"> would simply read</p>
<p><span id="eq-continuity"><img src="https://latex.codecogs.com/png.latex?%0A%5Cpartial_t%20p_t%20=%20-%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7D(%20p_t%20%5C,%20F).%0A%5Ctag%7B1%7D"></span></p>
<p>If one can find a vector field <img src="https://latex.codecogs.com/png.latex?F(t,x)"> such that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A-%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7D(%20p_t%20%5C,%20%5Cmu)%20+%20%5Cfrac12%20%5C,%20%5CDelta%20p_t%0A=%0A-%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7D(%20p_t%20%5C,%20F),%0A"></p>
<p>then one can basically replace diffusions by ODEs. The diffusion-ODE trick is the simple remark that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AF(t,x)%20=%20%5Cmu(t,x)%20%5C;%20%7B%5Ccolor%7Bred%7D%20-%20%5Cfrac12%20%5C,%20%5Cnabla_x%20%5Clog%20p_t(x)%7D%0A"></p>
<p>does exactly this, as algebra immediately shows it. The additional term <img src="https://latex.codecogs.com/png.latex?%7B%5Ccolor%7Bred%7D%20-%20%5Cfrac12%20%5C,%20%5Cnabla_x%20%5Clog%20p_t(x)%7D"> is intuitive. The coefficient <img src="https://latex.codecogs.com/png.latex?%7B%5Ccolor%7Bblue%7D%201/2%7D"> is because one is trying to match the term <img src="https://latex.codecogs.com/png.latex?%7B%5Ccolor%7Bblue%7D%20(1/2)%7D%20%5C,%20%5CDelta%20p_t"> in the Fokker-Planck equation. And the overall term <img src="https://latex.codecogs.com/png.latex?%7B%5Ccolor%7Bred%7D%20-%20%5Cfrac12%20%5C,%20%5Cnabla_x%20%5Clog%20p_t(x)%7D"> is just driving the ODE in direction where the probability density <img src="https://latex.codecogs.com/png.latex?p_t"> is small, i.e.&nbsp;it follows the negative gradient of the log-density: it is exactly trying to imitate the diffusion term <img src="https://latex.codecogs.com/png.latex?dB">.</p>
<p>What this means is that a diffusion process <img src="https://latex.codecogs.com/png.latex?dX%20=%20%5Cmu(t,X)%20%5C,%20dt%20+%20dB"> started from <img src="https://latex.codecogs.com/png.latex?X_0%20%5Csim%20p_0"> and marginal distribution <img src="https://latex.codecogs.com/png.latex?X_t%20%5Csim%20p_t(dx)"> can be imitated by an ODE process <img src="https://latex.codecogs.com/png.latex?dY/dt%20=%20%5Cmu(t,Y)%20-%20%5Cfrac12%20%5C,%20%5Cnabla%20%5Clog%20p_t(Y)"> started from <img src="https://latex.codecogs.com/png.latex?p_0">. At any time <img src="https://latex.codecogs.com/png.latex?t%3E0">, the marginal distributions of <img src="https://latex.codecogs.com/png.latex?X_t"> and <img src="https://latex.codecogs.com/png.latex?Y_t"> both exactly equal <img src="https://latex.codecogs.com/png.latex?p_t(dx)">.</p>
</section>
<section id="the-diffusion-ode-trick-application-to-ddpm" class="level3">
<h3 class="anchored" data-anchor-id="the-diffusion-ode-trick-application-to-ddpm">The diffusion-ODE trick: application to DDPM</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="sine_reverse_ODE.mp4" class="img-fluid" style="width:100.0%" controls=""><a href="sine_reverse_ODE.mp4">Video</a></video></p>
</figure>
</div>
<p>Consider a DDPM with forward dynamics given by an Ornstein-Uhlenbeck (OU) process</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ad%5Coverrightarrow%7BX%7D%20=%20-%5Cfrac12%20%5C,%20%5Coverrightarrow%7BX%7D%20%5C,%20dt%20+%20dB%0A"></p>
<p>and initial condition <img src="https://latex.codecogs.com/png.latex?X_0%20%5Csim%20p_0%20%5Cequiv%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D">. As explained in these <a href="../../notes/DDPM/DDPM.html">notes</a>, it is relatively straightforward to estimate the score function</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BS%7D(t,x_t)%20%5C;%20=%20%5C;%20%5Cnabla%20%5Clog%20p_t(x_t)%0A"></p>
<p>from data. This means that the forward OU process can be replaced by the forward ODE</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%7D%7Bdt%7D%20%5Coverrightarrow%7BY_t%7D%20=%20-%5Cfrac12%20%5C,%20%5Coverrightarrow%7BY_t%7D%20%20-%20%5Cfrac12%20%5Cmathcal%7BS%7D(t,%5Coverrightarrow%7BY_t%7D)%20=%20F(t,Y_t)%0A"></p>
<p>with <img src="https://latex.codecogs.com/png.latex?F(t,x)%20=%20-%5Ctfrac%7B1%7D%7B2%7D%20x%20-%5Ctfrac%7B1%7D%7B2%7D%20%5Cmathcal%7BS%7D(t,x)">. Similarly, the reverse diffusion (i.e.&nbsp;the “denoising” diffusion) defined as <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_s%20=%20X_%7BT-s%7D"> follows the dynamics</p>
<p><img src="https://latex.codecogs.com/png.latex?d%5Coverleftarrow%7BX%7D_s%20=%20%5Cfrac12%20%5Coverleftarrow%7BX%7D_s%20%5C,%20ds%20+%20%5Cmathcal%7BS%7D(T-s,%5Coverleftarrow%7BX%7D_s)%20+%20dW."></p>
<p>As described for the first time in the beautiful article <span class="citation" data-cites="song2020score">(Song et al. 2020)</span>, the diffusion-ODE trick now shows that the denoising diffusion can be replaced by a denoising ODE with dynamics</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cfrac%7Bd%7D%7Bds%7D%20%5Coverleftarrow%7BY_s%7D%20&amp;=%20%5Cfrac12%20%5Coverleftarrow%7BY_s%7D%20%5C,%20ds%20+%20%5Cmathcal%7BS%7D(T-s,%5Coverleftarrow%7BY_s%7D)%20-%20%5Cfrac12%20%5C,%20%5Cmathcal%7BS%7D(T-s,%5Coverleftarrow%7BX%7D_s)%5C%5C%0A&amp;=%0A%5Cfrac12%20%5Coverleftarrow%7BY_s%7D%20%5C,%20ds%20+%20%5Cfrac12%20%5C,%20%5Cmathcal%7BS%7D(T-s,%5Coverleftarrow%7BY_s%7D)%20=%20-F(T-s,%20%5Coverleftarrow%7BY_s%7D).%0A%5Cend%7Balign%7D%0A"></p>
<p>Interestingly [and I do not know whether there was an obvious way of seeing this from the start], this shows that the forward and backward ODE are actually the same but run forward and backward in time. They corresponds to the ODE described by the vector field</p>
<p><span id="eq-vector-field"><img src="https://latex.codecogs.com/png.latex?%0AF(t,x)%20%5C;%20=%20%5C;%20-%5Cfrac%7B1%7D%7B2%7D%20x%20-%5Cfrac%7B1%7D%7B2%7D%20%5Cmathcal%7BS%7D(t,x).%0A%5Ctag%7B2%7D"></span></p>
<p>The animation belows display the denoising ODE and the associated vector field Equation&nbsp;2.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="sine_reverse_vector_field.mp4" class="img-fluid" style="width:100.0%" controls=""><a href="sine_reverse_vector_field.mp4">Video</a></video></p>
</figure>
</div>
</section>
<section id="likelihood-computation" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-computation">Likelihood computation</h2>
<p>With the diffusion-ODE trick, we have just seen that it is possible to build a vector fields <img src="https://latex.codecogs.com/png.latex?F%5B0,T%5D%20%5Ctimes%20%5Cmathbb%7BR%7D%5Ed%20%5Cto%20%5Cmathbb%7BR%7D%5Ed"> such that the <em>forward</em> ODE</p>
<p><span id="eq-forward-ODE"><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%7D%7Bdt%7D%20%5Coverrightarrow%7BY_t%7D%20=%0AF(t,%5Coverrightarrow%7BY_t%7D)%0A%5Cqquad%20%5Ctextrm%7Binitialized%20at%7D%20%5Cqquad%0A%5Coverrightarrow%7BY%7D_0%20%5Csim%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D%0A%5Ctag%7B3%7D"></span></p>
<p>and the <em>backward</em> ODE defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%7D%7Bds%7D%20%5Coverleftarrow%7BY_s%7D%20=%0A-F(T-s,%5Coverleftarrow%7BY_s%7D)%0A%5Cqquad%20%5Ctextrm%7Binitialized%20at%7D%20%5Cqquad%0A%5Coverleftarrow%7BY%7D_0%20%5Csim%20%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%0A"></p>
<p>are such that <img src="https://latex.codecogs.com/png.latex?%5Coverrightarrow%7BY%7D_T%20%5Capprox%20%5Cpi_%7B%5Cmathrm%7Bref%7D%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BY%7D_T%20%5Capprox%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D">.</p>
<p>In general, consider a vector field <img src="https://latex.codecogs.com/png.latex?F(t,x)"> and a bunch of particles distributed according to a distribution <img src="https://latex.codecogs.com/png.latex?p_t"> at time <img src="https://latex.codecogs.com/png.latex?t">. If each particle follows the vector field for an amount of time <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Cll%201">, the particles that were in the vicinity of some <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathbb%7BR%7D%5Ed"> at time <img src="https://latex.codecogs.com/png.latex?t"> end up in the vicinity of <img src="https://latex.codecogs.com/png.latex?x%20+%20F(x,t)%20%5C,%20%5Cdelta"> at time <img src="https://latex.codecogs.com/png.latex?t+%5Cdelta">. At the same time, a volume element <img src="https://latex.codecogs.com/png.latex?dx"> around <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathbb%7BR%7D%5Ed"> gets stretch by a factor <img src="https://latex.codecogs.com/png.latex?1+%5Cdelta%20%5C,%20%5Cmathop%7B%5Cmathrm%7BTr%7D%7D%5B%5Cmathop%7B%5Cmathrm%7B%5Cmathrm%7BJac%7D%7D%7DF(x,t)%5D%20=%201%20+%20%5Cdelta%20%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7DF(x,t)"> while following the vector field <img src="https://latex.codecogs.com/png.latex?F">, which means that the density of particles at time <img src="https://latex.codecogs.com/png.latex?t+%5Cdelta"> and around <img src="https://latex.codecogs.com/png.latex?x%20+%20F(x,t)%20%5C,%20%5Cdelta"> equals <img src="https://latex.codecogs.com/png.latex?p_t(x)%20/%20%5B1%20+%20%5Cdelta%20%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7DF(x,t)%5D">. In other words <img src="https://latex.codecogs.com/png.latex?%5Clog%20p_%7Bt+%5Cdelta%7D(x%20+%20F(x,t)%20%5C,%20%5Cdelta)%20%5Capprox%20%5Clog%20p_t(x)%20-%20%5Cdelta%20%5C,%20%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7DF(x,t)">. This means that if we follows a trajectory of <img src="https://latex.codecogs.com/png.latex?%5Ctfrac%7Bd%7D%7Bdt%7D%20X_t%20=%20F(t,X_t)"> one gets</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Clog%20p_T(X_T)%20=%20%5Clog%20p_0(X_0)%20-%20%5Cint_%7Bt=0%7D%5E%7BT%7D%20%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7DF(X_t,t)%20%5C,%20dt.%0A"></p>
<p>That is the Lagrangian description of the density <img src="https://latex.codecogs.com/png.latex?p_t"> of particles. Indeed, one could directly get this identity by differentiating <img src="https://latex.codecogs.com/png.latex?p_t(X_t)"> with respect to time while using the continuity Equation&nbsp;1. When applied to the DDPM, this gives a way to assign likelihood the data samples, namely</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Clog%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D(x)%20=%20%5Clog%20%5Cpi_%7B%5Cmathrm%7Bref%7D%7D(%5Coverrightarrow%7BY_T%7D)%20+%20%5Cint_%7Bt=0%7D%5E%7BT%7D%20%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7DF(t,%20%5Coverrightarrow%7BY_t%7D)%5C,%20dt%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Coverrightarrow%7BY_t%7D"> is trajectory of the forward ODE Equation&nbsp;3 initialized as <img src="https://latex.codecogs.com/png.latex?%5Coverrightarrow%7BY_0%7D%20=%20x">. Note that in high-dimensional setting, it may be computationally expensive to compute the divergence term <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7DF(t,%20%5Coverrightarrow%7BY_t%7D)"> since it typically is <img src="https://latex.codecogs.com/png.latex?d"> times slower that a gradient computation; for this reason, it is often advocated to use the Hutchinson trace estimator to get an unbiased estimate of it at a much lower computational cost.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-song2020score" class="csl-entry">
Song, Yang, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2020. <span>“Score-Based Generative Modeling Through Stochastic Differential Equations.”</span> <em><span>ICLR</span> 2021</em>. <a href="https://arxiv.org/abs/2011.13456">https://arxiv.org/abs/2011.13456</a>.
</div>
</div></section></div> ]]></description>
  <category>DDPM</category>
  <guid>https://alexxthiery.github.io/notes/DDPM_deterministic/DDPM_deterministic.html</guid>
  <pubDate>Sat, 01 Jul 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Denoising Diffusion Probabilistic Models (DDPM)</title>
  <link>https://alexxthiery.github.io/notes/DDPM/DDPM.html</link>
  <description><![CDATA[ 




<section id="setting-goals" class="level3">
<h3 class="anchored" data-anchor-id="setting-goals">Setting &amp; Goals</h3>
<p>Consider <img src="https://latex.codecogs.com/png.latex?N"> samples <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D%5Cequiv%20%5C%7Bx_i%5C%7D_%7Bi=1%7D%5EN"> in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5ED"> from an unknown data distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D(dx)">. We would like to build an mechanism that can generate other samples from this data distribution. Implicitly, this means that we will be fitting a statistical model to this finite set of samples and have an algorithmic procedure to generate samples from the fitted probabilistic model. This type of models that directly define a stochastic procedure that generates data are called <em>implicit probabilistic models</em> in the ML literature.</p>
</section>
<section id="ornsteinuhlenbeck-noising-process" class="level3">
<h3 class="anchored" data-anchor-id="ornsteinuhlenbeck-noising-process">Ornstein–Uhlenbeck Noising process</h3>
<p>DDPMs work as follows. Consider a diffusion process <img src="https://latex.codecogs.com/png.latex?%5C%7B%20X_t%20%5C%7D_%7Bt=0%7D%5ET"> that starts from the data distribution <img src="https://latex.codecogs.com/png.latex?p_0(dx)%20%5Cequiv%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D(dx)"> at time <img src="https://latex.codecogs.com/png.latex?t=0">. The notation <img src="https://latex.codecogs.com/png.latex?p_t(dx)"> refers to the marginal distribution of the diffusion at time <img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20t%20%5Cleq%20T">. Assume furthermore that at time <img src="https://latex.codecogs.com/png.latex?t=T">, the marginal distribution is (very close to) a reference distribution <img src="https://latex.codecogs.com/png.latex?p_T(dx)%20=%20%5Cpi_%7B%5Cmathrm%7Bref%7D%7D(dx)"> that is straightforward to sample from. Typically, <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bref%7D%7D(dx)"> is an isotropic Gaussian distribution. This diffusion process is often called the <em>noising</em> process since it transform the data distribution into a reference measure that can be thought of as “pure noise”. It is often chosen as an <a href="https://en.wikipedia.org/wiki/Ornstein–Uhlenbeck_process">Ornstein–Uhlenbeck</a> (OU) diffusion,</p>
<p><span id="eq-OU"><img src="https://latex.codecogs.com/png.latex?%0AdX%20=%20-%20%5Cfrac12%20X%20%5C,%20dt%20+%20dW.%0A%5Ctag%7B1%7D"></span></p>
<p>This diffusion is reversible with respect to, and quickly converges to, the reference distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%20=%20%5Cmathcal%7BN%7D(0,%20I)"> and has the good taste of having simple transition densities: the law of <img src="https://latex.codecogs.com/png.latex?X_%7Bt+s%7D"> given that <img src="https://latex.codecogs.com/png.latex?X_t%20=%20x_t"> is the same as <img src="https://latex.codecogs.com/png.latex?e%5E%7B-s/2%7D%20x_t%20+%20%5Csqrt%7B1-e%5E%7B-s%7D%7D%20%5C,%20%5Cmathbf%7Bn%7D">, which we write as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha_s%20x%20+%20%5Csigma_s%20%5C,%20%5Cmathbf%7Bn%7D%5Cqquad%20%5Ctext%7Bwith%7D%20%5Cqquad%0A%5Cleft%5C%7B%0A%5Cbegin%7Baligned%7D%0A%5Calpha_s%20&amp;=%20%5Csqrt%7B1-%5Csigma_s%5E2%7D%5C%5C%0A%5Csigma%5E2_s%20&amp;=%201-e%5E%7B-s%7D%0A%5Cend%7Baligned%7D%0A%5Cright.%0A"></p>
<p>for isotropic Gaussian noise term <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bn%7D%5Csim%20%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%20=%20%5Cmathcal%7BN%7D(0,I)">. We have:</p>
<p><span id="eq-OU-forward"><img src="https://latex.codecogs.com/png.latex?%0AF(s,x,y)%20%5Cequiv%20%5Cmathop%7B%5Cmathrm%7BP%7D%7D(X_%7Bt+s%7D%20%5Cin%20dy%20%5C,%20%7C%20%5C,%20X_t%20=%20x%20)%0A%5C;%20%5Cpropto%20%5C;%0A%5Cexp%20%7B%5Cleft%5C%7B%20-%5Cfrac%7B(y%20-%20%5Calpha_s%20%5C,%20x)%5E2%7D%7B2%20%5C,%20%5Csigma%5E2_s%7D%20%5Cright%5C%7D%7D%20.%0A%5Ctag%7B2%7D"></span></p>
<p>where the notation <img src="https://latex.codecogs.com/png.latex?F(s,x,y)"> designates the forward transition from <img src="https://latex.codecogs.com/png.latex?x"> to <img src="https://latex.codecogs.com/png.latex?y"> in “<img src="https://latex.codecogs.com/png.latex?s">” amount of time. This also means that one can directly generate samples from <img src="https://latex.codecogs.com/png.latex?p_t(dx)"> by first choosing a data samples <img src="https://latex.codecogs.com/png.latex?x_i"> from the data distribution <img src="https://latex.codecogs.com/png.latex?p_%7B%5Cmathrm%7Bdata%7D%7D%20%5Cequiv%20p_0"> and blend it with noise by setting <img src="https://latex.codecogs.com/png.latex?x_i%5E%7B(t)%7D%20=%20%5Calpha_t%20%5C,%20x_i%20+%20%5Csigma_t%20%5C,%20%5Cmathbf%7Bn%7D">.</p>
</section>
<section id="the-reverse-diffusion" class="level3">
<h3 class="anchored" data-anchor-id="the-reverse-diffusion">The reverse diffusion</h3>
<p>In order to generate samples from the data distribution, the DDPM strategy consists in sampling from the Gaussian reference measure <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bref%7D%7D"> at time <img src="https://latex.codecogs.com/png.latex?t=T"> and simulate the OU process backward in time. In other words, one would like to simulate from the <em>reverse process</em> <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_t"> defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_s%20=%20X_%7BT-s%7D."></p>
<p>In other words, the reverse process is distributed as <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_0%20%5Csim%20%5Cpi_%7B%5Cmathrm%7Bref%7D%7D"> at time <img src="https://latex.codecogs.com/png.latex?t=0"> and, crucially, we have that <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_T%20%5Csim%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D">. Furthermore, and as explained in this <a href="../../notes/reverse_and_tweedie/reverse_and_tweedie.html">note</a>, the reverse diffusion follows the dynamics</p>
<p><span id="eq-OU-rev"><img src="https://latex.codecogs.com/png.latex?%0Ad%5Coverleftarrow%7BX%7D_t%20=%20%7B%5Ccolor%7Bred%7D%20+%20%7D%5Cfrac12%20%5Coverleftarrow%7BX%7D_t%20%5C,%20dt%0A%5C;%20%7B%5Ccolor%7Bred%7D%20+%20%5Cnabla%20%5Clog%20p_%7BT-t%7D(%5Coverleftarrow%7BX%7D_t)%20%5C,%20dt%7D%20%5C;%0A+%20dB%0A%5Ctag%7B3%7D"></span></p>
<p>where <img src="https://latex.codecogs.com/png.latex?B"> is another Wiener process. I have used the notation <img src="https://latex.codecogs.com/png.latex?B"> to emphasize that there is no link between this Wiener process and the one used to simulate the forward process. Note that if the initial data distribution <img src="https://latex.codecogs.com/png.latex?p_0%20%5Cequiv%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D"> were equal to the reference measure <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bref%7D%7D">, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?p_0%20=%20p_T%20=%20%5Cpi_%7B%5Cmathrm%7Bref%7D%7D"> then it is easy to see that the reverse diffusion would have exactly teh same dynamics as the forward diffusion. In order to simulate the reverse diffusion, one needs to be able to estimate the new term <img src="https://latex.codecogs.com/png.latex?%7B%5Ccolor%7Bred%7D%5Cnabla%20%5Clog%20p_%7BT-t%7D(x)%7D"> called the <em>score</em>. If one can estimate the score, it is straightforward to simulate the reverse process <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_t"> all the way to <img src="https://latex.codecogs.com/png.latex?t=T"> and obtain samples from the data distribution.</p>
</section>
<section id="denoising-to-estimating-the-score" class="level3">
<h3 class="anchored" data-anchor-id="denoising-to-estimating-the-score">Denoising to estimating the score</h3>
<p>In practice, the score is unknown and one has to build an approximation of it</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BS%7D(t,x)%20%5C;%20%5Capprox%20%5C;%20%5Cnabla_x%20%5Clog%20p_t(x)."></p>
<p>The approximate score <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BS%7D(t,x)"> is often parametrized by a neural network. Since the forward transitions are available and</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Clog%20p_t(x)%20%5C;%20=%20%5C;%20%5Clog%20%5Cint%20%5C;%20F(t,%20x_0,%20x)%5C;%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D(d%20x_0)%0A"></p>
<p>the analytical expression of <img src="https://latex.codecogs.com/png.latex?F(t,%20x_0,%20x)"> given in Equation&nbsp;2 readily gives that</p>
<p><span id="eq-score-denoising"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_x%20%5Clog%20p_t(x)%20%5C;%20=%20%5C;%20-%5Cfrac%7Bx%20-%20%5Calpha_t%20%5C,%20%5Cwidehat%7Bx%7D_0(x,t)%7D%7B%5Csigma_t%5E2%7D%0A%5Ctag%7B4%7D"></span></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0(x,t)"> is a “denoising” estimate of the initial position <img src="https://latex.codecogs.com/png.latex?x_0"> given a noisy estimate <img src="https://latex.codecogs.com/png.latex?X_t=x"> at time <img src="https://latex.codecogs.com/png.latex?t">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%7Bx%7D_0(x,t)%20%5C;%20=%20%5C;%20%5Cmathop%7B%5Cmathrm%7BE%7D%7D%5BX_0%20%20%5C;%20%5Cmid%20%5C;%20X_t%20=%20x%5D.%0A"></p>
<p>For simplifying notation, I will often write <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0(x_t,%20t)"> as <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0(x_t)"> when it is clear that <img src="https://latex.codecogs.com/png.latex?x_t"> is a sample obtained at time <img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20t%20%5Cleq%20T">. Equation&nbsp;4 means that to estimate the score, one only needs to train a denoising function</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%7Bx%7D_0(%5Ccdots):%20%5B0,T%5D%20%5Ctimes%20%5Cmathbb%7BR%7D%5Ed%20%5Cto%20%5Cmathbb%7BR%7D%5Ed.%0A"></p>
<p>It is a simple regression problem: take a bunch of pairs <img src="https://latex.codecogs.com/png.latex?(X_0,%20X_t)"> that can be generated as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AX_0%20%5Csim%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D%0A%5Cqquad%20%5Ctextrm%7Band%7D%20%5Cqquad%0AX_t%20=%20%5Calpha_t%20X_0%20+%20%5Csigma_t%20%5C,%20%5Cmathbf%7Bn%7D%0A"></p>
<p>with <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bn%7D%5Csim%20%5Cmathcal%7BN%7D(0,I)"> and minimize the Mean Squared Error (MSE) loss, i.e.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathop%7B%5Cmathrm%7BE%7D%7D%5C%7CX_0%20-%20%5Cwidehat%7Bx%7D_0(t,%20X_t)%5C%7C%5E2,%0A"></p>
<p>with stochastic gradient descent or any other stochastic optimization procedure. The score is then defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BS%7D(t,x)%20%5C;%20=%20%5C;%20-%5Cfrac%7Bx%20-%20%5Calpha_t%20%5C,%20%5Cwidehat%7Bx%7D_0(t,x)%7D%7B%5Csigma%5E2_t%7D.%0A"></p>
</section>
<section id="denoiser-practical-parametrization-and-training" class="level3">
<h3 class="anchored" data-anchor-id="denoiser-practical-parametrization-and-training">Denoiser: practical parametrization and training</h3>
<p>In practice, it may not be efficient, or stable, to try to directly parametrize the denoiser <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0(%5Ccdots)"> with a neural network and simply descend the loss</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathop%7B%5Cmathrm%7BE%7D%7D%5C%7CX_0%20-%20%5Cwidehat%7Bx%7D_0(t,%20X_t)%5C%7C%5E2.%0A"></p>
<p>For example, for <img src="https://latex.codecogs.com/png.latex?t%20%5Capprox%200">, we have that <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0(t,X_t)%20%5Capprox%20X_t%20%5Capprox%20X_0"> so that it is very easy to reconstruct <img src="https://latex.codecogs.com/png.latex?X_0"> from <img src="https://latex.codecogs.com/png.latex?X_t">. On the contrary, for large <img src="https://latex.codecogs.com/png.latex?t">, there is almost no information contained within <img src="https://latex.codecogs.com/png.latex?X_t"> to reconstruct <img src="https://latex.codecogs.com/png.latex?X_0">. This means that the typical value of the loss depends widely on <img src="https://latex.codecogs.com/png.latex?t">, which makes it difficult to optimize: with this parametrization, since large values of the loss will be typically concentrated to large values of <img src="https://latex.codecogs.com/png.latex?t">, the denoiser will not be accurate for <img src="https://latex.codecogs.com/png.latex?t%20%5Capprox%200">, leading to sub-optimal results. Since <img src="https://latex.codecogs.com/png.latex?X_t%20=%20%5Calpha_t%20%5C,%20X_0%20+%20%5Csigma_t%20%5C,%20%5Cmathbf%7Bn%7D">, one can defined the Signal-to-Noise-Ratio as</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BSNR%7D(t)%20=%20%5Cfrac%7B%5Calpha_t%7D%7B%5Csigma_t%7D"></p>
<p>and, in order to normalize by the reconstruction difficulty, it makes more sense to train a denoiser by minimizing the loss:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathop%7B%5Cmathrm%7BE%7D%7D%20%7B%5Cleft%5B%20%20%5Cmathrm%7BSNR%7D%5E2(t)%20%5Ctimes%20%5C%7CX_0%20-%20%5Cwidehat%7Bx%7D_0(t,%20X_t)%5C%7C%5E2%20%20%5Cright%5D%7D%20.%0A"></p>
<p>It turns out that it is entirely equivalent to minimizing the loss</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathop%7B%5Cmathrm%7BE%7D%7D%20%7B%5Cleft%5B%20%20%5C%7C%20%5Cmathbf%7Bn%7D-%20%5Cwidehat%7B%5Cmathbf%7Bn%7D%7D(t,%20X_t)%5C%7C%5E2%20%20%5Cright%5D%7D%20.%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?X_t%20=%20%5Calpha_t%20%5C,%20X_0%20+%20%5Csigma_t%20%5C,%20%5Cmathbf%7Bn%7D"> while the denoiser <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0(%5Cldots)"> and noise estimator <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathbf%7Bn%7D%7D(%5Cldots)"> are parametrized so that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AX_t%20=%20%5Calpha_t%20%5C,%20%5Cwidehat%7Bx%7D_0(t,%20X_t)%20+%20%5Csigma_t%20%5C,%20%5Cwidehat%7B%5Cmathbf%7Bn%7D%7D(t,X_t).%0A"></p>
<p>That is one of the reasons why most of the papers on DDPM are parametrizing the denoiser <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0(%5Cldots)"> by building instead a “noise estimator” <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathbf%7Bn%7D%7D(%5Cldots)"> with a neural network and setting</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%7Bx%7D_0(t,X_t)%20=%20%5Cfrac%7BX_t%20-%20%5Csigma_t%20%5C,%20%5Cwidehat%7B%5Cmathbf%7Bn%7D%7D(t,X_T)%7D%7B%5Calpha_t%7D.%0A"></p>
<p>Since <img src="https://latex.codecogs.com/png.latex?%5Calpha_t%20%5Cto%201"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma_t%20%5Cto%200"> for <img src="https://latex.codecogs.com/png.latex?t%20%5Cll%201">, this also implicitly ensures that <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0(t,X_t)%20%5Capprox%20X_t"> for <img src="https://latex.codecogs.com/png.latex?t%20%5Cll%201">, as required.</p>
<!-- This also means that the estimated score function is given by

$$
\cS(t,x) 
\; = \; -\frac{x - \alpha_t \, \widehat{x}_0(t,x)}{\sigma^2_t} \; = \; - \frac{\widehat{\bfn}(t,x)}{\sigma_t}.
$$ -->
</section>
<section id="the-denoising-diffusion" class="level3">
<h3 class="anchored" data-anchor-id="the-denoising-diffusion">The “denoising” diffusion</h3>
<p>Once the denoiser <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0(%5Cldots)"> has been trained, the reverse diffusion defined <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_s%20=%20X_%7BT-s%7D"> as to be simulated. Plugging Equation&nbsp;4 back in the expression of the dynamics of the reverse diffusion shows that</p>
<p><span id="eq-reverse-diff"><img src="https://latex.codecogs.com/png.latex?%0Ad%5Coverleftarrow%7BX%7D_s%20=%0A-%5Cfrac12%20%5C,%20%5Cfrac%7B1%7D%7B%5Ctanh((T-s)/2)%7D%20%5C,%0A%7B%5Cleft(%20%20%5Coverleftarrow%7BX%7D_s%20-%20%5Cfrac%7B%5Cwidehat%7Bx%7D_0(%5Coverleftarrow%7BX%7D_s)%7D%7B%5Ccosh((T-s)/2)%7D%20%20%5Cright)%7D%0A%5C;%20+%20%5C;%0AdB%0A%5Ctag%7B5%7D"></span></p>
<p>This dynamics is intuitive: as <img src="https://latex.codecogs.com/png.latex?s%20%5Cto%20T"> we have <img src="https://latex.codecogs.com/png.latex?%5Ccosh((T-s)/2)%20%5Cto%201"> and <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5E2%20%5Cequiv%20%5Ctanh((T-s)/2)%20%5Csim%20(T-s)/2%20%5Cto%200"> so that the dynamics is similar to</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ad%5Coverleftarrow%7BX%7D_s%20=%0A-%5Cfrac12%20%5C,%20%5Cfrac%7B1%7D%7B%5Cvarepsilon%5E2%7D%20%5C,%0A%7B%5Cleft(%20%20%5Coverleftarrow%7BX%7D_s%20-%20%5Cwidehat%7Bx%7D_0%20%5Cright)%7D%0A%5C;%20+%20%5C;%0AdB%0A"></p>
<p>which is OU process that converges quickly, i.e.&nbsp;on time-scale of order <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(%5Cvarepsilon%5E2)">, towards a Gaussian distribution with mean <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0"> (i.e.&nbsp;the denoised estimate) and variance <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5E2">.</p>
<p>To discretize the reverse dynamics Equation&nbsp;5 on a small interval <img src="https://latex.codecogs.com/png.latex?%5B%5Coverline%7Bs%7D,%20%5Coverline%7Bs%7D+%5Cdelta%5D">, one can for example consider the slightly simplified (linear) dynamics</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ad%5Coverleftarrow%7BX%7D_s%20=%0A-%5Cfrac12%20%5C,%20%5Cfrac%7B1%7D%7B%5Ctanh((T-s)/2)%7D%20%5C,%0A%7B%5Cleft(%20%20%5Coverleftarrow%7BX%7D_s%20-%20%5Cmu%20%5Cright)%7D%20.%0A%5C;%20+%20%5C;%0AdB%0A"></p>
<p>Here, <img src="https://latex.codecogs.com/png.latex?%5Cmu%20=%20%5Cwidehat%7Bx%7D_0(%5Coverleftarrow%7BX%7D_%7B%5Coverline%7Bs%7D%7D)%20/%20%5Ccosh((T-%5Coverline%7Bs%7D)/2)"> with <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_%7B%5Coverline%7Bs%7D%7D%20=%20%5Coverleftarrow%7Bx%7D_%7B%5Coverline%7Bs%7D%7D">. Algebra gives that, conditioned upon <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_%7B%5Coverline%7Bs%7D%7D%20=%20%5Coverleftarrow%7Bx%7D_%7B%5Coverline%7Bs%7D%7D">, we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%0A%5Cbegin%7Baligned%7D%0A%5Cmathop%7B%5Cmathrm%7BE%7D%7D%5B%20%5Coverleftarrow%7BX%7D_%7B%5Coverline%7Bs%7D%20+%20%5Cdelta%7D%20%5D%0A%5Cquad%20&amp;=%20%5Cquad%20%5Cmu%20+%20%5Clambda%20%5C,%20(%5Coverleftarrow%7Bx%7D_%7B%5Coverline%7Bs%7D%7D%20-%20%5Cmu)%5C%5C%0A%5Cmathop%7B%5Cmathrm%7BVar%7D%7D%5B%20%5Coverleftarrow%7BX%7D_%7B%5Coverline%7Bs%7D%20+%20%5Cdelta%7D%20%5D%0A%5Cquad%20&amp;=%20%5Cquad%0A%5Ctanh%20%7B%5Cleft(%20%5Cfrac%7BT-%5Coverline%7Bs%7D-%5Cdelta%7D%7B2%7D%20%5Cright)%7D%20%20%5C,%20(1-%5Clambda%5E2)%0A%5Cend%7Baligned%7D%0A%5Cright.%0A"></p>
<p>where the coefficient <img src="https://latex.codecogs.com/png.latex?0%3C%5Clambda%3C1"> is given by</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Clambda%20=%20%5Cfrac%7B%5Csinh(T-%5Coverline%7Bs%7D-%5Cdelta)%7D%7B%5Csinh(T-%5Coverline%7Bs%7D)%7D.%0A"></p>
<p>This discretization is more stable and efficient than a naive Euler-Maruyama approach since the drift term can get large as <img src="https://latex.codecogs.com/png.latex?s%20%5Cto%20T">. WIth the above discretization, one can easily simulate the reverse diffusion on <img src="https://latex.codecogs.com/png.latex?%5B0,T%5D"> and generate approximate samples from <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D">.</p>
<p>In the animation below, the method described in these notes was used. This very small example trains in less than a minute on an old laptop without a GPU. The noise estimator was parametrized with an MLP with <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Belu%7D(%5Cldots)"> non-linearity and two hidden-layers with size <img src="https://latex.codecogs.com/png.latex?H=128">. It was very useful to add a few Fourier features as input of the network, I could not make this work properly without.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="sine_reverse.mp4" class="img-fluid" style="width:100.0%" controls=""><a href="sine_reverse.mp4">Video</a></video></p>
</figure>
</div>
<p>The literature on DDPM is <a href="https://github.com/heejkoo/Awesome-Diffusion-Models">enormous and still growing</a>!</p>


</section>

 ]]></description>
  <category>DDPM</category>
  <guid>https://alexxthiery.github.io/notes/DDPM/DDPM.html</guid>
  <pubDate>Sat, 01 Jul 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Natural Gradients</title>
  <link>https://alexxthiery.github.io/notes/stats_basics/natural_gradients.html</link>
  <description><![CDATA[ 







 ]]></description>
  <category>gradient</category>
  <guid>https://alexxthiery.github.io/notes/stats_basics/natural_gradients.html</guid>
  <pubDate>Sun, 11 Jun 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Reverse diffusions, Score &amp; Tweedie</title>
  <link>https://alexxthiery.github.io/notes/reverse_and_tweedie/reverse_and_tweedie.html</link>
  <description><![CDATA[ 




<section id="reversing-a-diffusion" class="level2">
<h2 class="anchored" data-anchor-id="reversing-a-diffusion">Reversing a diffusion</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="reverse_cropped.mp4" class="img-fluid" style="width:100.0%" controls=""><a href="reverse_cropped.mp4">Video</a></video></p>
</figure>
</div>
<p>Imagine a scalar diffusion process <img src="https://latex.codecogs.com/png.latex?%5C%7BX_t%5C%7D_%7Bt=0%7D%5ET"> defined on the interval <img src="https://latex.codecogs.com/png.latex?%5B0,T%5D">,</p>
<p><img src="https://latex.codecogs.com/png.latex?dX%20=%20%5Cmu(X)%20%5C,%20dt%20+%20%5Csigma%20%5C,%20dW"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmu(X)"> is the drift term, <img src="https://latex.codecogs.com/png.latex?%5Csigma"> is the diffusion coefficient, and <img src="https://latex.codecogs.com/png.latex?dW"> is a Wiener process. Denote the distribution of this process at time <img src="https://latex.codecogs.com/png.latex?t"> (<img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20t%20%5Cleq%20T">) as <img src="https://latex.codecogs.com/png.latex?p_t(dx)"> with an initial distribution of <img src="https://latex.codecogs.com/png.latex?p_0(dx)">. Now, what happens if we reverse time and examine the process backward? In other words, consider the time-reversed process <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D"> defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_s%20=%20X_%7BT-s%7D."></p>
<p>Intuitively, the process <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D"> is also a diffusion on the interval <img src="https://latex.codecogs.com/png.latex?%5B0,%20T%5D">, but with an initial distribution <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_0%20%5Csim%20p_T(dx)">. To gain intuition, consider an Euler discretization of the forward process:</p>
<p><span id="eq-rev-wrong"><img src="https://latex.codecogs.com/png.latex?X_%7Bt+%5Cdelta%7D%20=%20X_%7Bt%7D%20+%20%5Cmu(X_t)%5C,%20%5Cdelta%20+%20%5Csigma%20%5C,%20%5Csqrt%7B%5Cdelta%7D%20%5C,%20%5Cmathbf%7Bn%7D%20%5Ctag%7B1%7D"></span></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bn%7D%5Csim%20%5Cmathcal%7BN%7D(0,1)"> represents a noise term independent from <img src="https://latex.codecogs.com/png.latex?X_%7Bt%7D">, and <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Cll%201"> is a time increment. Re-arranging terms and making the approximation <img src="https://latex.codecogs.com/png.latex?%5Cmu(X_t)%20%5Capprox%20%5Cmu(X_%7Bt+%5Cdelta%7D)"> gives that</p>
<p><span id="eq-rev-euler-wrong"><img src="https://latex.codecogs.com/png.latex?X_%7Bt%7D%20%5Capprox%20X_%7Bt+%5Cdelta%7D%20-%20%5Cmu(X_%7Bt+%5Cdelta%7D)%5C,%20%5Cdelta%20+%20%5Csigma%20%5C,%20%5Csqrt%7B%5Cdelta%7D%20%5C,%20(-%5Cmathbf%7Bn%7D).%20%5Ctag%7B2%7D"></span></p>
<p>This seems to suggest that the time-reversed process follows the dynamics <img src="https://latex.codecogs.com/png.latex?d%5Coverleftarrow%7BX%7D%20=%20-%5Cmu(%5Coverleftarrow%7BX%7D)%20%5C,%20dt%20+%20%5Csigma%20%5C,%20dW"> started from <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_0%20%5Csim%20p_T(dx)">. However, this conclusion is incorrect because this would suggest that the time-reversed of a standard Brownian motion (where <img src="https://latex.codecogs.com/png.latex?%5Cmu(x)%20%5Cequiv%200">) starting at zero is also a Brownian motion starting at <img src="https://latex.codecogs.com/png.latex?p_T(dx)%20=%20%5Cmathcal%7BN%7D(0,T)">, which is clearly not the case. The flaw in this argument lies in assuming that the noise term <img src="https://latex.codecogs.com/png.latex?Z"> is independent of <img src="https://latex.codecogs.com/png.latex?X_%7Bt+%5Cdelta%7D">, which is not true, rendering the Euler discretization argument invalid.</p>
<p>Deriving the dynamics of the backward process in a rigorous manner is not straightforward <span class="citation" data-cites="anderson1982reverse">(Anderson 1982)</span> <span class="citation" data-cites="haussmann1986time">(Haussmann and Pardoux 1986)</span>. What follows is a heuristic derivation that proceeds by estimating the mean and variance of <img src="https://latex.codecogs.com/png.latex?X_%7Bt%7D"> given <img src="https://latex.codecogs.com/png.latex?X_%7Bt+%5Cdelta%7D%20=%20x_%7Bt+%5Cdelta%7D">, assuming <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Cll%201">. Here, <img src="https://latex.codecogs.com/png.latex?x_%7Bt+%5Cdelta%7D"> is treated as a fixed and constant value, and we are only interested in the conditional distribution of <img src="https://latex.codecogs.com/png.latex?X_t"> given <img src="https://latex.codecogs.com/png.latex?x_%7Bt+%5Cdelta%7D">. Bayes’ law gives</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(X_t%20%5Cin%20dx%20%7C%20x_%7Bt+%5Cdelta%7D)%20%5Cpropto%20p_t(x)%20%5C,%20%5Cexp%5Cleft%5C%7B%20-%5Cfrac%7B%5Cleft(%20x_%7Bt+%5Cdelta%7D%20-%20%5Bx%20+%20%5Cmu(x)%20%5C,%20%5Cdelta%5D%20%5Cright)%5E2%7D%7B2%20%5Csigma%5E2%20%5C,%20%5Cdelta%7D%20%5Cright%5C%7D,"></p>
<p>where the exponential term corresponds to the transition of the forward diffusion for <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Cll%201">. Using the 1st order approximation</p>
<p><img src="https://latex.codecogs.com/png.latex?p_t(x)%20%5Capprox%20p_t(x_%7Bt+%5Cdelta%7D)%20%5C,%5Cexp%5Cleft(%20%5Clangle%20%5Cnabla%20%5Clog%20p_t(x_%7Bt+%5Cdelta%7D),%20(x-x_%7Bt+%5Cdelta%7D)%5Crangle%5Cright),"></p>
<p>eliminating multiplicative constants and higher-order error terms, we obtain:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(X_t%20%5Cin%20dx%20%7C%20x_%7Bt+%5Cdelta%7D)%0A%5Cpropto%0A%5Cexp%5Cleft%5C%7B%20-%5Cfrac%7B%5Cleft(%20x%20-%20%5Bx_%7Bt+%5Cdelta%7D%20-%20%5Cmu(x_%7Bt+%5Cdelta%7D)%20%5Cdelta%0A+%20%7B%5Ccolor%7Bred%7D%20%5Csigma%5E2%20%5C,%20%5Cnabla%20%5Clog%20p_t(x_%7Bt+%5Cdelta%7D)%20%5C,%20%5Cdelta%7D%20%5D%20%5Cright)%5E2%7D%7B2%20%5Csigma%5E2%20%5C,%20%5Cdelta%7D%20%5Cright%5C%7D."></p>
<p>For <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Cll%201">, this is transition of the reverse diffusion</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ad%5Coverleftarrow%7BX%7D_t%20=%20-%5Cmu(%5Coverleftarrow%7BX%7D_t)%20%5C,%20dt%20%5C;%20+%20%7B%5Ccolor%7Bred%7D%20%5Csigma%5E2%20%5C,%20%5Cnabla%20%5Clog%20p_%7BT-t%7D(%5Coverleftarrow%7BX%7D_t)%20%5C,%20dt%7D%20%5C;%20+%20%5C;%20%5Csigma%20%5C,%20dB.%0A"></p>
<p>The notation <img src="https://latex.codecogs.com/png.latex?B"> is used to emphasize that this Brownian motion is distinct from the one used in the forward diffusion. The additional drift term, denoted as <img src="https://latex.codecogs.com/png.latex?%7B%5Ccolor%7Bred%7D%20%5Csigma%5E2%20%5C,%20%5Cnabla%20%5Clog%20p_t(%5Coverleftarrow%7BX%7D)%7D">, is intuitive: it pushes the reverse diffusion toward regions where the forward diffusion spent a significant amount of time, i.e., where <img src="https://latex.codecogs.com/png.latex?p_t(dx)"> is large. The popular “denoising diffusion models” <span class="citation" data-cites="ho2020denoising">(Ho, Jain, and Abbeel 2020)</span> can be seen as discretizations of this backward process, employing various techniques to estimate the additional drift term from data.</p>
</section>
<section id="denoising-score-matching-tweedie-formula" class="level2">
<h2 class="anchored" data-anchor-id="denoising-score-matching-tweedie-formula">Denoising Score Matching &amp; Tweedie formula</h2>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://en.wikipedia.org/wiki/Maurice_Tweedie"><img src="https://alexxthiery.github.io/notes/reverse_and_tweedie/maurice_tweedie.gif" class="img-fluid figure-img" style="width:35.0%"></a></p>
<figcaption class="figure-caption">Maurice Tweedie (1919–1996)</figcaption>
</figure>
</div>
</div>
<p>The previous section shows that a quantity often referred to as the “score” in machine learning (ML) literature, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BS%7D_t(x)%20%5Cequiv%20%5Cnabla_x%20%5Clog%20p_t(x)">, naturally arises when a diffusion process runs backward in time. Interestingly, it is worth noting that since the beginning of times statisticians have been using the term <a href="https://en.wikipedia.org/wiki/Score_(statistics)">“score”</a> to refer to the other derivative, i.e.&nbsp;the derivative with respect to a model’s parameter, which is a completely different object!</p>
<p>Consider a Brownian diffusion process <img src="https://latex.codecogs.com/png.latex?dX%20=%20%5Csigma%20%5C,%20dW"> initiated from a distribution <img src="https://latex.codecogs.com/png.latex?%5Cmu(dx)%20=%20p_0(dx)">. If this process is ran forward for a duration <img src="https://latex.codecogs.com/png.latex?%5Cdelta%3E0">, we have:</p>
<p><img src="https://latex.codecogs.com/png.latex?X_%7B%5Cdelta%7D%20=%20X_0%20+%20%5Cmathcal%7BN%7D(0,%20%5Csigma%5E2%20%5C,%20%5Cdelta)"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?X_0%20%5Csim%20%5Cmu(dx)">. Now, focusing on a specific sample <img src="https://latex.codecogs.com/png.latex?y%20=%20X_%7B%5Cdelta%7D">, the backward dynamics <img src="https://latex.codecogs.com/png.latex?d%5Coverleftarrow%7BX%7D%20=%20%5Csigma%5E2%20%5C,%20%5Cnabla%20%5Clog%20p_t(%5Coverleftarrow%7BX%7D)%20,%20dt%20+%20%5Csigma%20%5C,%20dB"> suggests that for sufficiently small <img src="https://latex.codecogs.com/png.latex?%5Cdelta">, the following approximation holds:</p>
<p><span id="eq-rev-tweedie"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5BX_0%20%5C,%20%7C%20X_%7B%5Cdelta%7D%20=%20y%5D%20%5C;%20%7B%20%5Ccolor%7B%5Cred%7D%20%5Capprox%7D%20%5C;%20y%20+%20%5Cnabla%20%5Clog%20p_%7B%5Cdelta%7D(y)%20%5C,%20%5Csigma%5E2%20%5Cdelta.%0A%5Ctag%7B3%7D"></span></p>
<p>Maybe surprisingly, in the case of Brownian dynamics (no drift), this relationship holds exactly, even for arbitrarily large time increments <img src="https://latex.codecogs.com/png.latex?%5Cdelta%3E0">. This observation attributed in <span class="citation" data-cites="efron2011tweedie">(Efron 2011)</span> to <a href="https://en.wikipedia.org/wiki/Maurice_Tweedie">Maurice Tweedie (1919–1996)</a> has a straightforward proof. Specifically, if <img src="https://latex.codecogs.com/png.latex?X%20%5Csim%20%5Cmu(dx)">, then <img src="https://latex.codecogs.com/png.latex?Y%20=%20X%20+%20%5Cmathcal%7BN%7D(0,%20%5CGamma%5E2)"> has a density given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap_Y(dy)%20=%20%5Cint%20%5Cmu(x)%20%5C,%20%5Crho_%7B%5CGamma%7D(y-x)%20%5C,%20dx%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Crho_%7B%5CGamma%7D(z)%20%5Cpropto%20%5Cexp%5B-z%5E2/(2%20%5CGamma%5E2)%5D"> is a centred Gaussian with variance <img src="https://latex.codecogs.com/png.latex?%5CGamma%5E2">. It follows that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%20%5Cmathbb%7BE%7D%5BX%20%5C,%20%7C%20Y%20=%20y%5D-y%7D%7B%5CGamma%5E2%7D%20=%20%5Cfrac%7B%20%5Cint%20%5Cleft(%20%5Cfrac%20%7Bx-y%20%7D%7B%5CGamma%5E2%7D%20%5Cright)%5C,%20%5Cmu(x)%20%5C,%20%5Crho_%7B%5CGamma%7D(y-x)%20%5C,%20dx%20%7D%7B%5Cint%20%5Cmu(x)%20%5C,%20%5Crho_%7B%5CGamma%7D(y-x)%20%5C,%20dx%7D%0A=%0A%5Cnabla_y%20%5Clog%20%5Cleft%5C%7B%20%5Cint%20%5Cmu(x)%20%5C,%20%5Crho_%7B%5CGamma%7D(y-x)%20%5C,%20dx%20%5Cright%5C%7D%0A"></p>
<p>since <img src="https://latex.codecogs.com/png.latex?%5Cnabla_y%20%5Crho_%7B%5CGamma%7D(y-x)%20=%20(x-y)/%5CGamma%5E2">. This leads to Tweedie’s formula:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5BX%20%5C,%20%7C%20Y%20=%20y%5D%20%5C;%20%7B%5Ccolor%7Bred%7D%20=%7D%20%5C;%20y%20+%20%5CGamma%5E2%20%5C,%20%5Cnabla%20%5Clog%20p_Y(y),%0A"></p>
<p>which is exactly the same as Equation&nbsp;3 but with an equality sign: no approximation needed! Interestingly, this is what Machine-Learners often refer to as “denoising score matching” <span class="citation" data-cites="vincent2011connection">(Vincent 2011)</span>: if we have access to a large number of samples <img src="https://latex.codecogs.com/png.latex?(X_i,Y_i)">, where <img src="https://latex.codecogs.com/png.latex?X_i%20%5Csim%20%5Cmu(dx)"> and <img src="https://latex.codecogs.com/png.latex?Y_i%20=%20X_i%20+%20%5Cmathcal%7BN%7D(0,%20%5CGamma%5E2)">, and fit a regression model <img src="https://latex.codecogs.com/png.latex?F_%7B%5Ctheta%7D"> by minimizing the mean-squared error <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20%5Cmapsto%20%5Cmathbb%7BE%7D%20%5C%7CX%20-%20F_%7B%5Ctheta%7D(Y)%5C%7C%5E2">, then Tweedie formula shows that in the limit of infinite samples and with sufficient flexibility in the regression model <img src="https://latex.codecogs.com/png.latex?F_%7B%5Ctheta_%5Cstar%7D(y)%20=%20y%20+%20%5CGamma%5E2%20%5Cnabla%20%5Clog%20p_Y(y)">. This allows one to estimate <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20%5Clog%20p_Y"> from data, which is often a good approximation of <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20%5Clog%20%5Cmu"> if the variance <img src="https://latex.codecogs.com/png.latex?%5CGamma%5E2"> of the added noise is not too large. Indeed, things can go bad if <img src="https://latex.codecogs.com/png.latex?%5CGamma%5E2"> is very small and the number of training data is not large, no free lunch!</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-anderson1982reverse" class="csl-entry">
Anderson, Brian DO. 1982. <span>“Reverse-Time Diffusion Equation Models.”</span> <em>Stochastic Processes and Their Applications</em> 12 (3): 313–26.
</div>
<div id="ref-efron2011tweedie" class="csl-entry">
Efron, Bradley. 2011. <span>“Tweedie’s Formula and Selection Bias.”</span> <em>Journal of the American Statistical Association</em> 106 (496): 1602–14.
</div>
<div id="ref-haussmann1986time" class="csl-entry">
Haussmann, Ulrich G, and Etienne Pardoux. 1986. <span>“Time Reversal of Diffusions.”</span> <em>The Annals of Probability</em>, 1188–1205.
</div>
<div id="ref-ho2020denoising" class="csl-entry">
Ho, Jonathan, Ajay Jain, and Pieter Abbeel. 2020. <span>“Denoising Diffusion Probabilistic Models.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 6840–51.
</div>
<div id="ref-vincent2011connection" class="csl-entry">
Vincent, Pascal. 2011. <span>“A Connection Between Score Matching and Denoising Autoencoders.”</span> <em>Neural Computation</em> 23 (7): 1661–74.
</div>
</div></section></div> ]]></description>
  <category>SDE</category>
  <category>Generative-Model</category>
  <guid>https://alexxthiery.github.io/notes/reverse_and_tweedie/reverse_and_tweedie.html</guid>
  <pubDate>Sun, 11 Jun 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title></title>
  <link>https://alexxthiery.github.io/notes/index_notes.html</link>
  <description><![CDATA[ 



<h1 style="text-align: center;">
Notes
</h1>
<p>Notes for students, half-baked write-ups for myself, and probably other random maths/stats/ML texts. Certainly full of typos, comments welcome!</p>
<!-- ### Statistics: Foundations
* [Natural Gradients](./stats_basics/natural_gradients.qmd) -->
<section id="denoising-diffusion-probabilistic-models" class="level3">
<h3 class="anchored" data-anchor-id="denoising-diffusion-probabilistic-models">Denoising Diffusion Probabilistic Models</h3>
<ul>
<li><a href="../notes/DDPM/DDPM.html">Noising and Reverse Ornstein-Uhlenbeck</a></li>
<li><a href="../notes/DDPM_deterministic/DDPM_deterministic.html">From Denoising Diffusion to ODEs</a></li>
<li><a href="../notes/reverse_and_tweedie/reverse_and_tweedie.html">Reverse diffusions, Score &amp; Tweedie</a></li>
</ul>
</section>
<section id="information-theory" class="level3">
<h3 class="anchored" data-anchor-id="information-theory">Information Theory</h3>
<ul>
<li><a href="../notes/information_theory_basics/information_theory_entropy.html">Entropy and Basic Definitions</a></li>
</ul>


</section>

 ]]></description>
  <category>index</category>
  <guid>https://alexxthiery.github.io/notes/index_notes.html</guid>
  <pubDate>Sat, 31 Dec 2022 16:00:00 GMT</pubDate>
</item>
</channel>
</rss>
