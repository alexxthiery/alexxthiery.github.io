<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Alexandre Thiéry</title>
<link>https://alexxthiery.github.io/notes/index_notes_as_list.html</link>
<atom:link href="https://alexxthiery.github.io/notes/index_notes_as_list.xml" rel="self" type="application/rss+xml"/>
<description>Alex Thiery Notes</description>
<generator>quarto-1.3.353</generator>
<lastBuildDate>Sun, 22 Oct 2023 16:00:00 GMT</lastBuildDate>
<item>
  <title>Gaussian Assimilation</title>
  <link>https://alexxthiery.github.io/notes/Gaussian_Assimilation/gaussian_assimilation.html</link>
  <description><![CDATA[ 




<section id="ensemble-kalman-updates" class="level2">
<h2 class="anchored" data-anchor-id="ensemble-kalman-updates">Ensemble Kalman Updates</h2>
<p>Assume a prior Gaussian prior distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi_0%20%5Cequiv%20%5Cmathcal%7BN%7D(m_0,P_0)"> and a noisy observation <img src="https://latex.codecogs.com/png.latex?y_%5Cstar%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD_y%7D"> with</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay_%5Cstar%20=%20H%20x%20+%20%5Cxi%0A%5Cqquad%20%5Ctext%7Bwith%7D%20%5Cqquad%0A%5Cxi%20%5Csim%20%5Cmathcal%7BN%7D(0,R)%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD_x%7D"> is an unkown quantity of interest. The posterior distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi%20%5Cequiv%20%5Cmathcal%7BN%7D(m,P)"> is Gaussian and is given by</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cleft%5C%7B%0A%5Cbegin%7Baligned%7D%0Am%20&amp;=%20m_0%20+%20P_0%20H%5E%5Ctop%20%20%7B%5Cleft(%20H%20P_0%20H%5E%5Ctop%20+%20R%20%5Cright)%7D%20%5E%7B-1%7D(y_%5Cstar%20-%20H%20m)%5C%5C%0AP%20&amp;=%20P_0%20-%20P_0%20H%5E%5Ctop%20%20%7B%5Cleft(%20H%20P_0%20H%5E%5Ctop%20+%20R%20%5Cright)%7D%20%5E%7B-1%7D%20%5C,%20H%20P_0,%0A%5Cend%7Baligned%7D%0A%5Cright.%0A%5Cend%7Balign%7D%0A"></p>
<p>as standard <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">Gaussian conditioning</a> shows it. This can also be written as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cleft%5C%7B%0A%5Cbegin%7Baligned%7D%0Am%20&amp;=%20m_0%20+%20K%20%5C,%20(y_%5Cstar%20-%20H%20m_0)%5C%5C%0AP%20&amp;=%20(I%20-%20K%20%5C,%20H)%20P_0,%0A%5Cend%7Baligned%7D%0A%5Cright.%0A%5Cend%7Balign%7D%0A"></p>
<p>for <strong><a href="https://en.wikipedia.org/wiki/Kalman_filter">Kalman</a> Gain Matrix</strong> <img src="https://latex.codecogs.com/png.latex?K%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD_x,D_y%7D"> defined as</p>
<p><span id="eq-kalman"><img src="https://latex.codecogs.com/png.latex?%0AK%20%5C;%20=%20%5C;%20P_0%20H%5E%5Ctop%20%20%7B%5Cleft(%20H%20P_0%20H%5E%5Ctop%20+%20R%20%5Cright)%7D%20%5E%7B-1%7D.%0A%5Ctag%7B1%7D"></span></p>
<p>The important remark is that the posterior covariance matrix <img src="https://latex.codecogs.com/png.latex?P%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD_x,D_x%7D"> and the posterior mean <img src="https://latex.codecogs.com/png.latex?m%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD_x%7D"> can also be expressed as</p>
<p><span id="eq-gain"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cleft%5C%7B%0A%5Cbegin%7Baligned%7D%0Am%20&amp;=%20%20%5Ctextcolor%7Bred%7D%7B(I%20-%20K%20%5C,%20H)%7D%20%5C,%20m_0%20+%20%20%5Ctextcolor%7Bblue%7D%7BK%7D%20%5C,%20y_%5Cstar%5C%5C%0AP%20&amp;=%20%5C;%20%20%5Ctextcolor%7Bred%7D%7B(I%20-%20K%20%5C,%20H)%7D%20%5C,%20P_0%20%5C,%20%20%5Ctextcolor%7Bred%7D%7B(I%20-%20K%20%5C,%20H)%5E%5Ctop%7D%20+%20%20%5Ctextcolor%7Bblue%7D%7BK%7D%20R%20%20%5Ctextcolor%7Bblue%7D%7BK%5E%5Ctop%7D.%0A%5Cend%7Baligned%7D%0A%5Cright.%0A%5Cend%7Balign%7D%0A%5Ctag%7B2%7D"></span></p>
<p>This shows that <img src="https://latex.codecogs.com/png.latex?P"> is indeed positive semi-definite. More importantly, this gives a mechanism for transforming samples from the prior distributions into samples from the posterior distributions. Indeed, consider <img src="https://latex.codecogs.com/png.latex?N"> iid samples from the prior distribution, <img src="https://latex.codecogs.com/png.latex?x_1,%20%5Cldots,%20x_N%20%5Csim%20%5Cpi_0(dx)">, and set</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidetilde%7Bx%7D_i%0A=%0A%5Ctextcolor%7Bred%7D%7B(I%20-%20K%20%5C,%20H)%7D%20%5C,%20x_i%20+%20%20%5Ctextcolor%7Bblue%7D%7BK%7D(y_%5Cstar%20+%20%5Cxi_i)%0A"></p>
<p>for iid noise terms <img src="https://latex.codecogs.com/png.latex?%5Cxi_i%20%5Csim%20%5Cmathcal%7BN%7D(0,R)">. From Equation&nbsp;2 it is clear that <img src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7Bx%7D_1,%20%5Cldots,%20%5Cwidetilde%7Bx%7D_N"> are iid samples from the Gaussian posterior distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi%20=%20%5Cmathcal%7BN%7D(m,P)">. It is more intuitive to write this as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidetilde%7Bx%7D_i%0A=%0Ax_i%20+%20K%20%5C,%20(%20%20%5Ctextcolor%7Bgreen%7D%7B%20%5Cwidetilde%7By%7D_%5Cstar%7D%20-%20H%20%5C,%20x_i)%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7By%7D_%5Cstar%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD_y%7D"> are <strong>fake observations</strong> that are obtained by perturbing the actual observation <img src="https://latex.codecogs.com/png.latex?y_%5Cstar"> with additive Gaussian noise terms with covariance <img src="https://latex.codecogs.com/png.latex?R">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctextcolor%7Bgreen%7D%7B%5Cwidetilde%7By%7D_%5Cstar%20%5C,%20=%20%5C,%20y_%5Cstar%20+%20%5Cxi_i%7D.%0A"></p>
<section id="empirical-version-non-linearity-and-non-gaussianity" class="level3">
<h3 class="anchored" data-anchor-id="empirical-version-non-linearity-and-non-gaussianity">Empirical version: non-linearity and non-Gaussianity</h3>
<p>Suppose that we would like to estimate <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD_x%7D"> from the noisy observation</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay_%5Cstar%20=%20%5Cmathcal%7BH%7D(x)%20+%20%5Cxi%0A%5Cqquad%20%5Ctext%7Bwith%7D%20%5Cqquad%0A%5Cxi%20%5Csim%20%5Cmathcal%7BN%7D(0,R)%0A"></p>
<p>and possibly-nonlinear observation operator <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BH%7D:%20%5Cmathbb%7BR%7D%5E%7BD_x%7D%20%5Cto%20%5Cmathbb%7BR%7D%5E%7BD_y%7D">. Assume that we also have <img src="https://latex.codecogs.com/png.latex?N"> samples <img src="https://latex.codecogs.com/png.latex?x_1,%20%5Cldots,%20x_N"> generated from some (unkown) prior distribution. For example, these samples could come from another numerical procedure. In order to obtain <img src="https://latex.codecogs.com/png.latex?N"> approximate samples from the posterior distribution, one can set</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidetilde%7Bx%7D_i%0A=%0Ax_i%20+%20%5Cwidehat%7BK%7D%20%5C,%20%5B%20%20%5Ctextcolor%7Bgreen%7D%7B%20%5Cwidetilde%7By%7D_%5Cstar%7D%20-%20%5Cmathcal%7BH%7D(x_i)%5D%0A"></p>
<p>for fake observations <img src="https://latex.codecogs.com/png.latex?%20%5Ctextcolor%7Bgreen%7D%7B%5Cwidetilde%7By%7D_%5Cstar%7D%20=%20y_%5Cstar%20+%20%5Cxi_i">. The approximate Kalman gain matrix <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BK%7D"> is obtained by noting that in Equation&nbsp;1 giving</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AK%20%5C;%20=%20%5C;%20P_0%20H%5E%5Ctop%20%20%7B%5Cleft(%20H%20P_0%20H%5E%5Ctop%20+%20R%20%5Cright)%7D%20%5E%7B-1%7D%0A"></p>
<p>we have <img src="https://latex.codecogs.com/png.latex?P_0%20H%5E%5Ctop%20=%20%5Cmathop%7B%5Cmathrm%7BCov%7D%7D(X,HX)"> and <img src="https://latex.codecogs.com/png.latex?H%20P_0%20H%5E%5Ctop%20=%20%5Cmathop%7B%5Cmathrm%7BVar%7D%7D(HX)"> for <img src="https://latex.codecogs.com/png.latex?X%20%5Csim%20%5Cpi_0">. This means that an approximate Kalman matrix can be obtained using empirical estimates of these covariance matrices:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%7BK%7D%20%5C;%20=%20%5C;%20%5Cwidehat%7B%5Cmathop%7B%5Cmathrm%7BCov%7D%7D%7D(%5Bx_i%5D_i,%20%5B%5Cmathcal%7BH%7D(x_i)%5D_i)%20%5C,%20%20%7B%5Cleft(%20%5Cwidehat%7B%5Cmathop%7B%5Cmathrm%7BVar%7D%7D%7D(%5B%5Cmathcal%7BH%7D(x_i)%5D_i)%20+%20R%20%5Cright)%7D%20%5E%7B-1%7D.%0A"></p>
<p>These updates form the basis of the <a href="https://en.wikipedia.org/wiki/Ensemble_Kalman_filter">Ensemble Kalman filter (EnKF)</a>, and very successful and scalable approach to data-assimilation of high-dimensional dynamical systems. This method is operationally employed across various weather forecasting centers across the globe.</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/Gaussian_Assimilation/evensen.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption">EnKF Bible by Geir Evensen</figcaption>
</figure>
</div>
</div>


</section>
</section>

 ]]></description>
  <category>bayes</category>
  <guid>https://alexxthiery.github.io/notes/Gaussian_Assimilation/gaussian_assimilation.html</guid>
  <pubDate>Sun, 22 Oct 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Deriving Langevin MCMC</title>
  <link>https://alexxthiery.github.io/notes/on_Langevin_MCMC/on_Langevin_MCMC.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/on_Langevin_MCMC/besag.jpg" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Julian Besag (1945 – 2010)</figcaption>
</figure>
</div>
</div>
<p>Consider a target density <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)"> in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5ED">. Since the Langevin diffusion</p>
<p><span id="eq-langevin"><img src="https://latex.codecogs.com/png.latex?%0AdX_t%20=%20%5Cnabla%20%5Clog%20%5Cpi(X_t)%20%5C,%20dt%20+%20%5Csqrt%7B2%7D%20%5C,%20dW%0A%5Ctag%7B1%7D"></span></p>
<p>is reversible with respect to <img src="https://latex.codecogs.com/png.latex?%5Cpi">, it is natural to use a <a href="https://en.wikipedia.org/wiki/Euler–Maruyama_method">Euler-Maruyama</a> discretization of Equation&nbsp;1 to build MCMC proposals: in a MCMC simulation and for a time discretization parameter <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%3E%200">, if the current position is <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathbb%7BR%7D%5ED">, a proposal <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cmathbb%7BR%7D%5ED"> can be generated as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay%20=%20x%20+%20%5Cvarepsilon%5C,%20%5Cnabla%20%5Clog%20%5Cpi(X_t)%20+%20%5Csqrt%7B2%20%5Cvarepsilon%7D%20%5C,%20%5Cxi%0A"></p>
<p>with <img src="https://latex.codecogs.com/png.latex?%5Cxi%20%5Csim%20%5Cmathcal%7BN%7D(0,I)"> before being accepted-or-reject according to the usual <a href="https://en.wikipedia.org/wiki/Metropolis–Hastings_algorithm">Metropolis-Hastings</a> ratio. This MCMC method, first proposed by <a href="https://en.wikipedia.org/wiki/Julian_Besag">Julian Besag</a> in 1994, is commonly referred to as the <a href="https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm">Metropolis-Adjusted-Langevin-Algorithm</a> (MALA). But how can one come-up with this proposal mechanism without knowing before hand the existence of this reversible Langevin diffusion Equation&nbsp;1? While it is intuitively clear that following the direction of <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20%5Clog%20%5Cpi"> is not such a bad idea, i.e.&nbsp;one would like to move towards areas of “high probability mass”, where does this <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7B2%7D"> comes from? Naturally, one could look at proposals of the type <img src="https://latex.codecogs.com/png.latex?y%20=%20x%20+%20%5Cnabla%20%5Clog%20%5Cpi(X_t)%20%5C,%20%5Cvarepsilon+%20%5Clambda%20%5C,%20%5Cxi"> for some free parameter <img src="https://latex.codecogs.com/png.latex?%5Clambda%20%3E%200"> and study the behavior of the Metropolis-Hastings ratio in the regime <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5Cto%200">: as simple as it sounds, it is not entirely straightforward and requires quite a bit of algebra (do it!). Instead, I very much like the type of approaches described in <span class="citation" data-cites="titsias2018auxiliary">(Titsias and Papaspiliopoulos 2018)</span>. To summarize, we would like to generate a MCMC proposal <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cmathbb%7BR%7D%5ED"> that stays in the vicinity of the current position <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathbb%7BR%7D%5ED"> while exploiting the knowledge of <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20%5Clog%20%5Cpi(x)">. One cannot simply approximate the target distribution as <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)%20%5Capprox%20%5Cpi(x_k)%20e%5E%7B%5Cleft%3C%20%5Cnabla%20%5Clog%20%5Cpi(x_k),%20x-x_k%20%5Cright%3E%7D"> and sample from this approximation since it is typically not tractable. Instead, consider the following extended target distribution</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Coverline%7B%5Cpi%7D(x,z)%20%5C,%20%5Cpropto%20%5Cpi(x)%20%5C,%20%5Cexp%20%7B%5Cleft%5C%7B%20-%5Cfrac%7B1%7D%7B2%5Cvarepsilon%7D%5C%7Cz-x%5C%7C%5E2%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>In other words, we consider a Gaussian auxiliary variable <img src="https://latex.codecogs.com/png.latex?z%20%5Cin%20%5Cmathbb%7BR%7D%5ED"> that is centred at <img src="https://latex.codecogs.com/png.latex?x"> and at distance about <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7B%5Cvarepsilon%7D"> of it. Now, given the current position <img src="https://latex.codecogs.com/png.latex?x_k">, to generate a proposal <img src="https://latex.codecogs.com/png.latex?y_%5Cstar"> that stays in the vicinity of <img src="https://latex.codecogs.com/png.latex?x_k">, one can proceed in two steps, in the spirit of a Gibbs-sampling approach:</p>
<ol type="1">
<li><p>First, generate <img src="https://latex.codecogs.com/png.latex?z_%5Cstar%20%5Csim%20%5Coverline%7B%5Cpi%7D(dz%20%7C%20x_k)%20%5Csim%20%5Cmathcal%7BN%7D(x_k,%20%5Csqrt%7B%5Cvarepsilon%7DI)"></p></li>
<li><p>Second, sample from <img src="https://latex.codecogs.com/png.latex?y_%5Cstar%20%5Csim%20%5Coverline%7B%5Cpi%7D(dx%20%7C%20z_%5Cstar)">.</p></li>
</ol>
<p>Unfortunately, the second step is typically not tractable. Nevertheless, the conditional density <img src="https://latex.codecogs.com/png.latex?%5Coverline%7B%5Cpi%7D(dx%20%7C%20z_%5Cstar)"> is concentrated in a <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7B%5Cvarepsilon%7D">-neighborhood of <img src="https://latex.codecogs.com/png.latex?z_%5Cstar"> and a simple Gaussian approximation around <img src="https://latex.codecogs.com/png.latex?(x_k,%20z_%5Cstar)"> should be enough for our purpose. We have:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Clog%20%5Coverline%7B%5Cpi%7D(dx%20%7C%20z_%5Cstar)%0A&amp;=%0A%5Clog%20%5Cpi(x)%20-%20%5Cfrac%7B1%7D%7B2%20%5Cvarepsilon%7D%20%5C%7Cx%20-%20z_%5Cstar%5C%7C%5E2%20+%20%5Ctextrm%7B(Cst)%7D%5C%5C%0A&amp;%5Capprox%0A%5Cleft%3C%20%20%5Cnabla%20%5Clog%20%5Cpi(x_k),%20x-x_k%20%20%5Cright%3E%20-%20%5Cfrac%7B1%7D%7B2%20%5Cvarepsilon%7D%20%5C%7Cx%20-%20z_%5Cstar%5C%7C%5E2%20+%20%5Ctextrm%7B(Cst)%7D%5C%5C%0A&amp;=%0A-%20%5Cfrac%7B1%7D%7B2%20%5Cvarepsilon%7D%20%5C%7Cx%20-%20%5Bz_%5Cstar%20+%20%5Cvarepsilon%5C,%20%5Cnabla%20%5Clog%20%5Cpi(x_k)%5D%5C%7C%5E2%20+%20%5Ctextrm%7B(Cst)%7D.%0A%5Cend%7Balign%7D%0A"></p>
<p>This shows that the conditional <img src="https://latex.codecogs.com/png.latex?%5Coverline%7B%5Cpi%7D(dx%20%7C%20z_%5Cstar)"> can be approximated by a Gaussian distribution centred at <img src="https://latex.codecogs.com/png.latex?%5Bz_%5Cstar%20+%20%5Cnabla%20%5Clog%20%5Cpi(x_k)%5D"> and variance <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5C,%20I">. This means that the final proposal <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cmathbb%7BR%7D%5ED"> can be generated as <img src="https://latex.codecogs.com/png.latex?y%20%5Csim%20z_%5Cstar%20+%20%5Cnabla%20%5Clog%20%5Cpi(x_k)%20+%20%5Cxi"> where <img src="https://latex.codecogs.com/png.latex?%5Cxi%20%5Csim%20%5Cmathcal%7BN%7D(0,%5Cvarepsilon)">. But that is equivalent to setting</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay%20%5Csim%20x%20+%20%5Cvarepsilon%5C,%20%5Cnabla%20%5Clog%20%5Cpi(x_k)%20+%20%5Csqrt%7B2%20%5Cvarepsilon%7D%20%5C,%20%5Cxi%0A"></p>
<p>with <img src="https://latex.codecogs.com/png.latex?%5Cxi%20%5Csim%20%5Cmathcal%7BN%7D(0,I)"> since <img src="https://latex.codecogs.com/png.latex?z_%5Cstar%20%5Csim%20%5Cmathcal%7BN%7D(x,%20%5Csqrt%7B%5Cvarepsilon%7D%20I)">. It is exactly the MALA proposal. Naturally, one can also try to slightly more clever and use an extended distribution</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Coverline%7B%5Cpi%7D(x,z)%20%5C,%20%5Cpropto%20%5C,%20%5Cpi(x)%20%5C,%20%5Cexp%20%7B%5Cleft%5C%7B%20%20-%5Cfrac%7B1%7D%7B2%5Cvarepsilon%7D%20%5Cleft%3C%20(z-x),%20M%5E%7B-1%7D%20%5C,%20(z-x)%20%5Cright%3E%20%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>for some appropriate positive-definite “mass” matrix <img src="https://latex.codecogs.com/png.latex?M%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD,D%7D">. Indeed, this immediately leads to preconditioned MALA methods. While this derivation is neat, I really like this approach since it can be adapted and generalized to quite a few other situations!</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-titsias2018auxiliary" class="csl-entry">
Titsias, Michalis K, and Omiros Papaspiliopoulos. 2018. <span>“Auxiliary Gradient-Based Sampling Algorithms.”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 80 (4). Oxford University Press: 749–67.
</div>
</div></section></div> ]]></description>
  <category>MCMC</category>
  <guid>https://alexxthiery.github.io/notes/on_Langevin_MCMC/on_Langevin_MCMC.html</guid>
  <pubDate>Wed, 18 Oct 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Wasserstein Gradients &amp; Langevin Diffusions</title>
  <link>https://alexxthiery.github.io/notes/wasserstein_langevin/wasserstein_langevin.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/wasserstein_langevin/langevin.gif" class="img-fluid figure-img" style="width:95.0%"></p>
<figcaption class="figure-caption">Overdamped Langevin Diffusion as a gradient flow on the space of probability distributions endowed with the <img src="https://latex.codecogs.com/png.latex?2">-Wasserstein metric</figcaption>
</figure>
</div>
</div>
<p>Consider a target probability density <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)%20=%20%5Cfrac%7B%5Coverline%7B%5Cpi%7D(x)%7D%7B%5Cmathcal%7BZ%7D%7D"> on <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5ED"> that is known up to a normalizing constant <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BZ%7D%3E%200">. We also have a different probability density <img src="https://latex.codecogs.com/png.latex?p_0(x)">. The goal is to gradually tweak <img src="https://latex.codecogs.com/png.latex?p_0(x)"> so that it eventually matches <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)">. More concretely, we aim to perform a gradient descent on the space of probability distributions to reduce the functional</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BF%7D(p)%20%5C;%20=%20%5C;%20%5Cmathop%7B%5Cmathrm%7BD_%7B%5Ctext%7BKL%7D%7D%7D%7D%20%7B%5Cleft(%20p,%20%5Cpi%20%5Cright)%7D%20%20%5C;%20=%20%5C;%20%5Cint%20p(x)%20%5C,%20%5Clog%20%20%7B%5Cleft%5C%7B%20%5Cfrac%7Bp(x)%7D%7B%5Coverline%7B%5Cpi%7D(x)%7D%20%5Cright%5C%7D%7D%20%20%5C,%20dx%20%5C,%20+%20%5C,%20%5Ctextrm%7B(constant)%7D.%0A"></p>
<p>This approach can be discretized: assume <img src="https://latex.codecogs.com/png.latex?N%20%5Cgg%201"> particles <img src="https://latex.codecogs.com/png.latex?X_0%5E1,%20%5Cldots,%20X_0%5EN%20%5Cin%20%5Cmathbb%7BR%7D%5ED"> forming an empirical distribution that approximates <img src="https://latex.codecogs.com/png.latex?p_0(dx)">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap_0(dx)%20%5C;%20%5Capprox%20%5C;%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5EN%20%5C,%20%5Cdelta_%7BX_0%5Ei%7D(dx).%0A"></p>
<p>Define <img src="https://latex.codecogs.com/png.latex?X_%7B%5Cdelta%7D%5Ei%20=%20X_0%5Ei%20+%20%5Cdelta_t%20%5C,%20%5Cmu(X_0%5Ei)"> where <img src="https://latex.codecogs.com/png.latex?%5Cdelta_t%20%5Cll%201"> denotes a time discretization parameter and <img src="https://latex.codecogs.com/png.latex?%5Cmu:%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D%5ED"> is a “drift” function. Finding a suitable ‘drift function’ is the main problem. According to the <a href="https://en.wikipedia.org/wiki/Fokker–Planck_equation">Fokker-Planck</a> equation, the computed empirical distribution</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap_%7B%5Cdelta_t%7D(dx)%20%5C;%20%5Capprox%20%5C;%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5EN%20%5C,%20%5Cdelta_%7BX_%7B%5Cdelta_t%7D%5Ei%7D(dx)%0A"></p>
<p>approximates <img src="https://latex.codecogs.com/png.latex?p_%7B%5Cdelta_t%7D(x)"> given by</p>
<p><span id="eq-fokker"><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bp_%7B%5Cdelta_t%7D(x)-%20p_0(x)%7D%7B%5Cdelta_t%7D%20%5C;%20=%20%5C;%20-%5Cnabla%20%5Ccdot%20%20%7B%5Cleft%5B%20%5Cmu(x)%20%5C,%20p_0(x)%20%5Cright%5D%7D%20.%0A%5Ctag%7B1%7D"></span></p>
<p>What is the optimal drift function <img src="https://latex.codecogs.com/png.latex?%5Cmu:%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D%5ED"> that ensures that <img src="https://latex.codecogs.com/png.latex?p_%7B%5Cdelta_t%7D"> comes as close as possible to <img src="https://latex.codecogs.com/png.latex?%5Cpi">? Typically, we select <img src="https://latex.codecogs.com/png.latex?%5Cmu:%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D%5ED"> such that the quantity <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BF%7D(p_%7B%5Cdelta_t%7D)"> is minimized, provided that <img src="https://latex.codecogs.com/png.latex?p_%7B%5Cdelta_t%7D"> is not drastically different from <img src="https://latex.codecogs.com/png.latex?p_0">. One method is to use the <img src="https://latex.codecogs.com/png.latex?L%5E2"> <a href="https://en.wikipedia.org/wiki/Wasserstein_metric">Wasserstein distance</a> and assume the constraint</p>
<p><span id="eq-wass"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathop%7B%5Cmathrm%7BD_%7B%5Ctext%7BWass%7D%7D%7D%7D(p_%7B0%7D,%20p_%7B%5Cdelta_t%7D)%20%5Capprox%20%5Cint%20p_0(x)%20%5C,%20%5C%7C%20%5Cdelta_t%20%5C,%20%5Cmu(x)%20%5C%7C%5E2%20%5C,%20dx%20%5Cleq%20%5Cvarepsilon%0A%5Ctag%7B2%7D"></span></p>
<p>for a parameter <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5Cll%201">. More pragmatically, it is generally easier (eg. proximal methods) to minimize the joint objective</p>
<p><span id="eq-joint-obj"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BF%7D(p_%7B%5Cdelta_t%7D)%20+%20%5Cfrac%7B1%7D%7B2%20%5Cvarepsilon%7D%20%5C,%20%5Cmathop%7B%5Cmathrm%7BD_%7B%5Ctext%7BWass%7D%7D%7D%7D(p_%7B0%7D,%20p_%7B%5Cdelta_t%7D).%0A%5Ctag%7B3%7D"></span></p>
<p>Based on equations Equation&nbsp;1 and Equation&nbsp;2, a first-order expansion shows that the joint objective Equation&nbsp;3 can be approximated by</p>
<p><span id="eq-quad"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A-%5Cint%20&amp;%5Cnabla%20%5Ccdot%20%5CBig%5C%7B%20%20%5Ctextcolor%7Bred%7D%7B%5B%5Cdelta_t%20%5Cmu%5D%7D(x)%20%5C,%20p_0(x)%20%5CBig%5C%7D%20%5C,%20%5Clog%20%20%7B%5Cleft%5C%7B%20%20%5Cfrac%7Bp_0(x)%7D%7B%5Coverline%7B%5Cpi%7D(x)%7D%20%20%5Cright%5C%7D%7D%20%5C,%20dx%20%5C,%20%5C%5C%20&amp;%5Cqquad%20+%20%5Cqquad%20%5C,%20%5Cfrac%7B1%7D%7B2%20%5Cvarepsilon%7D%20%5C,%20%5Cint%20p_0(x)%20%5C,%20%5C%7C%20%20%5Ctextcolor%7Bred%7D%7B%5B%5Cdelta_t%20%5C,%20%5Cmu%5D%7D(x)%20%5C%7C%5E2%20%5C,%20dx,%0A%5Cend%7Balign%7D%0A%5Ctag%7B4%7D"></span></p>
<p>a relatively straightforward quadratic function of the drift function <img src="https://latex.codecogs.com/png.latex?%5Cmu:%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D%5ED">. The optimal drift function, ie. the minimizer of Equation&nbsp;4, is given by <img src="https://latex.codecogs.com/png.latex?%0A%5Cmu(x)%20%5C;%20=%20%5C;%20-%20%7B%5Cleft(%20%20%5Cfrac%7B%5Cvarepsilon%7D%7B%5Cdelta_t%7D%20%20%5Cright)%7D%20%20%5C,%20%5Cnabla%20%5Clog%20%20%7B%5Cleft%5C%7B%20%20%5Cfrac%7Bp_0(x)%7D%7B%5Coverline%7B%5Cpi%7D(x)%20%7D%20%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>Put simply, this suggests that we should select the drift function proportional to <img src="https://latex.codecogs.com/png.latex?-%5Cnabla%20%5Clog%5Bp_0(x)%20/%20%5Coverline%7B%5Cpi%7D(x)%5D">. To implement this scheme, we begin by sampling <img src="https://latex.codecogs.com/png.latex?N%20%5Cgg%201"> particles <img src="https://latex.codecogs.com/png.latex?X_0%5Ei%20%5Csim%20p_0(dx)"> and let evolve each particle according to the following differential equation</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%7D%7Bdt%7D%20X_t%5Ei%20%5C;%20=%20%5C;%20-%20%5Cnabla%20%5Clog%20%20%7B%5Cleft%5C%7B%20%20%5Cfrac%7Bp_t(X_t%5Ei)%20%7D%7B%20%5Coverline%7B%5Cpi%7D(X_t%5Ei)%20%7D%20%20%5Cright%5C%7D%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?p_t"> is the density of the set of particles at time <img src="https://latex.codecogs.com/png.latex?t">. It is the usual <a href="../../notes/DDPM_deterministic/DDPM_deterministic.html">diffusion-ODE trick</a> for describing the evolution of the density of an <a href="https://en.wikipedia.org/wiki/Brownian_dynamics">overdamped Langevin diffusion</a>,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX_t%20%5C;%20=%20%5C;%20-%5Cnabla%20%5Clog%20%5Coverline%7B%5Cpi%7D(X_t)%20%5C,%20dt%20%5C;%20+%20%5C;%20%5Csqrt%7B2%7D%20%5C,%20dW_t.%0A"></p>
<p>This can be shown by writing down the associated Fokker-Planck equation. This heuristic discussion shows that minimizing <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BD_%7B%5Ctext%7BKL%7D%7D%7D%7D(p,%20%5Cpi)"> by introducing a gradient flow in the space of probability distributions with the Wasserstein metric essentially produces a standard overdamped Langevin diffusion. Indeed, transforming this heuristic discussion into a formal statement is not trivial: the constructive proof in <span class="citation" data-cites="jordan1998variational">(Jordan, Kinderlehrer, and Otto 1998)</span> is now usually referred to as the JKO scheme.</p>
<p>The above derivation shows that the Wasserstein distance plays particularly well with minimizing functionals of the space of probability distributions. The same heuristic discussion shows that minimizing a functional of the type</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BF%7D(p)%20%5C;%20=%20%5C;%20%5Cint%20%5CPhi%5Bp(x)%5D%20%5C,%20%5Cnu(dx)%0A"></p>
<p>for some cost function <img src="https://latex.codecogs.com/png.latex?%5CPhi:%20(0,%20%5Cinfty)%20%5Cto%20%5Cmathbb%7BR%7D"> and distribution <img src="https://latex.codecogs.com/png.latex?%5Cnu(dx)"> leads to choosing a drift function <img src="https://latex.codecogs.com/png.latex?%5Cmu:%5Cmathbb%7BR%7D%5Cto%20%5Cmathbb%7BR%7D"> minimizing</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cint%20-%5Cnabla%20%5Ccdot%20%5CBig%5C%7B%20%20%5Ctextcolor%7Bred%7D%7B%5B%5Cdelta_t%20%5C,%20%5Cmu%5D%7D(x)%20%5C,%20p(x)%20%5CBig%5C%7D%20%5CPhi'%5Bp(x)%5D%20%5C,%20%5Cnu(dx)%0A%5C,%20+%20%5C,%20%5Cfrac%7B1%7D%7B2%20%5Cvarepsilon%7D%20%5C,%20%5Cint%20p_0(x)%20%5C,%20%5C%7C%20%20%5Ctextcolor%7Bred%7D%7B%5B%5Cdelta_t%20%5C,%20%5Cmu%5D%7D(x)%20%5C%7C%5E2%20%5C,%20dx.%0A"></p>
<p>This can be approached identically to what as been done in the case of minimizing <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BD_%7B%5Ctext%7BKL%7D%7D%7D%7D(p,%20%5Cpi)">.</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-jordan1998variational" class="csl-entry">
Jordan, Richard, David Kinderlehrer, and Felix Otto. 1998. <span>“The Variational Formulation of the Fokker–Planck Equation.”</span> <em>SIAM Journal on Mathematical Analysis</em> 29 (1). SIAM: 1–17.
</div>
</div></section></div> ]]></description>
  <category>diffusion</category>
  <guid>https://alexxthiery.github.io/notes/wasserstein_langevin/wasserstein_langevin.html</guid>
  <pubDate>Sun, 15 Oct 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Sanov’s Theorem</title>
  <link>https://alexxthiery.github.io/notes/sanov/sanov.html</link>
  <description><![CDATA[ 




<section id="sanovs-theorem" class="level3">
<h3 class="anchored" data-anchor-id="sanovs-theorem">Sanov’s Theorem</h3>
<p>Consider a random variable <img src="https://latex.codecogs.com/png.latex?X"> on the finite alphabet <img src="https://latex.codecogs.com/png.latex?%5C%7Ba_1,%20%5Cldots,%20a_K%5C%7D"> with <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BP%7D%7D(X=a_k)%20=%20p_k">. For <img src="https://latex.codecogs.com/png.latex?N%20%5Cgg%201">, consider a sequence <img src="https://latex.codecogs.com/png.latex?(x_1,%20%5Cldots,%20x_N)"> obtained by sampling <img src="https://latex.codecogs.com/png.latex?N"> times independently from <img src="https://latex.codecogs.com/png.latex?X"> and set</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%7Bp%7D_k%20=%20%5Cfrac%7B1%7D%7BN%7D%20%5C,%20%5Csum_%7Bi=1%7D%5EN%20%5C,%20%5Cmathbf%7B1%7D%20%7B%5Cleft(%20x_i%20=%20a_k%20%5Cright)%7D%0A"></p>
<p>the proportion of <img src="https://latex.codecogs.com/png.latex?a_k"> within this sequence. In other words, the empirical distribution obtained from the samples <img src="https://latex.codecogs.com/png.latex?(x_1,%20%5Cldots,%20x_N)"> reads</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%7Bp%7D%20=%20%5Csum_%7Bk=1%7D%5EK%20%5C,%20%5Cwidehat%7Bp%7D_k%20%5C,%20%5Cdelta_%7Ba_k%7D.%0A"></p>
<p>Indeed, the <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers">LLN</a> indicates that <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bp%7D_k%20%5Cto%20p_k"> as <img src="https://latex.codecogs.com/png.latex?N%20%5Cto%20%5Cinfty">, and it is important to estimate the probability that <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bp%7D_k"> significantly deviates from <img src="https://latex.codecogs.com/png.latex?p_k">. To this end, note that for another probability vector <img src="https://latex.codecogs.com/png.latex?q=(q_1,%20%5Cldots,%20q_K)"> the probability that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A(%5Cwidehat%7Bp%7D_1,%20%5Cldots,%20%5Cwidehat%7Bp%7D_K)%20%5C;%20=%20%5C;%20(q_1,%20%5Cldots,%20q_K)%0A"></p>
<p>is straightforward to compute and reads</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathop%7B%5Cmathrm%7BP%7D%7D(%5Cwidehat%7Bp%7D%20=%20q)%20%5C;%20=%20%5C;%20%5Cbinom%7BN%7D%7BN%20q_1,%20%5Cldots,%20N%20q_K%7D%20%5C,%20p_1%5E%7BN%20q_1%7D%20%5Cldots%20p_R%5E%7BN%20q_K%7D.%0A"></p>
<p><a href="https://en.wikipedia.org/wiki/Stirling%27s_approximation">Stirling’s approximation</a> <img src="https://latex.codecogs.com/png.latex?m!%20%5Casymp%20m%20%5C,%20%5Cln(m)"> then gives that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathop%7B%5Cmathrm%7BP%7D%7D(%5Cwidehat%7Bp%7D%20=%20q)%20%5C;%20%5Casymp%20%5C;%0A%5Cexp%20%7B%5Cleft(%20-N%20%5Ccdot%20%5Cmathop%7B%5Cmathrm%7BD_%7B%5Ctext%7BKL%7D%7D%7D%7D(q,p)%20%5Cright)%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BD_%7B%5Ctext%7BKL%7D%7D%7D%7D(q,p)%20=%20%5Csum_%7Bk=1%7D%5EK%20q_k%20%5C,%20%5Clog%5Bq_k%20/%20p_k%5D"> is the <a href="https://en.wikipedia.org/wiki/Kullback–Leibler_divergence">Kullback–Leibler divergence</a> of <img src="https://latex.codecogs.com/png.latex?q"> from <img src="https://latex.codecogs.com/png.latex?p">. In other words, as soon as <img src="https://latex.codecogs.com/png.latex?q%20%5Cneq%20p">, the probability of observing <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bp%7D%20%5Capprox%20q"> falls exponentially quickly to zero. With the language of <a href="https://en.wikipedia.org/wiki/Large_deviations_theory">Large Deviations</a>, one can make this statement slightly more precise, rigorous and general, but it is essentially the content of <a href="https://en.wikipedia.org/wiki/Sanov%27s_theorem">Sanov’s Theorem</a>.</p>
</section>
<section id="rare-events-happen-in-the-least-unlikely-manner" class="level3">
<h3 class="anchored" data-anchor-id="rare-events-happen-in-the-least-unlikely-manner">Rare events happen in the least unlikely manner</h3>
<p>Given a list of mutually exclusive events <img src="https://latex.codecogs.com/png.latex?E_1,%20%5Cldots,%20E_R"> and the knowledge that at least one of these events has taken place, the probability that the event <img src="https://latex.codecogs.com/png.latex?E_k"> was the one that happened is <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BP%7D%7D(E_k)%20/%20%5B%5Cmathop%7B%5Cmathrm%7BP%7D%7D(E_1)%20+%20%5Cldots%20+%20%5Cmathop%7B%5Cmathrm%7BP%7D%7D(E_R)%5D">. The implication is that if all the events are rare, that is <img src="https://latex.codecogs.com/png.latex?p_k%20%5Capprox%20e%5E%7B-N%20%5C,%20I_k%7D%20%5Cll%201">, and it is known that one event has indeed occurred, there is a high probability that the event with the smallest <img src="https://latex.codecogs.com/png.latex?I_k"> value was the one that happened: the rare event took place in the least unlikely manner.</p>
<p>Consider an iid sequence <img src="https://latex.codecogs.com/png.latex?(X_1,%20%5Cldots,%20X_N)"> of <img src="https://latex.codecogs.com/png.latex?N%20%5Cgg%201"> discrete real-valued random variables with <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BP%7D%7D(X%20=%20a_k)%20=%20p_k"> and mean <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BE%7D%7D(X)%20%5Cin%20%5Cmathbb%7BR%7D">. Suppose one observes the rare event</p>
<p><span id="eq-rare"><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5EN%20x_i%20%20%5Cgeq%20%5Cmu%0A%5Ctag%7B1%7D"></span></p>
<p>for some level <img src="https://latex.codecogs.com/png.latex?%5Cmu"> significantly above <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BE%7D%7D(X)">. Naturally, the least unlikely way for this to happen is if <img src="https://latex.codecogs.com/png.latex?(x_1%20+%20%5Cldots%20+%20x_N)%20/%20N%20%5C,%20%5Capprox%20%5C,%20%5Cmu">. Furthermore, one may be interested in the empirical distribution <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bp%7D"> associated to the sequence <img src="https://latex.codecogs.com/png.latex?(x_1,%20%5Cldots,%20x_N)"> when the rare event Equation&nbsp;1 does happen. The least unlikely empirical distribution is the one that minimizes <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BD_%7B%5Ctext%7BKL%7D%7D%7D%7D(%5Cwidehat%7Bp%7D,%20p)"> under the constraint that</p>
<p><span id="eq-contraint"><img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bi=1%7D%5EK%20a_k%20%5C,%20%5Cwidehat%7Bp%7D_k%20=%20%5Cmu.%0A%5Ctag%7B2%7D"></span></p>
<p>The function <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bp%7D%20%5Cmapsto%20%5Cmathop%7B%5Cmathrm%7BD_%7B%5Ctext%7BKL%7D%7D%7D%7D(%5Cwidehat%7Bp%7D,%20p)"> is convex and the introduction of Lagrange multipliers shows that the solution to this constraint minimization problem is given by the Boltzmann distribution <img src="https://latex.codecogs.com/png.latex?p_%7B%5Cbeta_%5Cmu%7D"> defined as</p>
<p><span id="eq-boltz"><img src="https://latex.codecogs.com/png.latex?%0Ap_%7B%5Cbeta_%5Cmu%7D(a_k)%20=%20%5Cfrac%7B%20p_k%20%5C,%20e%5E%7B-%5Cbeta_%7B%5Cmu%7D%20%5C,%20a_k%7D%20%7D%7BZ(%5Cbeta_%7B%5Cmu%7D)%7D.%0A%5Ctag%7B3%7D"></span></p>
<p>The parameter <img src="https://latex.codecogs.com/png.latex?%5Cbeta_%7B%5Cmu%7D%20%5Cin%20%5Cmathbb%7BR%7D"> is chosen so that the constraint Equation&nbsp;2 be satisfied and the minus sign is to follow the “physics” convention. Note in passing that, in fact, the joint function <img src="https://latex.codecogs.com/png.latex?(%5Cwidehat%7Bp%7D,p)%20%5Cmapsto%20%5Cmathop%7B%5Cmathrm%7BD_%7B%5Ctext%7BKL%7D%7D%7D%7D(%5Cwidehat%7Bp%7D,%20p)"> is convex! As usual, if one defines the log-partition function as <img src="https://latex.codecogs.com/png.latex?%5CPhi(%5Cbeta)%20=%20-%5Clog%20Z(%5Cbeta)">, with</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AZ(%5Cbeta)%20%5C;%20=%20%5C;%20%5Csum_%7Bk=1%7D%5EK%20%5C,%20p_k%20%5C,%20e%5E%7B-%5Cbeta%20%5C,%20a_k%7D,%0A"></p>
<p>one obtains that the constraint is equivalent to requiring <img src="https://latex.codecogs.com/png.latex?%5Cmu%20=%20%5Cfrac%7Bd%7D%7Bd%20%5Cbeta%7D%20%5CPhi(%5Cbeta)%20%5Cmid_%7B%5Cbeta%20=%20%5Cbeta_%5Cmu%7D">. Furthermore, since <img src="https://latex.codecogs.com/png.latex?%5CPhi"> is smooth and strictly concave, so is the function <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20%5Cmapsto%20%5Cleft%3C%20%5Cmu,%20%5Cbeta%20%5Cright%3E%20-%20%5CPhi(%5Cbeta)">, so that the condition <img src="https://latex.codecogs.com/png.latex?%5Cmu%20=%20%5Cfrac%7Bd%7D%7Bd%20%5Cbeta%7D%20%5CPhi(%5Cbeta)%20%5Cmid_%7B%5Cbeta%20=%20%5Cbeta_%5Cmu%7D"> is equivalent to setting</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbeta_%7B%5Cmu%7D%20%5C;%20=%20%5C;%20%5Cmathop%7B%5Cmathrm%7Bargmin%7D%7D_%7B%5Cbeta%20%5Cin%20%5Cmathbb%7BR%7D%7D%20%5C;%20%5Cleft%3C%20%5Cmu,%20%5Cbeta%20%5Cright%3E%20-%20%5CPhi(%5Cbeta).%0A"></p>
<p>Naturally, one can now also estimate the probability of the event <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BP%7D%7D%5B(X_1%20+%20%5Cldots%20+%20X_N)/N%20%5Capprox%20%5Calpha%5D"> happening since one now knows that it is equivalent (on a log scale) to <img src="https://latex.codecogs.com/png.latex?%5Cexp%5B-N%20%5C,%20%5Cmathop%7B%5Cmathrm%7BD_%7B%5Ctext%7BKL%7D%7D%7D%7D(p_%7B%5Cbeta_%5Cmu%7D,%20p)%5D">. Algebra gives</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cmathop%7B%5Cmathrm%7BD_%7B%5Ctext%7BKL%7D%7D%7D%7D(p_%7B%5Cbeta_%5Cmu%7D,%20p)%0A&amp;=%20%5CPhi(%5Cbeta_%5Cmu)%20-%20%5Cleft%3C%20%5Cmu,%20%5Cbeta_%5Cmu%20%5Cright%3E%5C%5C%0A&amp;=%20%5Cmax_%7B%5Cbeta%20%5Cin%20%5Cmathbb%7BR%7D%7D%20%5C;%20%5CPhi(%5Cbeta)%20-%20%5Cleft%3C%20%5Cmu,%20%5Cbeta%20%5Cright%3E.%0A%5Cend%7Balign%7D%0A"></p>
<p>As a sanity check, note that since <img src="https://latex.codecogs.com/png.latex?%5CPhi(0)=0">, we have that <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BD_%7B%5Ctext%7BKL%7D%7D%7D%7D(p_%7B%5Cbeta_%5Cmu%7D,%20p)%20%5Cgeq%200">, as required. The statement that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7BN%7D%20%5C,%20%5Clog%20%5Cmathop%7B%5Cmathrm%7BP%7D%7D%20%7B%5Cleft%5C%7B%20%5Cfrac%7BX_1%20+%20%5Cldots%20+%20X_N%7D%7BN%7D%20%5C;%20%5Capprox%20%5C;%20%5Cmu%20%5Cright%5C%7D%7D%20%20%5C;%20=%20%5C;%20-%20I(%5Cmu)%0A"></p>
<p>with a (Large Deviation) rate function given by</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AI(%5Cmu)%20%5C;%20=%20%5C;%20%5Cmax_%7B%5Cbeta%20%5Cin%20%5Cmathbb%7BR%7D%7D%20%5C;%20%5CPhi(%5Cbeta)%20-%20%5Cleft%3C%20%5Cmu,%20%5Cbeta%20%5Cright%3E%0A"></p>
<p>is more or less the content of <a href="https://en.wikipedia.org/wiki/Cramér%27s_theorem_(large_deviations)">Cramer’s Theorem</a>. The rate function <img src="https://latex.codecogs.com/png.latex?I(%5Cmu)"> and the function <img src="https://latex.codecogs.com/png.latex?%5Clog%20Z(%20%5Ctextcolor%7Bred%7D%7B-%7D%5Cbeta)"> are related by a <a href="https://en.wikipedia.org/wiki/Legendre_transformation">Legendre transform</a>.</p>
</section>
<section id="example-averaging-uniforms" class="level3">
<h3 class="anchored" data-anchor-id="example-averaging-uniforms">Example: averaging uniforms…</h3>
<p>Now, to illustrate the above discussion, consider <img src="https://latex.codecogs.com/png.latex?N=10"> iid uniform random variables on the interval <img src="https://latex.codecogs.com/png.latex?%5B0,1%5D">. It is straightforward to simulate these <img src="https://latex.codecogs.com/png.latex?N=10"> uniforms conditioned on the event that their mean exceeds the level <img src="https://latex.codecogs.com/png.latex?%5Cmu%20=%200.7">, which is a relatively rare event. When this is done repetitively and the empirical distribution is plotted, the resulting distribution is as follows:</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/sanov/boltzman.png" class="img-fluid figure-img" style="width:95.0%"></p>
<figcaption class="figure-caption">Mean of <img src="https://latex.codecogs.com/png.latex?10"> uniforms conditioned on being larger than <img src="https://latex.codecogs.com/png.latex?%5Cmu%20=%200.7"></figcaption>
</figure>
</div>
</div>
<p>Indeed, the distribution in blue is (very close to) the Boltzmann distribution with density <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7B%5Cbeta%7D(x)%20=%20e%5E%7B-%5Cbeta%20%5C,%20x%7D%20/%20Z(%5Cbeta)"> with <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20%5Cin%20%5Cmathbb%7BR%7D"> chosen so that <img src="https://latex.codecogs.com/png.latex?%5Cint_%7B0%7D%5E%7B1%7D%20x%20%5C,%20%5Cmathcal%7BD%7D_%7B%5Cbeta%7D(dx)%20=%20%5Cmu">.</p>


</section>

 ]]></description>
  <category>LargeDeviation</category>
  <guid>https://alexxthiery.github.io/notes/sanov/sanov.html</guid>
  <pubDate>Sun, 08 Oct 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Auxiliary variable trick</title>
  <link>https://alexxthiery.github.io/notes/auxiliary_variable_trick/auxiliary_variable_trick.html</link>
  <description><![CDATA[ 




<p>Consider a complicated distribution on the state space <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathcal%7BX%7D"> given by</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(x)%20=%20%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D%7D%20%5C,%20e%5E%7BC(x)%20+%20B(x)%7D%0A"></p>
<p>for a “complicated” functions <img src="https://latex.codecogs.com/png.latex?C(x)"> and a simpler one <img src="https://latex.codecogs.com/png.latex?B(x)">. In some situations, it is possible to introduce an auxiliary random variable <img src="https://latex.codecogs.com/png.latex?a%20%5Cin%20%5Cmathcal%7BA%7D"> and an extended probability distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi(x,a)"> on the extended space <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D%5Ctimes%20%5Cmathcal%7BA%7D">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(x,a)%20=%20%5Cpi(x)%20%5C,%20%20%5Ctextcolor%7Bred%7D%7B%5Cpi(a%20%7C%20x)%7D%20=%20%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D%7D%20e%5E%7BC(x)%20+%20B(x)%7D%20%5C,%20%20%5Ctextcolor%7Bred%7D%7Be%5E%7B-C(x)%20+%20D(x,%20a)%7D%7D,%0A"></p>
<p>with a tractable conditional probability <img src="https://latex.codecogs.com/png.latex?%5Cpi(a%20%7C%20x)">. This extended target distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi(x,a)%20=%20(1/%5Cmathcal%7BZ%7D)%20%5C,%20%5Cexp%5BB(x)%20+%20D(x,a)%5D"> can be often be easier to explore, for example when <img src="https://latex.codecogs.com/png.latex?a"> is continuous while <img src="https://latex.codecogs.com/png.latex?x"> is discrete, or to analyze, since the “complicated” term <img src="https://latex.codecogs.com/png.latex?C(x)"> has disappeared. Furthermore, there are a number of scenarios when the variable <img src="https://latex.codecogs.com/png.latex?x"> can be averaged out of the extended distribution, i.e.&nbsp;the distribution</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(a)%20=%20%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D%7D%20%5C,%20%5Cint_%7Bx%20%5Cin%20%5Cmathcal%7BX%7D%7D%20e%5E%7BB(x)%20+%20D(x,a)%7D%0A"></p>
<p>can be evaluated exactly.</p>
<section id="swendsenwang-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="swendsenwang-algorithm">Swendsen–Wang algorithm</h3>
<p>Consider a set of edges <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BE%7D"> on a graph with vertices <img src="https://latex.codecogs.com/png.latex?%5C%7B1,%20%5Cldots,%20N%5C%7D">. The Ising model is defined as <img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(x)%20%5Cpropto%20%5Cexp%20%20%7B%5Cleft%5C%7B%20%20%5Csum_%7B(i,j)%20%5Cin%20%5Cmathcal%7BE%7D%7D%20%5Cbeta%20x_i%20x_j%20%20%5Cright%5C%7D%7D%0A"></p>
<p>for spin configurations <img src="https://latex.codecogs.com/png.latex?x=(x_1,%20%5Cldots,%20x_N)%20%5Cin%20%5C%7B-1,1%5C%7D%5EN">. The term <img src="https://latex.codecogs.com/png.latex?%5Cexp%5B%5Cbeta%20x_i%20x_j%5D"> couples the two spins <img src="https://latex.codecogs.com/png.latex?x_i"> and <img src="https://latex.codecogs.com/png.latex?x_j"> for each edge <img src="https://latex.codecogs.com/png.latex?(i,j)%20%5Cin%20%5Cmathcal%7BE%7D">. The idea of the <a href="https://en.wikipedia.org/wiki/Swendsen–Wang_algorithm">Swendsen–Wang_algorithm</a> is to introduce an auxiliary variable <img src="https://latex.codecogs.com/png.latex?u_%7Bi,j%7D"> for each edge <img src="https://latex.codecogs.com/png.latex?(i,j)%20%5Cin%20%5Cmathcal%7BE%7D"> that is uniformly distributed on the interval <img src="https://latex.codecogs.com/png.latex?%5B0,%20%5Cexp(%5Cbeta%20x_i%20x_j)%5D">, i.e.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(u_%7Bi,j%7D%20%7C%20x)%20%5C;%20=%20%5C;%20%5Cfrac%7B%20%5Cmathbf%7B1%7D%20%20%7B%5Cleft%5C%7B%20%200%20%3C%20u_%7Bi,j%7D%20%3C%20%5Cexp%5B%5Cbeta%20x_i%20x_j%5D%20%20%5Cright%5C%7D%7D%20%20%7D%7B%5Cexp%5B%5Cbeta%20x_i%20x_j%5D%20%7D%0A"></p>
<p>It follows that the extended distribution on <img src="https://latex.codecogs.com/png.latex?%5C%7B-1,1%5C%7D%5EN%20%5Ctimes%20(0,%5Cinfty)%5E%7B%7C%5Cmathcal%7BE%7D%7C%7D"> reads</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(x,u)%20=%20%5Cfrac%7B1%7D%7BZ%7D%20%5Cprod_%7B(i,j)%20%5Cin%20%5Cmathcal%7BE%7D%7D%20%5C;%20%5Cmathbf%7B1%7D%20%20%7B%5Cleft%5C%7B%20%200%20%3C%20u_%7Bi,j%7D%20%3C%20%5Cexp%5B%5Cbeta%20x_i%20x_j%5D%20%20%5Cright%5C%7D%7D%0A"></p>
<p>for <img src="https://latex.codecogs.com/png.latex?x=(x_1,%20%5Cldots,%20x_N)"> and <img src="https://latex.codecogs.com/png.latex?u%20=%20(u_%7Bi,j%7D)_%7B(i,j)%20%5Cin%20%5Cmathcal%7BE%7D%7D">: the coupling term <img src="https://latex.codecogs.com/png.latex?%5Cexp%5B%5Cbeta%20x_i%20x_j%5D"> has disappeared. Furthermore, it is straightforward to sample from the conditional distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi(u%20%7C%20x)"> and, perhaps surprisingly, it is also relatively straightforward to sample from the other conditional distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi(x%20%7C%20u)"> – this boils down to finding the connect components of the graph on <img src="https://latex.codecogs.com/png.latex?%5C%7B1,%20%5Cldots,%20N%5C%7D"> with an edge <img src="https://latex.codecogs.com/png.latex?i%20%5Csim%20j"> present if <img src="https://latex.codecogs.com/png.latex?u_%7Bi,j%7D%20%3E%20e%5E%7B-%5Cbeta%7D"> and flipping a fair coin for setting each connected component to <img src="https://latex.codecogs.com/png.latex?%5Cpm%201">. This leads to an efficient Gibbs sampling scheme to sample from the extended distribution.</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/auxiliary_variable_trick/ising.gif" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Swendsen-Wang MCMC algorithm at critical temperature</figcaption>
</figure>
</div>
</div>
</section>
<section id="gaussian-integral-trick-curie-weiss-model" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-integral-trick-curie-weiss-model">Gaussian Integral trick: Curie-Weiss model</h3>
<p>For an inverse temperature <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20%3E%200">, consider the distribution on <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5C%7B-1,1%5C%7D%5EN"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(x)%20=%20%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D(%5Cbeta)%7D%20%5C,%20e%5E%7B%5Cbeta%20%5C,%20N%20%5C,%20m%5E2%7D%0A"></p>
<p>where the magnetization of the system of spins <img src="https://latex.codecogs.com/png.latex?x=(x_1,%20%5Cldots,%20x_N)"> is defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Am%20=%20%5Cfrac%7Bx_1%20+%20%5Cldots%20+%20x_N%7D%7BN%7D.%0A"></p>
<p>The distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)"> for <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20%5Cgg%201"> favours configurations with a magnetization close to <img src="https://latex.codecogs.com/png.latex?+1"> or <img src="https://latex.codecogs.com/png.latex?-1">. The normalization constant (i.e.&nbsp;partition function) <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BZ%7D(%5Cbeta)"> is a sum of <img src="https://latex.codecogs.com/png.latex?2%5EN"> terms,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BZ%7D(%5Cbeta)%20=%20%5Csum_%7Bs_1%20%5Cin%20%5C%7B%20%5Cpm%201%5C%7D%20%7D%20%5Cldots%20%5Csum_%7Bs_N%20%5Cin%20%5C%7B%20%5Cpm%201%5C%7D%20%7D%20%5Cexp%20%20%7B%5Cleft%5C%7B%20%20%5Cfrac%7B%5Cbeta%7D%7BN%7D%20%20%7B%5Cleft(%20%20%5Csum_%7Bi=1%7D%5EN%20x_i%20%20%5Cright)%7D%20%5E2%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>It is not difficult to estimate <img src="https://latex.codecogs.com/png.latex?%5Clog%20%5Cmathcal%7BZ%7D(%5Cbeta)"> as <img src="https://latex.codecogs.com/png.latex?N%20%5Cto%20%5Cinfty"> with combinatorial arguments. Nevertheless, another way to proceed is as follows. One can introduce a Gaussian auxiliary random variable <img src="https://latex.codecogs.com/png.latex?%5Cpi(a%20%7C%20x)%20=%20%5Cmathcal%7BN%7D(%5Calpha%20%5C,%20%20%7B%5Cleft(%20%5Csum_i%20x_i%20%5Cright)%7D%20%20,%20%5Csigma%5E2)"> with mean <img src="https://latex.codecogs.com/png.latex?%5Cmu%20=%20%5Calpha%20%5C,%20%20%7B%5Cleft(%20%5Csum_i%20x_i%20%5Cright)%7D%20"> and variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">: the parameters <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2%20%3E%200"> can then be judiciously chosen to cancel the bothering term <img src="https://latex.codecogs.com/png.latex?%5Cexp%5B%5Cfrac%7B%5Cbeta%7D%7BN%7D%20%5C,%20m%5E2%5D">. This approach is often called the a <a href="https://en.wikipedia.org/wiki/Hubbard–Stratonovich_transformation">Hubbard-Stratonovich transformation</a>. The bothering “coupling” term disappears when when choosing <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Calpha%5E2%7D%7B2%20%5Csigma%5E2%7D%20=%20%5Cfrac%7B%5Cbeta%7D%7BN%7D">. With such a choice, it follows that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(x,%20a)%20=%20%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D(%5Cbeta)%7D%20%5C,%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%20%5Cpi%20%5Csigma%5E2%7D%7D%20%5Cexp%20%7B%5Cleft%5C%7B%20%20-%20%5Cfrac%7Ba%5E2%7D%7B2%20%5Csigma%5E2%7D%20+%20%5Cfrac%7B%5Calpha%7D%7B%5Csigma%5E2%7D%20a%20%5C,%20%20%7B%5Cleft(%20%5Csum_i%20x_i%20%5Cright)%7D%20%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>Averaging out the <img src="https://latex.codecogs.com/png.latex?x_i%20%5Cin%20%5C%7B-1,%20+1%5C%7D"> gives that the partition function reads</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BZ%7D(%5Cbeta)%20%5C;%20=%20%5C;%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B2%20%5Cpi%20%5Csigma%5E2%7D%7D%20%5C,%0A%5Cint_%7Ba%20=%20-%5Cinfty%7D%5E%7Ba=%5Cinfty%7D%20%5C,%0A%5Cexp%20%7B%5Cleft%5C%7B%20-%5Cfrac%7Ba%5E2%7D%7B2%20%5Csigma%5E2%7D%20+%20%20%5Ctextcolor%7Bred%7D%7BN%7D%20%5C,%20%5Clog%5B%202%20%5C,%20%5Ccosh(%5Calpha%20a%20/%20%5Csigma%5E2)%5D%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>In order to use the <a href="https://en.wikipedia.org/wiki/Method_of_steepest_descent">method of steepest descent</a>, it would be useful to have an integrand of the type <img src="https://latex.codecogs.com/png.latex?%5Cexp%5BN%20%5Ctimes%20(%5Cldots)%5D">. One can choose <img src="https://latex.codecogs.com/png.latex?1/(2%20%5C,%20%5Csigma%5E2)%20=%20%5Cbeta%20%5C,%20N"> and <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%201/N">. This gives</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BZ%7D(%5Cbeta)%20%5C;%20=%20%5C;%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B2%20%5Cpi%20%5Csigma%5E2%7D%7D%20%5C,%0A%5Cint_%7Ba%20=%20-%5Cinfty%7D%5E%7Ba=%5Cinfty%7D%20%5C,%0A%5Cexp%20%20%7B%5Cleft%5C%7B%20%20%20%5Ctextcolor%7Bred%7D%7BN%7D%20%20%7B%5Cleft%5B%20%20-%5Cbeta%20%5C,%20a%5E2%20+%20%5Clog%5B%202%20%5C,%20%5Ccosh(2%20%5Cbeta%20a%20)%5D%20%5Cright%5D%7D%20%20%5Cright%5C%7D%7D%20%20%5C,%20da%0A"></p>
<p>from which one directly obtains that:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Clim_%7BN%20%5Cto%20%5Cinfty%7D%20%5C;%20-%5Cfrac%7B%5Clog%20%5Cmathcal%7BZ%7D(%5Cbeta)%7D%7BN%7D%0A%5C;%20=%20%5C;%0A%5Cmin_%7Ba%20%5Cin%20%5Cmathbb%7BR%7D%7D%20%5C;%20%5CBig%5C%7B%20%5Cbeta%20a%5E2%20-%20%5Clog%5B2%20%5C,%20%5Ccosh(2%20%5Cbeta%20a)%5D%20%5CBig%5C%7D.%0A"></p>
</section>
<section id="sherringtonkirkpatrick-model" class="level3">
<h3 class="anchored" data-anchor-id="sherringtonkirkpatrick-model">Sherrington–Kirkpatrick model</h3>
<p>Consider the distribution on <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5C%7B-1,1%5C%7D%5EN"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(x)%20=%20%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D(%5Cbeta)%7D%20%5C,%0A%5Cexp%20%7B%5Cleft%5C%7B%20%5Cfrac%2012%20%5C,%20%5Csum_%7Bi,j%7D%20W_%7Bij%7D%20x_i%20x_j%20%5Cright%5C%7D%7D%0A=%0A%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D(%5Cbeta)%7D%20%5C,%0A%5Cexp%20%7B%5Cleft%5C%7B%20%5Cfrac%2012%20%5C,%20%5Cleft%3C%20x,%20W%20x%20%5Cright%3E%20%5Cright%5C%7D%7D%0A"></p>
<p>where the <img src="https://latex.codecogs.com/png.latex?w_%7Bij%7D"> are some fixed weights with <img src="https://latex.codecogs.com/png.latex?w_%7Bij%7D%20=%20w_%7Bji%7D">. We assume that the matrix <img src="https://latex.codecogs.com/png.latex?W%20=%20%5BW_%7Bij%7D%5D_%7Bij%7D"> is positive definite: this can be achieved by adding <img src="https://latex.codecogs.com/png.latex?%5Clambda%20%5C,%20I_N"> to it if necessary, which does not change the distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi">. As described in <span class="citation" data-cites="zhang2012continuous">(Zhang et al. 2012)</span>, although I would not be surprised if similar ideas appeared in the physics literature much earlier, one can introduce a Gaussian auxiliary random variable <img src="https://latex.codecogs.com/png.latex?a%20=%20(a_1,%20%5Cldots,%20a_N)"> so that <img src="https://latex.codecogs.com/png.latex?%5Cpi(a%20%7C%20x)"> has mean <img src="https://latex.codecogs.com/png.latex?Fx"> and covariance <img src="https://latex.codecogs.com/png.latex?%5CGamma">. In other words,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(a%20%7C%20x)%20=%20%5Cfrac%7B1%7D%7B(2%20%5Cpi)%5E%7Bd/2%7D%20%5C,%20%7C%5CGamma%7C%5E%7B1/2%7D%7D%0A%5C,%20%5Cexp%20%7B%5Cleft%5C%7B%20-%5Cfrac%2012%20%5Cleft%3C%20(a%20-%20Fx),%20%5CGamma%5E%7B-1%7D%20(a%20-%20Fx)%20%5Cright%3E%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>In order to cancel-out the <img src="https://latex.codecogs.com/png.latex?%5Cleft%3C%20x,%20W,%20x%20%5Cright%3E"> it suffices to make sure that <img src="https://latex.codecogs.com/png.latex?F%5E%5Ctop%20%5C,%20%5CGamma%5E%7B-1%7D%20%5C,%20F%20=%20W">. There are a number of possibilities, the simplest approaches being perhaps</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A(F,%5CGamma)%20=%20(W%5E%7B1/2%7D,%20I_N)%0A%5Cquad%20%5Ctext%7Bor%7D%20%5Cquad%0A(F,%5CGamma)%20=%20(I,%20W%5E%7B-1%7D)%0A%5Cquad%20%5Ctext%7Bor%7D%20%5Cquad%0A(F,%5CGamma)%20=%20(W,%20W).%0A"></p>
<p>In any case, the joint distribution reads</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(x,a)%20%5C;%20=%20%5C;%20%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D%7D%20%5C,%20%5Cfrac%7B1%7D%7B(2%20%5Cpi)%5E%7Bd/2%7D%20%5C,%20%7C%5CGamma%7C%5E%7B1/2%7D%7D%20%5C,%0A%5Cexp%20%7B%5Cleft%5C%7B%20-%5Cfrac%7B1%7D%7B2%7D%20%5Cleft%3C%20a,%20%5CGamma%5E%7B-1%7D%20a%20%5Cright%3E%20+%20%5Cleft%3C%20x,%20F%5E%5Ctop%20%5CGamma%5E%7B-1%7D%20%5C,%20a%20%5Cright%3E%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>Indeed, one can try to implement some Gibbs-style update in order to explain this joint distribution since both <img src="https://latex.codecogs.com/png.latex?%5Cpi(x%7Ca)"> and <img src="https://latex.codecogs.com/png.latex?%5Cpi(a%7Cx)"> are straightforward to sample from: it is indeed related <a href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine">Restricted Boltzmann Machine</a> models. One can also average-out the spins <img src="https://latex.codecogs.com/png.latex?x_i%20%5Cin%20%5C%7B-1,1%5C%7D"> and obtain that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BZ%7D=%20%5Cint_%7B%5Cmathbb%7BR%7D%5EN%7D%0A%5Cexp%20%7B%5Cleft%5C%7B%20%20%5Csum_%7Bi=1%7D%5EN%20%5Clog%20%7B%5Cleft(%202%20%5C,%20%5Ccosh(%5BF%5E%5Ctop%20%5CGamma%5E%7B-1%7D%20%5C,%20a%5D_i)%20%5Cright)%7D%20%20%5Cright%5C%7D%7D%20%20%5C,%20%5Cmathcal%7BD%7D_%7B%5CGamma%7D(da)%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7B%5CGamma%7D"> is the density of a centred Gaussian distribution with covariance <img src="https://latex.codecogs.com/png.latex?%5CGamma">. [<strong>TODO:</strong> add SMC experiments to estimate <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BZ%7D">].</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-zhang2012continuous" class="csl-entry">
Zhang, Yichuan, Zoubin Ghahramani, Amos J Storkey, and Charles Sutton. 2012. <span>“Continuous Relaxations for Discrete Hamiltonian Monte Carlo.”</span> <em>Advances in Neural Information Processing Systems</em> 25.
</div>
</div></section></div> ]]></description>
  <category>auxiliary-variable</category>
  <guid>https://alexxthiery.github.io/notes/auxiliary_variable_trick/auxiliary_variable_trick.html</guid>
  <pubDate>Mon, 02 Oct 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Shearer’s lemma</title>
  <link>https://alexxthiery.github.io/notes/information_theory_shearer_lemma/shearer_lemma.html</link>
  <description><![CDATA[ 




<p>The <a href="https://en.wikipedia.org/wiki/Shearer%27s_inequality">Shearer’s lemma</a> <span class="citation" data-cites="chung1986some">(Chung et al. 1986)</span> is concerned with a generalization of the sub-additivity of the Shannon Entropy,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AH(X_1,%20%5Cldots,%20X_N)%20%5C;%20%5Cleq%20%5C;%20H(X_1)%20+%20%5Cldots%20+%20H(X_N).%0A"></p>
<p>Instead, consider an integer <img src="https://latex.codecogs.com/png.latex?t%20%5Cgeq%201"> and a family <img src="https://latex.codecogs.com/png.latex?S_1,%20%5Cldots,%20S_K"> of subsets of <img src="https://latex.codecogs.com/png.latex?%5C%7B1,%20%5Cldots,%20N%5C%7D"> such that any index <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20n%20%5Cleq%20N"> appears in at least <img src="https://latex.codecogs.com/png.latex?t"> of these subsets. Note that for a subset <img src="https://latex.codecogs.com/png.latex?S_i%20=%20%5C%7B%20%5Calpha_1,%20%5Cldots,%20%5Calpha_%7Br_i%7D%5C%7D"> with <img src="https://latex.codecogs.com/png.latex?%5Calpha_1%20%3C%20%5Cldots%20%3C%20%5Calpha_%7Br_i%7D"> we have</p>
<p><span id="eq-add"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AH(X_%7BS_i%7D)%20&amp;%5Cequiv%20H(X_%7B%5Calpha_1%7D,%20%5Cldots,%20X_%7B%5Calpha_%7Br_i%7D%7D)%5C%5C%0A&amp;=%20H(X_%7B%5Calpha_1%7D)%20+%20H(X_%7B%5Calpha_2%7D%20%7C%20X_%7B%5Calpha_1%7D)%20+%20%5Cldots%20+%20H(X_%7B%5Calpha_%7Br_i%7D%7D%20%7C%20X_%7B%5Calpha_%7Br_i-1%7D%7D,%20%5Cldots,%20X_%7B%5Calpha_1%7D%20)%20%5C%5C%0A&amp;%5Cgeq%20H(X_%7B%5Calpha_1%7D)%20+%20H(X_%7B%5Calpha_2%7D%20%7C%20X_%7B1:(%5Calpha_2-1)%7D)%20+%20%5Cldots%20+%20H(X_%7B%5Calpha_%7Br_i%7D%7D%20%7C%20X_%7B1:%5Calpha_%7Br_i%7D-1%7D).%0A%5Cend%7Balign%7D%0A%5Ctag%7B1%7D"></span></p>
<p>Since each index appears in at least <img src="https://latex.codecogs.com/png.latex?t"> of the subsets, summing Equation&nbsp;1 over all the subset <img src="https://latex.codecogs.com/png.latex?S_i"> yields</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bi=1%7D%5EK%20H(X_%7BS_k%7D)%20%5Cgeq%20t%20%5C,%20%5Csum_%7Bi=1%7D%5EN%20H(X_i%20%5C,%20%7C%20X_%7B1:(i-1)%7D)%20=%20t%20%5C,%20H(X).%0A"></p>
<p>This means that the following inequality holds,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AH(X)%20%5Cleq%20%5Cfrac%7B1%7D%7Bt%7D%20%5C,%20%5Csum_%7Bk=1%7D%5EK%20H(X_%7BS_k%7D)%0A"></p>
<p>Indeed, the standard sub-additivity property of the entropy corresponds to the set <img src="https://latex.codecogs.com/png.latex?S_k%20=%20%5Bk%5D"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20k%20%5Cleq%20N"> and <img src="https://latex.codecogs.com/png.latex?t=1">.</p>
<section id="application-projection-on-hyperplanes" class="level3">
<h3 class="anchored" data-anchor-id="application-projection-on-hyperplanes">Application: projection on hyperplanes</h3>
<p>Consider a measurable set <img src="https://latex.codecogs.com/png.latex?A%20%5Csubset%20%5Cmathbb%7BR%7D%5En"> and call <img src="https://latex.codecogs.com/png.latex?A_k"> the projection of <img src="https://latex.codecogs.com/png.latex?A"> on the hyperplane <img src="https://latex.codecogs.com/png.latex?%5C%7Bx=(x_1,%20%5Cldots,%20x_n)%20%5Cin%20%5Cmathbb%7BR%7D%5En%20%5C,%20:%20%5C,%20x_k=0%5C%7D">. A Theorem of Loomis and Whitney <span class="citation" data-cites="loomis1949inequality">(Loomis and Whitney 1949)</span> states that the lebesgue measure <img src="https://latex.codecogs.com/png.latex?%7CA%7C"> of the set <img src="https://latex.codecogs.com/png.latex?A"> satisfies</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7CA%7C%20%5C;%20%5Cleq%20%5C;%20%5Cprod_%7Bk=1%7D%5En%20%7CA_k%7C%5E%7B1/(n-1)%7D.%0A"></p>
<p>In other words, if all the projections <img src="https://latex.codecogs.com/png.latex?A_k"> of the set <img src="https://latex.codecogs.com/png.latex?A"> are small then, necessarily, the set <img src="https://latex.codecogs.com/png.latex?A"> itself is small. To proceed, one can approximate this set <img src="https://latex.codecogs.com/png.latex?A"> with the union <img src="https://latex.codecogs.com/png.latex?A_%7B%5Cvarepsilon%7D"> of small cubes of side <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon"> centred on <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5C,%20%5Cmathbb%7BZ%7D%5En">. If one can prove the statement for <img src="https://latex.codecogs.com/png.latex?A%5E%7B%5B%5Cvarepsilon%5D%7D">, the results follows from a standard approximation argument (ie. outer measure). Now, each cube can be indexed with a <img src="https://latex.codecogs.com/png.latex?n">-uple of integers <img src="https://latex.codecogs.com/png.latex?(x_1,%20%5Cldots,%20x_n)%20%5Cin%20%5Cmathbb%7BZ%7D%5En">, and one can consider the random variable <img src="https://latex.codecogs.com/png.latex?X=(X_1,%20%5Cldots,%20X_n)"> that is uniformly distributed on the set of cubes coordinates. Because <img src="https://latex.codecogs.com/png.latex?2%5E%7BH(X)%7D%20=%20%7CA%5E%7B%5B%5Cvarepsilon%5D%7D%7C%20/%20%5Cvarepsilon%5En"> and <img src="https://latex.codecogs.com/png.latex?2%5E%7BH(X_2,%20%5Cldots,%20X_n)%7D%20=%20%7CA_1%5E%7B%5B%5Cvarepsilon%5D%7D%7C%20/%20%5Cvarepsilon%5En"> etc…, choosing the subsets <img src="https://latex.codecogs.com/png.latex?S_i=%5B1:n%5D%20%5Csetminus%20%5C%7Bi%5C%7D"> and <img src="https://latex.codecogs.com/png.latex?t%20=%20(n-1)"> in Shearer’s Lemma immediately gives the conclusion.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-chung1986some" class="csl-entry">
Chung, Fan RK, Ronald L Graham, Peter Frankl, and James B Shearer. 1986. <span>“Some Intersection Theorems for Ordered Sets and Graphs.”</span> <em>Journal of Combinatorial Theory, Series A</em> 43 (1). Academic Press: 23–37.
</div>
<div id="ref-loomis1949inequality" class="csl-entry">
Loomis, Lynn H, and Hassler Whitney. 1949. <span>“An Inequality Related to the Isoperimetric Inequality.”</span>
</div>
</div></section></div> ]]></description>
  <category>infoTheory</category>
  <guid>https://alexxthiery.github.io/notes/information_theory_shearer_lemma/shearer_lemma.html</guid>
  <pubDate>Sun, 01 Oct 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Information Theory: References and Readings</title>
  <link>https://alexxthiery.github.io/notes/information_theory_references/information_theory_references.html</link>
  <description><![CDATA[ 




<section id="books" class="level3">
<h3 class="anchored" data-anchor-id="books">Books</h3>
<ul>
<li>“Elements of information theory” by T. M. Cover and J. A. Thomas – <em>perfect intro book to the topic.</em></li>
<li>“<a href="https://www.inference.org.uk/mackay/itila/">Information Theory, Inference, and Learning Algorithms</a>” by David J.C. MacKay</li>
<li>“<a href="http://www.stat.yale.edu/~yw562/ln.html">Information Theory From Coding to Learning</a>” by Yury Polyanskiy and Yihong Wu</li>
</ul>
</section>
<section id="lecture-notes-articles" class="level3">
<h3 class="anchored" data-anchor-id="lecture-notes-articles">Lecture Notes &amp; Articles</h3>
<ul>
<li>“<a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">A Mathematical Theory of Communication</a>” by C. Shannon (2948) – <em>entertaining and readable, even 70+ years later!</em></li>
<li>“<a href="https://web.stanford.edu/class/stats311/book.html">Lecture Notes on Statistics and Information Theory</a>” by John Duchi</li>
<li>“<a href="http://www.stat.yale.edu/~yw562/teaching/598/handouts.html">Information-theoretic methods for high-dimensional statistics</a>” by Yihong Wu</li>
</ul>


</section>

 ]]></description>
  <category>infoTheory</category>
  <guid>https://alexxthiery.github.io/notes/information_theory_references/information_theory_references.html</guid>
  <pubDate>Fri, 29 Sep 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Shannon Source Coding Theorem</title>
  <link>https://alexxthiery.github.io/notes/information_theory_shannon_coding/information_theory_shannon_coding.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/information_theory_shannon_coding/shannon.jpg" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Claude Shannon (1916 – 2001)</figcaption>
</figure>
</div>
</div>
<section id="transmission-through-a-noisy-channel" class="level3">
<h3 class="anchored" data-anchor-id="transmission-through-a-noisy-channel">Transmission through a noisy channel</h3>
<p>Consider a scenario involving a “noisy channel,” where a message <img src="https://latex.codecogs.com/png.latex?(x_1,x_2,%20%5Cldots)"> expressed in an alphabet <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D"> is transmitted before being received as a potentially different and corrupted message <img src="https://latex.codecogs.com/png.latex?(y_1,%20y_2,%5Cldots)"> expressed using a potentially different alphabet <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BY%7D">. One can assume that letter <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathcal%7BX%7D"> is transformed into <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cmathcal%7BY%7D"> with probability <img src="https://latex.codecogs.com/png.latex?p(x%20%5Cto%20y)"> so that the matrix <img src="https://latex.codecogs.com/png.latex?M_%7Bx,y%7D%20=%20%5Bp(x%20%5Cto%20y)%5D_%7B(x,y)%20%5Cin%20%5Cmathcal%7BX%7D%5Ctimes%20%5Cmathcal%7BY%7D%7D"> has rows summing-up to one, and that the “letters” of the message <img src="https://latex.codecogs.com/png.latex?(x_1%20x_2%20%5Cldots)"> are transmitted one by one and independently from each other (ie. there is no memory effect in the channel).</p>
<p>Now, imagine I have a text that needs to be transmitted through this channel. Assume that the text is represented using <img src="https://latex.codecogs.com/png.latex?N"> bits. My goal is to encode it in a way that introduces redundancy, utilizing the alphabet <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D">, so that it can be transmitted through the noisy channel. Eventually, I want it to be successfully decoded, allowing the original message to be recovered with minimal errors.</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/information_theory_shannon_coding/transmission.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">A Mathematical Theory of Communication</figcaption>
</figure>
</div>
</div>
<p>If transmitting each letter from the alphabet <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D"> takes <img src="https://latex.codecogs.com/png.latex?1"> unit of time, I need to estimate the overall time it will take to transmit the entire text of <img src="https://latex.codecogs.com/png.latex?N"> bits. In cases where the channel does not completely destroy the information, one can use about any non-idiotic encoding and transmit the same text multiple times to increase the chances of accurately recovering the original message. This can be achieved by employing techniques such as majority voting to decode the received messages, which are all distorted versions of the original text.</p>
<p>The transmission rate represents the inverse of the time required to transfer a single bit of information:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctextrm%7BR%20=%20(Transmission%20Rate)%7D%20=%20%5Cfrac%7B1%7D%7B%5Ctextrm%7B(average%20time%20it%20takes%20to%20transfer%20one%20bit)%7D%7D.%0A"></p>
<p>In other words, it takes about <img src="https://latex.codecogs.com/png.latex?N%20%5Ctimes%20R"> unit of times to transfer a text of <img src="https://latex.codecogs.com/png.latex?N"> bits. Moreover, the error rate refers to the percentage of errors in the decoded message, indicating the fraction of erroneous bits among the <img src="https://latex.codecogs.com/png.latex?N"> decoded bits. Naturally, there exists a tradeoff, and it is evident that one can reach a vanishing error rate if one is willing to allow an arbitrarily slow transmission rate (eg. majority voting after transmitting a very large number of times the same text). For example, if <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D=%20%5Cmathcal%7BY%7D=%20%5C%7B0,1%5C%7D"> and bits are flipped with probability <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctext%7Bflip%7D%7D%20%5Cll%201">, transmitting the text <img src="https://latex.codecogs.com/png.latex?(2K+1)"> times would lead to a transmission rate of <img src="https://latex.codecogs.com/png.latex?R%20=%201/(2K+1)"> and an error rate approximately equal to <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctext%7Bflip%7D%7D%5E%7BK+1%7D">.</p>
<p>The groundbreaking discovery made by Shannon is that it is possible to achieve a vanishing error rate even when transmitting at a finite transmission rate. He also managed to identify this optimal transmission rate. Shannon’s paper <span class="citation" data-cites="shannon1948mathematical">(Shannon 1948)</span> is beautifully written and surprisingly readable for a text written more than 50 years ago.</p>
</section>
<section id="vanishing-error-rate-shannon-codebooks" class="level3">
<h3 class="anchored" data-anchor-id="vanishing-error-rate-shannon-codebooks">Vanishing error rate: Shannon Codebooks</h3>
<p>Let’s imagine that we have a piece of information encoded in a variable, <img src="https://latex.codecogs.com/png.latex?X">. We send <img src="https://latex.codecogs.com/png.latex?X"> through a noisy channel, and at the other end we receive a somewhat distorted message, <img src="https://latex.codecogs.com/png.latex?Y">. So, how much of our original information actually was transmitted? To reconstruct our original message, <img src="https://latex.codecogs.com/png.latex?X">, using our received message, <img src="https://latex.codecogs.com/png.latex?Y">, we require an average of <img src="https://latex.codecogs.com/png.latex?H(X%7CY)"> additional bits of information. On average, <img src="https://latex.codecogs.com/png.latex?X"> contains <img src="https://latex.codecogs.com/png.latex?H(X)"> bits of information. So, if we encode <img src="https://latex.codecogs.com/png.latex?H(X)"> bits of useful information in <img src="https://latex.codecogs.com/png.latex?X">, the variable <img src="https://latex.codecogs.com/png.latex?Y"> that is correlated with <img src="https://latex.codecogs.com/png.latex?X"> still holds <img src="https://latex.codecogs.com/png.latex?I(X;Y)%20=%20H(X)%20-%20H(X%20%5C,%20%7C%20Y)"> bits of that original information. The quantity <img src="https://latex.codecogs.com/png.latex?I(X;Y)"> is the <a href="../../notes/information_theory_basics/information_theory_entropy.html">mutual information</a> between the random variables <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y">. In a noisy channel that transmits one “letter” at a time, the conditional probabilities <img src="https://latex.codecogs.com/png.latex?p(x%20%5Crightarrow%20y)"> are fixed. However, we can optimize the distribution of incoming messages. For instance, we can choose to transmit letters that are less likely to be corrupted. This discussion suggests that on average, transmitting <img src="https://latex.codecogs.com/png.latex?N"> symbols through the channel can provide up to <img src="https://latex.codecogs.com/png.latex?N%20%5Ctimes%20C"> bits of information, where <img src="https://latex.codecogs.com/png.latex?C%20=%20%5Cmax%20I(X;Y)">, the maximization being over the distribution of <img src="https://latex.codecogs.com/png.latex?X"> while keeping the conditional probabilities <img src="https://latex.codecogs.com/png.latex?p(x%20%5Crightarrow%20y)"> fixed. It may seem that this implies a noisy channel cannot transmit information at a rate higher than <img src="https://latex.codecogs.com/png.latex?C">. This hypothesis was precisely proven by Claude Shannon, who further established that this transmission rate can indeed be reached.</p>
<p>To prove that this transmission rate is achievable, Shannon’s idea was to simultaneously encode blocks of letters. To put it simply, consider the <img src="https://latex.codecogs.com/png.latex?2%5EN"> feasible blocks <img src="https://latex.codecogs.com/png.latex?%5C%7B%20t%5E%7B%5B1%5D%7D,%20%5Cldots,%20t%5E%7B%5B2%5EN%5D%7D%20%5C%7D"> of <img src="https://latex.codecogs.com/png.latex?N"> binary letters. Each block <img src="https://latex.codecogs.com/png.latex?t%5E%7B%5Bi%5D%7D%20%5Cin%20%5C%7B0,1%5C%7D%5EN"> has <img src="https://latex.codecogs.com/png.latex?N"> binary letters, <img src="https://latex.codecogs.com/png.latex?t%5E%7B%5Bi%5D%7D%20=%20(t_1%5E%7B%5Bi%5D%7D,%20%5Cldots,%20t_N%5E%7B%5Bi%5D%7D)">. Associate to each of block <img src="https://latex.codecogs.com/png.latex?t%5E%7B%5Bi%5D%7D%20%5Cin%20%5C%7B0,1%5C%7D%5EN"> a <strong>codeword</strong> <img src="https://latex.codecogs.com/png.latex?x%5E%7B%5Bi%5D%7D%20%5Cin%20%5Cmathcal%7BX%7D%5EK"> of size <img src="https://latex.codecogs.com/png.latex?K"> in the alphabet <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D">. The set of these <img src="https://latex.codecogs.com/png.latex?2%5EN"> codewords is usually called the <strong>codebook</strong>,</p>
<p><span id="eq-encode"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BC%7D=%20%5Cleft%5C%7B%20x%5E%7B%5B1%5D%7D,%20x%5E%7B%5B2%5D%7D,%20%5Cldots,%20x%5E%7B%5B2%5EN%5D%7D%20%5Cright%5C%7D%20%5C;%20%5Csubset%20%5Cmathcal%7BX%7D%5E%7BK%7D%0A%5Ctag%7B1%7D"></span></p>
<p>To transmit a block of <img src="https://latex.codecogs.com/png.latex?N"> letters from the original text, this block is first transformed into its associated codeword <img src="https://latex.codecogs.com/png.latex?x=(x_1,%20%5Cldots,%20x_K)%20%5Cin%20%5Cmathcal%7BX%7D%5EK">. This codeword is then sent through the noisy channel, resulting in a received message <img src="https://latex.codecogs.com/png.latex?(y_1,%20%5Cldots,%20y_K)%20%5Cin%20%5Cmathcal%7BY%7D%5EK">. The objective is to design a codebook with enough redundancy so that one can reconstruct the original codeword from the received message <img src="https://latex.codecogs.com/png.latex?(y_1,%20%5Cldots,%20y_K)">: the higher the ratio <img src="https://latex.codecogs.com/png.latex?K/N">, the larger the redundancy and the easier it should be to achieve this goal. The transmission rate is defined as <img src="https://latex.codecogs.com/png.latex?R%20=%20%5Cfrac%7BN%7D%7BK%7D"> since transmitting a binary text of length <img src="https://latex.codecogs.com/png.latex?N"> with vanishing errors takes <img src="https://latex.codecogs.com/png.latex?K"> units of time.</p>
<p>For generating the codebook in Equation&nbsp;1, Shannon adopted a simple approach consisting in generating each <img src="https://latex.codecogs.com/png.latex?x%5E%7B%5Bi%5D%7D_k"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%5Cleq%202%5EN"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20k%20%5Cleq%20K"> independently at random from some (encoding) distribution <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctext%7Bcode%7D%7D(dx)">. The choice of this encoding distribution can be optimized at a later stage.</p>
<p>Consider the codeword <img src="https://latex.codecogs.com/png.latex?x%5E%7B%5B0%5D%7D%20=%20(x%5E%7B%5B0%5D%7D_1,%20%5Cldots,%20x%5E%7B%5B0%5D%7D_K)">. After being transmitted through the noisy channel, this gives rise to a message <img src="https://latex.codecogs.com/png.latex?y_%7B%5Cstar%7D">. The codeword <img src="https://latex.codecogs.com/png.latex?x%5E%7B%5B0%5D%7D"> can be easily recovered if <img src="https://latex.codecogs.com/png.latex?(x%5E%7B%5B0%5D%7D,%20y_%5Cstar)"> is typical while all the other pairs <img src="https://latex.codecogs.com/png.latex?(x%5E%7B%5Bi%5D%7D,%20y_%5Cstar)"> for <img src="https://latex.codecogs.com/png.latex?2%20%5Cleq%20i%20%5Cleq%202%5EN"> are atypical. Since there are about <img src="https://latex.codecogs.com/png.latex?2%5E%7BK%20%5C,%20H(X%20%7C%20Y)%7D"> elements <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathcal%7BX%7D%5EK"> such that <img src="https://latex.codecogs.com/png.latex?(x,%20y_%5Cstar)"> is typical, and each codeword was chosen approximately uniformly at random within its typical set of size <img src="https://latex.codecogs.com/png.latex?2%5E%7BK%20%5C,%20H(X)%7D">, the probability for a random codeword to be atypical is about</p>
<p><img src="https://latex.codecogs.com/png.latex?1-2%5E%7B-K%20%5C,%20%5BH(X)%20-%20H(X%7CY)%5D%7D%20=%201%20-%202%5E%7B-K%20%5C,%20I(X;Y)%7D"></p>
<p>Consequently, the probability that all the other pairs <img src="https://latex.codecogs.com/png.latex?(x%5E%7B%5Bi%5D%7D,%20y_%5Cstar)"> for <img src="https://latex.codecogs.com/png.latex?2%20%5Cleq%20i%20%5Cleq%202%5EN"> are atypical is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap_%7B%5Ctext%7Bsuccess%7D%7D%20=%20(1%20-%202%5E%7B-K%20%5C,%20I(X;Y)%7D)%5E%7B2%5EN-1%7D%20%5Capprox%20(1%20-%202%5E%7B-K%20%5C,%20I(X;Y)%7D)%5E%7B2%5E%7BKR%7D%7D.%0A"></p>
<p>The probability <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctext%7Bsuccess%7D%7D%20%5Cto%201"> as soon as <img src="https://latex.codecogs.com/png.latex?R%20%3C%20I(X;Y)"> as <img src="https://latex.codecogs.com/png.latex?N%20%5Cto%20%5Cinfty">. Furthermore, remembering that one were free to optimize the encoding distribution <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctext%7Bcode%7D%7D(dx)">, a vanishing error rate is possible as soon as the transmission <img src="https://latex.codecogs.com/png.latex?R"> rate is lower than</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7B(Channel%20Capacity)%7D%20=%20C%20%5Cequiv%20%5Cmax_%7Bp_%7B%5Ctext%7Bcode%7D%7D%7D%20%5C;%20I(X;Y).%0A"></p>
<p>To sum-up, consider <img src="https://latex.codecogs.com/png.latex?p_%7B%5Cmathcal%7BC%7D,%20%5Ctext%7Bsuccess%7D%7D"> the success rate of the codebook <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D">, ie. the probability that a random codeword of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D"> is successfully decoded when passing through the noisy channel. The reasoning above shows that the averaged success rate <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctext%7Bsuccess%7D%7D%20=%20%5Cleft%3C%20p_%7B%5Cmathcal%7BC%7D,%20%5Ctext%7Bsuccess%7D%7D%20%5Cright%3E">, i.e.&nbsp;averaging <img src="https://latex.codecogs.com/png.latex?p_%7B%5Cmathcal%7BC%7D,%20%5Ctext%7Bsuccess%7D%7D"> over all possible codebooks <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D">, converges to one as long as the transmission rate is below the channel capacity <img src="https://latex.codecogs.com/png.latex?C">. This means that one can find at least one codebook that works well! This reasoning is an example of the “probabilistic method”… Indeed, one also expect <strong>most</strong> random codebook to work well!</p>
</section>
<section id="no-vanishing-error-below-the-channel-capacity" class="level3">
<h3 class="anchored" data-anchor-id="no-vanishing-error-below-the-channel-capacity">No vanishing error below the channel capacity</h3>
<p>To demonstrate that transmission at vanishing error-rate is impossible when the transmission rate exceeds the channel capacity, <img src="https://latex.codecogs.com/png.latex?C">, we can utilize <a href="../../notes/information_theory_fano/information_theory_fano.html">Fano’s inequality</a>.</p>
<p>Imagine selecting a message <img src="https://latex.codecogs.com/png.latex?M"> uniformly at random within <img src="https://latex.codecogs.com/png.latex?%5C%7B0,1%5C%7D%5EN"> and encode this message into the sequence <img src="https://latex.codecogs.com/png.latex?X=(X_1,%20...,%20X_K)%20%5Cin%20%5Cmathcal%7BX%7D%5EK">. We send <img src="https://latex.codecogs.com/png.latex?X"> through a channel with capacity <img src="https://latex.codecogs.com/png.latex?C"> and receive a corresponding, though somewhat distorted, signal <img src="https://latex.codecogs.com/png.latex?Y=(Y_1,%20...,%20Y_K)">. Finally, we decode this received message into <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BM%7D">, an estimate of our original message:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AM%20%5Crightarrow%20X%20%5Crightarrow%20Y%20%5Crightarrow%20%5Cwidehat%7BM%7D.%0A"></p>
<p>Fano’s inequality points out that the error probability, <img src="https://latex.codecogs.com/png.latex?p_E%20=%20%5Cmathop%7B%5Cmathrm%7BP%7D%7D(%5Cwidehat%7BM%7D%20%5Cneq%20M)"> is such that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AH(M%20%7C%20%5Cwidehat%7BM%7D)%0A&amp;%5Cleq%201%20+%20p_E%20%5C,%20%5Clog_2(%5C#%20%5Ctextrm%7Bpossible%20values%20of%20%7D%20M)%5C%5C%0A&amp;=%201%20+%20p_E%20%5C,%20N%0A%5Cend%7Balign%7D%0A"></p>
<p>Applying the data-processing inequality to <img src="https://latex.codecogs.com/png.latex?M%20%5Crightarrow%20X%20%5Crightarrow%20Y%20%5Crightarrow%20%5Cwidehat%7BM%7D"> proves:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AN%20&amp;=%20H(M)%20=%20H(M%20%7C%20%5Cwidehat%7BM%7D)%20+%20I(M;%20%5Cwidehat%7BM%7D)%20%5C%5C%0A&amp;%20%5Cleq%20H(M%20%7C%20%5Cwidehat%7BM%7D)%20+%20I(X;%20Y)%5C%5C%0A&amp;%20%5Cleq%201%20+%20N%20%5C,%20p_E%20+%20I(X;%20Y).%0A%5Cend%7Balign%7D%0A"></p>
<p>To wrap up, recall that each received letter <img src="https://latex.codecogs.com/png.latex?Y_i"> in the message (Y_1, , Y_K)$ depends solely on the corresponding letter <img src="https://latex.codecogs.com/png.latex?X_i"> in the message sent through the channel. This <a href="../../notes/information_theory_basics/information_theory_entropy.html">implies</a> that <img src="https://latex.codecogs.com/png.latex?I(X;%20Y)%20%5Cleq%20%5Csum_%7Bi=1%7D%5EK%20I(X_i;%20Y_i)%20%5Cleq%20K%20%5C,%20C">.This yields:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AN%20%5Cleq%201%20+%20N%20%5C,%20p_E%20+%20K%20%5C,%20C.%0A"></p>
<p>This reveals that for the probability of error to go to zero, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?p_E%20%5Crightarrow%200"> as <img src="https://latex.codecogs.com/png.latex?N%20%5Crightarrow%20%5Cinfty">, the transmission rate <img src="https://latex.codecogs.com/png.latex?N/K"> must be lower than <img src="https://latex.codecogs.com/png.latex?C">.</p>
</section>
<section id="experiment" class="level3">
<h3 class="anchored" data-anchor-id="experiment">Experiment</h3>
<p>Consider the <a href="https://en.wikipedia.org/wiki/Binary_symmetric_channel">Binary Symmetric Channel (BSC)</a> that randomly flips <img src="https://latex.codecogs.com/png.latex?0%20%5Cmapsto%201"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cmapsto%200"> with equal probability <img src="https://latex.codecogs.com/png.latex?0%3Cq%3C1">. The capacity of this channel is easily computed and equals <img src="https://latex.codecogs.com/png.latex?C%20=%201%20-%20h_2(q)"> where <img src="https://latex.codecogs.com/png.latex?h_2(q)%20=%20-%5Bq%20%5C,%20%5Clog_2(q)%20+%20(1-q)%20%5C,%20%5Clog_2(1-q)%5D"> is the binary entropy function: the optimal encoding distribution is</p>
<p><img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctext%7Bcode%7D%7D(0)%20=%20p_%7B%5Ctext%7Bcode%7D%7D(2)%20=%201/2."></p>
<p>For a flipping rate of <img src="https://latex.codecogs.com/png.latex?q=0.1"> the channel capacity equals <img src="https://latex.codecogs.com/png.latex?C=0.53">. To estimate the performance of the random Shannon codebook strategy, I chose <img src="https://latex.codecogs.com/png.latex?N=13"> and several values of <img src="https://latex.codecogs.com/png.latex?K%20%5Cgeq%20N">. This means generating a random codebook <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D=%20%5C%7Bx%5E%7B%5B1%5D%7D,%20%5Cldots,%20x%5E%7B%5B2%5EN%5D%7D%5C%7D"> of size <img src="https://latex.codecogs.com/png.latex?2%5E%7B13%7D%20=%208192"> consisting of random binary vectors of size <img src="https://latex.codecogs.com/png.latex?K">. For a randomly chosen codeword <img src="https://latex.codecogs.com/png.latex?x%5E%7B%5Bi%5D%7D">, a received message <img src="https://latex.codecogs.com/png.latex?y_%5Cstar"> is generated by flipping each of the <img src="https://latex.codecogs.com/png.latex?K"> coordinates of <img src="https://latex.codecogs.com/png.latex?x%5E%7B%5Bi%5D%7D"> independently with probability <img src="https://latex.codecogs.com/png.latex?q">. In the BSC setting, it is easily seen that the codeword of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D"> that was the most likely to have originated <img src="https://latex.codecogs.com/png.latex?y_%7B%5Cstar%7D"> is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax_%5Cstar%20%5C;%20=%20%5C;%20%5Cmathop%7B%5Cmathrm%7Bargmin%7D%7D_%7Bx%20%5Cin%20%5Cmathcal%7BC%7D%7D%20%5C;%20%5C%7Cx%20-%20y_%5Cstar%5C%7C_%7BL%5E2%7D.%0A"></p>
<p>The nearest neighbor <img src="https://latex.codecogs.com/png.latex?x_%5Cstar"> can be relatively efficiently computed with a nearest-neighbor routine (eg. <a href="https://github.com/facebookresearch/faiss/wiki">FAISS</a>). The figure below reports the probability of error (i.e.&nbsp;“Block Error Rate”),</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7B(Block%20Error%20Rate)%7D%20%5C;%20=%20%5C;%20%5Cmathop%7B%5Cmathrm%7BP%7D%7D(x_%5Cstar%20%5Cneq%20x%5E%7B%5Bi%5D%7D)%0A"></p>
<p>when the codeword <img src="https://latex.codecogs.com/png.latex?x%5E%7B%5Bi%5D%7D"> is chosen uniformly at random within the codebook.</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/information_theory_shannon_coding/shannon_code.png" class="img-fluid figure-img" style="width:80.0%"></p>
</figure>
</div>
</div>
<p>It can be seen that, although the error rate does go to zero for low transmission rate, the choice of <img src="https://latex.codecogs.com/png.latex?K%20=%20N%20/%20C"> where <img src="https://latex.codecogs.com/png.latex?C"> is the channel capacity still yields a relatively large block error rate. This indicates that the block size <img src="https://latex.codecogs.com/png.latex?N=13"> is still far too low for the “law of large number” arguments presented in the previous section to kick-in. I did try for <img src="https://latex.codecogs.com/png.latex?N=20"> and a codebook of <img src="https://latex.codecogs.com/png.latex?2%5E%7B20%7D%20%5Capprox%2010%5E6"> and the performace was still not impressive. This shows that even though the Shannon codebook approaches is an elegant construction, it is far from being practically useful. It requires a very large codebook of size <img src="https://latex.codecogs.com/png.latex?2%5EN"> and decoding requires doing a nearest-neighbors search that can become slow as <img src="https://latex.codecogs.com/png.latex?N"> increases.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-shannon1948mathematical" class="csl-entry">
Shannon, Claude Elwood. 1948. <span>“A Mathematical Theory of Communication.”</span> <em>The Bell System Technical Journal</em> 27 (3). Nokia Bell Labs: 379–423.
</div>
</div></section></div> ]]></description>
  <category>infoTheory</category>
  <guid>https://alexxthiery.github.io/notes/information_theory_shannon_coding/information_theory_shannon_coding.html</guid>
  <pubDate>Mon, 25 Sep 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Information Theory: Fano’s inequality</title>
  <link>https://alexxthiery.github.io/notes/information_theory_fano/information_theory_fano.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/information_theory_fano/robert_fano.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption"><a href="https://en.wikipedia.org/wiki/Robert_Fano">Robert Fano (1917 – 2016)</a></figcaption>
</figure>
</div>
</div>
<section id="fanos-inequality" class="level3">
<h3 class="anchored" data-anchor-id="fanos-inequality">Fano’s inequality</h3>
<p>Consider a three random variables forming a Markov chain,</p>
<p><span id="eq-markov"><img src="https://latex.codecogs.com/png.latex?%0AX%20%5Cmapsto%20Y%20%5Cmapsto%20%5Cwidehat%7BX%7D%0A%5Ctag%7B1%7D"></span></p>
<p>in the sense that <img src="https://latex.codecogs.com/png.latex?Y%20=%20%5Ctextrm%7Bfunction%7D(X,%20%5Ctext%7Bnoise%7D)"> and <img src="https://latex.codecogs.com/png.latex?Z%20=%20%5Ctextrm%7Bfunction%7D(Y,%20%5Ctext%7Bnoise%7D)">. Typical situations include:</p>
<ul>
<li><p>We select a parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> for a probabilistic model <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BP%7D%7D_%7B%5Ctheta%7D">. Afterward, we collect data <img src="https://latex.codecogs.com/png.latex?X"> from this model, and our goal is to estimate the parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> solely from the data <img src="https://latex.codecogs.com/png.latex?X">.</p></li>
<li><p>We generate data <img src="https://latex.codecogs.com/png.latex?X">, compress this data into <img src="https://latex.codecogs.com/png.latex?X_%7B%5Ctext%7Bzip%7D%7D">, and then attempt to recover the original data <img src="https://latex.codecogs.com/png.latex?X"> as closely as we can.</p></li>
</ul>
<p>Since each step in Equation&nbsp;1 destroys some information (eg. <a href="../../notes/information_theory_basics/information_theory_entropy.html">data processing</a>), it is important to measure how accurately <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BX%7D"> estimates the initial input, <img src="https://latex.codecogs.com/png.latex?X">. In other words, we want to know how much more information (expressed as ‘bits’) we need to reconstruct <img src="https://latex.codecogs.com/png.latex?X"> using knowledge of <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BX%7D"> alone, i.e.&nbsp;we would like to upper-bound <img src="https://latex.codecogs.com/png.latex?H(X%20%5C,%20%7C%20%5Cwidehat%7BX%7D)">. For this purpose, imagine an “error” variable <img src="https://latex.codecogs.com/png.latex?E"> that indicates whether <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BX%7D"> perfectly matches <img src="https://latex.codecogs.com/png.latex?X">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AE%20=%20%5Cmathbf%7B1%7D%20%7B%5Cleft(%20%5Cwidehat%7BX%7D%20%5Cneq%20X%20%5Cright)%7D%20.%0A"></p>
<p>The probability of error is <img src="https://latex.codecogs.com/png.latex?p_E%20=%20%5Cmathop%7B%5Cmathrm%7BP%7D%7D(%5Cwidehat%7BX%7D%20%5Cneq%20X)"> and <img src="https://latex.codecogs.com/png.latex?E%20=%20%5Ctext%7BBern%7D(p_E)">. To estimate <img src="https://latex.codecogs.com/png.latex?X"> from <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BX%7D">, we can start by learning if <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BX%7D"> equals <img src="https://latex.codecogs.com/png.latex?X">, which costs us <img src="https://latex.codecogs.com/png.latex?H(E%20%7C%20%5Cwidehat%7BX%7D)%20%5Cleq%20H(E)%20=%20h_2(p_E)"> ‘bits’ of information. If it turns out that <img src="https://latex.codecogs.com/png.latex?E=0">, we are done asking. If we find that <img src="https://latex.codecogs.com/png.latex?E=1">, however, we need to ask additional <img src="https://latex.codecogs.com/png.latex?H(X%20%7C%20%5Cwidehat%7BX%7D,%20E)"> questions. Crucially, <img src="https://latex.codecogs.com/png.latex?H(X%20%7C%20%5Cwidehat%7BX%7D,%20E)%20%5Cleq%20H(X)">, but also <img src="https://latex.codecogs.com/png.latex?H(X%20%7C%20%5Cwidehat%7BX%7D,%20E)%20%5Cleq%20%5Clog_2(%7C%5Cmathcal%7BX%7D%7C-1)"> since <img src="https://latex.codecogs.com/png.latex?X"> can take any value in <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D"> except <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BX%7D"> when <img src="https://latex.codecogs.com/png.latex?E=1">. Writing this reasoning quantitatively gives <a href="https://en.wikipedia.org/wiki/Fano%27s_inequality">Fano’s inequality</a>:</p>
<p><span id="eq-fano"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AH(X%20%7C%20%5Cwidehat%7BX%7D)%0A&amp;%5Cleq%20h_2(p_E)%20+%20p_E%20%5C,%20%5Clog_2(%7C%5Cmathcal%7BX%7D%7C-1).%0A%5Cend%7Balign%7D%0A%5Ctag%7B2%7D"></span></p>
<p>Apparently, this inequality was first derived by <a href="https://en.wikipedia.org/wiki/Robert_Fano">Robert Fano</a> in the 50s while teaching a Ph.D.&nbsp;seminar at MIT. In words: a large <img src="https://latex.codecogs.com/png.latex?H(X%20%7C%20%5Cwidehat%7BX%7D)"> means that <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BX%7D"> offers insufficient information about <img src="https://latex.codecogs.com/png.latex?X">, and as a result, the probability of error <img src="https://latex.codecogs.com/png.latex?p_E"> must be high.</p>
</section>
<section id="applications" class="level3">
<h3 class="anchored" data-anchor-id="applications">Applications:</h3>
<ul>
<li>Converse of <a href="../../notes/information_theory_shannon_coding/information_theory_shannon_coding.html">Shannon’s coding theorem</a></li>
</ul>


</section>

 ]]></description>
  <category>infoTheory</category>
  <guid>https://alexxthiery.github.io/notes/information_theory_fano/information_theory_fano.html</guid>
  <pubDate>Fri, 22 Sep 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Information Theory: Entropy and Basic Definitions</title>
  <link>https://alexxthiery.github.io/notes/information_theory_basics/information_theory_entropy.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/information_theory_basics/shannon_destruction.gif" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption class="figure-caption">Claude Shannon (1916 – 2001)</figcaption>
</figure>
</div>
</div>
<section id="shannon-entropy-compression" class="level3">
<h3 class="anchored" data-anchor-id="shannon-entropy-compression">Shannon Entropy &amp; Compression</h3>
<p>If Alice chooses a number <img src="https://latex.codecogs.com/png.latex?X"> uniformly at random from the set <img src="https://latex.codecogs.com/png.latex?%5C%7B1,2,%20%5Cldots,%20N%5C%7D">, Bob can use a simple “dichotomy” strategy to ask Alice <img src="https://latex.codecogs.com/png.latex?%5Clog_2(N)"> binary Yes/No questions and correctly identify the number. This result can be generalized to non-uniform distributions (cf.&nbsp;<a href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman codes</a>, and also <a href="https://en.wikipedia.org/wiki/Kraft–McMillan_inequality">Kraft-McMillan inequality</a>). If Alice chooses a number <img src="https://latex.codecogs.com/png.latex?X"> from <img src="https://latex.codecogs.com/png.latex?%5C%7B1,2,%20%5Cldots,%20N%5C%7D"> with probabilities <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BP%7D%7D(X=k)%20=%20p_k">, Bob can design a deterministic strategy to find the answer using, on average, about</p>
<p><span id="eq-OU"><img src="https://latex.codecogs.com/png.latex?%0AH(X)%20=%20-%20%5Csum_%7Bk=1%7D%5EN%20p_k%20%5C,%20%5Clog_2(p_k)%0A%5Ctag%7B1%7D"></span></p>
<p>binary questions, ie. bits. To be more precise, there are strategies that require at most <img src="https://latex.codecogs.com/png.latex?H(X)%20+%201"> questions on average, and none that can require less than <img src="https://latex.codecogs.com/png.latex?H(X)">. Note that applying this remark to an iid sequence <img src="https://latex.codecogs.com/png.latex?X_%7B1:T%7D%20=%20(X_1,%20%5Cldots,%20X_T)"> and using the the fact that <img src="https://latex.codecogs.com/png.latex?H(X_1,%20%5Cldots,%20X_T)%20=%20T%20%5C,%20H(X)">, this shows that one can exactly determining the sequence <img src="https://latex.codecogs.com/png.latex?X_%7B1:T%7D"> with at most <img src="https://latex.codecogs.com/png.latex?T%20%5C,%20H(X)%20+%201"> binary questions on average. The quantity <img src="https://latex.codecogs.com/png.latex?H(X)"> defined in Equation&nbsp;1, known as the <strong>Shannon Entropy</strong> of the distribution <img src="https://latex.codecogs.com/png.latex?(p_1,%20%5Cldots,%20p_N)">, also implies that there are strategies that can encode each integer <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20x%20%5Cleq%20N"> as a binary string of length <img src="https://latex.codecogs.com/png.latex?L(x)"> (i.e.&nbsp;with <img src="https://latex.codecogs.com/png.latex?L(x)"> bits), with the expected length <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BE%7D%7D%5BL(X)%5D"> approximately equal to <img src="https://latex.codecogs.com/png.latex?H(X)">. It is because a sequence of binary questions can be thought of as a binary tree, etc…</p>
<p>This remark can be used for compression. Imagine a very long sequence <img src="https://latex.codecogs.com/png.latex?(X_1,%20%5Cldots,%20X_T)"> of iid samples from <img src="https://latex.codecogs.com/png.latex?X">. Encoding each <img src="https://latex.codecogs.com/png.latex?X_i"> with <img src="https://latex.codecogs.com/png.latex?L(X_i)"> bits, one should be able to encode the resulting sequence with</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL(X_1)%20+%20%5Cldots%20+%20L(X_T)%20%5Capprox%20T%20%5C,%20%5Cmathop%7B%5Cmathrm%7BE%7D%7D%5BL(X)%5D%20%5Capprox%20T%20%5Ccdot%20H(X)%0A"></p>
<p>bits. Can the usual <strong>zip compression</strong> algorithm do this? To test this, choose a probability distribution on <img src="https://latex.codecogs.com/png.latex?%5C%7B1,%20%5Cldots,%20N%5C%7D">, generate an iid sequence of length <img src="https://latex.codecogs.com/png.latex?T%20%5Cgg%201">, compress this using the <img src="https://latex.codecogs.com/png.latex?%5Ctexttt%7Bgzip%7D(%5Cldots)"> command, and finally look at the size of the resulting file (in bits). I have done that a few times, with a few different values of <img src="https://latex.codecogs.com/png.latex?16%20%5Cleq%20N%20%5Cleq%20256"> and a few random distributions on <img src="https://latex.codecogs.com/png.latex?%5C%7B1,%20%5Cldots,%20N%5C%7D">, and with <img src="https://latex.codecogs.com/png.latex?T%20=%2010%5E6">. The plot of size of the compressed files versus the Shannon entropy <img src="https://latex.codecogs.com/png.latex?H"> looks as below:</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/information_theory_basics/shannon_vs_zip.png" class="img-fluid figure-img" style="width:90.0%"></p>
</figure>
</div>
</div>
<p>Seems like the zip-algorithm works almost optimally for compressing iid sequences.</p>
</section>
<section id="sequence-of-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="sequence-of-random-variables">Sequence of random variables</h3>
<p>Now consider a pair of discrete random variables <img src="https://latex.codecogs.com/png.latex?(X,Y)">. If Alice draws samples from this pair of rvs, one can ask <img src="https://latex.codecogs.com/png.latex?H(X,Y)"> binary questions on average to exactly find out these values. To do that, one can ask <img src="https://latex.codecogs.com/png.latex?H(X)"> questions to estimate <img src="https://latex.codecogs.com/png.latex?X">, and once <img src="https://latex.codecogs.com/png.latex?X=x"> is estimated, one can then ask about <img src="https://latex.codecogs.com/png.latex?H(Y%7CX=x)%20=%20-%5Csum_y%20%5Cmathop%7B%5Cmathrm%7BP%7D%7D(Y=y%7CX=x)%20%5C,%20%5Clog_2(%5Cmathop%7B%5Cmathrm%7BP%7D%7D(Y=y%7CX=x))"> to estimate <img src="https://latex.codecogs.com/png.latex?Y">. This strategy requires on average <img src="https://latex.codecogs.com/png.latex?H(X)%20+%20%5Csum_x%20%5Cmathop%7B%5Cmathrm%7BP%7D%7D(X=x)%20%5C,%20H(Y%7CX=x)"> binary questions and is actually optimal, showing that</p>
<p><span id="eq-chain-rule"><img src="https://latex.codecogs.com/png.latex?%0AH(X,Y)%20=%20H(X)%20+%20H(Y%20%7C%20X)%0A%5Ctag%7B2%7D"></span></p>
<p>where we have defined <img src="https://latex.codecogs.com/png.latex?H(Y%20%7C%20X)%20=%20%5Csum_x%20%5Cmathop%7B%5Cmathrm%7BP%7D%7D(X=x)%20%5C,%20H(Y%7CX=x)">.</p>
<p>Indeed, one can generalize these concepts to more than two random variables. Iterating Equation&nbsp;2 shows that the trajectory <img src="https://latex.codecogs.com/png.latex?X_%7B1:T%7D%20%5Cequiv%20(X_1,%20%5Cldots,%20X_N)"> of a stationary ergodic Markov chain can be estimated on average with <img src="https://latex.codecogs.com/png.latex?H(X_%7B1:T%7D)"> binary questions where</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AH(X_%7B1:T%7D)%0A&amp;=%20H(X_1)%20+%20H(X_2%7CX_1)%20+%20%5Cldots%20+%20H(X_%7BT%7D%20%7C%20X_%7Bt-1%7D)%5C%5C%0A&amp;%5Capprox%20T%20%5C,%20H(X_%7Bk+1%7D%20%7C%20X_k)%5C%5C%0A&amp;=%20-%20T%20%5C,%20%5Csum_x%20%5Cpi(x)%20%5C,%20%5Csum_%7By%7D%20p(x%20%5Cto%20y)%20%5C,%20%5Clog_2%5Bp(x%20%5Cto%20y)%5D.%0A%5Cend%7Balign%7D%0A"></p>
<p>Here, <img src="https://latex.codecogs.com/png.latex?%5Cpi(dx)"> is the equilibrium distribution of the Markov chain and <img src="https://latex.codecogs.com/png.latex?p(x%20%5Cto%20y)"> are the transition probabilities.</p>
<p>Can <img src="https://latex.codecogs.com/png.latex?%5Ctexttt%7Bgzip%7D(%5Cldots)"> compress Markovian trajectories and roughly achieve that level of compression? Indeed, one can test this by generating random Markov transition matrices (that are, with very high probability, ergodic with respect to an equilibrium distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi">). Doing this with trajectories of length <img src="https://latex.codecogs.com/png.latex?10%5E4"> (ie. quite short because it is quite slow to) on <img src="https://latex.codecogs.com/png.latex?%5C%7B1,%20%5Cldots,%20N%5C%7D"> with <img src="https://latex.codecogs.com/png.latex?2%20%5Cleq%20N%20%5Cleq%2064">, one get the following results:</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/information_theory_basics/shannon_vs_zip_markov.png" class="img-fluid figure-img" style="width:90.0%"></p>
</figure>
</div>
</div>
<p>In red is the entropy estimated without using the Markovian structure and assuming that the <img src="https://latex.codecogs.com/png.latex?X_i"> are iid samples. One can clearly see that the dependencies between successive samples are exploited for better compression. Nevertheless, the optimal compression rate given by the Shannon entropy is not reached. Indeed, <img src="https://latex.codecogs.com/png.latex?%5Ctexttt%7Bgzip%7D(%5Cldots)"> is not an optimal algorithm – it cannot even compress well enough the sequence <img src="https://latex.codecogs.com/png.latex?(1,2,3,1,2,3,1,2,3,%5Cldots)">!</p>
</section>
<section id="asymptotic-equipartition-property-aep" class="level3">
<h3 class="anchored" data-anchor-id="asymptotic-equipartition-property-aep">Asymptotic Equipartition Property (AEP)</h3>
<p>The AEP is simple remark that gives a convenient way of reasoning about long sequences random variables <img src="https://latex.codecogs.com/png.latex?X_%7B1:T%7D%20=%20(X_1,%20%5Cldots,%20X_T)"> with <img src="https://latex.codecogs.com/png.latex?T%20%5Cgg%201">. For example, assuming that the random variables <img src="https://latex.codecogs.com/png.latex?X_i"> are independent and identically distributed as the random variable <img src="https://latex.codecogs.com/png.latex?X">, the law of large numbers (LLN) gives that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A-%5Cfrac%7B1%7D%7BT%7D%20%5C,%20%5Clog_2(X_%7B1:T%7D)%20=%20-%5Cfrac%7B1%7D%7BT%7D%20%5C,%20%5Csum%20%5Clog_2%20p(X_i)%20%5Capprox%20H(X).%0A"></p>
<p>This means that <strong>any</strong> “typical” sequence has a probability about <img src="https://latex.codecogs.com/png.latex?2%5E%7B-T%20%5C,%20H(X)%7D"> of occurring, which also means that there are about <img src="https://latex.codecogs.com/png.latex?2%5E%7BT%20%5C,%20H(X)%7D"> such “typical” sequences. Indeed, one could use the LLN for Markov chains and obtain a similar statement for Markovian sequences. All this can be made rigorous with large deviation theory, for example. This also establishes the link with the “statistical physics” definition of entropy as the logarithm of the number of configurations… Set of the type</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AA_%7B%5Cvarepsilon%7D%20=%20%5Cleft%5C%7B%20x_%7B1:T%7D%20%5C;%20:%20%5C;%20%5Cleft%7C%20-%5Cfrac%7B1%7D%7BT%7D%20%5C,%20%5Clog_2(x_%7B1:T%7D)%20-%20H(X)%5Cright%7C%20%3C%20%5Cvarepsilon%5Cright%5C%7D%0A"></p>
<p>are usually called <strong>typical set</strong>: for any <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%3E%200">, the probability of <img src="https://latex.codecogs.com/png.latex?X_%7B1:T%7D"> to belongs to <img src="https://latex.codecogs.com/png.latex?A_%7B%5Cvarepsilon%7D"> goes to one as <img src="https://latex.codecogs.com/png.latex?T%20%5Cto%20%5Cinfty">. For these reasons, it is often a good heuristic to think of a draw of <img src="https://latex.codecogs.com/png.latex?(X_1,%20%5Cldots,%20X_T)"> as a uniformly distributed on the associated typical set. For example, if <img src="https://latex.codecogs.com/png.latex?(X_1,%20%5Cldots,%20X_N)"> are <img src="https://latex.codecogs.com/png.latex?N"> iid draws from a Bernoulli distribution with <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BP%7D%7D(X=1)%20=%201-%5Cmathop%7B%5Cmathrm%7BP%7D%7D(X=0)%20=p">, the set <img src="https://latex.codecogs.com/png.latex?A%20%5Csubset%20%5C%7B0,1%5C%7D%5EN"> of sequences such that <img src="https://latex.codecogs.com/png.latex?x_1%20+%20%5Cldots%20+%20x_N%20=%20Np"> has <img src="https://latex.codecogs.com/png.latex?%5Cbinom%7BN%7D%7BNp%7D%20%5Capprox%202%5E%7BN%20%5C,%20h_2(q)%7D"> elements where <img src="https://latex.codecogs.com/png.latex?h_2(q)%20=%20-%5Bq%20%5C,%20%5Clog_2(q)%20+%20(1-q)%20%5C,%20%5Clog_2(q)%5D"> is the entropy of a <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BBern%7D(q)"> random variable.</p>
</section>
<section id="mutual-information" class="level3">
<h3 class="anchored" data-anchor-id="mutual-information">Mutual information</h3>
<p>Consider a pair of random variables <img src="https://latex.codecogs.com/png.latex?(X,Y)">. Assuming that <img src="https://latex.codecogs.com/png.latex?X"> stores (on average) <img src="https://latex.codecogs.com/png.latex?H(X)"> bits of useful information, how much of it is still contained in <img src="https://latex.codecogs.com/png.latex?Y">? If <img src="https://latex.codecogs.com/png.latex?Y"> is independent from <img src="https://latex.codecogs.com/png.latex?X">, everything has been lost and, on the contrary, if <img src="https://latex.codecogs.com/png.latex?X=Y">, nothing has been lost. If one knows <img src="https://latex.codecogs.com/png.latex?Y">, one needs on average <img src="https://latex.codecogs.com/png.latex?H(X%7CY)"> binary questions (ie. bits of information) in order to determine <img src="https://latex.codecogs.com/png.latex?X"> certainly and recovered all the information contained in <img src="https://latex.codecogs.com/png.latex?X">. This means that the knowledge of <img src="https://latex.codecogs.com/png.latex?Y"> still contains <img src="https://latex.codecogs.com/png.latex?H(X)%20-%20H(X%7CY)"> useful bits of information! This quantity is called the <strong>mutual information</strong> of the two random variable <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y">, and it has the good taste of being symmetric:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AI(X;Y)%20&amp;=%20H(X)%20-%20H(X%7CY)%20%5C%5C%0A&amp;=%20H(X)%20+%20H(Y)%20-%20H(X,Y)%20%5C%5C%0A&amp;=%20H(Y;X).%0A%5Cend%7Balign%7D%0A"></p>
<p>Naturally, one can define conditional version of it by setting <img src="https://latex.codecogs.com/png.latex?I(X;Y%20%5C,%20%7CZ)%20=%20%5Csum_%7Bz%7D%20%5Cmathop%7B%5Cmathrm%7BP%7D%7D(Z=z)%20%5C,%20I(X_z%20%7C%20Y_z)"> where <img src="https://latex.codecogs.com/png.latex?(X_z,%20Y_z)"> has the law of <img src="https://latex.codecogs.com/png.latex?(X,Z)"> conditioned on <img src="https://latex.codecogs.com/png.latex?Z=z">. Since <img src="https://latex.codecogs.com/png.latex?I(X;Y%20%5C,%7CZ)"> is the reduction in uncertainty of <img src="https://latex.codecogs.com/png.latex?X"> due to <img src="https://latex.codecogs.com/png.latex?Y"> when <img src="https://latex.codecogs.com/png.latex?Z"> is given, there are indeed situations when <img src="https://latex.codecogs.com/png.latex?I(X;Y%20%5C,%20%7C%20Z)"> is larger than <img src="https://latex.codecogs.com/png.latex?I(X;Y)"> – it is to be contrasted to the intuitive inequality <img src="https://latex.codecogs.com/png.latex?H(X%7CZ)%20%5Cleq%20H(X)">, which is indeed true. A standard such examples is when <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> are independent <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BBern%7D(1/2)"> random variables and <img src="https://latex.codecogs.com/png.latex?Z%20=%20X+Y">: a short computation gives that <img src="https://latex.codecogs.com/png.latex?I(X;Y%20%5C,%20%7C%20Z)%20=%201/2"> while, indeed, <img src="https://latex.codecogs.com/png.latex?I(X;Y)%20=%200">. This definition of conditional mutual information leads to a chain-rule property,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AI(X;%20(Y_1,Y_2))%20=%20I(X;Y_1)%20+%20I(X;Y_2%20%7C%20Y_3),%0A"></p>
<p>which can indeed be generalized to any number of variables. Furthermore, if the <img src="https://latex.codecogs.com/png.latex?Y_i"> are conditionally independent given <img src="https://latex.codecogs.com/png.latex?X"> (eg. if <img src="https://latex.codecogs.com/png.latex?X=(X_1,%20%5Cldots,%20X_T)"> and <img src="https://latex.codecogs.com/png.latex?Y_i"> only depend on <img src="https://latex.codecogs.com/png.latex?X_i">), then the sub-additivity of the entropy readily gives that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AI(X;%20(Y_1,%20%5Cldots,%20Y_N))%20%5Cleq%20%5Csum_%7Bi=1%7D%5EN%20I(X;%20Y_i).%0A"></p>
<p>Importantly, algebra shows that <img src="https://latex.codecogs.com/png.latex?I(X;Y)"> can also be expressed as the <a href="https://en.wikipedia.org/wiki/Kullback–Leibler_divergence">Kullback-Leibler divergence</a> between the joint distribution <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BP%7D%7D_%7B(X,Y)%7D"> and the product of the marginals <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BP%7D%7D_X%20%5Cotimes%20%5Cmathop%7B%5Cmathrm%7BP%7D%7D_Y">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AI(X;Y)%20%5C;%20=%20%5C;%0A%5Cmathop%7B%5Cmathrm%7BD_%7B%5Ctext%7BKL%7D%7D%7D%7D%20%7B%5Cleft(%20%20(X,Y)%20%5C,%20%5C%7C%20%5C,%20X%20%5Cotimes%20Y%20%5Cright)%7D%20.%0A"></p>
<p>This diagram from <span class="citation" data-cites="mackay2003information">(MacKay 2003)</span> nicely illustrate the different fundamental quantities <img src="https://latex.codecogs.com/png.latex?H(X)"> and <img src="https://latex.codecogs.com/png.latex?H(X,Y)"> and <img src="https://latex.codecogs.com/png.latex?H(Y%7CX)"> and <img src="https://latex.codecogs.com/png.latex?I(X;Y)"> and <img src="https://latex.codecogs.com/png.latex?H(X,Y)">:</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/information_theory_basics/entropies.png" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption class="figure-caption">From: <a href="http://www.inference.org.uk/mackay/itila/book.html">Information Theory, Inference, and Learning Algorithms</a></figcaption>
</figure>
</div>
</div>
<p>Naturally, if one considers three random variables <img src="https://latex.codecogs.com/png.latex?X%20%5Cmapsto%20Y%20%5Cmapsto%20Z"> forming a “Markov chain”, we have the so-called <strong>data-processing</strong> inequality,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AI(X;Z)%20%5Cleq%20I(X;Y)%0A%5Cqquad%20%5Ctext%7Band%7D%20%5Cqquad%0AI(X;Z)%20%5Cleq%20I(Y;Z).%0A"></p>
<p>The first inequality is clear since all the useful information contained in <img src="https://latex.codecogs.com/png.latex?Z"> must be coming from <img src="https://latex.codecogs.com/png.latex?Y">, and <img src="https://latex.codecogs.com/png.latex?Y"> only contains <img src="https://latex.codecogs.com/png.latex?I(X;Y)"> bits about <img src="https://latex.codecogs.com/png.latex?X">. For the second inequality, note that if <img src="https://latex.codecogs.com/png.latex?Z"> contains <img src="https://latex.codecogs.com/png.latex?I(Y;Z)"> bits about <img src="https://latex.codecogs.com/png.latex?Y">, and <img src="https://latex.codecogs.com/png.latex?Y"> contains <img src="https://latex.codecogs.com/png.latex?H(Y;X)"> bits about <img src="https://latex.codecogs.com/png.latex?X">, then <img src="https://latex.codecogs.com/png.latex?Z"> cannot contain more than <img src="https://latex.codecogs.com/png.latex?I(Y;Z)"> bits of <img src="https://latex.codecogs.com/png.latex?X">:</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/information_theory_basics/mutual.jpg" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption class="figure-caption">Data Processing Inequality</figcaption>
</figure>
</div>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-mackay2003information" class="csl-entry">
MacKay, David JC. 2003. <em>Information Theory, Inference and Learning Algorithms</em>. Cambridge university press.
</div>
</div></section></div> ]]></description>
  <category>infoTheory</category>
  <guid>https://alexxthiery.github.io/notes/information_theory_basics/information_theory_entropy.html</guid>
  <pubDate>Fri, 22 Sep 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>From Denoising Diffusion to ODEs</title>
  <link>https://alexxthiery.github.io/notes/DDPM_deterministic/DDPM_deterministic.html</link>
  <description><![CDATA[ 




<section id="setting-goals" class="level3">
<h3 class="anchored" data-anchor-id="setting-goals">Setting &amp; Goals</h3>
<p>Consider an empirical data distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D">. In order to simulate approximate samples from <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D">, <a href="../../notes/DDPM/DDPM.html">Denoising Diffusion Probabilistic Models</a> (DDPM) simulate a forward diffusion process <img src="https://latex.codecogs.com/png.latex?%5C%7BX_t%5C%7D_%7B%5B0,T%5D%7D"> on an interval <img src="https://latex.codecogs.com/png.latex?%5B0,T%5D">. The diffusion is initialized at the data distribution, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?X_0%20%5Csim%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D">, and is chosen so that that the distribution of <img src="https://latex.codecogs.com/png.latex?X_T"> is very close to a known and tractable reference distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bref%7D%7D">, e.g.&nbsp;a Gaussian distribution. Denote by <img src="https://latex.codecogs.com/png.latex?p_t(dx)"> the marginal distribution at time <img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20t%20%5Cleq%20T">, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BP%7D%7D(X_t%20%5Cin%20dx)%20=%20p_t(dx)">. By choosing the forward distribution with simple and tractable transition probabilities, e.g.&nbsp;an Ornstein-Uhlenbeck, it is relatively easy to estimate <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20%5Clog%20p_t(x)"> from simulated data: this can be formulated as a simple regression problem. This allows one to simulate the diffusion backward in time and generate approximate samples from <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D">. Why this is useful is another question…</p>
<p>The fact that the mapping from data-samples at time <img src="https://latex.codecogs.com/png.latex?t=0"> to (approximate) Gaussian samples at time <img src="https://latex.codecogs.com/png.latex?t=T"> is stochastic and described by diffusion processes is cumbersome. This would be much more convenient to build a deterministic mapping between the data-distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D"> and the Gaussian reference distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bref%7D%7D">: this would allows one to associate a likelihood to data samples and to easily “encode”/“decode” data-samples. To so this, one can try to replace diffusion by Ordinary Differential Equations.</p>
</section>
<section id="the-diffusion-ode-trick" class="level3">
<h3 class="anchored" data-anchor-id="the-diffusion-ode-trick">The diffusion-ODE trick</h3>
<p>Consider an arbitrary diffusion process <img src="https://latex.codecogs.com/png.latex?dX%20=%20%5Cmu(t,X)%20%5C,%20dt%20+%20dB"> with associated distribution <img src="https://latex.codecogs.com/png.latex?p_t(dx)"> at time <img src="https://latex.codecogs.com/png.latex?t">. The <a href="https://en.wikipedia.org/wiki/Fokker–Planck_equation">Fokker-Planck</a> equation that describes the evolution of <img src="https://latex.codecogs.com/png.latex?p_t"> reads</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpartial_t%20p_t%20=%20-%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7D(%20p_t%20%5C,%20%5Cmu)%20+%20%5Cfrac12%20%5C,%20%5CDelta%20p_t%0A"></p>
<p>If there were no diffusion term <img src="https://latex.codecogs.com/png.latex?dB"> and <img src="https://latex.codecogs.com/png.latex?X"> was describing instead the evolution of differential equation <img src="https://latex.codecogs.com/png.latex?dX/dt%20=%20F(t,X)">, the associated evolution of the density of <img src="https://latex.codecogs.com/png.latex?X"> would simply read</p>
<p><span id="eq-continuity"><img src="https://latex.codecogs.com/png.latex?%0A%5Cpartial_t%20p_t%20=%20-%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7D(%20p_t%20%5C,%20F).%0A%5Ctag%7B1%7D"></span></p>
<p>If one can find a vector field <img src="https://latex.codecogs.com/png.latex?F(t,x)"> such that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A-%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7D(%20p_t%20%5C,%20%5Cmu)%20+%20%5Cfrac12%20%5C,%20%5CDelta%20p_t%0A=%0A-%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7D(%20p_t%20%5C,%20F),%0A"></p>
<p>then one can basically replace diffusions by ODEs. The diffusion-ODE trick is the simple remark that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AF(t,x)%20=%20%5Cmu(t,x)%20%5C;%20%7B%5Ccolor%7Bred%7D%20-%20%5Cfrac12%20%5C,%20%5Cnabla_x%20%5Clog%20p_t(x)%7D%0A"></p>
<p>does exactly this, as algebra immediately shows it. The additional term <img src="https://latex.codecogs.com/png.latex?%7B%5Ccolor%7Bred%7D%20-%20%5Cfrac12%20%5C,%20%5Cnabla_x%20%5Clog%20p_t(x)%7D"> is intuitive. The coefficient <img src="https://latex.codecogs.com/png.latex?%7B%5Ccolor%7Bblue%7D%201/2%7D"> is because one is trying to match the term <img src="https://latex.codecogs.com/png.latex?%7B%5Ccolor%7Bblue%7D%20(1/2)%7D%20%5C,%20%5CDelta%20p_t"> in the Fokker-Planck equation. And the overall term <img src="https://latex.codecogs.com/png.latex?%7B%5Ccolor%7Bred%7D%20-%20%5Cfrac12%20%5C,%20%5Cnabla_x%20%5Clog%20p_t(x)%7D"> is just driving the ODE in direction where the probability density <img src="https://latex.codecogs.com/png.latex?p_t"> is small, i.e.&nbsp;it follows the negative gradient of the log-density: it is exactly trying to imitate the diffusion term <img src="https://latex.codecogs.com/png.latex?dB">.</p>
<p>What this means is that a diffusion process <img src="https://latex.codecogs.com/png.latex?dX%20=%20%5Cmu(t,X)%20%5C,%20dt%20+%20dB"> started from <img src="https://latex.codecogs.com/png.latex?X_0%20%5Csim%20p_0"> and marginal distribution <img src="https://latex.codecogs.com/png.latex?X_t%20%5Csim%20p_t(dx)"> can be imitated by an ODE process <img src="https://latex.codecogs.com/png.latex?dY/dt%20=%20%5Cmu(t,Y)%20-%20%5Cfrac12%20%5C,%20%5Cnabla%20%5Clog%20p_t(Y)"> started from <img src="https://latex.codecogs.com/png.latex?p_0">. At any time <img src="https://latex.codecogs.com/png.latex?t%3E0">, the marginal distributions of <img src="https://latex.codecogs.com/png.latex?X_t"> and <img src="https://latex.codecogs.com/png.latex?Y_t"> both exactly equal <img src="https://latex.codecogs.com/png.latex?p_t(dx)">.</p>
</section>
<section id="the-diffusion-ode-trick-application-to-ddpm" class="level3">
<h3 class="anchored" data-anchor-id="the-diffusion-ode-trick-application-to-ddpm">The diffusion-ODE trick: application to DDPM</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="sine_reverse_ODE.mp4" class="img-fluid" style="width:100.0%" controls=""><a href="sine_reverse_ODE.mp4">Video</a></video></p>
</figure>
</div>
<p>Consider a DDPM with forward dynamics given by an Ornstein-Uhlenbeck (OU) process</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ad%5Coverrightarrow%7BX%7D%20=%20-%5Cfrac12%20%5C,%20%5Coverrightarrow%7BX%7D%20%5C,%20dt%20+%20dB%0A"></p>
<p>and initial condition <img src="https://latex.codecogs.com/png.latex?X_0%20%5Csim%20p_0%20%5Cequiv%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D">. As explained in these <a href="../../notes/DDPM/DDPM.html">notes</a>, it is relatively straightforward to estimate the score function</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BS%7D(t,x_t)%20%5C;%20=%20%5C;%20%5Cnabla%20%5Clog%20p_t(x_t)%0A"></p>
<p>from data. This means that the forward OU process can be replaced by the forward ODE</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%7D%7Bdt%7D%20%5Coverrightarrow%7BY_t%7D%20=%20-%5Cfrac12%20%5C,%20%5Coverrightarrow%7BY_t%7D%20%20-%20%5Cfrac12%20%5Cmathcal%7BS%7D(t,%5Coverrightarrow%7BY_t%7D)%20=%20F(t,Y_t)%0A"></p>
<p>with <img src="https://latex.codecogs.com/png.latex?F(t,x)%20=%20-%5Ctfrac%7B1%7D%7B2%7D%20x%20-%5Ctfrac%7B1%7D%7B2%7D%20%5Cmathcal%7BS%7D(t,x)">. Similarly, the reverse diffusion (i.e.&nbsp;the “denoising” diffusion) defined as <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_s%20=%20X_%7BT-s%7D"> follows the dynamics</p>
<p><img src="https://latex.codecogs.com/png.latex?d%5Coverleftarrow%7BX%7D_s%20=%20%5Cfrac12%20%5Coverleftarrow%7BX%7D_s%20%5C,%20ds%20+%20%5Cmathcal%7BS%7D(T-s,%5Coverleftarrow%7BX%7D_s)%20+%20dW."></p>
<p>As described for the first time in the beautiful article <span class="citation" data-cites="song2020score">(Song et al. 2020)</span>, the diffusion-ODE trick now shows that the denoising diffusion can be replaced by a denoising ODE with dynamics</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cfrac%7Bd%7D%7Bds%7D%20%5Coverleftarrow%7BY_s%7D%20&amp;=%20%5Cfrac12%20%5Coverleftarrow%7BY_s%7D%20%5C,%20ds%20+%20%5Cmathcal%7BS%7D(T-s,%5Coverleftarrow%7BY_s%7D)%20-%20%5Cfrac12%20%5C,%20%5Cmathcal%7BS%7D(T-s,%5Coverleftarrow%7BX%7D_s)%5C%5C%0A&amp;=%0A%5Cfrac12%20%5Coverleftarrow%7BY_s%7D%20%5C,%20ds%20+%20%5Cfrac12%20%5C,%20%5Cmathcal%7BS%7D(T-s,%5Coverleftarrow%7BY_s%7D)%20=%20-F(T-s,%20%5Coverleftarrow%7BY_s%7D).%0A%5Cend%7Balign%7D%0A"></p>
<p>Interestingly [and I do not know whether there was an obvious way of seeing this from the start], this shows that the forward and backward ODE are actually the same but run forward and backward in time. They corresponds to the ODE described by the vector field</p>
<p><span id="eq-vector-field"><img src="https://latex.codecogs.com/png.latex?%0AF(t,x)%20%5C;%20=%20%5C;%20-%5Cfrac%7B1%7D%7B2%7D%20x%20-%5Cfrac%7B1%7D%7B2%7D%20%5Cmathcal%7BS%7D(t,x).%0A%5Ctag%7B2%7D"></span></p>
<p>The animation belows display the denoising ODE and the associated vector field Equation&nbsp;2.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="sine_reverse_vector_field.mp4" class="img-fluid" style="width:100.0%" controls=""><a href="sine_reverse_vector_field.mp4">Video</a></video></p>
</figure>
</div>
</section>
<section id="likelihood-computation" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-computation">Likelihood computation</h2>
<p>With the diffusion-ODE trick, we have just seen that it is possible to build a vector fields <img src="https://latex.codecogs.com/png.latex?F%5B0,T%5D%20%5Ctimes%20%5Cmathbb%7BR%7D%5Ed%20%5Cto%20%5Cmathbb%7BR%7D%5Ed"> such that the <em>forward</em> ODE</p>
<p><span id="eq-forward-ODE"><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%7D%7Bdt%7D%20%5Coverrightarrow%7BY_t%7D%20=%0AF(t,%5Coverrightarrow%7BY_t%7D)%0A%5Cqquad%20%5Ctextrm%7Binitialized%20at%7D%20%5Cqquad%0A%5Coverrightarrow%7BY%7D_0%20%5Csim%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D%0A%5Ctag%7B3%7D"></span></p>
<p>and the <em>backward</em> ODE defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%7D%7Bds%7D%20%5Coverleftarrow%7BY_s%7D%20=%0A-F(T-s,%5Coverleftarrow%7BY_s%7D)%0A%5Cqquad%20%5Ctextrm%7Binitialized%20at%7D%20%5Cqquad%0A%5Coverleftarrow%7BY%7D_0%20%5Csim%20%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%0A"></p>
<p>are such that <img src="https://latex.codecogs.com/png.latex?%5Coverrightarrow%7BY%7D_T%20%5Capprox%20%5Cpi_%7B%5Cmathrm%7Bref%7D%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BY%7D_T%20%5Capprox%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D">.</p>
<p>In general, consider a vector field <img src="https://latex.codecogs.com/png.latex?F(t,x)"> and a bunch of particles distributed according to a distribution <img src="https://latex.codecogs.com/png.latex?p_t"> at time <img src="https://latex.codecogs.com/png.latex?t">. If each particle follows the vector field for an amount of time <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Cll%201">, the particles that were in the vicinity of some <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathbb%7BR%7D%5Ed"> at time <img src="https://latex.codecogs.com/png.latex?t"> end up in the vicinity of <img src="https://latex.codecogs.com/png.latex?x%20+%20F(x,t)%20%5C,%20%5Cdelta"> at time <img src="https://latex.codecogs.com/png.latex?t+%5Cdelta">. At the same time, a volume element <img src="https://latex.codecogs.com/png.latex?dx"> around <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathbb%7BR%7D%5Ed"> gets stretch by a factor <img src="https://latex.codecogs.com/png.latex?1+%5Cdelta%20%5C,%20%5Cmathop%7B%5Cmathrm%7BTr%7D%7D%5B%5Cmathop%7B%5Cmathrm%7B%5Cmathrm%7BJac%7D%7D%7DF(x,t)%5D%20=%201%20+%20%5Cdelta%20%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7DF(x,t)"> while following the vector field <img src="https://latex.codecogs.com/png.latex?F">, which means that the density of particles at time <img src="https://latex.codecogs.com/png.latex?t+%5Cdelta"> and around <img src="https://latex.codecogs.com/png.latex?x%20+%20F(x,t)%20%5C,%20%5Cdelta"> equals <img src="https://latex.codecogs.com/png.latex?p_t(x)%20/%20%5B1%20+%20%5Cdelta%20%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7DF(x,t)%5D">. In other words <img src="https://latex.codecogs.com/png.latex?%5Clog%20p_%7Bt+%5Cdelta%7D(x%20+%20F(x,t)%20%5C,%20%5Cdelta)%20%5Capprox%20%5Clog%20p_t(x)%20-%20%5Cdelta%20%5C,%20%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7DF(x,t)">. This means that if we follows a trajectory of <img src="https://latex.codecogs.com/png.latex?%5Ctfrac%7Bd%7D%7Bdt%7D%20X_t%20=%20F(t,X_t)"> one gets</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Clog%20p_T(X_T)%20=%20%5Clog%20p_0(X_0)%20-%20%5Cint_%7Bt=0%7D%5E%7BT%7D%20%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7DF(X_t,t)%20%5C,%20dt.%0A"></p>
<p>That is the Lagrangian description of the density <img src="https://latex.codecogs.com/png.latex?p_t"> of particles. Indeed, one could directly get this identity by differentiating <img src="https://latex.codecogs.com/png.latex?p_t(X_t)"> with respect to time while using the continuity Equation&nbsp;1. When applied to the DDPM, this gives a way to assign likelihood the data samples, namely</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Clog%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D(x)%20=%20%5Clog%20%5Cpi_%7B%5Cmathrm%7Bref%7D%7D(%5Coverrightarrow%7BY_T%7D)%20+%20%5Cint_%7Bt=0%7D%5E%7BT%7D%20%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7DF(t,%20%5Coverrightarrow%7BY_t%7D)%5C,%20dt%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Coverrightarrow%7BY_t%7D"> is trajectory of the forward ODE Equation&nbsp;3 initialized as <img src="https://latex.codecogs.com/png.latex?%5Coverrightarrow%7BY_0%7D%20=%20x">. Note that in high-dimensional setting, it may be computationally expensive to compute the divergence term <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7Bdiv%7D%7DF(t,%20%5Coverrightarrow%7BY_t%7D)"> since it typically is <img src="https://latex.codecogs.com/png.latex?d"> times slower that a gradient computation; for this reason, it is often advocated to use the Hutchinson trace estimator to get an unbiased estimate of it at a much lower computational cost.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-song2020score" class="csl-entry">
Song, Yang, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2020. <span>“Score-Based Generative Modeling Through Stochastic Differential Equations.”</span> <em><span>ICLR</span> 2021</em>. <a href="https://arxiv.org/abs/2011.13456">https://arxiv.org/abs/2011.13456</a>.
</div>
</div></section></div> ]]></description>
  <category>DDPM</category>
  <category>score</category>
  <guid>https://alexxthiery.github.io/notes/DDPM_deterministic/DDPM_deterministic.html</guid>
  <pubDate>Sat, 01 Jul 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Denoising Diffusion Probabilistic Models (DDPM)</title>
  <link>https://alexxthiery.github.io/notes/DDPM/DDPM.html</link>
  <description><![CDATA[ 




<section id="setting-goals" class="level3">
<h3 class="anchored" data-anchor-id="setting-goals">Setting &amp; Goals</h3>
<p>Consider <img src="https://latex.codecogs.com/png.latex?N"> samples <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D%5Cequiv%20%5C%7Bx_i%5C%7D_%7Bi=1%7D%5EN"> in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5ED"> from an unknown data distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D(dx)">. We would like to build an mechanism that can generate other samples from this data distribution. Implicitly, this means that we will be fitting a statistical model to this finite set of samples and have an algorithmic procedure to generate samples from the fitted probabilistic model. This type of models that directly define a stochastic procedure that generates data are called <em>implicit probabilistic models</em> in the ML literature.</p>
</section>
<section id="ornsteinuhlenbeck-noising-process" class="level3">
<h3 class="anchored" data-anchor-id="ornsteinuhlenbeck-noising-process">Ornstein–Uhlenbeck Noising process</h3>
<p>DDPMs work as follows. Consider a diffusion process <img src="https://latex.codecogs.com/png.latex?%5C%7B%20X_t%20%5C%7D_%7Bt=0%7D%5ET"> that starts from the data distribution <img src="https://latex.codecogs.com/png.latex?p_0(dx)%20%5Cequiv%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D(dx)"> at time <img src="https://latex.codecogs.com/png.latex?t=0">. The notation <img src="https://latex.codecogs.com/png.latex?p_t(dx)"> refers to the marginal distribution of the diffusion at time <img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20t%20%5Cleq%20T">. Assume furthermore that at time <img src="https://latex.codecogs.com/png.latex?t=T">, the marginal distribution is (very close to) a reference distribution <img src="https://latex.codecogs.com/png.latex?p_T(dx)%20=%20%5Cpi_%7B%5Cmathrm%7Bref%7D%7D(dx)"> that is straightforward to sample from. Typically, <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bref%7D%7D(dx)"> is an isotropic Gaussian distribution. This diffusion process is often called the <em>noising</em> process since it transform the data distribution into a reference measure that can be thought of as “pure noise”. It is often chosen as an <a href="https://en.wikipedia.org/wiki/Ornstein–Uhlenbeck_process">Ornstein–Uhlenbeck</a> (OU) diffusion,</p>
<p><span id="eq-OU"><img src="https://latex.codecogs.com/png.latex?%0AdX%20=%20-%20%5Cfrac12%20X%20%5C,%20dt%20+%20dW.%0A%5Ctag%7B1%7D"></span></p>
<p>This diffusion is reversible with respect to, and quickly converges to, the reference distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%20=%20%5Cmathcal%7BN%7D(0,%20I)"> and has the good taste of having simple transition densities: the law of <img src="https://latex.codecogs.com/png.latex?X_%7Bt+s%7D"> given that <img src="https://latex.codecogs.com/png.latex?X_t%20=%20x_t"> is the same as <img src="https://latex.codecogs.com/png.latex?e%5E%7B-s/2%7D%20x_t%20+%20%5Csqrt%7B1-e%5E%7B-s%7D%7D%20%5C,%20%5Cmathbf%7Bn%7D">, which we write as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha_s%20x%20+%20%5Csigma_s%20%5C,%20%5Cmathbf%7Bn%7D%5Cqquad%20%5Ctext%7Bwith%7D%20%5Cqquad%0A%5Cleft%5C%7B%0A%5Cbegin%7Baligned%7D%0A%5Calpha_s%20&amp;=%20%5Csqrt%7B1-%5Csigma_s%5E2%7D%5C%5C%0A%5Csigma%5E2_s%20&amp;=%201-e%5E%7B-s%7D%0A%5Cend%7Baligned%7D%0A%5Cright.%0A"></p>
<p>for isotropic Gaussian noise term <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bn%7D%5Csim%20%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%20=%20%5Cmathcal%7BN%7D(0,I)">. We have:</p>
<p><span id="eq-OU-forward"><img src="https://latex.codecogs.com/png.latex?%0AF(s,x,y)%20%5Cequiv%20%5Cmathop%7B%5Cmathrm%7BP%7D%7D(X_%7Bt+s%7D%20%5Cin%20dy%20%5C,%20%7C%20%5C,%20X_t%20=%20x%20)%0A%5C;%20%5Cpropto%20%5C;%0A%5Cexp%20%7B%5Cleft%5C%7B%20-%5Cfrac%7B(y%20-%20%5Calpha_s%20%5C,%20x)%5E2%7D%7B2%20%5C,%20%5Csigma%5E2_s%7D%20%5Cright%5C%7D%7D%20.%0A%5Ctag%7B2%7D"></span></p>
<p>where the notation <img src="https://latex.codecogs.com/png.latex?F(s,x,y)"> designates the forward transition from <img src="https://latex.codecogs.com/png.latex?x"> to <img src="https://latex.codecogs.com/png.latex?y"> in “<img src="https://latex.codecogs.com/png.latex?s">” amount of time. This also means that one can directly generate samples from <img src="https://latex.codecogs.com/png.latex?p_t(dx)"> by first choosing a data samples <img src="https://latex.codecogs.com/png.latex?x_i"> from the data distribution <img src="https://latex.codecogs.com/png.latex?p_%7B%5Cmathrm%7Bdata%7D%7D%20%5Cequiv%20p_0"> and blend it with noise by setting <img src="https://latex.codecogs.com/png.latex?x_i%5E%7B(t)%7D%20=%20%5Calpha_t%20%5C,%20x_i%20+%20%5Csigma_t%20%5C,%20%5Cmathbf%7Bn%7D">.</p>
</section>
<section id="the-reverse-diffusion" class="level3">
<h3 class="anchored" data-anchor-id="the-reverse-diffusion">The reverse diffusion</h3>
<p>In order to generate samples from the data distribution, the DDPM strategy consists in sampling from the Gaussian reference measure <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bref%7D%7D"> at time <img src="https://latex.codecogs.com/png.latex?t=T"> and simulate the OU process backward in time. In other words, one would like to simulate from the <em>reverse process</em> <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_t"> defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_s%20=%20X_%7BT-s%7D."></p>
<p>In other words, the reverse process is distributed as <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_0%20%5Csim%20%5Cpi_%7B%5Cmathrm%7Bref%7D%7D"> at time <img src="https://latex.codecogs.com/png.latex?t=0"> and, crucially, we have that <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_T%20%5Csim%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D">. Furthermore, and as explained in this <a href="../../notes/reverse_and_tweedie/reverse_and_tweedie.html">note</a>, the reverse diffusion follows the dynamics</p>
<p><span id="eq-OU-rev"><img src="https://latex.codecogs.com/png.latex?%0Ad%5Coverleftarrow%7BX%7D_t%20=%20%7B%5Ccolor%7Bred%7D%20+%20%7D%5Cfrac12%20%5Coverleftarrow%7BX%7D_t%20%5C,%20dt%0A%5C;%20%7B%5Ccolor%7Bred%7D%20+%20%5Cnabla%20%5Clog%20p_%7BT-t%7D(%5Coverleftarrow%7BX%7D_t)%20%5C,%20dt%7D%20%5C;%0A+%20dB%0A%5Ctag%7B3%7D"></span></p>
<p>where <img src="https://latex.codecogs.com/png.latex?B"> is another Wiener process. I have used the notation <img src="https://latex.codecogs.com/png.latex?B"> to emphasize that there is no link between this Wiener process and the one used to simulate the forward process. Note that if the initial data distribution <img src="https://latex.codecogs.com/png.latex?p_0%20%5Cequiv%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D"> were equal to the reference measure <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bref%7D%7D">, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?p_0%20=%20p_T%20=%20%5Cpi_%7B%5Cmathrm%7Bref%7D%7D"> then it is easy to see that the reverse diffusion would have exactly teh same dynamics as the forward diffusion. In order to simulate the reverse diffusion, one needs to be able to estimate the new term <img src="https://latex.codecogs.com/png.latex?%7B%5Ccolor%7Bred%7D%5Cnabla%20%5Clog%20p_%7BT-t%7D(x)%7D"> called the <em>score</em>. If one can estimate the score, it is straightforward to simulate the reverse process <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_t"> all the way to <img src="https://latex.codecogs.com/png.latex?t=T"> and obtain samples from the data distribution.</p>
</section>
<section id="denoising-to-estimating-the-score" class="level3">
<h3 class="anchored" data-anchor-id="denoising-to-estimating-the-score">Denoising to estimating the score</h3>
<p>In practice, the score is unknown and one has to build an approximation of it</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BS%7D(t,x)%20%5C;%20%5Capprox%20%5C;%20%5Cnabla_x%20%5Clog%20p_t(x)."></p>
<p>The approximate score <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BS%7D(t,x)"> is often parametrized by a neural network. Since the forward transitions are available and</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Clog%20p_t(x)%20%5C;%20=%20%5C;%20%5Clog%20%5Cint%20%5C;%20F(t,%20x_0,%20x)%5C;%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D(d%20x_0)%0A"></p>
<p>the analytical expression of <img src="https://latex.codecogs.com/png.latex?F(t,%20x_0,%20x)"> given in Equation&nbsp;2 readily gives that</p>
<p><span id="eq-score-denoising"><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_x%20%5Clog%20p_t(x)%20%5C;%20=%20%5C;%20-%5Cfrac%7Bx%20-%20%5Calpha_t%20%5C,%20%5Cwidehat%7Bx%7D_0(x,t)%7D%7B%5Csigma_t%5E2%7D%0A%5Ctag%7B4%7D"></span></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0(x,t)"> is a “denoising” estimate of the initial position <img src="https://latex.codecogs.com/png.latex?x_0"> given a noisy estimate <img src="https://latex.codecogs.com/png.latex?X_t=x"> at time <img src="https://latex.codecogs.com/png.latex?t">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%7Bx%7D_0(x,t)%20%5C;%20=%20%5C;%20%5Cmathop%7B%5Cmathrm%7BE%7D%7D%5BX_0%20%20%5C;%20%5Cmid%20%5C;%20X_t%20=%20x%5D.%0A"></p>
<p>For simplifying notation, I will often write <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0(x_t,%20t)"> as <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0(x_t)"> when it is clear that <img src="https://latex.codecogs.com/png.latex?x_t"> is a sample obtained at time <img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20t%20%5Cleq%20T">. Equation&nbsp;4 means that to estimate the score, one only needs to train a denoising function</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%7Bx%7D_0(%5Ccdots):%20%5B0,T%5D%20%5Ctimes%20%5Cmathbb%7BR%7D%5Ed%20%5Cto%20%5Cmathbb%7BR%7D%5Ed.%0A"></p>
<p>It is a simple regression problem: take a bunch of pairs <img src="https://latex.codecogs.com/png.latex?(X_0,%20X_t)"> that can be generated as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AX_0%20%5Csim%20%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D%0A%5Cqquad%20%5Ctextrm%7Band%7D%20%5Cqquad%0AX_t%20=%20%5Calpha_t%20X_0%20+%20%5Csigma_t%20%5C,%20%5Cmathbf%7Bn%7D%0A"></p>
<p>with <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bn%7D%5Csim%20%5Cmathcal%7BN%7D(0,I)"> and minimize the Mean Squared Error (MSE) loss, i.e.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathop%7B%5Cmathrm%7BE%7D%7D%5C%7CX_0%20-%20%5Cwidehat%7Bx%7D_0(t,%20X_t)%5C%7C%5E2,%0A"></p>
<p>with stochastic gradient descent or any other stochastic optimization procedure. The score is then defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BS%7D(t,x)%20%5C;%20=%20%5C;%20-%5Cfrac%7Bx%20-%20%5Calpha_t%20%5C,%20%5Cwidehat%7Bx%7D_0(t,x)%7D%7B%5Csigma%5E2_t%7D.%0A"></p>
</section>
<section id="denoiser-practical-parametrization-and-training" class="level3">
<h3 class="anchored" data-anchor-id="denoiser-practical-parametrization-and-training">Denoiser: practical parametrization and training</h3>
<p>In practice, it may not be efficient, or stable, to try to directly parametrize the denoiser <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0(%5Ccdots)"> with a neural network and simply descend the loss</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathop%7B%5Cmathrm%7BE%7D%7D%5C%7CX_0%20-%20%5Cwidehat%7Bx%7D_0(t,%20X_t)%5C%7C%5E2.%0A"></p>
<p>For example, for <img src="https://latex.codecogs.com/png.latex?t%20%5Capprox%200">, we have that <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0(t,X_t)%20%5Capprox%20X_t%20%5Capprox%20X_0"> so that it is very easy to reconstruct <img src="https://latex.codecogs.com/png.latex?X_0"> from <img src="https://latex.codecogs.com/png.latex?X_t">. On the contrary, for large <img src="https://latex.codecogs.com/png.latex?t">, there is almost no information contained within <img src="https://latex.codecogs.com/png.latex?X_t"> to reconstruct <img src="https://latex.codecogs.com/png.latex?X_0">. This means that the typical value of the loss depends widely on <img src="https://latex.codecogs.com/png.latex?t">, which makes it difficult to optimize: with this parametrization, since large values of the loss will be typically concentrated to large values of <img src="https://latex.codecogs.com/png.latex?t">, the denoiser will not be accurate for <img src="https://latex.codecogs.com/png.latex?t%20%5Capprox%200">, leading to sub-optimal results. Since <img src="https://latex.codecogs.com/png.latex?X_t%20=%20%5Calpha_t%20%5C,%20X_0%20+%20%5Csigma_t%20%5C,%20%5Cmathbf%7Bn%7D">, one can defined the Signal-to-Noise-Ratio as</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BSNR%7D(t)%20=%20%5Cfrac%7B%5Calpha_t%7D%7B%5Csigma_t%7D"></p>
<p>and, in order to normalize by the reconstruction difficulty, it makes more sense to train a denoiser by minimizing the loss:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathop%7B%5Cmathrm%7BE%7D%7D%20%7B%5Cleft%5B%20%20%5Cmathrm%7BSNR%7D%5E2(t)%20%5Ctimes%20%5C%7CX_0%20-%20%5Cwidehat%7Bx%7D_0(t,%20X_t)%5C%7C%5E2%20%20%5Cright%5D%7D%20.%0A"></p>
<p>It turns out that it is entirely equivalent to minimizing the loss</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathop%7B%5Cmathrm%7BE%7D%7D%20%7B%5Cleft%5B%20%20%5C%7C%20%5Cmathbf%7Bn%7D-%20%5Cwidehat%7B%5Cmathbf%7Bn%7D%7D(t,%20X_t)%5C%7C%5E2%20%20%5Cright%5D%7D%20.%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?X_t%20=%20%5Calpha_t%20%5C,%20X_0%20+%20%5Csigma_t%20%5C,%20%5Cmathbf%7Bn%7D"> while the denoiser <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0(%5Cldots)"> and noise estimator <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathbf%7Bn%7D%7D(%5Cldots)"> are parametrized so that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AX_t%20=%20%5Calpha_t%20%5C,%20%5Cwidehat%7Bx%7D_0(t,%20X_t)%20+%20%5Csigma_t%20%5C,%20%5Cwidehat%7B%5Cmathbf%7Bn%7D%7D(t,X_t).%0A"></p>
<p>That is one of the reasons why most of the papers on DDPM are parametrizing the denoiser <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0(%5Cldots)"> by building instead a “noise estimator” <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathbf%7Bn%7D%7D(%5Cldots)"> with a neural network and setting</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%7Bx%7D_0(t,X_t)%20=%20%5Cfrac%7BX_t%20-%20%5Csigma_t%20%5C,%20%5Cwidehat%7B%5Cmathbf%7Bn%7D%7D(t,X_T)%7D%7B%5Calpha_t%7D.%0A"></p>
<p>Since <img src="https://latex.codecogs.com/png.latex?%5Calpha_t%20%5Cto%201"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma_t%20%5Cto%200"> for <img src="https://latex.codecogs.com/png.latex?t%20%5Cll%201">, this also implicitly ensures that <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0(t,X_t)%20%5Capprox%20X_t"> for <img src="https://latex.codecogs.com/png.latex?t%20%5Cll%201">, as required.</p>
<!-- This also means that the estimated score function is given by

$$
\cS(t,x) 
\; = \; -\frac{x - \alpha_t \, \widehat{x}_0(t,x)}{\sigma^2_t} \; = \; - \frac{\widehat{\bfn}(t,x)}{\sigma_t}.
$$ -->
</section>
<section id="the-denoising-diffusion" class="level3">
<h3 class="anchored" data-anchor-id="the-denoising-diffusion">The “denoising” diffusion</h3>
<p>Once the denoiser <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0(%5Cldots)"> has been trained, the reverse diffusion defined <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_s%20=%20X_%7BT-s%7D"> as to be simulated. Plugging Equation&nbsp;4 back in the expression of the dynamics of the reverse diffusion shows that</p>
<p><span id="eq-reverse-diff"><img src="https://latex.codecogs.com/png.latex?%0Ad%5Coverleftarrow%7BX%7D_s%20=%0A-%5Cfrac12%20%5C,%20%5Cfrac%7B1%7D%7B%5Ctanh((T-s)/2)%7D%20%5C,%0A%7B%5Cleft(%20%20%5Coverleftarrow%7BX%7D_s%20-%20%5Cfrac%7B%5Cwidehat%7Bx%7D_0(%5Coverleftarrow%7BX%7D_s)%7D%7B%5Ccosh((T-s)/2)%7D%20%20%5Cright)%7D%0A%5C;%20+%20%5C;%0AdB%0A%5Ctag%7B5%7D"></span></p>
<p>This dynamics is intuitive: as <img src="https://latex.codecogs.com/png.latex?s%20%5Cto%20T"> we have <img src="https://latex.codecogs.com/png.latex?%5Ccosh((T-s)/2)%20%5Cto%201"> and <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5E2%20%5Cequiv%20%5Ctanh((T-s)/2)%20%5Csim%20(T-s)/2%20%5Cto%200"> so that the dynamics is similar to</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ad%5Coverleftarrow%7BX%7D_s%20=%0A-%5Cfrac12%20%5C,%20%5Cfrac%7B1%7D%7B%5Cvarepsilon%5E2%7D%20%5C,%0A%7B%5Cleft(%20%20%5Coverleftarrow%7BX%7D_s%20-%20%5Cwidehat%7Bx%7D_0%20%5Cright)%7D%0A%5C;%20+%20%5C;%0AdB%0A"></p>
<p>which is OU process that converges quickly, i.e.&nbsp;on time-scale of order <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(%5Cvarepsilon%5E2)">, towards a Gaussian distribution with mean <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bx%7D_0"> (i.e.&nbsp;the denoised estimate) and variance <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5E2">.</p>
<p>To discretize the reverse dynamics Equation&nbsp;5 on a small interval <img src="https://latex.codecogs.com/png.latex?%5B%5Coverline%7Bs%7D,%20%5Coverline%7Bs%7D+%5Cdelta%5D">, one can for example consider the slightly simplified (linear) dynamics</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ad%5Coverleftarrow%7BX%7D_s%20=%0A-%5Cfrac12%20%5C,%20%5Cfrac%7B1%7D%7B%5Ctanh((T-s)/2)%7D%20%5C,%0A%7B%5Cleft(%20%20%5Coverleftarrow%7BX%7D_s%20-%20%5Cmu%20%5Cright)%7D%20.%0A%5C;%20+%20%5C;%0AdB%0A"></p>
<p>Here, <img src="https://latex.codecogs.com/png.latex?%5Cmu%20=%20%5Cwidehat%7Bx%7D_0(%5Coverleftarrow%7BX%7D_%7B%5Coverline%7Bs%7D%7D)%20/%20%5Ccosh((T-%5Coverline%7Bs%7D)/2)"> with <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_%7B%5Coverline%7Bs%7D%7D%20=%20%5Coverleftarrow%7Bx%7D_%7B%5Coverline%7Bs%7D%7D">. Algebra gives that, conditioned upon <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_%7B%5Coverline%7Bs%7D%7D%20=%20%5Coverleftarrow%7Bx%7D_%7B%5Coverline%7Bs%7D%7D">, we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%0A%5Cbegin%7Baligned%7D%0A%5Cmathop%7B%5Cmathrm%7BE%7D%7D%5B%20%5Coverleftarrow%7BX%7D_%7B%5Coverline%7Bs%7D%20+%20%5Cdelta%7D%20%5D%0A%5Cquad%20&amp;=%20%5Cquad%20%5Cmu%20+%20%5Clambda%20%5C,%20(%5Coverleftarrow%7Bx%7D_%7B%5Coverline%7Bs%7D%7D%20-%20%5Cmu)%5C%5C%0A%5Cmathop%7B%5Cmathrm%7BVar%7D%7D%5B%20%5Coverleftarrow%7BX%7D_%7B%5Coverline%7Bs%7D%20+%20%5Cdelta%7D%20%5D%0A%5Cquad%20&amp;=%20%5Cquad%0A%5Ctanh%20%7B%5Cleft(%20%5Cfrac%7BT-%5Coverline%7Bs%7D-%5Cdelta%7D%7B2%7D%20%5Cright)%7D%20%20%5C,%20(1-%5Clambda%5E2)%0A%5Cend%7Baligned%7D%0A%5Cright.%0A"></p>
<p>where the coefficient <img src="https://latex.codecogs.com/png.latex?0%3C%5Clambda%3C1"> is given by</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Clambda%20=%20%5Cfrac%7B%5Csinh(T-%5Coverline%7Bs%7D-%5Cdelta)%7D%7B%5Csinh(T-%5Coverline%7Bs%7D)%7D.%0A"></p>
<p>This discretization is more stable and efficient than a naive Euler-Maruyama approach since the drift term can get large as <img src="https://latex.codecogs.com/png.latex?s%20%5Cto%20T">. WIth the above discretization, one can easily simulate the reverse diffusion on <img src="https://latex.codecogs.com/png.latex?%5B0,T%5D"> and generate approximate samples from <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B%5Cmathrm%7Bdata%7D%7D">.</p>
<p>In the animation below, the method described in these notes was used. This very small example trains in less than a minute on an old laptop without a GPU. The noise estimator was parametrized with an MLP with <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Belu%7D(%5Cldots)"> non-linearity and two hidden-layers with size <img src="https://latex.codecogs.com/png.latex?H=128">. It was very useful to add a few Fourier features as input of the network, I could not make this work properly without.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="sine_reverse.mp4" class="img-fluid" style="width:100.0%" controls=""><a href="sine_reverse.mp4">Video</a></video></p>
</figure>
</div>
<p>The literature on DDPM is <a href="https://github.com/heejkoo/Awesome-Diffusion-Models">enormous and still growing</a>!</p>


</section>

 ]]></description>
  <category>DDPM</category>
  <category>score</category>
  <guid>https://alexxthiery.github.io/notes/DDPM/DDPM.html</guid>
  <pubDate>Sat, 01 Jul 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Reverse diffusions, Score &amp; Tweedie</title>
  <link>https://alexxthiery.github.io/notes/reverse_and_tweedie/reverse_and_tweedie.html</link>
  <description><![CDATA[ 




<section id="reversing-a-diffusion" class="level2">
<h2 class="anchored" data-anchor-id="reversing-a-diffusion">Reversing a diffusion</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="reverse_cropped.mp4" class="img-fluid" style="width:100.0%" controls=""><a href="reverse_cropped.mp4">Video</a></video></p>
</figure>
</div>
<p>Imagine a scalar diffusion process <img src="https://latex.codecogs.com/png.latex?%5C%7BX_t%5C%7D_%7Bt=0%7D%5ET"> defined on the interval <img src="https://latex.codecogs.com/png.latex?%5B0,T%5D">,</p>
<p><img src="https://latex.codecogs.com/png.latex?dX%20=%20%5Cmu(X)%20%5C,%20dt%20+%20%5Csigma%20%5C,%20dW"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmu(X)"> is the drift term, <img src="https://latex.codecogs.com/png.latex?%5Csigma"> is the diffusion coefficient, and <img src="https://latex.codecogs.com/png.latex?dW"> is a Wiener process. Denote the distribution of this process at time <img src="https://latex.codecogs.com/png.latex?t"> (<img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20t%20%5Cleq%20T">) as <img src="https://latex.codecogs.com/png.latex?p_t(dx)"> with an initial distribution of <img src="https://latex.codecogs.com/png.latex?p_0(dx)">. Now, what happens if we reverse time and examine the process backward? In other words, consider the time-reversed process <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D"> defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_s%20=%20X_%7BT-s%7D."></p>
<p>Intuitively, the process <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D"> is also a diffusion on the interval <img src="https://latex.codecogs.com/png.latex?%5B0,%20T%5D">, but with an initial distribution <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_0%20%5Csim%20p_T(dx)">. To gain intuition, consider an Euler discretization of the forward process:</p>
<p><span id="eq-rev-wrong"><img src="https://latex.codecogs.com/png.latex?X_%7Bt+%5Cdelta%7D%20=%20X_%7Bt%7D%20+%20%5Cmu(X_t)%5C,%20%5Cdelta%20+%20%5Csigma%20%5C,%20%5Csqrt%7B%5Cdelta%7D%20%5C,%20%5Cmathbf%7Bn%7D%20%5Ctag%7B1%7D"></span></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bn%7D%5Csim%20%5Cmathcal%7BN%7D(0,1)"> represents a noise term independent from <img src="https://latex.codecogs.com/png.latex?X_%7Bt%7D">, and <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Cll%201"> is a time increment. Re-arranging terms and making the approximation <img src="https://latex.codecogs.com/png.latex?%5Cmu(X_t)%20%5Capprox%20%5Cmu(X_%7Bt+%5Cdelta%7D)"> gives that</p>
<p><span id="eq-rev-euler-wrong"><img src="https://latex.codecogs.com/png.latex?X_%7Bt%7D%20%5Capprox%20X_%7Bt+%5Cdelta%7D%20-%20%5Cmu(X_%7Bt+%5Cdelta%7D)%5C,%20%5Cdelta%20+%20%5Csigma%20%5C,%20%5Csqrt%7B%5Cdelta%7D%20%5C,%20(-%5Cmathbf%7Bn%7D).%20%5Ctag%7B2%7D"></span></p>
<p>This seems to suggest that the time-reversed process follows the dynamics <img src="https://latex.codecogs.com/png.latex?d%5Coverleftarrow%7BX%7D%20=%20-%5Cmu(%5Coverleftarrow%7BX%7D)%20%5C,%20dt%20+%20%5Csigma%20%5C,%20dW"> started from <img src="https://latex.codecogs.com/png.latex?%5Coverleftarrow%7BX%7D_0%20%5Csim%20p_T(dx)">. However, this conclusion is incorrect because this would suggest that the time-reversed of a standard Brownian motion (where <img src="https://latex.codecogs.com/png.latex?%5Cmu(x)%20%5Cequiv%200">) starting at zero is also a Brownian motion starting at <img src="https://latex.codecogs.com/png.latex?p_T(dx)%20=%20%5Cmathcal%7BN%7D(0,T)">, which is clearly not the case. The flaw in this argument lies in assuming that the noise term <img src="https://latex.codecogs.com/png.latex?Z"> is independent of <img src="https://latex.codecogs.com/png.latex?X_%7Bt+%5Cdelta%7D">, which is not true, rendering the Euler discretization argument invalid.</p>
<p>Deriving the dynamics of the backward process in a rigorous manner is not straightforward <span class="citation" data-cites="anderson1982reverse">(Anderson 1982)</span> <span class="citation" data-cites="haussmann1986time">(Haussmann and Pardoux 1986)</span>. What follows is a heuristic derivation that proceeds by estimating the mean and variance of <img src="https://latex.codecogs.com/png.latex?X_%7Bt%7D"> given <img src="https://latex.codecogs.com/png.latex?X_%7Bt+%5Cdelta%7D%20=%20x_%7Bt+%5Cdelta%7D">, assuming <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Cll%201">. Here, <img src="https://latex.codecogs.com/png.latex?x_%7Bt+%5Cdelta%7D"> is treated as a fixed and constant value, and we are only interested in the conditional distribution of <img src="https://latex.codecogs.com/png.latex?X_t"> given <img src="https://latex.codecogs.com/png.latex?x_%7Bt+%5Cdelta%7D">. Bayes’ law gives</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(X_t%20%5Cin%20dx%20%7C%20x_%7Bt+%5Cdelta%7D)%20%5Cpropto%20p_t(x)%20%5C,%20%5Cexp%5Cleft%5C%7B%20-%5Cfrac%7B%5Cleft(%20x_%7Bt+%5Cdelta%7D%20-%20%5Bx%20+%20%5Cmu(x)%20%5C,%20%5Cdelta%5D%20%5Cright)%5E2%7D%7B2%20%5Csigma%5E2%20%5C,%20%5Cdelta%7D%20%5Cright%5C%7D,"></p>
<p>where the exponential term corresponds to the transition of the forward diffusion for <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Cll%201">. Using the 1st order approximation</p>
<p><img src="https://latex.codecogs.com/png.latex?p_t(x)%20%5Capprox%20p_t(x_%7Bt+%5Cdelta%7D)%20%5C,%5Cexp%5Cleft(%20%5Clangle%20%5Cnabla%20%5Clog%20p_t(x_%7Bt+%5Cdelta%7D),%20(x-x_%7Bt+%5Cdelta%7D)%5Crangle%5Cright),"></p>
<p>eliminating multiplicative constants and higher-order error terms, we obtain:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(X_t%20%5Cin%20dx%20%7C%20x_%7Bt+%5Cdelta%7D)%0A%5Cpropto%0A%5Cexp%5Cleft%5C%7B%20-%5Cfrac%7B%5Cleft(%20x%20-%20%5Bx_%7Bt+%5Cdelta%7D%20-%20%5Cmu(x_%7Bt+%5Cdelta%7D)%20%5Cdelta%0A+%20%7B%5Ccolor%7Bred%7D%20%5Csigma%5E2%20%5C,%20%5Cnabla%20%5Clog%20p_t(x_%7Bt+%5Cdelta%7D)%20%5C,%20%5Cdelta%7D%20%5D%20%5Cright)%5E2%7D%7B2%20%5Csigma%5E2%20%5C,%20%5Cdelta%7D%20%5Cright%5C%7D."></p>
<p>For <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Cll%201">, this is transition of the reverse diffusion</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ad%5Coverleftarrow%7BX%7D_t%20=%20-%5Cmu(%5Coverleftarrow%7BX%7D_t)%20%5C,%20dt%20%5C;%20+%20%7B%5Ccolor%7Bred%7D%20%5Csigma%5E2%20%5C,%20%5Cnabla%20%5Clog%20p_%7BT-t%7D(%5Coverleftarrow%7BX%7D_t)%20%5C,%20dt%7D%20%5C;%20+%20%5C;%20%5Csigma%20%5C,%20dB.%0A"></p>
<p>The notation <img src="https://latex.codecogs.com/png.latex?B"> is used to emphasize that this Brownian motion is distinct from the one used in the forward diffusion. The additional drift term, denoted as <img src="https://latex.codecogs.com/png.latex?%7B%5Ccolor%7Bred%7D%20%5Csigma%5E2%20%5C,%20%5Cnabla%20%5Clog%20p_t(%5Coverleftarrow%7BX%7D)%7D">, is intuitive: it pushes the reverse diffusion toward regions where the forward diffusion spent a significant amount of time, i.e., where <img src="https://latex.codecogs.com/png.latex?p_t(dx)"> is large. The popular “denoising diffusion models” <span class="citation" data-cites="ho2020denoising">(Ho, Jain, and Abbeel 2020)</span> can be seen as discretizations of this backward process, employing various techniques to estimate the additional drift term from data.</p>
</section>
<section id="denoising-score-matching-tweedie-formula" class="level2">
<h2 class="anchored" data-anchor-id="denoising-score-matching-tweedie-formula">Denoising Score Matching &amp; Tweedie formula</h2>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://en.wikipedia.org/wiki/Maurice_Tweedie"><img src="https://alexxthiery.github.io/notes/reverse_and_tweedie/maurice_tweedie.gif" class="img-fluid figure-img" style="width:35.0%"></a></p>
<figcaption class="figure-caption">Maurice Tweedie (1919–1996)</figcaption>
</figure>
</div>
</div>
<p>The previous section shows that a quantity often referred to as the “score” in machine learning (ML) literature, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BS%7D_t(x)%20%5Cequiv%20%5Cnabla_x%20%5Clog%20p_t(x)">, naturally arises when a diffusion process runs backward in time. Interestingly, it is worth noting that since the beginning of times statisticians have been using the term <a href="https://en.wikipedia.org/wiki/Score_(statistics)">“score”</a> to refer to the other derivative, i.e.&nbsp;the derivative with respect to a model’s parameter, which is a completely different object!</p>
<p>Consider a Brownian diffusion process <img src="https://latex.codecogs.com/png.latex?dX%20=%20%5Csigma%20%5C,%20dW"> initiated from a distribution <img src="https://latex.codecogs.com/png.latex?%5Cmu(dx)%20=%20p_0(dx)">. If this process is ran forward for a duration <img src="https://latex.codecogs.com/png.latex?%5Cdelta%3E0">, we have:</p>
<p><img src="https://latex.codecogs.com/png.latex?X_%7B%5Cdelta%7D%20=%20X_0%20+%20%5Cmathcal%7BN%7D(0,%20%5Csigma%5E2%20%5C,%20%5Cdelta)"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?X_0%20%5Csim%20%5Cmu(dx)">. Now, focusing on a specific sample <img src="https://latex.codecogs.com/png.latex?y%20=%20X_%7B%5Cdelta%7D">, the backward dynamics <img src="https://latex.codecogs.com/png.latex?d%5Coverleftarrow%7BX%7D%20=%20%5Csigma%5E2%20%5C,%20%5Cnabla%20%5Clog%20p_t(%5Coverleftarrow%7BX%7D)%20,%20dt%20+%20%5Csigma%20%5C,%20dB"> suggests that for sufficiently small <img src="https://latex.codecogs.com/png.latex?%5Cdelta">, the following approximation holds:</p>
<p><span id="eq-rev-tweedie"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5BX_0%20%5C,%20%7C%20X_%7B%5Cdelta%7D%20=%20y%5D%20%5C;%20%7B%20%5Ccolor%7B%5Cred%7D%20%5Capprox%7D%20%5C;%20y%20+%20%5Cnabla%20%5Clog%20p_%7B%5Cdelta%7D(y)%20%5C,%20%5Csigma%5E2%20%5Cdelta.%0A%5Ctag%7B3%7D"></span></p>
<p>Maybe surprisingly, in the case of Brownian dynamics (no drift), this relationship holds exactly, even for arbitrarily large time increments <img src="https://latex.codecogs.com/png.latex?%5Cdelta%3E0">. This observation attributed in <span class="citation" data-cites="efron2011tweedie">(Efron 2011)</span> to <a href="https://en.wikipedia.org/wiki/Maurice_Tweedie">Maurice Tweedie (1919–1996)</a> has a straightforward proof. Specifically, if <img src="https://latex.codecogs.com/png.latex?X%20%5Csim%20%5Cmu(dx)">, then <img src="https://latex.codecogs.com/png.latex?Y%20=%20X%20+%20%5Cmathcal%7BN%7D(0,%20%5CGamma%5E2)"> has a density given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap_Y(dy)%20=%20%5Cint%20%5Cmu(x)%20%5C,%20%5Crho_%7B%5CGamma%7D(y-x)%20%5C,%20dx%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Crho_%7B%5CGamma%7D(z)%20%5Cpropto%20%5Cexp%5B-z%5E2/(2%20%5CGamma%5E2)%5D"> is a centred Gaussian with variance <img src="https://latex.codecogs.com/png.latex?%5CGamma%5E2">. It follows that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%20%5Cmathbb%7BE%7D%5BX%20%5C,%20%7C%20Y%20=%20y%5D-y%7D%7B%5CGamma%5E2%7D%20=%20%5Cfrac%7B%20%5Cint%20%5Cleft(%20%5Cfrac%20%7Bx-y%20%7D%7B%5CGamma%5E2%7D%20%5Cright)%5C,%20%5Cmu(x)%20%5C,%20%5Crho_%7B%5CGamma%7D(y-x)%20%5C,%20dx%20%7D%7B%5Cint%20%5Cmu(x)%20%5C,%20%5Crho_%7B%5CGamma%7D(y-x)%20%5C,%20dx%7D%0A=%0A%5Cnabla_y%20%5Clog%20%5Cleft%5C%7B%20%5Cint%20%5Cmu(x)%20%5C,%20%5Crho_%7B%5CGamma%7D(y-x)%20%5C,%20dx%20%5Cright%5C%7D%0A"></p>
<p>since <img src="https://latex.codecogs.com/png.latex?%5Cnabla_y%20%5Crho_%7B%5CGamma%7D(y-x)%20=%20(x-y)/%5CGamma%5E2">. This leads to Tweedie’s formula:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5BX%20%5C,%20%7C%20Y%20=%20y%5D%20%5C;%20%7B%5Ccolor%7Bred%7D%20=%7D%20%5C;%20y%20+%20%5CGamma%5E2%20%5C,%20%5Cnabla%20%5Clog%20p_Y(y),%0A"></p>
<p>which is exactly the same as Equation&nbsp;3 but with an equality sign: no approximation needed! Interestingly, this is what Machine-Learners often refer to as “denoising score matching” <span class="citation" data-cites="vincent2011connection">(Vincent 2011)</span>: if we have access to a large number of samples <img src="https://latex.codecogs.com/png.latex?(X_i,Y_i)">, where <img src="https://latex.codecogs.com/png.latex?X_i%20%5Csim%20%5Cmu(dx)"> and <img src="https://latex.codecogs.com/png.latex?Y_i%20=%20X_i%20+%20%5Cmathcal%7BN%7D(0,%20%5CGamma%5E2)">, and fit a regression model <img src="https://latex.codecogs.com/png.latex?F_%7B%5Ctheta%7D"> by minimizing the mean-squared error <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20%5Cmapsto%20%5Cmathbb%7BE%7D%20%5C%7CX%20-%20F_%7B%5Ctheta%7D(Y)%5C%7C%5E2">, then Tweedie formula shows that in the limit of infinite samples and with sufficient flexibility in the regression model <img src="https://latex.codecogs.com/png.latex?F_%7B%5Ctheta_%5Cstar%7D(y)%20=%20y%20+%20%5CGamma%5E2%20%5Cnabla%20%5Clog%20p_Y(y)">. This allows one to estimate <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20%5Clog%20p_Y"> from data, which is often a good approximation of <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20%5Clog%20%5Cmu"> if the variance <img src="https://latex.codecogs.com/png.latex?%5CGamma%5E2"> of the added noise is not too large. Indeed, things can go bad if <img src="https://latex.codecogs.com/png.latex?%5CGamma%5E2"> is very small and the number of training data is not large, no free lunch!</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-anderson1982reverse" class="csl-entry">
Anderson, Brian DO. 1982. <span>“Reverse-Time Diffusion Equation Models.”</span> <em>Stochastic Processes and Their Applications</em> 12 (3): 313–26.
</div>
<div id="ref-efron2011tweedie" class="csl-entry">
Efron, Bradley. 2011. <span>“Tweedie’s Formula and Selection Bias.”</span> <em>Journal of the American Statistical Association</em> 106 (496): 1602–14.
</div>
<div id="ref-haussmann1986time" class="csl-entry">
Haussmann, Ulrich G, and Etienne Pardoux. 1986. <span>“Time Reversal of Diffusions.”</span> <em>The Annals of Probability</em>, 1188–1205.
</div>
<div id="ref-ho2020denoising" class="csl-entry">
Ho, Jonathan, Ajay Jain, and Pieter Abbeel. 2020. <span>“Denoising Diffusion Probabilistic Models.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 6840–51.
</div>
<div id="ref-vincent2011connection" class="csl-entry">
Vincent, Pascal. 2011. <span>“A Connection Between Score Matching and Denoising Autoencoders.”</span> <em>Neural Computation</em> 23 (7): 1661–74.
</div>
</div></section></div> ]]></description>
  <category>SDE</category>
  <category>score</category>
  <guid>https://alexxthiery.github.io/notes/reverse_and_tweedie/reverse_and_tweedie.html</guid>
  <pubDate>Sun, 11 Jun 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Notes</title>
  <link>https://alexxthiery.github.io/notes/index_notes.html</link>
  <description><![CDATA[ 




<!-- <h1 style="text-align: center;">Notes</h3> -->
<p>Notes for students, half-baked write-ups for myself, and probably other random maths/stats/ML texts. Certainly full of typos, comments welcome!</p>
<!-- ### Statistics: Foundations
* [Natural Gradients](./stats_basics/natural_gradients.qmd) -->
<hr>
<p>Notes indexed by <a href="../notes/index_notes_as_list.html">categories</a>.</p>
<hr>
<section id="denoising-diffusion-probabilistic-models" class="level3">
<h3 class="anchored" data-anchor-id="denoising-diffusion-probabilistic-models">Denoising Diffusion Probabilistic Models</h3>
<ul>
<li><a href="../notes/DDPM/DDPM.html">Noising and Reverse Ornstein-Uhlenbeck</a></li>
<li><a href="../notes/DDPM_deterministic/DDPM_deterministic.html">From Denoising Diffusion to ODEs</a></li>
<li><a href="../notes/reverse_and_tweedie/reverse_and_tweedie.html">Reverse diffusions, Score &amp; Tweedie</a></li>
</ul>
</section>
<section id="information-theory" class="level3">
<h3 class="anchored" data-anchor-id="information-theory">Information Theory</h3>
<ul>
<li><a href="../notes/information_theory_references/information_theory_references.html">References &amp; Readings</a></li>
<li><a href="../notes/information_theory_basics/information_theory_entropy.html">Entropy and Basic Definitions</a></li>
<li><a href="../notes/information_theory_shannon_coding/information_theory_shannon_coding.html">Shannon Source Coding Theorem</a></li>
<li><a href="../notes/information_theory_fano/information_theory_fano.html">Fano’s inequality</a></li>
<li><a href="../notes/information_theory_shearer_lemma/shearer_lemma.html">Shearer’s Lemma</a></li>
</ul>
</section>
<section id="monte-carlo-methods" class="level3">
<h3 class="anchored" data-anchor-id="monte-carlo-methods">Monte-Carlo methods</h3>
<ul>
<li><a href="../notes/on_Langevin_MCMC/on_Langevin_MCMC.html">Deriving the Langevin MCMC algorithm</a></li>
<li><a href="../notes/Gaussian_Assimilation/gaussian_assimilation.html">Gaussian Assimilation</a></li>
</ul>
</section>
<section id="probability-misc" class="level3">
<h3 class="anchored" data-anchor-id="probability-misc">Probability Misc</h3>
<ul>
<li><a href="../notes/auxiliary_variable_trick/auxiliary_variable_trick.html">Auxiliary variable trick</a></li>
<li><a href="../notes/sanov/sanov.html">Sanov’s Theorem</a></li>
<li><a href="../notes/wasserstein_langevin/wasserstein_langevin.html">Wasserstein Gradients &amp; Langevin Diffusions</a></li>
</ul>


</section>

 ]]></description>
  <category>index</category>
  <guid>https://alexxthiery.github.io/notes/index_notes.html</guid>
  <pubDate>Sat, 31 Dec 2022 16:00:00 GMT</pubDate>
</item>
</channel>
</rss>
