<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Alexandre Thiéry</title>
<link>https://alexxthiery.github.io/notes/index_notes_as_list.html</link>
<atom:link href="https://alexxthiery.github.io/notes/index_notes_as_list.xml" rel="self" type="application/rss+xml"/>
<description>Alex Thiery Notes</description>
<generator>quarto-1.3.353</generator>
<lastBuildDate>Fri, 04 Apr 2025 16:00:00 GMT</lastBuildDate>
<item>
  <title>Self Avoiding Walks</title>
  <link>https://alexxthiery.github.io/notes/SAW/SAW.html</link>
  <description><![CDATA[ 




<!-- \begin{figure}[h]
\centering
\includegraphics[width=0.3\textwidth]{polymer-selfavoiding.png}
\caption{A 2D self-avoiding walk}
\end{figure} -->
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/SAW/polymer-selfavoiding.png" class="img-fluid figure-img" style="width:35.0%"></p>
<figcaption class="figure-caption"><a href="https://en.wikipedia.org/wiki/Self-avoiding_walk">A 2D self-avoiding walk</a></figcaption>
</figure>
</div>
</div>
<p><em>These notes present comments on the “Self-avoiding walks” assignment given to the “ST3247: Simulations” class. Most of the drafts that have been submitted so far describe variations of importance sampling. The purpose of these notes is to suggest directions for slightly more advanced Monte Carlo methods that can be used to estimate the connective constant <img src="https://latex.codecogs.com/png.latex?%5Cmu"> of self-avoiding walks. These are only pointers and suggestions.</em></p>
<section id="the-problems-and-notations" class="level3">
<h3 class="anchored" data-anchor-id="the-problems-and-notations">The problems and notations</h3>
<p>Recall that we are trying to estimate the <a href="https://en.wikipedia.org/wiki/Connective_constant">connective constant</a> <img src="https://latex.codecogs.com/png.latex?%5Cmu"> of self-avoiding walks (SAW) in the 2D lattice <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BZ%7D%5E2">. If <img src="https://latex.codecogs.com/png.latex?c_L"> denotes the number of SAWs of length <img src="https://latex.codecogs.com/png.latex?L">, we have the following asymptotic behavior:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ac_L%20%5C;%20%5Csim%20%5C;%20A%20%5C,%20%5Cmu%5EL%20%5C,%20L%5E%7B%5Cgamma%7D%0A"></p>
<p>for some unknown constants <img src="https://latex.codecogs.com/png.latex?A">, <img src="https://latex.codecogs.com/png.latex?%5Cmu">, and <img src="https://latex.codecogs.com/png.latex?%5Cgamma">. The main objective of the assignment is to estimate <img src="https://latex.codecogs.com/png.latex?%5Cmu">, which can also be expressed as the limit of <img src="https://latex.codecogs.com/png.latex?c_L%5E%7B1/L%7D"> as <img src="https://latex.codecogs.com/png.latex?L%20%5Cto%20%5Cinfty">. As of today, the <a href="https://en.wikipedia.org/wiki/Connective_constant">best known estimate</a> is <img src="https://latex.codecogs.com/png.latex?%5Cmu%20%5Capprox%202.638158533032790(3)">, which required several tens of thousand hours of CPU time to compute. To estimate <img src="https://latex.codecogs.com/png.latex?%5Cmu">, one must approximate the number of SAWs of length <img src="https://latex.codecogs.com/png.latex?L"> starting at the origin for large values of <img src="https://latex.codecogs.com/png.latex?L"> if one hopes to get a good estimate.</p>
<p>Consider a sequence <img src="https://latex.codecogs.com/png.latex?z_%7B0:L%7D%20=%20(z_0,%20z_1,%20%5Cdots,%20z_L)"> of <img src="https://latex.codecogs.com/png.latex?L+1"> distinct vertices in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BZ%7D%5E2"> with <img src="https://latex.codecogs.com/png.latex?z_0%20=%20(0,0)"> and <img src="https://latex.codecogs.com/png.latex?%5C%7Cz_%7Bk+1%7D%20-%20z_k%5C%7C=1"> for all <img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20k%20%5Cleq%20L-1">, i.e., a walk of length <img src="https://latex.codecogs.com/png.latex?L">. For notational convenience, let us introduce the function <img src="https://latex.codecogs.com/png.latex?%5Cvarphi%5E%7B%5Ctextrm%7Bwalk%7D%7D(z_%7B:L%7D)"> that returns one if <img src="https://latex.codecogs.com/png.latex?z_%7B0:L%7D"> is a correct walk of length <img src="https://latex.codecogs.com/png.latex?L">, and zero otherwise. In particular, this function returns zero if two consecutive vertices are the same, or if the walk does not start at zero. Similarly, introduce the function <img src="https://latex.codecogs.com/png.latex?%5Cvarphi%5E%7B%5Ctextrm%7BSAW%7D%7D(z_%7B:L%7D)"> that returns one if <img src="https://latex.codecogs.com/png.latex?z_%7B0:L%7D"> is a SAW of length <img src="https://latex.codecogs.com/png.latex?L">. One can define two important probability mass functions:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap%5E%7B%5Ctextrm%7Bwalk%7D%7D_%7BL%7D(z_%7B0:L%7D)%20=%20%5Cfrac%7B%5Cvarphi_L%5E%7B%5Ctextrm%7Bwalk%7D%7D(z_%7B0:L%7D)%7D%7B4%5EL%7D%0A%5Cqquad%20%5Ctextrm%7Band%7D%20%5Cqquad%0Ap%5E%7B%5Ctextrm%7BSAW%7D%7D_%7BL%7D(z_%7B0:L%7D)%20=%20%5Cfrac%7B%5Cvarphi_L%5E%7B%5Ctextrm%7BSAW%7D%7D(z_%7B0:L%7D)%7D%7Bc_L%7D.%0A"></p>
<p>They describe the uniform distributions on all the walks of length <img src="https://latex.codecogs.com/png.latex?L"> and all the SAWs of length <img src="https://latex.codecogs.com/png.latex?L">, respectively.</p>
</section>
<section id="importance-sampling" class="level3">
<h3 class="anchored" data-anchor-id="importance-sampling">Importance sampling</h3>
<p>One can approximate <img src="https://latex.codecogs.com/png.latex?c_L"> with naive Monte Carlo by estimating the proportion <img src="https://latex.codecogs.com/png.latex?p_L"> of walks that are SAWs,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap_L%20=%20%5Cmathbb%7BE%7D_%7Bp%5E%7B%5Ctextrm%7Bwalk%7D%7D_%7BL%7D%7D%20%5Cleft%5B%20%5Cvarphi_L%5E%7B%5Ctextrm%7BSAW%7D%7D(z_%7B0:L%7D)%20%5Cright%5D%0A=%0A%5Cfrac%7B1%7D%7B4%5EL%7D%20%5Csum_%7Bz_%7B0:L%7D%7D%20%5Cvarphi_L%5E%7B%5Ctextrm%7BSAW%7D%7D(z_%7B0:L%7D).%0A"></p>
<p>This is an absolute disaster since the proportion of SAWs among all walks is extremely small. One can do significantly better using importance sampling. For this, consider a proposal distribution that starts at the origin and continues by choosing uniformly among the four neighbors of the last vertex that have not been visited yet. If there are no unvisited neighbors, the walk continues by standing still until length <img src="https://latex.codecogs.com/png.latex?L"> is reached: the resulting path is not even a valid walk, so <img src="https://latex.codecogs.com/png.latex?p%5E%7B%5Ctextrm%7Bwalk%7D%7D_%7BL%7D(z_%7B0:L%7D)%20=%200"> as well as <img src="https://latex.codecogs.com/png.latex?p%5E%7B%5Ctextrm%7BSAW%7D%7D_%7BL%7D(z_%7B0:L%7D)%20=%200">. The probability mass function of the proposal distribution is easy to compute, so estimating <img src="https://latex.codecogs.com/png.latex?p_L"> with importance sampling is straightforward. This is usually called the Rosenbluth method <span class="citation" data-cites="rosenbluth1955monte">(Rosenbluth and Rosenbluth 1955)</span>. <em>[<strong>Note to students</strong>: make it much clearer in your report that the Rosenbluth method is just importance sampling. Do note that even the “rejected” walks have to be taken into account!]</em></p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/SAW/SAW_SMC.png" class="img-fluid figure-img" style="width:95.0%"></p>
<figcaption class="figure-caption">Importance Sampling (Rosenbluth method)</figcaption>
</figure>
</div>
</div>
<p>As one can see, the quality quickly deteriorates as <img src="https://latex.codecogs.com/png.latex?L"> increases. This is because the number of accepted walks is very small, and, among them, the importance weights are highly unequal.<br>
<em>[<strong>Note to students</strong>: you should explain this much more clearly, and possibly explore this more quantitatively. The reason it is failing is not only that the number of accepted walks is small]</em></p>
</section>
<section id="recursive-formulation" class="level3">
<h3 class="anchored" data-anchor-id="recursive-formulation">Recursive formulation</h3>
<p>We have just seen that importance sampling will not be able to estimate <img src="https://latex.codecogs.com/png.latex?c_L"> for large values of <img src="https://latex.codecogs.com/png.latex?L">. This makes accurate estimates of <img src="https://latex.codecogs.com/png.latex?%5Cmu"> difficult to obtain this way.</p>
<p>To make progress, one can exploit the recursive structure of the problem. Let us define the concatenation of two walks. Given a first walk <img src="https://latex.codecogs.com/png.latex?z%5E%7B(A)%7D_%7B0:L_A%7D"> and a second walk <img src="https://latex.codecogs.com/png.latex?z%5E%7B(B)%7D_%7B0:L_B%7D">, one can define a new walk of length <img src="https://latex.codecogs.com/png.latex?L_A%20+%20L_B"> by starting at the origin, following the <img src="https://latex.codecogs.com/png.latex?L_A"> increments of the first walk, then the <img src="https://latex.codecogs.com/png.latex?L_B"> increments of the second. The concatenation of two SAWs is not always a SAW. However, it is not hard to prove the following. Define <img src="https://latex.codecogs.com/png.latex?B(L_A,%20L_B)%20%5Cin%20(0,1)"> as the probability that, when sampling SAWs <img src="https://latex.codecogs.com/png.latex?z%5E%7B(A)%7D_%7B0:L_A%7D"> and <img src="https://latex.codecogs.com/png.latex?z%5E%7B(B)%7D_%7B0:L_B%7D"> independently and uniformly at random, their concatenation is still a SAW. Then:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AB(L_A,%20L_B)%20%5C;%20=%20%5C;%20%5Cfrac%7Bc_%7BL_A%20+%20L_B%7D%7D%7Bc_%7BL_A%7D%20%5C,%20c_%7BL_B%7D%7D.%0A"></p>
<p><em>[<strong>Note to students</strong>: it is OK for you to use this fact. It’s even better if you can prove it, but not absolutely necessary.]</em></p>
<p>Assuming one can generate SAWs of length <img src="https://latex.codecogs.com/png.latex?L"> uniformly at random ( a problem that will be discussed later), we can estimate <img src="https://latex.codecogs.com/png.latex?%5Cmu"> in several ways:</p>
<ol type="1">
<li><p>For small values of <img src="https://latex.codecogs.com/png.latex?L_1">, the number of SAWs <img src="https://latex.codecogs.com/png.latex?c_%7BL_1%7D"> is known exactly (e.g., <img src="https://latex.codecogs.com/png.latex?c_1%20=%204">, <img src="https://latex.codecogs.com/png.latex?c_%7B10%7D%20=%2044100">). Suppose one can generate SAWs of length <img src="https://latex.codecogs.com/png.latex?L_2%20%5Cgg%201">. One can then estimate <img src="https://latex.codecogs.com/png.latex?B(L_1,%20L_2)"> empirically. Since <img src="https://latex.codecogs.com/png.latex?c_L%20%5C;%20%5Csim%20%5C;%20A%20%5C,%20%5Cmu%5EL%20%5C,%20L%5E%7B%5Cgamma%7D">, it follows that, for <img src="https://latex.codecogs.com/png.latex?L_1"> fixed and <img src="https://latex.codecogs.com/png.latex?L_2%20%5Cto%20%5Cinfty">, <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bc_%7BL_1+L_2%7D%7D%7Bc_%7BL_2%7D%7D%20%5Capprox%20%5Cmu%5E%7BL_1%7D.%0A"> Using the fact that <img src="https://latex.codecogs.com/png.latex?B(L_1,%20L_2)%20=%20c_%7BL_1+L_2%7D%20/%20(c_%7BL_2%7D%20c_%7BL_1%7D)">, one can then estimate <img src="https://latex.codecogs.com/png.latex?%5Cmu"> from the estimate of <img src="https://latex.codecogs.com/png.latex?B(L_1,%20L_2)">.</p></li>
<li><p>Alternatively, one can estimate <img src="https://latex.codecogs.com/png.latex?c_L"> for large <img src="https://latex.codecogs.com/png.latex?L"> recursively. For example, starting from <img src="https://latex.codecogs.com/png.latex?c_%7B10%7D%20=%2044100">, estimate <img src="https://latex.codecogs.com/png.latex?B(10,10)"> to compute <img src="https://latex.codecogs.com/png.latex?c_%7B20%7D">, then use <img src="https://latex.codecogs.com/png.latex?B(20,20)"> to compute <img src="https://latex.codecogs.com/png.latex?c_%7B40%7D">, and so on. Using this method and about <img src="https://latex.codecogs.com/png.latex?5"> hours of CPU time (see below for details) with <img src="https://latex.codecogs.com/png.latex?10,000"> SAWs of lengths <img src="https://latex.codecogs.com/png.latex?10,%2020,%20%5Cdots,%202560">, I obtained <img src="https://latex.codecogs.com/png.latex?%5Cmu%20%5Capprox%202.643">.</p></li>
</ol>
</section>
<section id="generating-saws" class="level3">
<h3 class="anchored" data-anchor-id="generating-saws">Generating SAWs</h3>
<p>The previous discussion shows that, once we know how to generate uniform SAWs, we can estimate <img src="https://latex.codecogs.com/png.latex?%5Cmu"> relatively easily. One of the most common methods is the pivot algorithm: see <a href="https://clisby.net/projects/sm_simulator/">here</a> for a nice visualization. The principle is simple: given a SAW, randomly select a pivot site and apply a symmetry operation (like rotation or reflection) to one part of the walk. If the resulting walk remains self-avoiding, accept it; otherwise, reject it. Repeating this process generates diverse, approximately uniform SAWs.<br>
<em>[<strong>Note to students</strong>: explain this much more clearly if you decide to use it]</em></p>
<p>In short, the pivot algorithm updates a SAW by applying a symmetry operation to a subpath. Given a SAW <img src="https://latex.codecogs.com/png.latex?z_%7B0:L%7D">, one can obtain another SAW by applying to it the pivot algorithm a (large) number of times. To obtain a nearly independent SAW of length <img src="https://latex.codecogs.com/png.latex?L"> starting from <img src="https://latex.codecogs.com/png.latex?z_%7B0:L%7D">, one typically need to apply about <img src="https://latex.codecogs.com/png.latex?L"> pivot steps. While it can be slow for large <img src="https://latex.codecogs.com/png.latex?L">, it is far more efficient than naive importance sampling.<br>
<em>[<strong>Note to students</strong>: efficiently implementing the pivot algorithm is non-trivial, but LLM assistants can help a lot, and are actually quite useful for code optimization]</em></p>
</section>
<section id="sequential-monte-carlo" class="level3">
<h3 class="anchored" data-anchor-id="sequential-monte-carlo">Sequential Monte Carlo</h3>
<p>To estimate <img src="https://latex.codecogs.com/png.latex?c_L"> for large <img src="https://latex.codecogs.com/png.latex?L">, one can use Sequential Monte Carlo (SMC). The idea is to grow a population of <img src="https://latex.codecogs.com/png.latex?N"> SAWs in parallel and estimate <img src="https://latex.codecogs.com/png.latex?c_L"> by recursively estimating the ratios <img src="https://latex.codecogs.com/png.latex?c_%7BL+1%7D/c_L">. Suppose you have <img src="https://latex.codecogs.com/png.latex?N"> SAWs of length <img src="https://latex.codecogs.com/png.latex?L">. Try to extend each SAW by choosing a neighbor of the last vertex that has not been visited yet. This is a form of importance sampling, giving <img src="https://latex.codecogs.com/png.latex?N"> new walks of length <img src="https://latex.codecogs.com/png.latex?L+1"> with associated weights (some of them being non-valid walks!). Then, <em>resample</em> <img src="https://latex.codecogs.com/png.latex?N"> times from this weighted set to get <img src="https://latex.codecogs.com/png.latex?N"> new SAWs of length <img src="https://latex.codecogs.com/png.latex?L+1"> (with possible duplicates). Apply the pivot algorithm to eliminate these duplicates and generate more diverse SAWs.<br>
<em>[<strong>Note to students</strong>: if you decide to use SMC, explain it much more clearly. It’s not entirely straightforward to understand or implement, but it is one of the most powerful and versatile Monte Carlo methods to this day. A good investment of your time if you decide to understand SMC]</em></p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/SAW/SMC_mu_estimates.png" class="img-fluid figure-img" style="width:95.0%"></p>
<figcaption class="figure-caption">Sequential Monte Carlo</figcaption>
</figure>
</div>
</div>
</section>
<section id="improving-the-estimation-of-mu" class="level3">
<h3 class="anchored" data-anchor-id="improving-the-estimation-of-mu">Improving the estimation of <img src="https://latex.codecogs.com/png.latex?%5Cmu"></h3>
<p>Suppose you have estimates of <img src="https://latex.codecogs.com/png.latex?(%5Clog%20c_L)/L"> for various <img src="https://latex.codecogs.com/png.latex?L">. Since</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Clog%20c_L%7D%7BL%7D%20%5Capprox%20%5Clog%20A%20%5Ccdot%20%5Cfrac%7B1%7D%7BL%7D%20+%20%5Clog%20%5Cmu%20+%20%5Cgamma%20%5Ccdot%20%5Cfrac%7B%5Clog%20L%7D%7BL%7D,%0A"></p>
<p>you can fit a linear regression to estimate <img src="https://latex.codecogs.com/png.latex?%5Clog%20A">, <img src="https://latex.codecogs.com/png.latex?%5Clog%20%5Cmu">, and <img src="https://latex.codecogs.com/png.latex?%5Cgamma">. I tried this approach using a naive and non-optimized SMC implementation with <img src="https://latex.codecogs.com/png.latex?N=1000"> and <img src="https://latex.codecogs.com/png.latex?L=1000">, running for 10 hours on a free (and bad) online CPU, and obtained <img src="https://latex.codecogs.com/png.latex?%5Cmu%20%5Capprox%202.6366">.<br>
<em>[<strong>Note to students</strong>: can you do much better?]</em></p>
</section>
<section id="running-long-simulations" class="level3">
<h3 class="anchored" data-anchor-id="running-long-simulations">Running long simulations</h3>
<p>The best known estimate of <img src="https://latex.codecogs.com/png.latex?%5Cmu"> required several tens of thousands of CPU hours. While writing these notes, I was able to run simulations easily and for free using <a href="https://deepnote.com">deepNote</a>: it was my first time using it, and it was very user friendly. This allowed me to run simulations for 8 hours on a (free but slow) CPU without issue. Launch simulations in the evening and let them run overnight. <em>[<strong>Note to students</strong>: for the more motivated ones, you can try writing GPU-friendly code to run simulations, possibly on Google Colab]</em></p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-rosenbluth1955monte" class="csl-entry">
Rosenbluth, Marshall N, and Arianna W Rosenbluth. 1955. <span>“Monte Carlo Calculation of the Average Extension of Molecular Chains.”</span> <em>The Journal of Chemical Physics</em> 23 (2). American Institute of Physics: 356–59.
</div>
</div></section></div> ]]></description>
  <category>monte-carlo</category>
  <guid>https://alexxthiery.github.io/notes/SAW/SAW.html</guid>
  <pubDate>Fri, 04 Apr 2025 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Jarzynski and Crooks</title>
  <link>https://alexxthiery.github.io/notes/jarzynski/jarzynski.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/jarzynski/jarzynski_crooks.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption"><a href="https://en.wikipedia.org/wiki/Christopher_Jarzynski">Christopher Jarzynski</a> and <a href="https://en.wikipedia.org/wiki/Gavin_E._Crooks">Gavin Crooks</a></figcaption>
</figure>
</div>
</div>
<p>Consider a sequence of densities on <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5ED"> indexed by time parameter <img src="https://latex.codecogs.com/png.latex?t%20%5Cin%20%5B0,T%5D">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi_t(x)%20%5C;%20=%20%5C;%20%5Cfrac%7B%20e%5E%7B-U_t(x)%7D%7D%7BZ_t%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?U_t:%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D"> is time-dependent potential function and <img src="https://latex.codecogs.com/png.latex?Z_t"> is the normalizing constant. We are in fact really interested in studying the final density <img src="https://latex.codecogs.com/png.latex?%5Cpi_T"> and the bridging sequence of densities <img src="https://latex.codecogs.com/png.latex?%5Cpi_t"> is just a tool to get there, starting from an initial and tractable density <img src="https://latex.codecogs.com/png.latex?%5Cpi_0">. If one initializes a particle <img src="https://latex.codecogs.com/png.latex?X_0%20%5Csim%20%5Cpi_0"> and evolves it according to the <a href="https://en.wikipedia.org/wiki/Langevin_equation">Langevin</a> dynamics</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX_t%20%5C;%20=%20%5C;%20-%5Cnabla%20U_t(X_t)%20%5C,%20dt%20+%20%5Csqrt%7B2%7D%20%5C,%20dW_t%0A"></p>
<p>one can hope that the distribution of <img src="https://latex.codecogs.com/png.latex?X_T"> will be close to <img src="https://latex.codecogs.com/png.latex?%5Cpi_T">. This would be the case if one evolved the particle according <img src="https://latex.codecogs.com/png.latex?dX_t%20%5C;%20=%20%5C;%20-%5Cgamma%20%5Cnabla%20U_t(X_t)%20%5C,%20dt%20+%20%5Csqrt%7B2%20%5Cgamma%7D%20%5C,%20dW_t"> and let <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20%5Cto%20%5Cinfty"> since in that case <img src="https://latex.codecogs.com/png.latex?X_t"> would be distributed according to <img src="https://latex.codecogs.com/png.latex?%5Cpi_t"> for all <img src="https://latex.codecogs.com/png.latex?t">. Can one correct the distribution of <img src="https://latex.codecogs.com/png.latex?X_T"> with importance sampling weights?</p>
<p>I like the approach presented in <span class="citation" data-cites="vargas2023transport">(Vargas et al. 2024)</span> and these notes are my attempt to understand it. One very fruitful idea that has been used in a number of works in the Monte-Carlo literature is to look at a probability distribution of interest as the marginal of a joint distribution and to carry out computations and build numerical methods on the joint distribution <span class="citation" data-cites="del2006sequential">(Del Moral, Doucet, and Jasra 2006)</span>. Indeed, there is a lot of flexibility in the choice of the joint distribution.</p>
<p>Here, we can also consider the diffusion process <img src="https://latex.codecogs.com/png.latex?Y_T"> that runs backward in times and that is initialized according to <img src="https://latex.codecogs.com/png.latex?%5Cpi_T"> and follows the same Langevin dynamics. Again, one expects the distribution of <img src="https://latex.codecogs.com/png.latex?Y_t"> to be close to <img src="https://latex.codecogs.com/png.latex?%5Cpi_t">. It is more intuitive to discuss discretized version of the process. For a time discretization <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20=%20T/N">, we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%0A%5Cbegin%7Baligned%7D%0Ax_%7Bt%20+%20%5Cdelta%7D%20&amp;=%20x_t%20-%20%5Cnabla%20U_t(x_t)%20%5C,%20%5Cdelta%20+%20%5Csqrt%7B2%20%5Cdelta%7D%20%5C,%20%5Cxi_t%5C%5C%0Ay_%7Bt%7D%20&amp;=%20y_%7Bt%20+%20%5Cdelta%7D%20-%20%5Cnabla%20U_%7Bt%20+%20%5Cdelta%7D(y_%7Bt%20+%20%5Cdelta%7D)%20%5C,%20%5Cdelta%20+%20%5Csqrt%7B2%20%5Cdelta%7D%20%5C,%20%5Cxi_t%0A%5Cend%7Baligned%7D%0A%5Cright.%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cxi_t%20%5Csim%20%5Cmathcal%7BN%7D(0,I)"> are i.i.d. standard Gaussian random variables. Let us continue with these discretized versions and denote by <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5E%7BX%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5E%7BY%7D"> the probability measures associated with the discretized processes. The crucial remark is that the marginal distribution of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5EY"> at time <img src="https://latex.codecogs.com/png.latex?T"> is our distribution of interest <img src="https://latex.codecogs.com/png.latex?%5Cpi_T">. For a discretized path <img src="https://latex.codecogs.com/png.latex?%5Cunderline%7Bz%7D%20=%20(z_0,%20z_%7B%5Cdelta%7D,%20%5Cldots,%20z_%7BT%7D)"> we have:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BP%7D%5EX(%5Cunderline%7Bz%7D)%20&amp;=%0A%5Cpi_0(z_0)%20%5C,%20%5Cexp%20%7B%5Cleft%5C%7B%20-%5Cfrac%7B1%7D%7B4%20%5Cdelta%7D%20%5Csum_%7Bk=0%7D%5E%7BN-1%7D%20%5C%7Cz_%7Bt_%7Bk+1%7D%7D%20-%20%5Bz_%7Bt_k%7D%20-%20%5Cnabla%20U_%7Bt_k%7D(z_%7Bt_k%7D)%5C,%5Cdelta%5D%5C%7C%5E2%20%5Cright%5C%7D%7D%20%5C%5C%0A%5Cmathbb%7BP%7D%5EY(%5Cunderline%7Bz%7D)%20&amp;=%0A%5Cpi_T(z_T)%20%5C,%20%5Cexp%20%7B%5Cleft%5C%7B%20-%5Cfrac%7B1%7D%7B4%20%5Cdelta%7D%20%5Csum_%7Bk=0%7D%5E%7BN-1%7D%20%5C%7Cz_%7Bt_%7Bk%7D%7D%20-%20%5Bz_%7Bt_%7Bk+1%7D%7D%20-%20%5Cnabla%20U_%7Bt_%7Bk+1%7D%7D(z_%7Bt_%7Bk+1%7D%7D)%5C,%5Cdelta%5D%5C%7C%5E2%20%5Cright%5C%7D%7D%20.%0A%5Cend%7Baligned%7D%0A"></p>
<p>One can compute the ratio <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5EY(z)%20/%20%5Cmathbb%7BP%7D%5EX(z)"> and examine its limit as <img src="https://latex.codecogs.com/png.latex?N%20%5Cto%20%5Cinfty">. Algebra gives:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20%5Cmathbb%7BP%7D%5EY%7D%7Bd%20%5Cmathbb%7BP%7D%5EX%7D(%5Cunderline%7Bz%7D)%20=%0A%5Cfrac%7B%5Cpi_T(z_T)%7D%7B%5Cpi_0(z_0)%7D%20%5C,%0A%5Cexp%20%7B%5Cleft%5C%7B%20%5Csum_%7Bk=0%7D%5E%7BN-1%7D%20%5Cleft%3C%20z_%7Bt_%7Bk+1%7D%7D%20-%20z_%7Bt_k%7D,%20%5Cfrac%7B%5Cnabla%20U_%7Bt_k%7D(z_%7Bt_k%7D)%20+%20%5Cnabla%20U_%7Bt_%7Bk+1%7D%7D(z_%7Bt_%7Bk+1%7D%7D)%7D%7B2%7D%20%20%5Cright%3E%20+%20%5Cmathcal%7BO%7D(%5Cdelta%5E2)%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>One could probably use some <a href="https://en.wikipedia.org/wiki/Stratonovich_integral">Stratonovich</a> calculus to study this, but I always forget these things, so let’s use Ito instead. Write</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cnabla%20U_%7Bt_k%7D(z_%7Bt_k%7D)%20+%20%5Cnabla%20U_%7Bt_%7Bk+1%7D%7D(z_%7Bt_%7Bk+1%7D%7D)%7D%7B2%7D%0A%5Capprox%0A%5Cnabla%20U_%7Bt_k%7D(z_%7Bt_k%7D)%20+%20%5Cfrac%7B1%7D%7B2%7D%20%5Cmathrm%7BHess%7D_%7BU_%7Bt_%7Bk+1%7D%7D%7D%20(z_%7Bt_%7Bk+1%7D%7D)%20(z_%7Bt_%7Bk+1%7D%7D%20-%20z_%7Bt_k%7D)%0A+%0A%5Cfrac12%20%5C,%20%5Cpartial_t%20U_%7Bt_k%7D(z_%7Bt_k%7D)%20%5C,%20%5Cdelta.%0A"></p>
<p>The term <img src="https://latex.codecogs.com/png.latex?%5Cpartial_t%20U_%7Bt_k%7D(z_%7Bt_k%7D)%20%5C,%20%5Cdelta"> is too small to matter in the limit <img src="https://latex.codecogs.com/png.latex?N%20%5Cto%20%5Cinfty"> and Ito formula <img src="https://latex.codecogs.com/png.latex?d%20U_t(z_t)%20=%20%5Cpartial_t%20U_t(z_t)%20%5C,%20dt%20+%20%5Cleft%3C%20%5Cnabla%20U_t(z_t),%20dz_t%20%5Cright%3E%20+%20%5Cfrac%7B1%7D%7B2%7D%20%5Cleft%3C%20dx,%20%5Cmathrm%7BHess%7D_%7BU_t%7D(z_t)%20%5C,%20dz_t%20%5Cright%3E"> shows that in the limit <img src="https://latex.codecogs.com/png.latex?N%20%5Cto%20%5Cinfty">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20%5Cmathbb%7BP%7D%5EY%7D%7Bd%20%5Cmathbb%7BP%7D%5EX%7D(%5Cunderline%7Bz%7D)%0A=%0A%5Cfrac%7B%5Cpi_T(z_T)%7D%7B%5Cpi_0(z_0)%7D%20%5C,%0A%5Cexp%20%7B%5Cleft%5C%7B%20%20U_T(z_T)%20-%20U_0(z_0)%20-%20%5Cint_0%5ET%20%5Cpartial_t%20U_t(z_t)%20%5C,%20dt%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>Since <img src="https://latex.codecogs.com/png.latex?%5Cpi_t(z_t)%20=%20%5Cexp(-U_t(z_t))%20/%20Z_t">, this gives the <a href="https://en.wikipedia.org/wiki/Crooks_fluctuation_theorem">Crooks relation</a>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20%5Cmathbb%7BP%7D%5EY%7D%7Bd%20%5Cmathbb%7BP%7D%5EX%7D(%5Cunderline%7Bz%7D)%0A=%0A%5Cfrac%7BZ_0%7D%7BZ_T%7D%20%5C,%0A%5Cexp%20%7B%5Cleft%5C%7B%20%20-%20%5Cint_0%5ET%20%5Cpartial_t%20U_t(z_t)%20%5C,%20dt%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>Integrating over trajectories of <img src="https://latex.codecogs.com/png.latex?X_t">, since <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7BX%7D%5B(d%20%5Cmathbb%7BP%7D%5EY%20/%20d%20%5Cmathbb%7BP%7D%5EX)(X)%5D%20=%201">, one obtains the <a href="https://en.wikipedia.org/wiki/Jarzynski_equality">Jarzynski equality</a> <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7BZ_T%7D%7BZ_0%7D%20%5C;%20=%20%5C;%20%5Cmathbb%7BE%7D_%7BX%7D%20%20%7B%5Cleft%5C%7B%20%20%5Cexp%20%7B%5Cleft%5C%7B%20%20-%20%5Cint_0%5ET%20%5Cpartial_t%20U_t(X_t)%20%5C,%20dt%20%5Cright%5C%7D%7D%20%20%5Cright%5C%7D%7D%0A"></p>
<p>which is indeed also central to sequential Monte-Carlo methods. As described in <span class="citation" data-cites="vargas2023transport">(Vargas et al. 2024)</span>, the same approach can be used to slightly generalize the Crooks relation. Indeed, suppose that one instead consider the dynamics:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX_t%20%5C;%20=%20%5C;%20-%5Cnabla%20U_t(X_t)%20%5C,%20dt%20%20%5Ctextcolor%7Bblue%7D%7B+%20b_t(X_t)%20%5C,%20dt%7D%20+%20%5Csqrt%7B2%7D%20%5C,%20dW_t%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?b:%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D%5ED"> is a control function. One can consider the backward dynamics <img src="https://latex.codecogs.com/png.latex?Y_t"> that is initialized according to <img src="https://latex.codecogs.com/png.latex?%5Cpi_T"> and follows the dynamics <img src="https://latex.codecogs.com/png.latex?dY_t%20=%20-%5Cnabla%20U_t(Y_t)%20%5C,%20dt%20%20%5Ctextcolor%7Bred%7D%7B-%7D%20b(Y_t)%20%5C,%20dt%20+%20%5Csqrt%7B2%7D%20%5C,%20dW_t"> backward in time, i.e.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%0A%5Cbegin%7Baligned%7D%0Ax_%7Bt%20+%20%5Cdelta%7D%20&amp;=%20x_t%20-%20%5Cnabla%20U_t(x_t)%20%5C,%20%5Cdelta%20+%20b_t(x_t)%20%5C,%20%5Cdelta%20+%20%5Csqrt%7B2%20%5Cdelta%7D%20%5C,%20%5Cxi_t%5C%5C%0Ay_%7Bt%7D%20&amp;=%20y_%7Bt%20+%20%5Cdelta%7D%20-%20%5Cnabla%20U_%7Bt%20+%20%5Cdelta%7D(y_%7Bt%20+%20%5Cdelta%7D)%20%5C,%20%5Cdelta%20%20%5Ctextcolor%7Bred%7D%7B-%7D%20b_%7Bt%20+%20%5Cdelta%7D(y_%7Bt%20+%20%5Cdelta%7D)%20%5C,%20%5Cdelta%20+%20%5Csqrt%7B2%20%5Cdelta%7D%20%5C,%20%5Cxi_t.%0A%5Cend%7Baligned%7D%0A%5Cright.%0A"></p>
<p>The minus sign for the backward dynamics <img src="https://latex.codecogs.com/png.latex?Y_t"> is natural since one knows that this exactly gives the <a href="../../notes/reverse_and_tweedie/reverse_and_tweedie.html">backward dynamics</a> of the forward process <img src="https://latex.codecogs.com/png.latex?X_t"> in the case when <img src="https://latex.codecogs.com/png.latex?X_t%20%5Csim%20%5Cpi_t(dx)"> for all time <img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20t%20%5Cleq%20T">. One can then follow the exact same steps, using that the quadratic variation is <img src="https://latex.codecogs.com/png.latex?%5Cleft%3C%20dz_t,%20dz_t%20%5Cright%3E%20=%202%20%5C,%20dt">, to obtain that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20%5Cmathbb%7BP%7D%5EY%7D%7Bd%20%5Cmathbb%7BP%7D%5EX%7D(%5Cunderline%7Bz%7D)%0A=%0A%5Cfrac%7BZ_0%7D%7BZ_T%7D%20%5C,%0A%5Cexp%20%7B%5Cleft%5C%7B%20%20%5Cint_0%5ET%20-%5Cpartial_t%20U_t(z_t)%20%20%5Ctextcolor%7Bblue%7D%7B+%20%5Cnabla%20%5Ccdot%20b_t(z_t)%20-%20%5Cleft%3C%20%5Cnabla%20U_t(z_t),%20b_t(z_t)%20%5Cright%3E%7D%20%5C,%20dt%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>This for example shows that, for <img src="https://latex.codecogs.com/png.latex?dX_t%20%5C;%20=%20%5C;%20-%5Cnabla%20U_t(X_t)%20%5C,%20dt%20%20%5Ctextcolor%7Bblue%7D%7B+%20b_t(X_t)%20%5C,%20dt%7D%20+%20%5Csqrt%7B2%7D%20%5C,%20dW_t"> initialized according to <img src="https://latex.codecogs.com/png.latex?%5Cpi_0">, we have:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7BZ_T%7D%7BZ_0%7D%20%5C;%20=%20%5C;%20%5Cmathbb%7BE%7D_%7BX%7D%20%20%7B%5Cleft%5C%7B%20%20%5Cexp%20%7B%5Cleft%5C%7B%20%20%5Cint_0%5ET%20-%5Cpartial_t%20U_t(X_t)%20%20%5Ctextcolor%7Bblue%7D%7B+%20%5Cnabla%20%5Ccdot%20b_t(X_t)%20-%20%5Cleft%3C%20%5Cnabla%20U_t(X_t),%20b_t(X_t)%20%5Cright%3E%7D%20%5C,%20dt%20%5Cright%5C%7D%7D%20%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>This generalization of the Crooks relation is also explored in <span class="citation" data-cites="albergo2024nets">(Albergo and Vanden-Eijnden 2024)</span> where an alternative derivation by directly exploiting the <a href="https://en.wikipedia.org/wiki/Fokker–Planck_equation">Fokker-Planck</a> equation. Crucially, <span class="citation" data-cites="albergo2024nets">(Albergo and Vanden-Eijnden 2024)</span> note that, if the control function <img src="https://latex.codecogs.com/png.latex?b_t:%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D%5ED"> is chosen so that</p>
<p><span id="eq-nets"><img src="https://latex.codecogs.com/png.latex?%0A-%5Cpartial_t%20U_t(x)%20+%20%5Cnabla%20%5Ccdot%20b_t(x)%20-%20%5Cleft%3C%20%5Cnabla%20U_t(x),%20b_t(x)%20%5Cright%3E%0A=%0A%5Cfrac%7Bd%7D%7Bdt%7D%20%5C,%20%5Clog%20Z_t%0A%5Ctag%7B1%7D"></span></p>
<p>then the term <img src="https://latex.codecogs.com/png.latex?%5Cint_0%5ET%20-%5Cpartial_t%20U_t(X_t)%20+%20%5Cnabla%20%5Ccdot%20b_t(X_t)%20-%20%5Cleft%3C%20%5Cnabla%20U_t(X_t),%20b_t(X_t)%20%5Cright%3E%20%5C,%20dt"> is indeed constant, which gives a zero-variance estimator of the free energy difference <img src="https://latex.codecogs.com/png.latex?%5Clog(Z_T/Z_0)">. Indeed, it is a formidable challenge to solve the high-dimensional PDE Equation&nbsp;1 and <span class="citation" data-cites="albergo2024nets">(Albergo and Vanden-Eijnden 2024)</span> propose interesting <a href="https://en.wikipedia.org/wiki/Physics-informed_neural_networks">PINNs</a>-based methods to do so.</p>
<section id="some-references" class="level3">
<h3 class="anchored" data-anchor-id="some-references">Some References:</h3>
<ul>
<li>The original papers by Jarzynski <span class="citation" data-cites="jarzynski1997nonequilibrium">(Jarzynski 1997)</span> and Crooks <span class="citation" data-cites="crooks1999entropy">(Crooks 1999)</span>.</li>
<li>The book <span class="citation" data-cites="stoltz2010free">(Stoltz, Rousset, et al. 2010)</span> is excellent!</li>
<li>The two papers that prompted these notes: <span class="citation" data-cites="vargas2023transport">(Vargas et al. 2024)</span> and <span class="citation" data-cites="albergo2024nets">(Albergo and Vanden-Eijnden 2024)</span>.</li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-albergo2024nets" class="csl-entry">
Albergo, Michael S, and Eric Vanden-Eijnden. 2024. <span>“Nets: A Non-Equilibrium Transport Sampler.”</span> <em>arXiv Preprint arXiv:2410.02711</em>.
</div>
<div id="ref-crooks1999entropy" class="csl-entry">
Crooks, Gavin E. 1999. <span>“Entropy Production Fluctuation Theorem and the Nonequilibrium Work Relation for Free Energy Differences.”</span> <em>Physical Review E</em> 60 (3). APS: 2721.
</div>
<div id="ref-del2006sequential" class="csl-entry">
Del Moral, Pierre, Arnaud Doucet, and Ajay Jasra. 2006. <span>“Sequential Monte Carlo Samplers.”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 68 (3). Oxford University Press: 411–36.
</div>
<div id="ref-jarzynski1997nonequilibrium" class="csl-entry">
Jarzynski, Christopher. 1997. <span>“Nonequilibrium Equality for Free Energy Differences.”</span> <em>Physical Review Letters</em> 78 (14). APS: 2690.
</div>
<div id="ref-stoltz2010free" class="csl-entry">
Stoltz, Gabriel, Mathias Rousset, et al. 2010. <em>Free Energy Computations: A Mathematical Perspective</em>. World Scientific.
</div>
<div id="ref-vargas2023transport" class="csl-entry">
Vargas, Francisco, Shreyas Padhy, Denis Blessing, and Nikolas Nüsken. 2024. <span>“Transport Meets Variational Inference: Controlled Monte Carlo Diffusions.”</span> <em>ICLR 2024</em>.
</div>
</div></section></div> ]]></description>
  <category>SDE</category>
  <category>markov</category>
  <guid>https://alexxthiery.github.io/notes/jarzynski/jarzynski.html</guid>
  <pubDate>Fri, 21 Feb 2025 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Basic Conformal Inference</title>
  <link>https://alexxthiery.github.io/notes/conformal_inference/conformal.html</link>
  <description><![CDATA[ 




<p>Unfortunately, I am totally ignorant about <a href="https://en.wikipedia.org/wiki/Conformal_prediction">conformal inference</a>. However, in today’s seminar, I attended a very interesting talk on the topic, and I think it’s time I try implementing the most basic version of it. It seems like a useful concept, and I might even explain it next semester in my simulation class. What I’ll describe below is the simplest version of conformal inference. There appear to be many extensions and variations of it, most of which I don’t yet understand. For now, I just want to spend a few minutes implementing it myself to ensure I grasp the basic idea.</p>
<p>Consider the (simulated) 1D dataset <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D%20=%20%5C%7Bx_i,%20y_i%5C%7D_%7Bi=1%7D%5EN"> below; our goal is to build prediction confidence intervals <img src="https://latex.codecogs.com/png.latex?%5BL(x),%20U(x)%5D"> for the target variable <img src="https://latex.codecogs.com/png.latex?y"> given a new input <img src="https://latex.codecogs.com/png.latex?x">. Crucially, we would like these predictions to be well-calibrated in the sense that <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5BL(x),%20U(x)%5D"> with probability <img src="https://latex.codecogs.com/png.latex?90%5C%25">, say.</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/conformal_inference/data.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">1D regression dataset</figcaption>
</figure>
</div>
</div>
<p>I am lazy so I will be using a simple KNN regressor to predict the target variable <img src="https://latex.codecogs.com/png.latex?y"> given a new input <img src="https://latex.codecogs.com/png.latex?x">. For this purpose, split the dataset <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D"> into two parts <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7B%5Ctext%7Btrain%7D%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7B%5Ctext%7BCal%7D%7D">. The regressor is fitted on <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7B%5Ctext%7Btrain%7D%7D">. To calibrate the prediction intervals, compute the residuals <img src="https://latex.codecogs.com/png.latex?r_i%20=%20%7Cy_i%20-%20%5Chat%7By%7D_i%7C"> on the calibration set <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7B%5Ctext%7BCal%7D%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i%20=%20%5Chat%7By%7D(x_i)"> is the prediction of the regressor on <img src="https://latex.codecogs.com/png.latex?x_i">. One can then compute the <img src="https://latex.codecogs.com/png.latex?90%5C%25"> quantile <img src="https://latex.codecogs.com/png.latex?%5Cgamma_%7B90%5C%25%7D"> of the residuals: with probability <img src="https://latex.codecogs.com/png.latex?90%5C%25"> we have that <img src="https://latex.codecogs.com/png.latex?y_i%20%5Cin%20%5B%5Chat%7By%7D_i%20-%20%5Cgamma_%7B90%5C%25%7D,%20%5Chat%7By%7D_i%20+%20%5Cgamma_%7B90%5C%25%7D%5D"> on the calibration set, and this can be used to build the prediction intervals, as displayed below:</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/conformal_inference/conformal_basic.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">1D regression dataset: basic conformal inference</figcaption>
</figure>
</div>
</div>
<p>Not terribly impressive, but at least it is entirely straightforward to implement and it has the correct (marginal) coverage: for a new pair <img src="https://latex.codecogs.com/png.latex?(X,Y)"> coming from the same distribution as the training data, the probability that <img src="https://latex.codecogs.com/png.latex?Y"> falls within the prediction interval is indeed <img src="https://latex.codecogs.com/png.latex?90%5C%25">, up to a bit of nitpicking. Note that it is much much less impressive than saying that</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%20%7B%5Cleft(%20Y%20%5Cin%20%5B%5Chat%7By%7D(x)%20-%20%5Cgamma_%7B90%5C%25%7D,%20%5Chat%7By%7D(x)%20+%20%5Cgamma_%7B90%5C%25%7D%5D%20%5C;%20%7C%20%5C;%20X=x%20%5Cright)%7D%20%20=%2090%5C%25,"></p>
<p>which is clearly not true as can be seen from the figure above, but it is a good start. As a matter of fact, I’ve learned today from the very nice talk that without other assumptions, it is impossible to design a procedure that would guarantee the above so-called conditional coverage <span class="citation" data-cites="lei2014distribution">(Lei and Wasserman 2014)</span>. But let’s face it, the figure above is terribly unimpressive. Nevertheless, one can indeed make it slightly less useless by calibrating using a different strategy. For example, I can use the training set to estimate the Mean Absolute Deviation (MAD) of the residuals <img src="https://latex.codecogs.com/png.latex?%5Csigma(x)%20=%20%5Cmathbb%7BE%7D%5B%20%7CY%20-%20%5Chat%7By%7D(x)%20%7C%20%5C;%20%7C%20%5C;%20X=x%5D"> (again with a naive KNN regressor) and use the calibration set to estimate the <img src="https://latex.codecogs.com/png.latex?90%5C%25"> quantile <img src="https://latex.codecogs.com/png.latex?%5Cgamma_%7B90%5C%25%7D"> of the quantities <img src="https://latex.codecogs.com/png.latex?%7Cy_i%20-%20%5Chat%7By%7D_i%7C%20/%20%5Csigma(x_i)">. This allows one to produce calibrated prediction intervals of the type <img src="https://latex.codecogs.com/png.latex?%5B%5Chat%7By%7D_i%20-%20%5Cgamma_%7B90%5C%25%7D%20%5Csigma(x_i),%20%5Chat%7By%7D_i%20+%20%5Cgamma_%7B90%5C%25%7D%20%5Csigma(x_i)%5D">, which are displayed below:</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/conformal_inference/conformal.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">1D regression dataset: less useless conformal inference</figcaption>
</figure>
</div>
</div>
<p>It is slightly more useful, and it is again surprisingly straightforward to implement, literally 5 lines of code. I think I will have to read more about this in the future and I am pretty sure I will introduce the idea to the next batch of students!</p>
<section id="readings" class="level4">
<h4 class="anchored" data-anchor-id="readings">Readings:</h4>
<ul>
<li>The introduction paper <span class="citation" data-cites="lei2018distribution">(Lei et al. 2018)</span> is really good</li>
<li>I am really curious about <span class="citation" data-cites="gibbs2023conformal">(Gibbs, Cherian, and Candès 2023)</span> and it’s next on my reading list</li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-gibbs2023conformal" class="csl-entry">
Gibbs, Isaac, John J Cherian, and Emmanuel J Candès. 2023. <span>“Conformal Prediction with Conditional Guarantees.”</span> <em>arXiv Preprint arXiv:2305.12616</em>.
</div>
<div id="ref-lei2018distribution" class="csl-entry">
Lei, Jing, Max G’Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. 2018. <span>“Distribution-Free Predictive Inference for Regression.”</span> <em>Journal of the American Statistical Association</em> 113 (523). Taylor &amp; Francis: 1094–1111.
</div>
<div id="ref-lei2014distribution" class="csl-entry">
Lei, Jing, and Larry Wasserman. 2014. <span>“Distribution-Free Prediction Bands for Non-Parametric Regression.”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 76 (1). Oxford University Press: 71–96.
</div>
</div></section></div> ]]></description>
  <category>conformal</category>
  <guid>https://alexxthiery.github.io/notes/conformal_inference/conformal.html</guid>
  <pubDate>Sat, 07 Dec 2024 16:00:00 GMT</pubDate>
</item>
<item>
  <title>VIASM mini-course on diffusions and flows</title>
  <link>https://alexxthiery.github.io/notes/VIASM_2024/VIASM_2024.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/VIASM_2024/viasm_2024.jpg" class="img-fluid figure-img" style="width:95.0%"></p>
</figure>
</div>
</div>
<p>The slides for this short course on diffusion models (denoising diffusions, probability flows) and other flow methods (stochastic interpolants, flow-matching) are available <a href="https://alexxthiery.github.io/viasm_2024/">here</a>. There are a few animations, so loading the slides may be slow…</p>



 ]]></description>
  <category>diffusion</category>
  <guid>https://alexxthiery.github.io/notes/VIASM_2024/VIASM_2024.html</guid>
  <pubDate>Mon, 29 Jul 2024 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Doob, Girsanov and Bellman</title>
  <link>https://alexxthiery.github.io/notes/HJB/HJB.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/HJB/bellman.jpg" class="img-fluid figure-img" style="width:35.0%"></p>
<figcaption class="figure-caption"><a href="https://en.wikipedia.org/wiki/Richard_E._Bellman">Richard Bellman</a> (1920 – 1984)</figcaption>
</figure>
</div>
</div>
<p>Consider a diffusion in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5ED"> with deterministic starting position <img src="https://latex.codecogs.com/png.latex?x_0%20%5Cin%20%5Cmathbb%7BR%7D%5ED"> and dynamics</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX_t%20=%20b(X_t)dt%20+%20%5Csigma(X_t)%20%5C,%20dW_t%0A"></p>
<p>for a drift and volatility functions <img src="https://latex.codecogs.com/png.latex?b:%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D%5ED"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma:%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D%5E%7BD%20%5Ctimes%20D%7D">. On the time interval <img src="https://latex.codecogs.com/png.latex?%5B0,T%5D">, this defines a probability <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D"> on the path-space <img src="https://latex.codecogs.com/png.latex?C(%5B0,T%5D;%5Cmathbb%7BR%7D%5ED)">. For two functions <img src="https://latex.codecogs.com/png.latex?f:%20%5B0,T%5D%20%5Ctimes%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D"> and <img src="https://latex.codecogs.com/png.latex?g:%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D">, consider the probability distribution <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BQ%7D"> defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20%5Cmathbb%7BQ%7D%7D%7Bd%20%5Cmathbb%7BP%7D%7D%20=%20%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D%7D%20%5Cexp%20%20%7B%5Cleft%5C%7B%20%20%5Cint_0%5ET%20f(X_s)%20%5C,%20ds%20+%20g(X_T)%20%20%5Cright%5C%7D%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BZ%7D"> denotes the normalizing constant <span id="eq-normalizing-constant"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BZ%7D%5C;%20=%20%5C;%20%5Cmathbb%7BE%7D%20%7B%5Cleft%5B%20%20%5Cexp%20%20%7B%5Cleft%5C%7B%20%20%5Cint_0%5ET%20f(X_s)%20%5C,%20ds%20+%20g(X_T)%20%20%5Cright%5C%7D%7D%20%20%20%5Cright%5D%7D%20.%0A%5Ctag%7B1%7D"></span></p>
<p>The distribution <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BQ%7D"> places more probability mass on trajectories such that <img src="https://latex.codecogs.com/png.latex?%5Cint_0%5ET%20f(X_s)%20%5C,%20ds%20+%20g(X_T)"> is large. As described in these notes on <a href="../../notes/doob_transforms/doob.html">Doob h-transforms</a>, the tilted probability distribution <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BQ%7D"> can be described by a diffusion process <img src="https://latex.codecogs.com/png.latex?X%5E%5Cstar"> with dynamics</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX%5E%5Cstar%20=%20b(X%5E%5Cstar)dt%20+%20%5Csigma(X%5E%5Cstar)%20%5C,%20%20%7B%5Cleft%5C%7B%20%20dW_t%20+%20%20%5Ctextcolor%7Bblue%7D%7Bu%5E%5Cstar(t,%20X%5E%5Cstar)%7D%20%5C,%20dt%20%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>The control function <img src="https://latex.codecogs.com/png.latex?%20%5Ctextcolor%7Bblue%7D%7Bu%5E%5Cstar:%20%5B0,T%5D%20%5Ctimes%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D%5ED%7D"> is of the gradient form</p>
<p><span id="eq-u-star"><img src="https://latex.codecogs.com/png.latex?%0A%5Ctextcolor%7Bblue%7D%7Bu%5E%5Cstar(t,%20x)%7D%20%5C;%20=%20%5C;%20%5Csigma%5E%5Ctop(x)%20%5C,%20%5Cnabla%20%5Clog%5B%20%20%5Ctextcolor%7Bgreen%7D%7Bh(t,x)%7D%20%5D%0A%5Ctag%7B2%7D"></span></p>
<p>and the function <img src="https://latex.codecogs.com/png.latex?%20%5Ctextcolor%7Bgreen%7D%7Bh(t,x)%7D"> is described by the conditional expectation,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctextcolor%7Bgreen%7D%7Bh(t,x)%20=%20%5Cmathbb%7BE%7D%20%7B%5Cleft%5B%20%20%5Cexp%20%20%7B%5Cleft%5C%7B%20%20%5Cint_t%5ET%20f(X_s)%20%5C,%20ds%20+%20g(X_T)%20%20%5Cright%5C%7D%7D%20%20%20%5Cmid%20X_t%20=%20x%20%20%5Cright%5D%7D%20%7D.%0A"></p>
<p>The expression <img src="https://latex.codecogs.com/png.latex?%20%5Ctextcolor%7Bblue%7D%7Bu%5E%5Cstar(t,%20x)%7D%20%5C;%20=%20%5C;%20%5Csigma%5E%5Ctop(x)%20%5C,%20%5Cnabla%20%5Clog%5B%20%20%5Ctextcolor%7Bgreen%7D%7Bh(t,x)%7D%20%5D"> is quite intuitive; in order to describe the tilted measure <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BQ%7D"> that places more probability mass on trajectories such that <img src="https://latex.codecogs.com/png.latex?%5Cint_0%5ET%20f(X_s)%20%5C,%20ds%20+%20g(X_T)"> is large, the optimal control <img src="https://latex.codecogs.com/png.latex?u%5E%5Cstar(t,x)"> should be in the direction of states such that the “reward-to-go” quantity <img src="https://latex.codecogs.com/png.latex?%5Cint_t%5ET%20f(X_s)%20%5C,%20ds%20+%20g(X_T)"> is large.</p>
<p>To obtain a variational description of the optimal control function <img src="https://latex.codecogs.com/png.latex?%20%5Ctextcolor%7Bblue%7D%7Bu%5E%5Cstar%7D">, it suffices to express it as the solution of an optimization problem. It turns out that KL-divergences between diffusion processes are the right tool for this: we will write <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BQ%7D"> as the minimizer of <img src="https://latex.codecogs.com/png.latex?D_%7B%5Ctext%7BKL%7D%7D(%5Cmathbb%7BP%7D%5Eu%20%5C%7C%20%5Cmathbb%7BQ%7D)"> for a class of tractable probability distributions <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5Eu"> described by controlled diffusions. As described in these notes on the <a href="../../notes/girsanov/girsanov.html">Girsanov Theorem</a>, for any control function <img src="https://latex.codecogs.com/png.latex?u(t,x)">, the controlled diffusion <img src="https://latex.codecogs.com/png.latex?X%5Eu"> with dynamics</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX%5Eu%20=%20b(X%5Eu)dt%20+%20%5Csigma(X%5Eu)%20%5C,%20%20%7B%5Cleft%5C%7B%20%20dW_t%20+%20%20%5Ctextcolor%7Bblue%7D%7Bu(t,%20X%5Eu)%7D%20%5C,%20dt%20%20%5Cright%5C%7D%7D%0A"></p>
<p>and started at <img src="https://latex.codecogs.com/png.latex?x_0"> induces a probability distribution <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5Eu"> on path-space given by</p>
<p><span id="eq-girsanov"><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%5Cmathbb%7BP%7D%7D%7Bd%5Cmathbb%7BP%7D%5Eu%7D(x)%0A%5C;%20=%20%5C;%0A%5Cexp%20%7B%5Cleft%5C%7B%20-%5Cfrac%2012%20%5Cint_%7B0%7D%5E%7BT%7D%20%5C%7Cu(s,X%5Eu_S)%5C%7C%5E2%20%5C,%20ds%20-%20%5Cint_%7B0%7D%5E%7BT%7D%20u(s,X%5Eu_s)%5E%5Ctop%20%5C,%20dW_s%20%5Cright%5C%7D%7D%20.%0A%5Ctag%7B3%7D"></span></p>
<p>This allows one to write down explicitly the expression for the negative KL divergence</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A-D_%7B%5Ctext%7BKL%7D%7D(%5Cmathbb%7BP%7D%5Eu%20%5C%7C%20%5Cmathbb%7BQ%7D)%20=%0A%5Cmathbb%7BE%7D_u%20%7B%5Cleft%5B%20%20%20%5Clog%20%7B%5Cleft%5C%7B%20%5Cfrac%7Bd%5Cmathbb%7BQ%7D%7D%7Bd%5Cmathbb%7BP%7D%5Eu%7D(X%5Eu)%20%5Cright%5C%7D%7D%20%20%5Cright%5D%7D%0A"></p>
<p>between <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5Eu"> and the tilted distribution <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BQ%7D">. The notation <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_u"> denotes the expectation with respect to the controlled diffusion <img src="https://latex.codecogs.com/png.latex?X%5Eu">. The negative KL is, up to a constant, the usual Evidence Lower Bound (ELBO) used in variational inference. Since the quantity <img src="https://latex.codecogs.com/png.latex?%5Clog%20%7B%5Cleft%5C%7B%20%5Cfrac%7Bd%5Cmathbb%7BQ%7D%7D%7Bd%5Cmathbb%7BP%7D%5Eu%7D(X%5Eu)%20%5Cright%5C%7D%7D%20"> can be expressed as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Clog%20%7B%5Cleft%5C%7B%20%5Cfrac%7Bd%5Cmathbb%7BP%7D%7D%7Bd%5Cmathbb%7BP%7D%5Eu%7D(X%5Eu)%20%5Cright%5C%7D%7D%20%20-%20%5Clog(%5Cmathcal%7BZ%7D)%0A+%20%5Cint_0%5ET%20f(X%5Eu_s)%20%5C,%20ds%20+%20g(X%5Eu_T)%0A"></p>
<p>it follows from Equation&nbsp;3 that <img src="https://latex.codecogs.com/png.latex?-D_%7B%5Ctext%7BKL%7D%7D(%5Cmathbb%7BP%7D%5Eu%20%5C%7C%20%5Cmathbb%7BQ%7D)"> equals</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A-%5Clog(%5Cmathcal%7BZ%7D)%20+%0A%5Cmathbb%7BE%7D%20%7B%5Cleft%5B%20%20%5Cint_%7B0%7D%5E%7BT%7D%20%20%7B%5Cleft%5C%7B%20%20-%5Ctfrac%2012%20%5C%7Cu(s,X%5Eu_s)%5C%7C%5E2%20+%20f(X%5Eu_s)%20%20%5Cright%5C%7D%7D%20%20%5C,%20ds%20+%20g(X%5Eu_T)%20%5Cright%5D%7D%20.%0A"></p>
<p>Since the KL divergence is positive and the optimal control <img src="https://latex.codecogs.com/png.latex?u%5E%5Cstar"> in Equation&nbsp;2 drives the KL divergence to zero, we have that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmax_u%20%5C;%20%5Ctext%7BELBO%7D(u)%20=%20%5Clog%20%5Cmathcal%7BZ%7D%0A"></p>
<p>where the minimization is over all (reasonably well-behaved) control functions <img src="https://latex.codecogs.com/png.latex?u:%20%5B0,T%5D%20%5Ctimes%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D%5ED"> and</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BELBO%7D(u)%20%5C;%20=%20%5C;%20%5Cmathbb%7BE%7D%20%7B%5Cleft%5B%20%20%5Cint_%7B0%7D%5E%7BT%7D%20%20%7B%5Cleft%5C%7B%20%20-%5Ctfrac%2012%20%5C%7Cu(s,X%5Eu_s)%5C%7C%5E2%20+%20f(X%5Eu_s)%20%20%5Cright%5C%7D%7D%20%20%5C,%20ds%20+%20g(X%5Eu_T)%20%5Cright%5D%7D%20.%0A"></p>
<p>For maximizing the ELBO, the control needs to drive the trajectories to regions where <img src="https://latex.codecogs.com/png.latex?%5Cint_%7B0%7D%5E%7BT%7D%20f(X%5Eu_s)%20%5C,%20ds%20+%20g(X%5Eu_T)"> is large while at the same time keep the control effort <img src="https://latex.codecogs.com/png.latex?%5Cint_%7B0%7D%5E%7BT%7D%20%5C%7Cu(s,X%5Eu_s)%5C%7C%5E2%20%5C,%20ds"> small. The optimal control <img src="https://latex.codecogs.com/png.latex?u%5E%5Cstar"> is given by Equation&nbsp;2 and Equation&nbsp;1 gives that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Clog%20%5Cmathcal%7BZ%7D%0A&amp;=%20%5Clog%20%5Cmathbb%7BE%7D%20%7B%5Cleft%5B%20%20%5Cexp%20%20%7B%5Cleft%5C%7B%20%20%5Cint_0%5ET%20f(X_s)%20%5C,%20ds%20+%20g(X_T)%20%20%5Cright%5C%7D%7D%20%20%20%5Cright%5D%7D%20%5C%5C%0A&amp;=%20%5Clog%5B%20%20%5Ctextcolor%7Bgreen%7D%7B%20h(0,x_0)%20%7D%20%5D.%0A%5Cend%7Balign%7D%0A"></p>
<p>Since there was nothing really special about the starting point <img src="https://latex.codecogs.com/png.latex?x_0"> and the time horizon <img src="https://latex.codecogs.com/png.latex?T%3E0">, the above derivation gives the solution to the following stochastic optimal control problem. It is written as a maximization problem although a large part of the control and physics literature writes it as an equivalent minimization problem. Consider the reward-to-go function (a.k.a. value function) defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AV(t,x)%20=%20%5Csup_u%20%5C;%20%5Cmathbb%7BE%7D%20%7B%5Cleft%5B%20%20%5Cint_%7Bt%7D%5E%7BT%7D%20%20%7B%5Cleft%5C%7B%20%20-%5Ctfrac%2012%20%5C%7Cu(s,X%5Eu_s)%5C%7C%5E2%20+%20f(X%5Eu_s)%20%20%5Cright%5C%7D%7D%20%20%5C,%20ds%20+%20g(X%5Eu_T)%20%5Cmid%20X_t%20=%20x%20%5Cright%5D%7D%20.%0A"></p>
<p>We have that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AV(t,x)%0A&amp;=%20%5Clog%20%5Cmathbb%7BE%7D%20%7B%5Cleft%5B%20%20%5Cexp%20%20%7B%5Cleft%5C%7B%20%20%5Cint_t%5ET%20f(X_s)%20%5C,%20ds%20+%20g(X_T)%20%20%5Cright%5C%7D%7D%20%20%5Cmid%20X_t%20=%20x%20%5Cright%5D%7D%20%5C%5C%0A&amp;=%20%5Clog%5B%20%20%5Ctextcolor%7Bgreen%7D%7Bh(t,%20x)%7D%20%5D.%0A%5Cend%7Balign%7D%0A"></p>
<p>This shows that optimal control <img src="https://latex.codecogs.com/png.latex?u%5E%5Cstar"> can also be expressed as</p>
<p><span id="eq-u-star-V"><img src="https://latex.codecogs.com/png.latex?%0Au%5E%5Cstar(t,x)%20=%20%5Csigma%5E%5Ctop(x)%20%5Cnabla%20%5Clog%5B%20%20%5Ctextcolor%7Bgreen%7D%7B%20h(t,x)%20%7D%5D%0A=%20%5Csigma%5E%5Ctop(x)%20%5C,%20%5Cnabla%20V(t,x)%20.%0A%5Ctag%7B4%7D"></span></p>
<p>The expression <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E%5Ctop(x)%20%5C,%20%5Cnabla%20V(t,x)"> is intuitive: since we are trying to maximize the reward-to-go function, the optimal control should be in the direction of the gradient of the reward-to-go function.</p>
<p>Finally, let us mention that one can easily derive the <a href="https://en.wikipedia.org/wiki/Hamilton–Jacobi–Bellman_equation">Hamilton-Jacobi-Bellman</a> equation for the reward-to-go function <img src="https://latex.codecogs.com/png.latex?V(t,x)">. We have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AV(t,x)%20=%20%5Csup_u%20%5C;%20%5Cmathbb%7BE%7D%20%7B%5Cleft%5B%20%20%5Cint_%7Bt%7D%5ET%20C_s%20%5C,%20ds%20+%20g(X%5Eu_T)%20%5Cright%5D%7D%0A"></p>
<p>with <img src="https://latex.codecogs.com/png.latex?C_s%20=%20-%5Ctfrac12%20%5C%7Cu(s,X%5Eu_s)%5C%7C%5E2%20+%20f(X%5Eu_s)">. For <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Cll%201">, we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AV(t,x)%0A&amp;%5C;%20=%20%5C;%0A%5Csup_u%20%5C;%20%20%7B%5Cleft%5C%7B%20%20C_t%20%5C,%20%5Cdelta%20+%20%5Cmathbb%7BE%7D%20%7B%5Cleft%5B%20%20V(t+%5Cdelta,%20X%5Eu_%7Bt+%5Cdelta%7D)%20%5Cmid%20X%5Eu_t=x%20%5Cright%5D%7D%20%20%20%5Cright%5C%7D%7D%20%20+%20o(%5Cdelta)%5C%5C%0A&amp;%5C;%20=%20%5C;%0AV(t,x)%20+%20%5Cdelta%20%5C,%20%5Csup_%7Bu(t,x)%7D%20%5C;%20%20%7B%5Cleft%5C%7B%20%20C_t%20+%20(%5Cpartial_t%20+%20%5Cmathcal%7BL%7D+%20%5Csigma(x)%20%5C,%20u(t,x)%20%5C,%20%5Cnabla)%20%5C,%20V(t,x)%20%5Cright%5C%7D%7D%20%20+%20o(%5Cdelta)%0A%5Cend%7Balign%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D=%20b%20%5Cnabla%20+%20%5Csigma%20%5Csigma%5E%5Ctop%20:%20%5Cnabla%5E2"> is the generator of the uncontrolled diffusion. Since <img src="https://latex.codecogs.com/png.latex?C_t%20=%20-%5Ctfrac12%20%5C%7Cu(t,x)%5C%7C%5E2%20+%20f(x)"> is a simple quadratic function, the supremum over the control <img src="https://latex.codecogs.com/png.latex?u(t,x)"> can be computed in closed form,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0Au%5E%5Cstar(t,x)%0A&amp;=%20%5Cmathop%7B%5Cmathrm%7Bargmax%7D%7D_%7Bz%20%5Cin%20%5Cmathbb%7BR%7D%5ED%7D%20%5C;%20-%5Ctfrac12%20%5C%7Cz%5C%7C%5E2%20+%20%5Cleft%3C%20z,%20%5Csigma%5E%5Ctop(x)%20%5Cnabla%20V(t,x)%20%20%5Cright%3E%5C%5C%0A&amp;=%20%5Csigma%5E%5Ctop(x)%20%5C,%20%5Cnabla%20V(t,x),%0A%5Cend%7Balign%7D%0A"></p>
<p>as we already knew from Equation&nbsp;4. This implies that the reward-to-go function <img src="https://latex.codecogs.com/png.latex?V(t,x)"> satisfies the HJB equation</p>
<p><span id="eq-hjb"><img src="https://latex.codecogs.com/png.latex?%0A%7B%5Cleft(%20%5Cpartial_t%20+%20%5Cmathcal%7BL%7D%20%5Cright)%7D%20V%20+%20%5Cfrac12%20%5C%7C%20%5Csigma%5E%5Ctop%20%5Cnabla%20V%20%5C%7C%5E2%20+%20f%20=%200%0A%5Ctag%7B5%7D"></span></p>
<p>with terminal condition <img src="https://latex.codecogs.com/png.latex?V(T,x)%20=%20g(x)">. Another route to derive Equation&nbsp;5 is to simply use the fact that <img src="https://latex.codecogs.com/png.latex?V(t,x)%20=%20%5Clog%20h(t,x)">; since the <a href="https://en.wikipedia.org/wiki/Feynman–Kac_formula">Feynman-Kac</a> gives that the function <img src="https://latex.codecogs.com/png.latex?h(t,x)"> satisfies <img src="https://latex.codecogs.com/png.latex?(%5Cpartial_t%20+%20%5Cmathcal%7BL%7D+%20f)%20h%20=%200">, the conclusion follows from a few lines of algebra by starting writing <img src="https://latex.codecogs.com/png.latex?%5Cpartial_t%20V%20=%20h%5E%7B-1%7D%20%5C,%20%5Cpartial_t%20h%20=%20-h%5E%7B-1%7D(%5Cmathcal%7BL%7D+%20f)%5Bh%5D">, expanding <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7Dh"> and expressing everything back in terms of <img src="https://latex.codecogs.com/png.latex?V">. The term <img src="https://latex.codecogs.com/png.latex?%5C%7C%5Csigma%5E%5Ctop%20%5Cnabla%20V%5C%7C%5E2"> naturally arises when expressing the diffusion term <img src="https://latex.codecogs.com/png.latex?%5Csigma%20%5Csigma%5E%5Ctop%20:%20%5Cnabla%5E2%20h"> as a function of the second derivative of <img src="https://latex.codecogs.com/png.latex?V">; it is the idea of the standard <a href="https://en.wikipedia.org/wiki/Cole–Hopf_transformation">Cole-Hopf transformation</a>.</p>



 ]]></description>
  <category>SDE</category>
  <category>markov</category>
  <guid>https://alexxthiery.github.io/notes/HJB/HJB.html</guid>
  <pubDate>Mon, 10 Jun 2024 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Girsanov and Importance Sampling</title>
  <link>https://alexxthiery.github.io/notes/girsanov/girsanov.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/girsanov/girsanov_portrait.jpg" class="img-fluid figure-img" style="width:35.0%"></p>
<figcaption class="figure-caption"><a href="https://en.wikipedia.org/wiki/Igor_Girsanov">Igor Girsanov</a> (1934 – 1967)</figcaption>
</figure>
</div>
</div>
<p>Let <img src="https://latex.codecogs.com/png.latex?q(dx)%20%5Cequiv%20%5Cmathcal%7BN%7D(%5Cmu,%5CGamma)"> be the Gaussian distribution with mean <img src="https://latex.codecogs.com/png.latex?%5Cmu%20%5Cin%20%5Cmathbb%7BR%7D%5ED"> and covariances <img src="https://latex.codecogs.com/png.latex?%5CGamma%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD%20%5Ctimes%20D%7D">. For a direction <img src="https://latex.codecogs.com/png.latex?u%20%5Cin%20%5Cmathbb%7BR%7D%5ED">, consider the distribution <img src="https://latex.codecogs.com/png.latex?q%5E%7Bu%7D(dx)%20%5Cequiv%20%5Cmathcal%7BN%7D(%5Cmu%20+%20%5CGamma%5E%7B1/2%7D%20%5C,%20u,%20%5CGamma)">, i.e.&nbsp;the same Gaussian distribution but shifted by an amount <img src="https://latex.codecogs.com/png.latex?%5CGamma%5E%7B1/2%7D%20%5C,%20u">. Algebra directly gives that</p>
<p><span id="eq-girsanov"><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bq%5E%7Bu%7D(x)%7D%7Bq(x)%7D%0A=%0A%5Cexp%20%7B%5Cleft%5C%7B%20-%20%5Cfrac%7B1%7D%7B2%7D%20%5C%7C%20u%5C%7C%5E2%20+%20%5Cleft%3C%20u,%20%5C,%20%5CGamma%5E%7B-1/2%7D(x-%5Cmu)%20%5Cright%3E%20%5Cright%5C%7D%7D%20.%0A%5Ctag%7B1%7D"></span></p>
<p>We will see that, not very surprisingly, a similar change-of-probability result holds in continuous time. On the time interval <img src="https://latex.codecogs.com/png.latex?%5B0,T%5D">, let <img src="https://latex.codecogs.com/png.latex?W_t"> be a standard Brownian motion in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5ED"> and <img src="https://latex.codecogs.com/png.latex?X_t"> be the solution to the <a href="https://en.wikipedia.org/wiki/Stochastic_differential_equation">SDE</a></p>
<p><span id="eq-sde-original"><img src="https://latex.codecogs.com/png.latex?%0AdX_t%20%5C;%20=%20%5C;%20b(X_t)%20%5C,%20dt%20+%20%5Csigma(X_t)%20%5C,%20dW_t%0A%5Ctag%7B2%7D"></span></p>
<p>for some drift <img src="https://latex.codecogs.com/png.latex?b:%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D%5ED"> and diffusion <img src="https://latex.codecogs.com/png.latex?%5Csigma:%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D%5E%7BD%20%5Ctimes%20D%7D"> and initial distribution <img src="https://latex.codecogs.com/png.latex?%5Cmu_0(dx_0)">. This SDE defines a probability measure <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D"> on the <a href="https://en.wikipedia.org/wiki/Classical_Wiener_space">path-space</a> <img src="https://latex.codecogs.com/png.latex?C(%5B0,T%5D;%20%5Cmathbb%7BR%7D%5ED)">, the space of continuous functions from <img src="https://latex.codecogs.com/png.latex?%5B0,T%5D"> to <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5ED">. Consider a perturbation drift function <img src="https://latex.codecogs.com/png.latex?u:%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D%5ED"> and associated perturbed SDE given by</p>
<p><span id="eq-sde-perturbed"><img src="https://latex.codecogs.com/png.latex?%0AdX_t%5Eu%20%5C;%20=%20%5C;%20b(X_t%5Eu)%20%5C,%20dt%20+%20%5Csigma(X_t%5Eu)%20%5C,%20%20%7B%5Cleft%5C%7B%20%20dW_t%20+%20%20%5Ctextcolor%7Bblue%7D%7Bu(X_t%5Eu)%20%5C,%20dt%7D%20%20%5Cright%5C%7D%7D%20.%0A%5Ctag%7B3%7D"></span></p>
<p>This perturbed SDE, started from the same initial distribution <img src="https://latex.codecogs.com/png.latex?%5Cmu_0(dx_0)">, defines a probability measure <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5Eu"> on the path-space <img src="https://latex.codecogs.com/png.latex?C(%5B0,T%5D;%20%5Cmathbb%7BR%7D%5ED)"> and it is often useful to understand the Radon-Nikodym derivative of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5Eu"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D">. I have never really liked the way this is <a href="https://en.wikipedia.org/wiki/Girsanov_theorem">usually</a> derived, and also never really remember the result. It takes only a few lines of algebra to re-derive these results, at least informally. For this purpose, consider a simpler <a href="https://en.wikipedia.org/wiki/Euler–Maruyama_method">Euler discretization</a> of the SDE with time-discretization <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20=%20T/N"> for <img src="https://latex.codecogs.com/png.latex?N%20%5Cgg%201">. Consider a discretized paths <img src="https://latex.codecogs.com/png.latex?(x_0,%20x_%7B%5Cdelta%7D,%20%5Cldots,%20x_%7BT%7D)"> of Equation&nbsp;2 obtained by iterating the update</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax_%7Bt_%7Bk+1%7D%7D%20%5C;%20=%20%5C;%20x_%7Bt_k%7D%20+%20b(x_%7Bt_k%7D)%5C,%5Cdelta%20+%20%5Csigma(x_%7Bt_k%7D)%20%5C,%20(%5CDelta%20W_%7Bt_k%7D)%0A"></p>
<p>with <img src="https://latex.codecogs.com/png.latex?t_k%20=%20k%5Cdelta"> and <img src="https://latex.codecogs.com/png.latex?%5CDelta%20W_%7Bt_k%7D%20=%20W_%7Bt_%7Bk+1%7D%7D%20-%20W_%7Bt_k%7D">. The probability of observing such a path reads <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D%7D%20%5C,%20%5Cmu_0(x_0)%20%5C,%20%5Cexp%20%7B%5Cleft%5C%7B%20-%5Cfrac%7B1%7D%7B2%20%5Cdelta%7D%20%5Csum_%7Bk=0%7D%5E%7BN-1%7D%0A%5C%7Cx_%7Bt_%7Bk+1%7D%7D%20-%20%5Bx_%7Bt_k%7D%20+%20b(x_%7Bt_k%7D)%5C,%5Cdelta%5D%5C%7C%5E2_%7B%5CGamma%5E%7B-1%7D(x_%7Bt_k%7D)%20%7D%20%5Cright%5C%7D%7D%0A"></p>
<p>with <img src="https://latex.codecogs.com/png.latex?%5CGamma(x)%20%5Cequiv%20%5Csigma(x)%20%5Csigma%5E%5Ctop(x)"> the volatility matrix and an irrelevant multiplicative constant <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BZ%7D">. One obtains a similar expression for a discretized path of the perturbed SDE Equation&nbsp;3 and the ratio of these two quantities equals</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20%5Cwidetilde%7B%5Cmathbb%7BP%7D%7D%5E%7Bu%7D%7D%7Bd%20%5Cwidetilde%7B%5Cmathbb%7BP%7D%7D%7D(x)%20=%20%5Cexp%20%7B%5Cleft%5C%7B%20%5Csum_%7Bk=0%7D%5E%7BN-1%7D%20-%5Cfrac%7B%5Cdelta%7D%7B2%7D%20%5C%7Cu(x_%7Bt_k%7D)%5C%7C%5E2%20%20+%0A%5Cleft%3C%20x_%7Bt_%7Bk+1%7D%7D-x_%7Bt_k%7D-b(x_%7Bt_k%7D)%5Cdelta,%20%5Csigma(x_%7Bt_k%7D)%20%5C,%20u(x_%7Bt_k%7D)%20%5Cright%3E_%7B%5CGamma%5E%7B-1%7D(x_%7Bt_k%7D)%7D%20%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>where the tilde notation denotes the discretized version of the measures. Since</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax_%7Bt_%7Bk+1%7D%7D-x_%7Bt_k%7D-b(x_%7Bt_k%7D)%5Cdelta%20=%20%5Csigma(x_%7Bt_k%7D)%20%5C,%20%5CDelta%20W_%7Bt_k%7D,%0A"> for a path <img src="https://latex.codecogs.com/png.latex?dx_t%20%5C;%20=%20%5C;%20b(x_t)%20%5C,%20dt%20+%20%5Csigma(x_t)%20%5C,%20dW_t"> and taking the limit <img src="https://latex.codecogs.com/png.latex?N%20%5Cto%20%5Cinfty"> gives</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20%5Cmathbb%7BP%7D%5E%7Bu%7D%7D%7Bd%20%5Cmathbb%7BP%7D%7D(x)%20%5C;%20=%20%5C;%20%5Cexp%20%7B%5Cleft%5C%7B%0A-%5Cfrac%2012%20%5C,%20%5Cint_0%5ET%20%5C%7Cu(x_t)%5C%7C%5E2%20%5C,%20dt%20+%20%5Cint_%7B0%7D%5ET%20u%5E%5Ctop(x_t)%20%5C,%20dW_t%0A%5Cright%5C%7D%7D%20.%0A"></p>
<p>Similarly, for a path <img src="https://latex.codecogs.com/png.latex?dx%5E%7Bu%7D_t%20%5C;%20=%20%5C;%20b(x%5Eu_t)%20%5C,%20dt%20+%20%5Csigma(x%5Eu_t)%20%5C,%20%20%7B%5Cleft(%20%20dW_t%20+%20u(x%5Eu_t)%20%5Cright)%7D%20">, we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20%5Cmathbb%7BP%7D%7D%7Bd%20%5Cmathbb%7BP%7D%5Eu%7D(x%5Eu)%20%5C;%20=%20%5C;%20%5Cexp%20%7B%5Cleft%5C%7B%0A-%5Cfrac%2012%20%5C,%20%5Cint_0%5ET%20%5C%7Cu(x%5Eu_t)%5C%7C%5E2%20%5C,%20dt%20-%20%5Cint_%7B0%7D%5ET%20u%5E%5Ctop(x%5Eu_t)%20%5C,%20dW_t%0A%5Cright%5C%7D%7D%20.%0A"></p>
<p>These results remain identical for time-dependent drift and volatility functions, as is clear from this non-rigorous argument. The above two formulas for <img src="https://latex.codecogs.com/png.latex?d%5Cmathbb%7BP%7D%5Eu/d%5Cmathbb%7BP%7D(x)"> and <img src="https://latex.codecogs.com/png.latex?d%5Cmathbb%7BP%7D/d%5Cmathbb%7BP%7D%5Eu(x)"> may be slightly confusing since they are not immediately recognizable as inverse of each other. Furthermore, these probability ratios evaluated along a path <img src="https://latex.codecogs.com/png.latex?x"> or <img src="https://latex.codecogs.com/png.latex?x%5Eu"> are expressed in terms of the Brownian trajectory that defines them, which can be confusing. In short, this would be better to express <img src="https://latex.codecogs.com/png.latex?d%5Cmathbb%7BP%7D%5Eu/d%5Cmathbb%7BP%7D(x)"> directly in terms of the path <img src="https://latex.codecogs.com/png.latex?x">, and not in terms of the Brownian motion <img src="https://latex.codecogs.com/png.latex?W_t">, even though it is indeed equivalent. For these reasons, it is often convenient to use the following equivalent expressions:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%0A%5Cbegin%7Baligned%7D%0A%5Cfrac%7Bd%20%5Cmathbb%7BP%7D%5E%7Bu%7D%7D%7Bd%20%5Cmathbb%7BP%7D%7D(x)%20&amp;=%20%5Cexp%20%7B%5Cleft%5C%7B%0A%5Ctextcolor%7Bblue%7D%7B-%7D%5Cfrac%2012%20%5C,%20%5Cint_0%5ET%20%5C%7Cu(x_t)%5C%7C%5E2%20%5C,%20dt%20%20%5Ctextcolor%7Bblue%7D%7B+%7D%20%5Cint_%7B0%7D%5ET%20u%5E%5Ctop(x_t)%20%5C,%20%5Cfrac%7Bdx_t%20-%20b(x_t)%20dt%7D%7B%5Csigma(x_t)%7D%20%5Cright%5C%7D%7D%20%5C%5C%0A%5Cfrac%7Bd%20%5Cmathbb%7BP%7D%7D%7Bd%20%5Cmathbb%7BP%7D%5E%7B(u)%7D%7D(x)%20&amp;=%20%5Cexp%20%7B%5Cleft%5C%7B%0A%5Ctextcolor%7Bblue%7D%7B+%7D%5Cfrac%2012%20%5C,%20%5Cint_0%5ET%20%5C%7Cu(x_t)%5C%7C%5E2%20%5C,%20dt%20%20%5Ctextcolor%7Bblue%7D%7B-%7D%20%5Cint_%7B0%7D%5ET%20u%5E%5Ctop(x_t)%20%5C,%20%5Cfrac%7Bdx_t%20-%20b(x_t)%20dt%7D%7B%5Csigma(x_t)%7D%20%5Cright%5C%7D%7D%0A%5Cend%7Baligned%7D%0A%5Cright.%0A"></p>
<p>From these expression, the fact that <img src="https://latex.codecogs.com/png.latex?d%5Cmathbb%7BP%7D%5Eu/d%5Cmathbb%7BP%7D(x)"> and <img src="https://latex.codecogs.com/png.latex?d%5Cmathbb%7BP%7D/d%5Cmathbb%7BP%7D%5Eu(x)"> are indeed inverse of each other is clear. Another entirely equivalent formulation, slightly more symmetrical again, is as follows. Consider the two measures <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5E%7B(1)%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5E%7B(2)%7D"> associated to</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX_t%5E%7B(i)%7D%20%5C;%20=%20%5C;%20b%5E%7B(i)%7D(X_t)%20%5C,%20dt%20+%20%5Csigma(X_t)%20%5C,%20dW_t%0A"></p>
<p>for two drift functions <img src="https://latex.codecogs.com/png.latex?b%5E%7B(1)%7D:%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D%5ED"> and <img src="https://latex.codecogs.com/png.latex?b%5E%7B(2)%7D:%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D%5ED">. Then, the Radon-Nikodym derivative between these two measures is given by</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20%5Cmathbb%7BP%7D%5E%7B(2)%7D%7D%7Bd%20%5Cmathbb%7BP%7D%5E%7B(1)%7D%7D(x)%20=%20%5Cexp%20%7B%5Cleft%5C%7B%0A-%5Cfrac%7B1%7D%7B2%7D%5Cint_%7B0%7D%5ET%20%20%7B%5Cleft(%20%5Cfrac%7B%5C%7Cb%5E%7B(2)%7D_t%5C%7C%5E2%20-%20%5C%7Cb%5E%7B(1)%7D_t%5C%7C%5E2%7D%7B%5Csigma%5E2_t%7D%20%5Cright)%7D%20%20%5C,%20dt%0A+%0A%5Cint_%7B0%7D%5ET%20%5Cleft%3C%20%20%5Cfrac%7Bb%5E%7B(2)%7D_t%20-%20b%5E%7B(1)%7D_t%7D%7B%5Csigma_t%5E2%7D,%20dx_t%20%5Cright%3E%0A%5Cright%5C%7D%7D%0A"></p>
<p>with the shorthand <img src="https://latex.codecogs.com/png.latex?b%5E%7B(i)%7D_t%20=%20b%5E%7B(i)%7D(x_t)"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma_t%20=%20%5Csigma(x_t)"> and <img src="https://latex.codecogs.com/png.latex?%5C%7Cv%5C%7C%5E2/%5Csigma%5E2%20=%20%5Cleft%3C%20v,%20%5B%5Csigma%20%5Csigma%5E%5Ctop%5D%5E%7B-1%7D%20v%20%5Cright%3E"> and <img src="https://latex.codecogs.com/png.latex?%5Cleft%3C%20u,v%20%5Cright%3E%20/%20%5Csigma%5E2%20=%20%5Cleft%3C%20u,%20%5B%5Csigma%20%5Csigma%5E%5Ctop%5D%5E%7B-1%7D%20v%20%5Cright%3E">. Again, this follows immediately from a discretized version of the SDEs. As described below, these change of variables formulae are often useful when performing importance sampling on path-space. As a sanity check, one can see that in the case of a scalar Brownian motion <img src="https://latex.codecogs.com/png.latex?dX%20=%20%5Csigma%20%5C,%20dW"> and drifted version of it <img src="https://latex.codecogs.com/png.latex?dX%5Eu%20=%20%5Csigma%20%5C,%20dW%20+%20u%20%5C,%20dt">, we indeed have that <img src="https://latex.codecogs.com/png.latex?d%5Cmathbb%7BP%7D%5Eu/d%5Cmathbb%7BP%7D(x)"> has unit expectation under <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D"> since it is equivalent to the fact <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5B%5Cexp(%5Csigma%20%5C,%20%5Cxi)%5D%20=%20%5Cexp(%5Csigma%5E2/2)"> for a standard Gaussian random variable <img src="https://latex.codecogs.com/png.latex?%5Cxi">. Finally, note that the <a href="https://en.wikipedia.org/wiki/Kullback–Leibler_divergence">Kullback-Leibler divergence</a> between <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5Eu"> has a particularly simple form. Since <img src="https://latex.codecogs.com/png.latex?D_%7B%5Ctext%7BKL%7D%7D(%5Cmathbb%7BP%7D,%20%5Cmathbb%7BP%7D%5Eu)%20=%20%5Cmathbb%7BE%7D_%7B%5Cmathbb%7BP%7D%7D%20%7B%5Cleft%5B%20-%5Clog%20%7B%5Cleft%5C%7B%20%5Cfrac%7Bd%20%5Cmathbb%7BP%7D%5E%7Bu%7D%7D%7Bd%20%5Cmathbb%7BP%7D%7D(X)%20%5Cright%5C%7D%7D%20%20%5Cright%5D%7D%20"> one obtains</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AD_%7B%5Ctext%7BKL%7D%7D(%5Cmathbb%7BP%7D,%20%5Cmathbb%7BP%7D%5Eu)%20=%20%5Cfrac12%20%5Cmathbb%7BE%7D%20%7B%5Cleft%5B%20%20%5Cint_0%5ET%20%5C%7Cu(X_t)%5C%7C%5E2%20%5C,%20dt%20%20%5Cright%5D%7D%20.%0A"></p>
<section id="importance-sampling-on-path-space" class="level3">
<h3 class="anchored" data-anchor-id="importance-sampling-on-path-space">Importance Sampling on path-space</h3>
<p>Consider a functional <img src="https://latex.codecogs.com/png.latex?%5CPhi:%20C(%5B0,T%5D;%20%5Cmathbb%7BR%7D%5ED)%20%5Cto%20%5Cmathbb%7BR%7D"> on path-space; a typical example is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5CPhi(x)%20=%20%5Cexp%20%7B%5Cleft%5C%7B%20%5Cint_0%5ET%20f(X_t)%20%5C,%20dt%20%5C,%20+%20%5C,%20g(X_T)%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>Suppose that we would like to evaluate the expectation of <img src="https://latex.codecogs.com/png.latex?%5CPhi"> under the measure <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D">. Naive Monte-Carlo (MC) would require sampling <img src="https://latex.codecogs.com/png.latex?M"> trajectories from Equation&nbsp;2 and computing the average of <img src="https://latex.codecogs.com/png.latex?%5CPhi"> on these trajectories. To reduce the variance of this naive MC estimator, one can also use importance sampling by sampling <img src="https://latex.codecogs.com/png.latex?M"> trajectories <img src="https://latex.codecogs.com/png.latex?x%5E%7B1,u%7D,%20%5Cldots,%20x%5E%7BM,u%7D"> from the measure <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5Eu"> and compute the average</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7BM%7D%20%5C,%20%5Csum_%7Bi=1%7D%5EM%20%5CPhi(x%5E%7Bi,u%7D)%20%5C,%20W(x%5E%7Bi,u%7D)%0A"></p>
<p>with weights given by the Radon-Nikodym derivative</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AW(x%5E%7Bi,u%7D)%20%5C;%20=%20%5C;%20%5Cexp%20%7B%5Cleft%5C%7B%0A-%5Cfrac%2012%20%5C,%20%5Cint_0%5ET%20%5C%7Cu(x%5E%7Bi,u%7D_t)%5C%7C%5E2%20%5C,%20dt%20-%20%5Cint_%7B0%7D%5ET%20u%5E%5Ctop(x%5E%7Bi,u%7D_t)%20%5C,%20dW_t%0A%5Cright%5C%7D%7D%20.%0A"></p>
<p>Choosing the optimal “control” function <img src="https://latex.codecogs.com/png.latex?u"> that minimizes the variance of the estimator is not entirely straightforward, although this <a href="../../notes/doob_transforms/doob.html">previous note</a> already gives the answer. More on this in <a href="../../notes/HJB/HJB.html">another note</a>.</p>


</section>

 ]]></description>
  <category>SDE</category>
  <category>markov</category>
  <guid>https://alexxthiery.github.io/notes/girsanov/girsanov.html</guid>
  <pubDate>Sun, 02 Jun 2024 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Joe Doob &amp; Change of measures on path-space</title>
  <link>https://alexxthiery.github.io/notes/doob_transforms/doob.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/doob_transforms/joseph_doob.jpg" class="img-fluid figure-img" style="width:35.0%"></p>
<figcaption class="figure-caption"><a href="https://en.wikipedia.org/wiki/Joseph_L._Doob">Joseph Doob</a> (1910 – 2004)</figcaption>
</figure>
</div>
</div>
<p>Consider a continuous time Markov process <img src="https://latex.codecogs.com/png.latex?X_t"> on the time interval <img src="https://latex.codecogs.com/png.latex?%5B0,T%5D"> and with value in the state space <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D">. This defines a probability <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D"> on the set of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D">-valued paths. Now, it is often the case that one has to consider a perturbed probability distribution <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BQ%7D"> defined as</p>
<p><span id="eq-change-pb"><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%5Cmathbb%7BQ%7D%7D%7Bd%5Cmathbb%7BP%7D%7D(x_%7B%5B0,T%5D%7D)%20=%20%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D%7D%20%5C,%20%5Cexp%5Bg(X_T)%5D%0A%5Ctag%7B1%7D"></span></p>
<p>for a (typically unknown) normalization constant <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BZ%7D"> and some function <img src="https://latex.codecogs.com/png.latex?g:%20%5Cmathcal%7BX%7D%5Cto%20%5Cmathbb%7BR%7D">. For example, collecting a noisy observation <img src="https://latex.codecogs.com/png.latex?y_T%20%5Csim%20%5Cmathcal%7BF%7D(X_T)%20+%20%5Ctextrm%7B(noise)%7D"> at time <img src="https://latex.codecogs.com/png.latex?T">, the distribution <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BQ%7D"> defined with the log-likelihood function <img src="https://latex.codecogs.com/png.latex?g(x)%20=%20%5Clog%20%5Cmathbb%7BP%7D(y_T%20%5Cmid%20X_T=x)"> describes the dynamics of the Markov process <img src="https://latex.codecogs.com/png.latex?X_t"> conditioned on the observation <img src="https://latex.codecogs.com/png.latex?y_T">; we will use this interpretation in the following since this is the most common use case and gives the most intuitive interpretation. Doob h-transforms are a powerful tool to describe the dynamics of the conditioned process.</p>
<p>For convenience, let us use the notation <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_x%5B%5Cldots%5D%20%5Cequiv%20%5Cmathbb%7BE%7D%5B%5Cldots%20%5Cmid%20x_t=x%5D">. For a test function <img src="https://latex.codecogs.com/png.latex?%5Cvarphi:%20%5Cmathcal%7BX%7D%5Cto%20%5Cmathbb%7BR%7D"> and a time increment <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%3E%200">, we have</p>
<p><span id="eq-def"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cmathbb%7BE%7D%5B%5Cvarphi(x_%7Bt+%5Cdelta%7D)%20%7C%20x_t,%20y_T%5D%20&amp;=%20%5Cmathbb%7BE%7D_%7Bx_t%7D%5B%5Cvarphi(x_%7Bt+%5Cdelta%7D)%20%5C,%20%5Cexp(g(x_T))%20%5D%20%5C,%20/%20%5C,%20%5Cmathbb%7BE%7D_%7Bx_t%7D%5B%5Cexp(g(x_T))%5D%5C%5C%0A&amp;=%20%5Cfrac%7B%20%5Cmathbb%7BE%7D_%7Bx_t%7D%5B%5Cvarphi(x_%7Bt+%5Cdelta%7D)%20%5C,%20h(t+%5Cdelta,%20x_%7Bt+%5Cdelta%7D)%5D%20%7D%7Bh(t,%20x)%7D.%0A%5Cend%7Balign*%7D%0A%5Ctag%7B2%7D"></span></p>
<p>We have introduced the important function <img src="https://latex.codecogs.com/png.latex?h:%5B0,T%5D%20%5Ctimes%20%5Cmathcal%7BX%7D%5Cto%20%5Cmathbb%7BR%7D"> defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ah(t,%20x)%20%5C;%20=%20%5C;%20%5Cmathbb%7BE%7D%20%7B%5Cleft%5B%20%20%5Cexp%5Bg(x_T)%5D%20%5Cmid%20x_t%20=%20x%20%5Cright%5D%7D%20%20%5C;%20=%20%5C;%20%5Cmathbb%7BP%7D(y_T%20%5Cmid%20x_t%20=%20x).%0A"></p>
<p>One can readily check that the function <img src="https://latex.codecogs.com/png.latex?h"> satisfies the <a href="https://en.wikipedia.org/wiki/Kolmogorov_equations">Kolmogorov equation</a></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A(%5Cpartial_t%20+%20%5Cmathcal%7BL%7D)%20%5C,%20h%20=%200%0A"></p>
<p>with boundary condition <img src="https://latex.codecogs.com/png.latex?h(T,x)%20=%20%5Cexp%5Bg(x)%5D">. Furthermore, denoting by <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D"> the infinitesimal generator of the Markov process <img src="https://latex.codecogs.com/png.latex?X_t">, we have:</p>
<p><span id="eq-kolmogorov"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cmathbb%7BE%7D_%7Bx_t%7D%5B%5Cvarphi(x_%7Bt+%5Cdelta%7D)%20&amp;%20h(t+%5Cdelta,%20x_%7Bt+%5Cdelta%7D)%20%5D%0A%5C;%20%5Capprox%20%5C;%0A%5Cvarphi(x_t)%20h(t,%20x_t)%20%5C%5C%0A&amp;+%20%5C;%20%5Cdelta%20%5C,%20(%5Cpartial_t%20+%20%5Cmathcal%7BL%7D)%5Bh%20%5C,%20%5Cvarphi%5D%20%5C,%20(t,%20x_t)%0A%5C;%20+%20%5C;%20o(%5Cdelta).%0A%5Cend%7Balign*%7D%0A%5Ctag%7B3%7D"></span></p>
<p>The infinitesimal generator <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D%5E%7B%5Cstar%7D"> of the conditioned process is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D%5E%7B%5Cstar%7D%20%5Cvarphi(t,%20x_t)%20=%20%5Clim_%7B%5Cdelta%20%5Cto%200%5E+%7D%20%5C;%20%5Cfrac%7B%5Cmathbb%7BE%7D%5B%5Cvarphi(x_%7Bt+%5Cdelta%7D)%20%7C%20x_t,%20y_T%5D%20-%20%5Cvarphi(x_t)%7D%7B%5Cdelta%7D.%0A"></p>
<p>Plugging Equation&nbsp;3 within Equation&nbsp;2 directly gives that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D%5E%7B%5Cstar%7D%20%5Cvarphi%5C;%20=%20%5C;%20%5Cmathcal%7BL%7D%5Cvarphi+%20%5Cfrac%7B%5Cmathcal%7BL%7D%5B%5Cvarphi%5C,%20h%5D%7D%7Bh%7D%20+%20%5Cvarphi%5Cfrac%7B%5Cpartial_t%20h%7D%7Bh%7D.%0A"></p>
<p>The generator <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D%5E%7B%5Cstar%7D"> describes the dynamics of the conditioned process. In fact, the same computation holds with a more general change of measure of the type <span id="eq-change-gen"><img src="https://latex.codecogs.com/png.latex?%0A%5Ctextcolor%7Bgreen%7D%7B%5Cfrac%7Bd%5Cmathbb%7BQ%7D%7D%7Bd%5Cmathbb%7BP%7D%7D(x_%7B%5B0,T%5D%7D)%20=%20%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D%7D%20%5C,%20%5Cexp%20%7B%5Cleft%5C%7B%20%5Cint_%7B0%7D%5ET%20f(s,%20X_s)%20%5C,%20ds%20+%20g(x_T)%20%5Cright%5C%7D%7D%20%20%7D%0A%5Ctag%7B4%7D"></span></p>
<p>for some function <img src="https://latex.codecogs.com/png.latex?f:%5B0,T%5D%20%5Ctimes%20%5Cmathcal%7BX%7D%5Cto%20%5Cmathbb%7BR%7D">. One can define the function <img src="https://latex.codecogs.com/png.latex?h"> similarly as</p>
<p><span id="eq-h-gen"><img src="https://latex.codecogs.com/png.latex?%0A%5Ctextcolor%7Bgreen%7D%7B%20h(t,%20x_t)%20%5C;%20=%20%5C;%20%5Cmathbb%7BE%7D%20%7B%5Cleft%5B%20%20%5Cexp%20%7B%5Cleft%5C%7B%20%5Cint_%7Bt%7D%5ET%20f(X_s)%20%5C,%20ds%20+%20g(x_T)%20%5Cright%5C%7D%7D%20%20%5Cmid%20x_t%20%5Cright%5D%7D%20%7D.%0A%5Ctag%7B5%7D"></span></p>
<p>This function satisfies the <a href="https://en.wikipedia.org/wiki/Feynman–Kac_formula">Feynman-Kac formula</a> <img src="https://latex.codecogs.com/png.latex?(%5Cpartial_t%20+%20%5Cmathcal%7BL%7D+%20f)%20%5C,%20h%20=%200"> and one obtains entirely similarly that the probability distribution <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BQ%7D"> describes a Markov process with infinitesimal generator</p>
<p><span id="eq-doob-transforms"><img src="https://latex.codecogs.com/png.latex?%0A%5Ctextcolor%7Bgreen%7D%7B%5Cmathcal%7BL%7D%5E%7B%5Cstar%7D%20%5Cvarphi%5C;%20=%20%5C;%20%5Cmathcal%7BL%7D%5Cvarphi+%20%5Cfrac%7B%5Cmathcal%7BL%7D%5Bh%20%5C,%20%5Cvarphi%5D%7D%7Bh%7D%20+%20%20%7B%5Cleft(%20%20%5Cfrac%7B%5Cpartial_t%20h%7D%20%7Bh%7D%20+%20f%20%5Cright)%7D%20%20%5C,%20%5Cvarphi.%7D%0A%5Ctag%7B6%7D"></span></p>
<p>To see how this works, let us see a few examples:</p>
<section id="general-diffusions" class="level3">
<h3 class="anchored" data-anchor-id="general-diffusions">General diffusions</h3>
<p>Consider a diffusion process</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX%20=%20b(X)%20%5C,%20dt%20+%20%5Csigma(X)%20%5C,%20dW%0A"></p>
<p>with generator <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D%5Cvarphi=%20b%20%5Cnabla%20%5Cvarphi+%20%5Ctfrac12%20%5C,%20%5Csigma%20%5Csigma%5E%5Ctop%20:%20%5Cnabla%5E2%20%5Cvarphi"> and initial distribution <img src="https://latex.codecogs.com/png.latex?%5Cmu_0(dx)">. We are interested in describing the dynamics of the “conditioned” process given by the probability distribution <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BQ%7D"> defined in Equation&nbsp;4. Algebra applied to Equation&nbsp;5 then shows that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D%5E%5Cstar%20%5Cvarphi%5C;%20=%20%5C;%20%5Cmathcal%7BL%7D%5Cvarphi+%20%5Cunderbrace%7B%5Cfrac%7B%5Cvarphi%5C,%20(%5Cpartial_t%20+%20%5Cmathcal%7BL%7D+%20f)%5Bh%5D%7D%7Bh%7D%7D_%7B=0%7D%0A+%20%5Csigma%20%5C,%20%5Csigma%5E%5Ctop%20%5C,%20(%5Cnabla%20%5Clog%20h)%20%5C,%20%5Cnabla%20%5Cvarphi%0A"></p>
<p>where the function <img src="https://latex.codecogs.com/png.latex?h"> is described in Equation&nbsp;5. Since <img src="https://latex.codecogs.com/png.latex?(%5Cpartial_t%20+%20%5Cmathcal%7BL%7D+%20f)%20%5C,%20h%20=%200">, this reveals that the probability distribution <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BQ%7D"> describes a diffusion process <img src="https://latex.codecogs.com/png.latex?X%5E%5Cstar"> with dynamics</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX%5E%5Cstar%20=%20b(X%5E%5Cstar)%20%5C,%20dt%20+%20%5Csigma(X%5E%5Cstar)%20%5C,%20%20%7B%5Cleft%5C%7B%20%20dW%20+%20%20%5Ctextcolor%7Bblue%7D%7Bu%5E%5Cstar(t,%20X%5E%5Cstar)%7D%20%5C,%20dt%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>The additional drift term <img src="https://latex.codecogs.com/png.latex?%5Csigma(X%5E%5Cstar)%20%5C,%20%20%5Ctextcolor%7Bblue%7D%7Bu%5E%5Cstar(t,%20X%5E%5Cstar)%7D%20%5C,%20dt"> is involves a “control” <img src="https://latex.codecogs.com/png.latex?%20%5Ctextcolor%7Bblue%7D%7Bu%5E%5Cstar(t,%20X%5E%5Cstar)%7D"> with <span id="eq-diffusion-doob"><img src="https://latex.codecogs.com/png.latex?%0A%5Ctextcolor%7Bblue%7D%7Bu%5E%5Cstar(t,%20x)%20=%20%5Csigma%5E%5Ctop(x)%20%5C,%20%5Cnabla%20%5Clog%20h(t,%20x)%7D.%0A%5Ctag%7B7%7D"></span></p>
<p>Note that the initial distribution of the conditioned process is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmu_0%5E%5Cstar(dx)%20=%20%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D%7D%20%5C,%20%5Cmu_0(dx)%20%5C,%20h(0,x).%0A"></p>
<p>Unfortunately, apart from a few straightforward cases such as a Brownian motion or an Ornstein-Uhlenbeck process, the function <img src="https://latex.codecogs.com/png.latex?h"> is generally intractable. However, there are indeed several numerical methods available to approximate it effectively.</p>
</section>
<section id="brownian-bridge" class="level3">
<h3 class="anchored" data-anchor-id="brownian-bridge">Brownian bridge</h3>
<p>What about a Brownian motion in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5ED"> conditioned to hit the state <img src="https://latex.codecogs.com/png.latex?x_%5Cstar%20%5Cin%20%5Cmathbb%7BR%7D%5ED"> at time <img src="https://latex.codecogs.com/png.latex?t=T">, i.e.&nbsp;a <a href="https://en.wikipedia.org/wiki/Brownian_bridge">Brownian bridge</a>? In that case, the function <img src="https://latex.codecogs.com/png.latex?h"> is given by</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ah(t,x)%20=%20%5Cmathbb%7BP%7D(B_T%20=%20x_%5Cstar%20%5Cmid%20B_t%20=%20x)%0A=%0A%5Cexp%20%7B%5Cleft%5C%7B%20-%5Cfrac%7B%5C%7Cx-x_%5Cstar%5C%7C%5E2%7D%7B2%20%5C,%20(T-t)%7D%20%5Cright%5C%7D%7D%20%20/%20%5Cmathcal%7BZ%7D_%7BT-t%7D%0A"></p>
<p>for some irrelevant normalization constant <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BZ%7D_%7BT-t%7D"> that only depends on <img src="https://latex.codecogs.com/png.latex?T-t">. Plugging this into Equation&nbsp;7 gives that the conditioned Brownian <img src="https://latex.codecogs.com/png.latex?X%5E%7B%5Cstar%7D"> motion has dynamics</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX%5E%5Cstar%20%5C;=%5C;%20%20%5Ctextcolor%7Bblue%7D%7B-%20%5Cfrac%7BX%5E%5Cstar%20-%20x_%5Cstar%7D%7BT-t%7D%20%5C,%20dt%7D%20+%20dB.%0A"></p>
<p>The additional drift term <img src="https://latex.codecogs.com/png.latex?-(X%5E%5Cstar%20-%20x_%5Cstar)/(T-t)"> is intuitive: it points in the direction of <img src="https://latex.codecogs.com/png.latex?x%5E%5Cstar"> and gets increasingly large as <img src="https://latex.codecogs.com/png.latex?t%20%5Cto%20T">.</p>
</section>
<section id="positive-brownian-motion" class="level3">
<h3 class="anchored" data-anchor-id="positive-brownian-motion">Positive Brownian motion</h3>
<p>What about a scalar Brownian conditioned to stay positive at all times? Let us consider <img src="https://latex.codecogs.com/png.latex?T"> and let us condition first on the event that the Brownian motion stays positive within <img src="https://latex.codecogs.com/png.latex?%5B0,T%5D"> and later consider the limit <img src="https://latex.codecogs.com/png.latex?T%20%5Cto%20%5Cinfty">. The function <img src="https://latex.codecogs.com/png.latex?h"> reads</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ah(t,x)%20=%20%5Cmathbb%7BP%7D%20%7B%5Cleft(%20%5Ctext%7B$B_t$%20stays%20$%3E0$%20on%20$%5Bt,T%5D$%7D%20%5Cmid%20B_t=x%20%5Cright)%7D%20.%0A"></p>
<p>This can easily be calculated with the <a href="https://en.wikipedia.org/wiki/Reflection_principle_(Wiener_process)">reflection principle</a>. It equals</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ah(t,x)%20=%201%20-%202%20%5C,%20%5Cmathbb%7BP%7D(B_T%20%3C%200%20%5Cmid%20B_T%20=%20x)%0A=%0A%5Cmathbb%7BP%7D(%5Csqrt%7BT-t%7D%20%5C,%20%5C%7C%20%5Cxi%20%5C%7C%20%3C%20x)%0A"></p>
<p>for a standard Gaussian <img src="https://latex.codecogs.com/png.latex?%5Cxi%20%5Csim%20%5Cmathcal%7BN%7D(0,1)">. Plugging this into Equation&nbsp;7 gives that the additional drift term is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla%20%5Clog%20h(t,x)%20=%20%5Cfrac%7B%5Cexp%20%7B%5Cleft(%20-x%5E2%20/%20(2%20%5C,%20(T-t))%20%5Cright)%7D%20%7D%7Bx%7D%20%5Cquad%20%5Cto%20%5Cquad%20%5Cfrac%7B1%7D%7Bx%7D%0A"></p>
<p>as <img src="https://latex.codecogs.com/png.latex?T%20%5Cto%20%5Cinfty">. This shows that a Brownian motion conditioned to stay positive at all times has a upward drift of size <img src="https://latex.codecogs.com/png.latex?1/x">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX%5E%5Cstar%20%5C;=%5C;%20%5Cfrac%7B1%7D%7BX%5E%7B%5Cstar%7D%7D%20+%20dB.%0A"></p>
<p>Incidentally, it is the dynamics of a <a href="https://en.wikipedia.org/wiki/Bessel_process">Bessel process</a> of dimension <img src="https://latex.codecogs.com/png.latex?d=3">, i.e.&nbsp;the law of the modulus of a three-dimensional Brownian motion. More generally, if one conditions a Brownian motion to stay within a closed domain <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D">, the conditioned dynamics exhibit a repulsive drift term of size about <img src="https://latex.codecogs.com/png.latex?1/%5Ctextrm%7Bdist%7D(x,%20%5Cpartial%20%5Cmathcal%7BD%7D)"> near the boundary <img src="https://latex.codecogs.com/png.latex?%5Cpartial%20%5Cmathcal%7BD%7D"> of the domain, as described below.</p>
</section>
<section id="brownian-motion-staying-in-a-domain" class="level3">
<h3 class="anchored" data-anchor-id="brownian-motion-staying-in-a-domain">Brownian motion staying in a domain</h3>
<p>What about a Brownian motion conditioned to stay within a domain <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D"> forever? As before, consider an time horizon <img src="https://latex.codecogs.com/png.latex?T"> and define the function <img src="https://latex.codecogs.com/png.latex?h"> as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ah(t,x)%20=%20%5Cmathbb%7BP%7D%20%7B%5Cleft(%20%5Ctext%7B$B_t$%20stays%20in%20$%5Cmathcal%7BD%7D$%20on%20$%5Bt,T%5D$%7D%20%5Cmid%20B_t=x%20%5Cright)%7D%20.%0A"></p>
<p>One can see that the function <img src="https://latex.codecogs.com/png.latex?h"> satisfies the PDE</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A(%5Cpartial_t%20+%20%5CDelta)%20%5C,%20h%20=%200%0A"></p>
<p>and equals zero on the boundary <img src="https://latex.codecogs.com/png.latex?%5Cpartial%20%5Cmathcal%7BD%7D"> of the domain. Furthermore <img src="https://latex.codecogs.com/png.latex?h(t,x)%20%5Cto%201"> as <img src="https://latex.codecogs.com/png.latex?t%20%5Cto%20T"> for all <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathcal%7BD%7D">. Consider the eigenfunctions <img src="https://latex.codecogs.com/png.latex?%5Cpsi_k"> of the negative Laplacian <img src="https://latex.codecogs.com/png.latex?-%5CDelta"> with Dirichlet boundary conditions on <img src="https://latex.codecogs.com/png.latex?%5Cpartial%20%5Cmathcal%7BD%7D">. Recall that <img src="https://latex.codecogs.com/png.latex?-%5CDelta"> is a positive operator with a discrete spectrum <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%20%5Cleq%20%5Clambda_2%20%5Cleq%20%5Cldots"> of non-negative eigenvalues. The eigenfunction corresponding to the smallest eigenvalue <img src="https://latex.codecogs.com/png.latex?%5Clambda_1"> is the principal eigenfunction <img src="https://latex.codecogs.com/png.latex?%5Cpsi_1"> and it is standard that it is a positive function within the domain <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D">, as a “slight” generalization of the <a href="https://en.wikipedia.org/wiki/Perron–Frobenius_theorem">Perron-Frobenius</a> in linear algebra shows it. Expanding <img src="https://latex.codecogs.com/png.latex?h"> in the basis of eigenfunctions <img src="https://latex.codecogs.com/png.latex?%5Cpsi_k"> gives that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ah(t,x)%20=%20%5Cunderbrace%7Bc_1%20%5C,%20e%5E%7B-%5Clambda_1%20%5C,%20(T-t)%7D%20%5C,%20%5Cpsi_1(x)%7D_%7B%5Ctextrm%7Bdominant%20contribution%7D%7D%20+%20%5Csum_%7Bk%20%5Cgeq%202%7D%20c_k%20%5C,%20e%5E%7B-%5Clambda_k%20%5C,%20(T-t)%7D%20%5C,%20%5Cpsi_k(x).%0A"></p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/doob_transforms/eigenfunctions.jpg" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption class="figure-caption">Eigenfunctions of the Laplacian</figcaption>
</figure>
</div>
</div>
<p>Since we are interested in the regime <img src="https://latex.codecogs.com/png.latex?T%20%5Cto%20%5Cinfty">, it holds that</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cnabla_x%20%5Clog%20h(t,x)%20%5C;%20%5Cto%20%5C;%20%5Cnabla%20%5Clog%20%5Cpsi_1(x)."></p>
<p>This shows that the conditioned Brownian motion has a drift term expressed in terms of the principal eigenfunction <img src="https://latex.codecogs.com/png.latex?%5Cpsi_1"> of the Laplacian:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX%5E%5Cstar%20%5C;=%5C;%20%20%5Ctextcolor%7Bblue%7D%7B%20%5Cnabla%20%5Clog%20%5Cpsi_1(X%5E%5Cstar)%20%5C,%20dt%7D%20+%20dB.%0A"></p>
<p>For example, if <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D%5Cequiv%20%5B0,L%5D"> for a 1D Brownian motion, the principal eigenfunction is <img src="https://latex.codecogs.com/png.latex?%5Cpsi_1(x)%20=%20%5Csin(%5Cpi%20%5C,%20x%20/L)">. This shows that there is a upward drift of size <img src="https://latex.codecogs.com/png.latex?%5Csim%201/x"> near <img src="https://latex.codecogs.com/png.latex?x%20%5Capprox%200"> and a downward drift of size <img src="https://latex.codecogs.com/png.latex?%5Csim%201/(L-x)"> near <img src="https://latex.codecogs.com/png.latex?x%20%5Capprox%20L">.</p>


</section>

 ]]></description>
  <category>SDE</category>
  <category>markov</category>
  <guid>https://alexxthiery.github.io/notes/doob_transforms/doob.html</guid>
  <pubDate>Mon, 13 May 2024 16:00:00 GMT</pubDate>
</item>
<item>
  <title>RWM &amp; HMC on manifolds</title>
  <link>https://alexxthiery.github.io/notes/MCMC_on_manifold/mcmc_manifold.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/MCMC_on_manifold/mcmc_manifold.jpg" class="img-fluid figure-img" style="width:80.0%"></p>
</figure>
</div>
</div>
<p>Consider a smooth manifold <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D%5Csubset%20%5Cmathbb%7BR%7D%5En"> of dimension <img src="https://latex.codecogs.com/png.latex?d_%7B%5Cmathcal%7BM%7D%7D%20=%20(n-d)"> defined as the zero set of a well-behaved “constraint” function <img src="https://latex.codecogs.com/png.latex?C:%20%5Cmathbb%7BR%7D%5En%20%5Cto%20%5Cmathbb%7BR%7D%5Ed">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BM%7D=%20%5C%7B%20x%20%5Cin%20%5Cmathbb%7BR%7D%5En%20%5C;%20%5Ctext%7Bsuch%20that%7D%20%5C;%20C(x)%20=%200%20%5C%7D.%0A"></p>
<p>We would like to use MCMC to sample from a probability distribution supported on <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D"> with density <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)"> with respect to the uniform <a href="https://en.wikipedia.org/wiki/Hausdorff_measure">Hausdorff measure</a> on <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D">. It is relatively straightforward to adapt standard MCMC methods when dealing with simple manifolds such as a sphere or a torus since their geodesics and several other geometric quantities are analytically tractable. Maybe surprisingly, it is in fact relatively straightforward to design MCMC samplers on general implicitly defined manifold such as <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D">. The article <span class="citation" data-cites="zappa2018monte">(Zappa, Holmes-Cerfon, and Goodman 2018)</span> explains these ideas beautifully.</p>
<section id="manifold-random-walk-metropolis-hastings" class="level3">
<h3 class="anchored" data-anchor-id="manifold-random-walk-metropolis-hastings">Manifold Random Walk Metropolis-Hastings</h3>
<p>Assume that <img src="https://latex.codecogs.com/png.latex?x_n%20%5Cin%20%5Cmathcal%7BM%7D"> is the current position of the MCMC chain. To generate a proposal <img src="https://latex.codecogs.com/png.latex?y_n%20%5Cin%20%5Cmathcal%7BM%7D"> that will eventually be accepted or rejected, one can proceed very similarly to the standard RWM algorithm with Gaussian perturbations with variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">. First, generate a vector <img src="https://latex.codecogs.com/png.latex?v%20%5Cin%20T_%7Bx_n%7D"> from a centred Gaussian distribution with covariance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2%20%5C,%20I"> on the tangent space <img src="https://latex.codecogs.com/png.latex?T_%7Bx_n%7D"> to <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D"> at <img src="https://latex.codecogs.com/png.latex?x_n">. To do so, it suffices for example to generate a standard Gaussian vector <img src="https://latex.codecogs.com/png.latex?z%20%5Csim%20%5Cmathcal%7BN%7D(0,%20%5Csigma%5E2%20I_n)"> in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En"> and orthogonal-project it onto <img src="https://latex.codecogs.com/png.latex?T_%7Bx_n%7D">. Indeed, one cannot simply define the proposal as <img src="https://latex.codecogs.com/png.latex?x_n%20+%20v"> since it would not necessarily lie on <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D">. Instead, one projects <img src="https://latex.codecogs.com/png.latex?x_n%20+%20v"> back to <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D">. To do so, one needs to define the direction used for the projection and the manifold RWM algorithm uses <img src="https://latex.codecogs.com/png.latex?T_%7Bx_n%7D%5E%5Cperp">, for reasons that will become clear later. In other words, the proposal <img src="https://latex.codecogs.com/png.latex?y_n"> is obtained by seeking a vector <img src="https://latex.codecogs.com/png.latex?w%20%5Cin%20T_%7Bx_n%7D%5E%7B%5Cperp%7D"> such that <img src="https://latex.codecogs.com/png.latex?x_n%20+%20v%20+%20w%20%5Cin%20%5Cmathcal%7BM%7D">.</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/MCMC_on_manifold/projection_onto_M.jpg" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Projection onto <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D"> from <span class="citation" data-cites="zappa2018monte">(Zappa, Holmes-Cerfon, and Goodman 2018)</span></figcaption>
</figure>
</div>
</div>
<p>If one calls <img src="https://latex.codecogs.com/png.latex?J_%7Bx_n%7D"> the Jacobian matrix of <img src="https://latex.codecogs.com/png.latex?C"> at <img src="https://latex.codecogs.com/png.latex?x_n">, i.e.&nbsp;the matrix whose <strong>rows</strong> are the gradients of the components of <img src="https://latex.codecogs.com/png.latex?C">, this projection operation boils down to finding a vector <img src="https://latex.codecogs.com/png.latex?%5Clambda%20%5Cin%20%5Cmathbb%7BR%7D%5Ed"> such that</p>
<p><span id="eq-projection"><img src="https://latex.codecogs.com/png.latex?%0AC(%20%5C,%20x_n%20+%20v%20+%20J_%7Bx_n%7D%5E%5Ctop%20%5Clambda)%20=%200%20%5Cin%20%5Cmathbb%7BR%7D%5Ed.%0A%5Ctag%7B1%7D"></span></p>
<p>Note that Equation&nbsp;1 is a non-linear equation in <img src="https://latex.codecogs.com/png.latex?%5Clambda"> that can have no solution, one solution or many solutions – this can seem like a fundamental roadblock to the design of a valid MCMC algorithm, but we will see that it is not! Before discussing in slightly more details the resolution of Equation&nbsp;1, assume that a standard root-finding algorithm takes the pair <img src="https://latex.codecogs.com/png.latex?(x_n+v,%20J_%7Bx_n%7D)"> as input and attempts to produces the projection <img src="https://latex.codecogs.com/png.latex?y_n">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BProj%7D:%20%5Cquad%20(x_n+v,%20J_%7Bx_n%7D)%20%5C;%20%5Cunderbrace%7B%5Cmapsto%7D_%7B%5Ctext%7Broot-finding%7D%7D%20%5C;%20y_n%20%5Cin%20%5Cmathcal%7BM%7D.%0A"></p>
<p>The algorithm will either converge to one of the possible solutions or fail. If the algorithm fails to converge, one can simply reject the proposal <img src="https://latex.codecogs.com/png.latex?y_n"> and set <img src="https://latex.codecogs.com/png.latex?y_n%20=%20%5Ctext%7B(Failed)%7D"> and set <img src="https://latex.codecogs.com/png.latex?x_%7Bn+1%7D%20=%20x_n">. If the algorithm converges, this defines a valid proposal <img src="https://latex.codecogs.com/png.latex?y_n%20%5Cin%20%5Cmathcal%7BM%7D">. To ensure reversibility, and it is one of the main novelty of the article <span class="citation" data-cites="zappa2018monte">(Zappa, Holmes-Cerfon, and Goodman 2018)</span>, one needs to verify that the reverse proposal <img src="https://latex.codecogs.com/png.latex?y_n%20%5Cmapsto%20x_n"> is possible.</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/MCMC_on_manifold/reverse_mcmc.jpg" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Reversibility check <span class="citation" data-cites="zappa2018monte">(Zappa, Holmes-Cerfon, and Goodman 2018)</span></figcaption>
</figure>
</div>
</div>
<p>To do so, note that the only possibility for the reverse move <img src="https://latex.codecogs.com/png.latex?y_n%20%5Cto%20x_n"> to happen is if <img src="https://latex.codecogs.com/png.latex?x_n%20=%20%5Ctext%7BProj%7D(y_n%20+%20v',%20J_%7By_n%7D)"> where</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax_n-y_n%20%5C;=%5C;%20%5Cunderbrace%7Bv'%7D_%7B%5Cin%20T_%7By_n%7D%7D%20%20%5C,%20+%20%5C,%20%5Cunderbrace%7Bw'%7D_%7B%5Cin%20T_%7By_n%7D%5E%7B%5Cperp%7D%7D.%0A"></p>
<p>The uniqueness follows from the decomposition <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En%20%5Cequiv%20T_%7By_n%7D%20%5Cotimes%20T_%7By_n%7D%5E%7B%5Cperp%7D">. The reverse move is consequently possible if and only if the following <strong>reversibility check</strong> condition is satisfied,</p>
<p><span id="eq-reversibility"><img src="https://latex.codecogs.com/png.latex?%0Ax_n%20=%20%5Ctext%7BProj%7D(y_n%20+%20v',%20J_%7By_n%7D).%0A%5Ctag%7B2%7D"></span></p>
<p>This reversibility check is necessary as it is not guaranteed that the root-finding algorithm started from <img src="https://latex.codecogs.com/png.latex?y_n%20+%20v'"> converges at all, or converges to <img src="https://latex.codecogs.com/png.latex?x_n"> in the case when there are several solutions. If Equation&nbsp;2 is not satisfied, the proposal <img src="https://latex.codecogs.com/png.latex?y_n"> is rejected and one sets <img src="https://latex.codecogs.com/png.latex?x_%7Bn+1%7D%20=%20x_n">. On the other hand, if Equation&nbsp;2 is satisfied, the proposal <img src="https://latex.codecogs.com/png.latex?y_n"> is accepted with the usual Metropolis-Hastings probability</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmin%20%5Cleft%5C%7B1,%20%5Cfrac%7B%5Cpi(y_n)%20%5C,%20p(v'%7Cx_n)%7D%7B%5Cpi(x_n)%20%5C,%20p(v%7Cx_n)%7D%20%5Cright%5C%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?p(v%7Cx)%20=%20Z%5E%7B-1%7D%20%5C,%20%5Cexp(-%5C%7Cv%5C%7C%5E2%20/%202%20%5Csigma%5E2)"> denotes the Gaussian density on the tangent space <img src="https://latex.codecogs.com/png.latex?T_%7Bx_n%7D"> The above description defines a valid MCMC algorithm on <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D"> that is reversible with respect to the target distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)">.</p>
</section>
<section id="projection-onto-the-manifold" class="level3">
<h3 class="anchored" data-anchor-id="projection-onto-the-manifold">Projection onto the manifold</h3>
<p>As described above, the main difficulty is to solve the non-linear equation Equation&nbsp;1 describing the projection of the proposal <img src="https://latex.codecogs.com/png.latex?(x_n%20+%20v)"> back to the manifold <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D">. The projection is along the space spanned by the columns of <img src="https://latex.codecogs.com/png.latex?J_%7Bx_n%7D%5E%5Ctop%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn,d%7D">, i.e.&nbsp;find a vector <img src="https://latex.codecogs.com/png.latex?%5Clambda%20%5Cin%20%5Cmathbb%7BR%7D%5Ed"> such that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5CPhi(%5Clambda)%20=%20C(%20%5C,%20x_n%20+%20v%20+%20J_%7Bx_n%7D%5E%5Ctop%20%5Clambda)%20=%200%20%5Cin%20%5Cmathbb%7BR%7D%5Ed.%0A"></p>
<p>One can use a standard Newton’s method to solve this equation started from <img src="https://latex.codecogs.com/png.latex?%5Clambda_0=0">. Setting for notational convenience <img src="https://latex.codecogs.com/png.latex?q(%5Clambda)%20=%20x_n%20+%20v%20+%20J_%7Bx_n%7D%5ET%20%5C,%20%5Clambda">, this boils down to iterating</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Clambda_%7Bk+1%7D%20-%20%5Clambda_%7Bk%7D%0A=%0A-%20%5Cleft(%20J_%7Bq(%5Clambda_k)%7D%20%5C,%20J_%7Bx_n%7D%5E%5Ctop%20%5Cright)%5E%7B-1%7D%20%5C,%20%5CPhi(%5Clambda_k).%0A"></p>
<p>As described in <span class="citation" data-cites="barth1995algorithms">(Barth et al. 1995)</span>, it can sometimes be computationally advantageous to use a quasi-Newton method and use instead</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Clambda_%7Bk+1%7D%20-%20%5Clambda_%7Bk%7D%0A=%0A-%20G%5E%7B-1%7D%20%5C,%20%5CPhi(%5Clambda_k)%0A"></p>
<p>with <strong>fixed</strong> positive definite matrix <img src="https://latex.codecogs.com/png.latex?G%20=%20J_%7Bx_n%7D%20%5C,%20J_%7Bx_n%7D%5E%5Ctop"> since one can then pre-compute a decomposition of <img src="https://latex.codecogs.com/png.latex?G"> and use it to solve the linear systems at each iterations. In some recent and related work <span class="citation" data-cites="au2020manifold">(Au, Graham, and Thiery 2022)</span>, we observed that the standard Newton method performed well in the settings we considered and there was most of the time no computational advantage to using a quasi-Newton method. In practice, the main computational bottleneck is to compute the Jacobian matrix <img src="https://latex.codecogs.com/png.latex?J_%7Bx_n%7D">, although it is problem-dependent and some structure can typically be exploited. In practice, only a relatively small number of iterations are performed and the root-finding algorithm is stopped as soon as <img src="https://latex.codecogs.com/png.latex?%5C%7C%5CPhi(%5Clambda_k)%5C%7C"> is below a certain threshold. If the stepsize is small, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?%5C%7Cv%5C%7C%20%5Cll%201">, it is typically the case that the Newton’s method will converge to a solution in only a very small number of iterations – indeed, <a href="https://en.wikipedia.org/wiki/Newton%27s_method">Newton’s method</a> is quadratically convergent when close to a solution.</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/MCMC_on_manifold/RWM_double_torus.gif" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption class="figure-caption">30k RWM chains ran in parallel to explore a double torus.</figcaption>
</figure>
</div>
</div>
<p>In the figure above, I have implemented the RWM algorithm above described to sample from the uniform distribution supported on a double torus described by the constraint function <img src="https://latex.codecogs.com/png.latex?C:%20%5Cmathbb%7BR%7D%5E3%20%5Cto%20%5Cmathbb%7BR%7D"> given as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AC(x,y,z)%20=%20(x%5E2%20%5C,%20(x%5E2%20-%201)%20+%20y%5E2)%5E2+z%5E2-0.03.%0A"></p>
<p>The figure shows <img src="https://latex.codecogs.com/png.latex?30,000"> chains ran in parallel, which is straightforward to implement in practice with JAX <span class="citation" data-cites="jax2018github">(Bradbury et al. 2018)</span>. All the chains are initialized from the same position so that one can visualize the evolution of the density of particles.</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/MCMC_on_manifold/RWM_manifold_tuning.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Tuning of manifold-RWM</figcaption>
</figure>
</div>
</div>
<p>One can for example monitor the usual expected squared jump distance</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctextrm%7B(ESJD)%7D%20%5Cequiv%20%5Cmathbb%7BE%7D%5B%5C%7CX_%7Bn+1%7D%20-%20X_n%5C%7C%5E2%5D%0A"></p>
<p>and maximize it to tune the RWM step-size; it would probably make slightly more sense to monitor the squared geodesic distances instead the naive squared norm <img src="https://latex.codecogs.com/png.latex?%5C%7CX_%7Bn+1%7D%20-%20X_n%5C%7C%5E2">, but that’s way to much hassle and would probably make only a negligible difference. In the figure above, I have plotted the expected squared jump distance as a function of the acceptance rate for different step-sizes. It is interesting to see a pattern extremely similar to the one observed in the standard RWM algorithm <span class="citation" data-cites="roberts2001optimal">(Roberts and Rosenthal 2001)</span>: in this double torus example, the optimal acceptance rate is around <img src="https://latex.codecogs.com/png.latex?25%5C%25">. Note that since the target distribution is uniform, the rate of acceptance is only very slightly lower than the proportion of successful reversibility checks.</p>
</section>
<section id="hamiltonian-monte-carlo-hmc-on-manifolds" class="level3">
<h3 class="anchored" data-anchor-id="hamiltonian-monte-carlo-hmc-on-manifolds">Hamiltonian Monte Carlo (HMC) on manifolds</h3>
<p>While the Random Walk Metropolis-Hastings algorithm is interesting, exploiting gradient information is often necessary to design efficient MCMC samplers. Consider a single iteration of a standard <a href="https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo">Hamiltonian Monte Carlo (HMC)</a> sampler targeting a density <img src="https://latex.codecogs.com/png.latex?%5Cpi(q)"> on <img src="https://latex.codecogs.com/png.latex?q%20%5Cin%20%5Cmathbb%7BR%7D%5En">. The method proceeds by simulating from a dynamics that is reversible with respect to an extended target density <img src="https://latex.codecogs.com/png.latex?%5Cbar%7B%5Cpi%7D(q,p)"> on <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En%20%5Cotimes%20%5Cmathbb%7BR%7D%5En"> defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cbar%7B%5Cpi%7D(q,p)%0A&amp;%5Cpropto%20%5Cpi(q)%20%5C,%20%5Cexp%20%5Cleft%5C%7B%20-%5Cfrac%7B1%7D%7B2m%7D%20%5C%7Cp%5C%7C%5E2%20%5Cright%5C%7D%5C%5C%0A&amp;=%20%5Cexp%5Cleft%5C%7B%20%5Clog%20%5Cpi(q)%20-%20K(p)%20%5Cright%5C%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>for a user-defined mass parameter <img src="https://latex.codecogs.com/png.latex?m%20%3E%200">. In general, the mass parameter is a positive definite <strong>matrix</strong> but generalizing this to manifolds is slightly less useful in practice. For a time-discretization step <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%3E%200"> and a current position <img src="https://latex.codecogs.com/png.latex?(q_n,p_n)">, the method proceeds by generating a proposal <img src="https://latex.codecogs.com/png.latex?(q_%7B*%7D,p_%7B*%7D)"> defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%0A%5Cbegin%7Baligned%7D%0Ap_%7Bn+1/2%7D%20&amp;=%20p_n%20+%20%5Cfrac%7B%5Cvarepsilon%7D%7B2%7D%20%5Cnabla%20%5Clog%20%5Cpi(q_n)%5C%5C%0Aq_%7B*%7D%20&amp;=%20q_n%20+%20%5Cvarepsilon%5C,%20m%5E%7B-1%7D%20%5C,%20p_%7Bn+1/2%7D%5C%5C%0Ap_%7B*%7D%20&amp;=%20p_%7Bn+1/2%7D%20+%20%5Cfrac%7B%5Cvarepsilon%7D%7B2%7D%20%5Cnabla%20%5Clog%20%5Cpi(q_%7B*%7D).%0A%5Cend%7Baligned%7D%0A%5Cright.%0A"></p>
<p>This proposal is accepted with probability <img src="https://latex.codecogs.com/png.latex?%5Cmin%5Cleft(%201,%20%5Cbar%7B%5Cpi%7D(q_*,%20p_*)/%5Cbar%7B%5Cpi%7D(q_n,%20p_n)%20%5Cright)">. Indeed, in standard implementation, several leapfrog steps are performed instead of a single one. One can also choose to perform a single leapfrog step as above and only do a partial refreshment of the momentum after each leapfrog step – this may be more efficient or easier to implement when running a large number of HMC chains in parallel on a GPU for example. To adapt the HMC algorithm to sample from a density <img src="https://latex.codecogs.com/png.latex?%5Cpi(q)"> supported on a manifold <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D">, one can proceed similarly to the RWM algorithm by interleaving additional projection steps. These projections are needed to ensure that the momentum vectors <img src="https://latex.codecogs.com/png.latex?p_n"> remain in the right tangent spaces and the position vectors <img src="https://latex.codecogs.com/png.latex?q_n"> remain on the manifold <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A(q_n,%20p_n)%20%5C;%20%5Cin%20%5C;%20%5Cmathcal%7BM%7D%5Ctimes%20T_%7Bq_n%7D.%0A"></p>
<p>As in the RWM algorithm, reversibility checks need to be performed to ensure that the overall algorithm is reversible with respect to the target distribution <img src="https://latex.codecogs.com/png.latex?%5Coverline%7B%5Cpi%7D(q,p)">. The resulting algorithm for generating a proposal <img src="https://latex.codecogs.com/png.latex?(q_n,%20p_n)%20%5Cmapsto%20(q_*,%20p_*)"> reads as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%0A%5Cbegin%7Baligned%7D%0A%5Cwidetilde%7Bp%7D_%7Bn+1/2%7D%20&amp;=%20p_n%20+%20%5Cfrac%7B%5Cvarepsilon%7D%7B2%7D%20%5Cnabla%20%5Clog%20%5Cpi(q_n)%5C%5C%0Ap_%7Bn+1/2%7D%20&amp;=%20%20%5Ctextcolor%7Bred%7D%7B%5Ctext%7Borthogonal%20project%20$%5Cwidetilde%7Bp%7D_%7Bn+1/2%7D$%20onto%20$T_%7Bq_n%7D$%7D%7D%20%5C%5C%0A%5Cwidetilde%7Bq%7D_%7B*%7D%20&amp;=%20q_n%20+%20%5Cvarepsilon%5C,%20m%5E%7B-1%7D%20%5C,%20p_%7Bn+1/2%7D%5C%5C%0Aq_%7B*%7D%20&amp;=%20%20%5Ctextcolor%7Bred%7D%7B%5Ctext%7BProj$(%5Cwidetilde%7Bq%7D_%7B*%7D,%20J_%7Bq_n%7D)$%7D%7D%5C%5C%0A%5Coverline%7Bp%7D_%7Bn+1/2%7D%20&amp;=%20%20%5Ctextcolor%7Bred%7D%7B%5Ctext%7Borthogonal%20project%20$(q_%7B*%7D-q_n)%20%5C,%20m%20/%20%5Cvarepsilon$%20onto%20$T_%7Bq_%7B*%7D%7D$%7D%7D%20%5C%5C%0A%5Cwidetilde%7Bp%7D_%7B*%7D%20&amp;=%20%5Coverline%7Bp%7D_%7Bn+1/2%7D%20+%20%5Cfrac%7B%5Cvarepsilon%7D%7B2%7D%20%5Cnabla%20%5Clog%20%5Cpi(q_%7B*%7D)%5C%5C%0Ap_%7B*%7D%20&amp;=%20%20%5Ctextcolor%7Bred%7D%7B%5Ctext%7Borthogonal%20project%20$%5Cwidetilde%7Bp%7D_%7B*%7D$%20onto%20$T_%7Bq_%7B*%7D%7D$%7D%7D.%0A%5Cend%7Baligned%7D%0A%5Cright.%0A"></p>
<p>If any of the projection operations fail, the proposal is rejected. If no failure occurs, a reversibility check is performed by running the algorithm backward starting from <img src="https://latex.codecogs.com/png.latex?(q_*,%20-p_*)">. If the reversibility check is successful, the proposal is accepted with the usual Metropolis-Hastings probability <img src="https://latex.codecogs.com/png.latex?%5Cmin%5Cleft(%201,%20%5Cbar%7B%5Cpi%7D(q_*,%20p_*)/%5Cbar%7B%5Cpi%7D(q_n,%20p_n)%20%5Cright)">.</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/MCMC_on_manifold/HMC_double_torus_compressed.gif" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption class="figure-caption">5k HMC chains ran in parallel: the momentum is not refreshed</figcaption>
</figure>
</div>
</div>
<p>The article <span class="citation" data-cites="lelievre2019hybrid">(Lelièvre, Rousset, and Stoltz 2019)</span> provides a detailed description of several of these ideas along with detailed analysis and extensions.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-au2020manifold" class="csl-entry">
Au, Khai Xiang, Matthew M Graham, and Alexandre H Thiery. 2022. <span>“Manifold Lifting: Scaling MCMC to the Vanishing Noise Regime.”</span> <em>Journal of the Royal Statistical Society: Series B</em>. <a href="https://arxiv.org/abs/2003.03950">https://arxiv.org/abs/2003.03950</a>.
</div>
<div id="ref-barth1995algorithms" class="csl-entry">
Barth, Eric, Krzysztof Kuczera, Benedict Leimkuhler, and Robert D Skeel. 1995. <span>“Algorithms for Constrained Molecular Dynamics.”</span> <em>Journal of Computational Chemistry</em> 16 (10). Wiley Online Library: 1192–1209. <a href="https://doi.org/10.1002/jcc.540161003">https://doi.org/10.1002/jcc.540161003</a>.
</div>
<div id="ref-jax2018github" class="csl-entry">
Bradbury, James, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, et al. 2018. <span>“<span>JAX</span>: Composable Transformations of <span>P</span>ython+<span>N</span>um<span>P</span>y Programs.”</span> <a href="http://github.com/google/jax">http://github.com/google/jax</a>.
</div>
<div id="ref-lelievre2019hybrid" class="csl-entry">
Lelièvre, Tony, Mathias Rousset, and Gabriel Stoltz. 2019. <span>“Hybrid Monte Carlo Methods for Sampling Probability Measures on Submanifolds.”</span> <em>Numerische Mathematik</em> 143 (2). Springer: 379–421. <a href="https://arxiv.org/abs/1807.02356">https://arxiv.org/abs/1807.02356</a>.
</div>
<div id="ref-roberts2001optimal" class="csl-entry">
Roberts, Gareth O, and Jeffrey S Rosenthal. 2001. <span>“Optimal Scaling for Various Metropolis-Hastings Algorithms.”</span> <em>Statistical Science</em> 16 (4). Institute of Mathematical Statistics: 351–67. <a href="https://doi.org/10.1214/ss/1015346320">https://doi.org/10.1214/ss/1015346320</a>.
</div>
<div id="ref-zappa2018monte" class="csl-entry">
Zappa, Emilio, Miranda Holmes-Cerfon, and Jonathan Goodman. 2018. <span>“Monte Carlo on Manifolds: Sampling Densities and Integrating Functions.”</span> <em>Communications on Pure and Applied Mathematics</em> 71 (12). Wiley Online Library: 2609–47. <a href="https://arxiv.org/abs/1702.08446">https://arxiv.org/abs/1702.08446</a>.
</div>
</div></section></div> ]]></description>
  <category>MCMC</category>
  <category>manifold</category>
  <guid>https://alexxthiery.github.io/notes/MCMC_on_manifold/mcmc_manifold.html</guid>
  <pubDate>Fri, 08 Mar 2024 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Metropolis-Hastings ratio with deterministic proposals</title>
  <link>https://alexxthiery.github.io/notes/MCMC_deterministic_proposals/MCMC_deterministic.html</link>
  <description><![CDATA[ 




<p>Consider a probability density <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)"> on <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Ed"> and a (deterministic) function <img src="https://latex.codecogs.com/png.latex?F:%20%5Cmathbb%7BR%7D%5Ed%20%5Cto%20%5Cmathbb%7BR%7D%5Ed">. Assume further that <img src="https://latex.codecogs.com/png.latex?F"> is an <a href="https://en.wikipedia.org/wiki/Involution_(mathematics)">involution</a> in the sense that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AF(F(x))%20=%20x%0A"></p>
<p>for all <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathbb%7BR%7D%5Ed">. To keep things simple since it is not really the point of this short note, suppose that <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)%3E0"> everywhere and that <img src="https://latex.codecogs.com/png.latex?F"> is smooth. This type of transformations can be used to define Markov Chain Monte Carlo algorithms, eg. the standard <a href="https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo">Hamiltonian Monte Carlo (HMC)</a> algorithm. To design a MCMC scheme with this involution <img src="https://latex.codecogs.com/png.latex?F">, one needs to answer the following basic question: suppose that <img src="https://latex.codecogs.com/png.latex?X%20%5Csim%20%5Cpi(dx)"> and the proposal <img src="https://latex.codecogs.com/png.latex?Y%20=%20F(X)"> is constructed and accepted with probability <img src="https://latex.codecogs.com/png.latex?%5Calpha(X)">, how should the acceptance probability function <img src="https://latex.codecogs.com/png.latex?%5Calpha:%20%5Cmathbb%7BR%7D%5Ed%20%5Cto%20%5B0,1%5D"> be chosen so that the resulting random variable <img src="https://latex.codecogs.com/png.latex?Z%20%5C;%20=%20%5C;%20Y%20%5C,%20B%20+%20(1-B)%20%5C,%20X"> is also distributed according to <img src="https://latex.codecogs.com/png.latex?%5Cpi(dx)">? The Bernoulli random variable <img src="https://latex.codecogs.com/png.latex?B"> is such that <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(B=1%7CX=x)=%5Calpha(x)">. In other words, for any test function <img src="https://latex.codecogs.com/png.latex?%5Cvarphi:%20%5Cmathbb%7BR%7D%5Ed%20%5Cto%20%5Cmathbb%7BR%7D">, we would like <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5B%5Cvarphi(Z)%5D%20=%20%5Cmathbb%7BE%7D%5B%5Cvarphi(X)%5D">, which means that</p>
<p><span id="eq-necessary"><img src="https://latex.codecogs.com/png.latex?%0A%5Cint%20%20%7B%5Cleft%5C%7B%20%20%5Cvarphi(F(x))%20%5C,%20%5Calpha(x)%20+%20%5Cvarphi(x)%20%5C,%20(1-%5Calpha(x))%20%20%5Cright%5C%7D%7D%20%20%5C,%20%5Cpi(dx)%20=%20%5Cint%20%5Cvarphi(x)%20%5C,%20%5Cpi(dx).%0A%5Ctag%7B1%7D"></span></p>
<p>Requiring for Equation&nbsp;1 to hold for any test function <img src="https://latex.codecogs.com/png.latex?%5Cvarphi"> is easily seen to be equivalent to asking for the equation</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha(x)%20%5C,%20%5Cpi(x)%20%5C;%20=%20%5C;%20%5Calpha(y)%20%5C,%20%5Cpi(y)%20%5C,%20%7CJ_F(x)%7C%0A"></p>
<p>to hold for any <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathbb%7BR%7D%5Ed"> where <img src="https://latex.codecogs.com/png.latex?y=F(x)"> and <img src="https://latex.codecogs.com/png.latex?J_F(x)"> is the Jacobian of <img src="https://latex.codecogs.com/png.latex?F"> at <img src="https://latex.codecogs.com/png.latex?x">. Since <img src="https://latex.codecogs.com/png.latex?%7CJ_F(y)%7C%20%5Ctimes%20%7CJ_F(x)%7C%20=%201"> because the function <img src="https://latex.codecogs.com/png.latex?F"> is an involution, this also reads</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha(x)%20%5C,%20%5Cfrac%7B%5Cpi(x)%20%7D%7B%7CJ_F(x)%7C%5E%7B1/2%7D%7D%20%5C;%20=%20%5C;%0A%5Calpha(y)%20%5C,%20%5Cfrac%7B%5Cpi(y)%20%7D%7B%7CJ_F(y)%7C%5E%7B1/2%7D%7D.%0A"></p>
<p>At this point, it becomes clear to anyone familiar with the the correctness-proof of the usual <a href="https://en.wikipedia.org/wiki/Metropolis–Hastings_algorithm">Metropolis-Hastings algorithm</a> that a possible solution is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha(x)%20%5C;%20=%20%5C;%20%5Cmin%20%7B%5Cleft%5C%7B%201,%20%5Cfrac%7B%5Cpi(y)%20/%20%7CJ_F(y)%7C%5E%7B1/2%7D%7D%7B%5Cpi(x)%20/%20%7CJ_F(x)%7C%5E%7B1/2%7D%7D%20%5Cright%5C%7D%7D%0A"></p>
<p>although there are indeed many other possible solutions. Since <img src="https://latex.codecogs.com/png.latex?%7CJ_F(y)%7C%20%5Ctimes%20%7CJ_F(x)%7C%20=%201">, this also reads</p>
<p><span id="eq-MH"><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha(x)%20%5C;%20=%20%5C;%20%5Cmin%20%7B%5Cleft%5C%7B%201,%20%5Cfrac%7B%5Cpi(y)%7D%7B%5Cpi(x)%7D%20%5C,%20%7CJ_F(x)%7C%20%5Cright%5C%7D%7D%20.%0A%5Ctag%7B2%7D"></span></p>
<p>One can reach a similar conclusion by looking at the Radon-Nikodym ratio <img src="https://latex.codecogs.com/png.latex?%5B%5Cpi(dx)%20%5Cotimes%20q(x,dy)%5D%20/%20%5B%5Cpi(dy)%20%5Cotimes%20q(y,dx)%5D"> where <img src="https://latex.codecogs.com/png.latex?q(x,dy)"> is the markov kernel described the deterministic transformation <span class="citation" data-cites="green1995reversible">(Green 1995)</span>, but I do not find this approach significantly simpler. The very neat article <span class="citation" data-cites="andrieu2020general">(Andrieu, Lee, and Livingstone 2020)</span> describes much more sophisticated and interesting generalizations. Indeed, Equation&nbsp;2 is often used in the simpler case when <img src="https://latex.codecogs.com/png.latex?F"> is volume preserving, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?%7CJ_F(x)%7C=1">, as is the case for the <a href="https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo">Hamiltonian Monte Carlo (HMC)</a>. The discussion above was prompted by a student implementing a variant of this but with the wrong acceptance ratio <img src="https://latex.codecogs.com/png.latex?%5Calpha(x)%20=%20%5Cmin%20%7B%5Cleft%5C%7B%201,%20%5Cfrac%7B%5Cpi(y)%7D%7B%5Cpi(x)%7D%20%5C,%20%5Cfrac%7B%7CJ_F(x)%7C%7D%7B%7CJ_F(y)%7C%7D%20%5Cright%5C%7D%7D%20"> and us taking quite a bit of time to find the bug…</p>
<p>Note that there are interesting and practical situations when the function <img src="https://latex.codecogs.com/png.latex?F"> satisfies the involution property <img src="https://latex.codecogs.com/png.latex?F(F(x))=x"> only when <img src="https://latex.codecogs.com/png.latex?x"> belongs to a subset of the state-space. For instance, this can happen when implementing MCMC on a manifold <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D%20%5Csubset%20%5Cmathbb%7BR%7D%5Ed"> and the function <img src="https://latex.codecogs.com/png.latex?F"> involves a “projection” on the manifold <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D">, as for example described in the really interesting article <span class="citation" data-cites="zappa2018monte">(Zappa, Holmes-Cerfon, and Goodman 2018)</span>. In that case, it suffices to add a “reversibility check”, i.e.&nbsp;make sure that when applying <img src="https://latex.codecogs.com/png.latex?F"> to the proposal <img src="https://latex.codecogs.com/png.latex?y=F(x)">, one goes back to <img src="https://latex.codecogs.com/png.latex?x"> in the sense that <img src="https://latex.codecogs.com/png.latex?F(y)=x">. The acceptance probability in that case should be amended and expressed as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha(x)%20%5C;%20=%20%5C;%20%5Cmin%20%7B%5Cleft%5C%7B%201,%20%5Cfrac%7B%5Cpi(y)%7D%7B%5Cpi(x)%7D%20%5C,%20%7CJ_F(x)%7C%20%5Cright%5C%7D%7D%20%20%5C,%20%5Cmathbf%7B1%7D%20%7B%5Cleft(%20F(y)=x%20%5Cright)%7D%20.%0A"></p>
<p>In other words, if applying <img src="https://latex.codecogs.com/png.latex?F"> to the proposal <img src="https://latex.codecogs.com/png.latex?y=F(x)"> does not lead back to <img src="https://latex.codecogs.com/png.latex?x">, the proposal is always rejected.</p>
<section id="same-but-without-involution" class="level3">
<h3 class="anchored" data-anchor-id="same-but-without-involution">Same, but without involution</h3>
<p>in some situations, the requirement for <img src="https://latex.codecogs.com/png.latex?F"> to be an involution can seem cumbersome. What if we consider the more general situation of a smooth bijection <img src="https://latex.codecogs.com/png.latex?T:%20%5Cmathbb%7BR%7D%5Ed%20%5Cto%20%5Cmathbb%7BR%7D%5Ed"> and its inverse <img src="https://latex.codecogs.com/png.latex?T%5E%7B-1%7D">? In that case, one can directly apply what has been described in the previous section: it suffices to consider an extended state-space <img src="https://latex.codecogs.com/png.latex?(x,%5Cvarepsilon)"> obtained by including an index <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5Cin%20%5C%7B-1,1%5C%7D"> and the involution <img src="https://latex.codecogs.com/png.latex?F"> defined as</p>
<p><span id="eq-extended"><img src="https://latex.codecogs.com/png.latex?%0AF:%0A%5Cleft%5C%7B%0A%5Cbegin%7Balign%7D%0A(x,%5Cvarepsilon=+1)%20&amp;%5Cmapsto%20(T(x),%20%5Cvarepsilon=-1)%5C%5C%0A(x,%5Cvarepsilon=-1)%20&amp;%5Cmapsto%20(T%5E%7B-1%7D(x),%20%5Cvarepsilon=+1).%0A%5Cend%7Balign%7D%0A%5Cright.%0A%5Ctag%7B3%7D"></span></p>
<p>This allows one to define a Markov kernel that lets the distribution <img src="https://latex.codecogs.com/png.latex?%5Coverline%7B%5Cpi%7D(x,%20%5Cvarepsilon)%20=%20%5Cpi(dx)/2"> invariant. Things can even start to get a bit more interesting if a deterministic “flip” <img src="https://latex.codecogs.com/png.latex?(x,%20%5Cvarepsilon)%20%5Cmapsto%20(x,%20-%5Cvarepsilon)"> is applied after each application of the Markov kernel above describe: doing so avoids immediately coming back to <img src="https://latex.codecogs.com/png.latex?x"> in the event the move <img src="https://latex.codecogs.com/png.latex?(x,%5Cvarepsilon)%20%5Cmapsto%20(T%5E%7B%5Cvarepsilon%7D(x),%20-%5Cvarepsilon)"> is accepted. There are indeed quite a few papers exploiting this type of ideas.</p>
</section>
<section id="a-mixture-of-deterministic-transformations" class="level3">
<h3 class="anchored" data-anchor-id="a-mixture-of-deterministic-transformations">A mixture of deterministic transformations?</h3>
<p>To conclude these notes, here is a small riddle whose answer I do not have. One can check that for any <img src="https://latex.codecogs.com/png.latex?c%20%5Cin%20%5Cmathbb%7BR%7D">, the function <img src="https://latex.codecogs.com/png.latex?F_%7Bc%7D(x)%20=%20c%20+%201/(x-c)"> is an involution of the real line. This means that for any target density <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)"> on the real line, one can build the associated Markov kernel <img src="https://latex.codecogs.com/png.latex?M_c"> defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AM_c(x,%20dy)%20=%20%5Calpha_c(x)%20%5C,%20%5Cdelta_%7BF_c(x)%7D(dy)%20+%20(1-%5Calpha_c(x))%20%5C,%20%5Cdelta_x(dy)%0A"></p>
<p>for an acceptance probability <img src="https://latex.codecogs.com/png.latex?%5Calpha_c(x)"> described as above,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha_c(x)%20=%20%5Cmin%20%7B%5Cleft%5C%7B%201,%20%5Cfrac%7B%5Cpi%5BF_c(x)%5D%7D%7B%5Cpi(x)%7D%20%7CF'_c(x)%7C%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>Finally, choose a <img src="https://latex.codecogs.com/png.latex?N%20%5Cgeq%202"> values <img src="https://latex.codecogs.com/png.latex?c_1,%20%5Cldots,%20c_N%20%5Cin%20%5Cmathbb%7BR%7D"> and consider the mixture of Markov kernels</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AM(x,dy)%20%5C;%20=%20%5C;%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5EN%20M_%7Bc_i%7D(x,%20dy).%0A"></p>
<p>The Markov kernel <img src="https://latex.codecogs.com/png.latex?M(x,%20dy)"> lets the distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi"> invariant since each Markov kernel <img src="https://latex.codecogs.com/png.latex?M_%7Bc_i%7D(x,%20dy)"> does, but it is not clear at all (to me) under what conditions the associated MCMC algorithm does converge to <img src="https://latex.codecogs.com/png.latex?%5Cpi">. One can empirically check that if <img src="https://latex.codecogs.com/png.latex?N"> is very small, things can break down quite easily. On the other, for <img src="https://latex.codecogs.com/png.latex?N"> large, the mixuture of Markov kernels <img src="https://latex.codecogs.com/png.latex?M(x,dy)"> empirically seems to behave as if it were ergodic with respect to <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/MCMC_deterministic_proposals/mcmc_deterministic.png" class="img-fluid figure-img" style="width:80.0%"></p>
</figure>
</div>
</div>
<p>For <img src="https://latex.codecogs.com/png.latex?N=5"> values <img src="https://latex.codecogs.com/png.latex?c_1,%20%5Cldots,%20c_5%20%5Cin%20%5Cmathbb%7BR%7D"> chosen at random, the illustration aboves shows the empirical distribution of the associated Markov chain ran for <img src="https://latex.codecogs.com/png.latex?T=10%5E6"> iterations and targeting the standard Gaussian distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi(dx)%20%5Cequiv%20%5Cmathcal%7BN%7D(0,1)">: the fit seems almost perfect.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-andrieu2020general" class="csl-entry">
Andrieu, Christophe, Anthony Lee, and Sam Livingstone. 2020. <span>“A General Perspective on the Metropolis-Hastings Kernel.”</span> <em>arXiv Preprint arXiv:2012.14881</em>.
</div>
<div id="ref-green1995reversible" class="csl-entry">
Green, Peter J. 1995. <span>“Reversible Jump Markov Chain Monte Carlo Computation and Bayesian Model Determination.”</span> <em>Biometrika</em> 82 (4). Oxford University Press: 711–32.
</div>
<div id="ref-zappa2018monte" class="csl-entry">
Zappa, Emilio, Miranda Holmes-Cerfon, and Jonathan Goodman. 2018. <span>“Monte Carlo on Manifolds: Sampling Densities and Integrating Functions.”</span> <em>Communications on Pure and Applied Mathematics</em> 71 (12). Wiley Online Library: 2609–47.
</div>
</div></section></div> ]]></description>
  <category>auxiliary-variable</category>
  <guid>https://alexxthiery.github.io/notes/MCMC_deterministic_proposals/MCMC_deterministic.html</guid>
  <pubDate>Sun, 17 Dec 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Averaging and homogenization</title>
  <link>https://alexxthiery.github.io/notes/averaging_homogenization/averaging_homogenization.html</link>
  <description><![CDATA[ 




<section id="averaging" class="level3">
<h3 class="anchored" data-anchor-id="averaging">Averaging</h3>
<p>Consider a pair of (coupled) Markov processes <img src="https://latex.codecogs.com/png.latex?X_t%20%5Cin%20%5Cmathcal%7BX%7D"> and <img src="https://latex.codecogs.com/png.latex?Y_t%20%5Cin%20%5Cmathcal%7BY%7D"> with dynamics that can informally be described as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%0A%5Cbegin%7Balign%7D%0AdX%5E%7B%5Cvarepsilon%7D/dt%20&amp;=%20F(X%5E%7B%5Cvarepsilon%7D,Y%5E%7B%5Cvarepsilon%7D,%20W%5EX)%5C%5C%0AdY%5E%7B%5Cvarepsilon%7D/dt%20&amp;=%20%20%5Ctextcolor%7Bred%7D%7B%5Cvarepsilon%5E%7B-1%7D%7D%20%5C,%20G(X%5E%7B%5Cvarepsilon%7D,Y%5E%7B%5Cvarepsilon%7D,%20W%5EY)%5C%5C%0A%5Cend%7Balign%7D%0A%5Cright.%0A"></p>
<p>for two independent “noise” terms <img src="https://latex.codecogs.com/png.latex?W%5EX"> and <img src="https://latex.codecogs.com/png.latex?W%5EY"> and a time-scale parameter <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5Cll%201">. We assume that <img src="https://latex.codecogs.com/png.latex?X"> is a <strong>slow component</strong> that moves by <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(%5Cdelta)"> in on the time interval <img src="https://latex.codecogs.com/png.latex?%5Bt,%20t+%5Cdelta%5D">. The scaling <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5E%7B-1%7D"> in the dynamics of <strong>fast process</strong> <img src="https://latex.codecogs.com/png.latex?Y%5E%7B%5Cvarepsilon%7D"> indicates that we expect the process <img src="https://latex.codecogs.com/png.latex?Y"> to evolve on a time scale of order <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(%5Cvarepsilon)">. We are interested in the limit <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5Cto%200"> and hope to “average out” the fast process <img src="https://latex.codecogs.com/png.latex?Y%5E%7B%5Cvarepsilon%7D"> and be able to describe the slow (and interesting) process <img src="https://latex.codecogs.com/png.latex?X%5E%7B%5Cvarepsilon%7D"> without referring to the fast process. Informally, we would like to describe the process <img src="https://latex.codecogs.com/png.latex?X%5E%7B%5Cvarepsilon%7D">, in the limit <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5Cto%200">, as following an effective Markovian dynamics</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX/dt%20=%20%5Coverline%7BF%7D(X,%20W%5EX).%0A"></p>
<p>For describing the averaging phenomenon, we typically assume some ergodicity conditions on the fast process <img src="https://latex.codecogs.com/png.latex?Y">. Here, we assume that for each fixed <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathcal%7BX%7D">, the fast process process <img src="https://latex.codecogs.com/png.latex?Y%5E%7B%5Bx%5D%7D"> with fixed slow-component <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathcal%7BX%7D">, i.e.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdY%5E%7B%5Bx%5D%7D/dt%20=%20G(x,Y%5E%7B%5Bx%5D%7D,%20W%5EY)%0A"></p>
<p>is ergodic with respect to some probability distribution <img src="https://latex.codecogs.com/png.latex?%5Crho_x(dy)">. Although the averaging phenomenon is quite general, it is somewhat easier to illustrate it for diffusion processes. In this case, let us assume that the slow process is given by</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX%5E%7B%5Cvarepsilon%7D%20=%20%5Cmu(X%5E%7B%5Cvarepsilon%7D,%20Y%5E%7B%5Cvarepsilon%7D)%20%5C,%20dt%20+%20%5Csigma(X%5E%7B%5Cvarepsilon%7D,%20Y%5E%7B%5Cvarepsilon%7D)%20%5C,%20dW%5Ex.%0A"></p>
<p>For <img src="https://latex.codecogs.com/png.latex?X%5E%7B%5Cvarepsilon%7D_%7Bt%7D%20=%20x"> and for a time increment <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Cll%201">, since the process <img src="https://latex.codecogs.com/png.latex?X%5E%7B%5Cvarepsilon%7D"> can be considered constant we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AX%5E%7B%5Cvarepsilon%7D_%7Bt+%5Cdelta%7D%20-%20x%0A&amp;%5Capprox%20%5C;%0A%7B%5Cleft(%20%20%5Cfrac%7B%20%5Cint_%7Bt%7D%5E%7Bt+%5Cdelta%7D%20%5Cmu(x,%20Y%5E%7B%5Cvarepsilon%7D)%20%5C,%20dt%7D%7B%5Cdelta%7D%20%20%5Cright)%7D%20%20%5C,%20%5Cdelta%20+%20%5C%0A%7B%5Cleft(%20%20%5Cfrac%7B%20%5Cint_%7Bt%7D%5E%7Bt+%5Cdelta%7D%20%5Csigma%5E2(x,%20Y%5E%7B%5Cvarepsilon%7D)%20%7D%7B%5Cdelta%7D%20%5Cright)%7D%20%5E%7B1/2%7D%20%5C,%20%5Cmathcal%7BN%7D(0,%20%5Cdelta).%0A%5Cend%7Balign%7D%0A"></p>
<p>This can be regarded as a time-discretization of the <strong>averaged process</strong></p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX%20%5C,%20=%20%5C;%20%5Coverline%7B%5Cmu%7D(X)%20%5C,%20dt%20+%20%5Coverline%7B%5Csigma%7D(X)%20%5C,%20dW%0A"></p>
<p>for averaged drift and volatility functions give by</p>
<p><span id="eq-av-diff"><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%0A%5Cbegin%7Balign%7D%0A%5Coverline%7B%5Cmu%7D(x)%0A&amp;=%0A%5Cint%20%5Cmu(x,y)%20%5C,%20%5Crho_x(dy)%20%5C%5C%0A%5Coverline%7B%5Csigma%7D%5E2(x)%0A&amp;=%20%5Cint%20%5Csigma%5E2(x,y)%20%5C,%20%5Crho_x(dy).%0A%5Cend%7Balign%7D%0A%5Cright.%0A%5Ctag%7B1%7D"></span></p>
<p>One standard approach for proving this type of results is to write the Kolmogorov equations</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%7D%7Bdt%7D%5Cvarphi%5E%7B%5Cvarepsilon%7D(x,y,t)%20=%20%5Cmathcal%7BL%7D%5E%7B%5Cvarepsilon%7D%20%5Cvarphi%5E%7B%5Cvarepsilon%7D(x,y,t)"> for <img src="https://latex.codecogs.com/png.latex?%5Cvarphi%5E%7B%5Cvarepsilon%7D(x,y,t)%20=%20%5Cmathbb%7BE%7D%5B%5Cvarphi(X%5E%7B%5Cvarepsilon%7D_%7Bt%7D,%20Y%5E%7B%5Cvarepsilon%7D_%7Bt%7D,%20t)%20%7C%20X%5E%7B%5Cvarepsilon%7D_%7B0%7D=x,%20Y%5E%7B%5Cvarepsilon%7D_%7B0%7D=y%5D"> and perform a <a href="https://en.wikipedia.org/wiki/Method_of_matched_asymptotic_expansions">multiscale expansion</a> <span class="citation" data-cites="hinch_1991">(Hinch 1991)</span> <span class="citation" data-cites="pavliotis2008multiscale">(Pavliotis and Stuart 2008)</span> <span class="citation" data-cites="weinan2011principles">(Weinan 2011)</span></p>
<p><span id="eq-multiscale"><img src="https://latex.codecogs.com/png.latex?%0A%5Cvarphi%5E%7B%5Cvarepsilon%7D(x,y,t)%0A=%0AA(x,t)%20+%20%5Cvarepsilon%20B(x,y,t)%20+%20%5Cmathcal%7BO%7D(%5Cvarepsilon%5E2).%0A%5Ctag%7B2%7D"></span></p>
<p>Indeed, the first order term <img src="https://latex.codecogs.com/png.latex?A(x,t)"> is expected to not depend on the initial condition <img src="https://latex.codecogs.com/png.latex?y"> since the process <img src="https://latex.codecogs.com/png.latex?(X%5E%7B%5Cvarepsilon%7D_t,%20Y%5E%7B%5Cvarepsilon%7D_t)"> forgets <img src="https://latex.codecogs.com/png.latex?Y%5E%7B%5Cvarepsilon%7D_0%20=%20y"> on time scales of order <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon"> and we are interested in the regime <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5Cto%200">. From Equation&nbsp;2 one can obtain the dynamics of the averaged process described by the function <img src="https://latex.codecogs.com/png.latex?A(x,t)">. One finds that <img src="https://latex.codecogs.com/png.latex?A"> is described by the averaged generator of the slow component, i.e.&nbsp;averaging <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D%5E%7BX%5E%7B%5Cvarepsilon%7D%7D"> under <img src="https://latex.codecogs.com/png.latex?%5Crho_x(dy)">; this exactly gives Equation&nbsp;1 in the case of diffusions. A typical example could be as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%0A%5Cbegin%7Balign%7D%0AdX%5E%7B%5Cvarepsilon%7D%20&amp;=%20%5Cmu(X%5E%7B%5Cvarepsilon%7D,%20Y%5E%7B%5Cvarepsilon%7D)%20dt%20+%20%5Csigma(X%5E%7B%5Cvarepsilon%7D,%20Y%5E%7B%5Cvarepsilon%7D)%20%5C,%20dW%5EX%5C%5C%0AdY%5E%7B%5Cvarepsilon%7D%20&amp;=%20-%20%5Ctextcolor%7Bred%7D%7B%5Cvarepsilon%5E%7B-1%7D%7D%20%5Cfrac%7B%20(Y%5E%7B%5Cvarepsilon%7D%20-%20X%5E%7B%5Cvarepsilon%7D)%20%7D%7B%5Csigma%5E2%7D%20%5C,%20dt%20+%20%5Csqrt%7B2%20%20%5Ctextcolor%7Bred%7D%7B%5Cvarepsilon%5E%7B-1%7D%7D%20%7D%20%5C,%20dW%5EY.%5C%5C%0A%5Cend%7Balign%7D%0A%5Cright.%0A"></p>
<p>The fast process <img src="https://latex.codecogs.com/png.latex?Y%5E%7B%5Cvarepsilon%7D_t"> is a <a href="https://en.wikipedia.org/wiki/Ornstein–Uhlenbeck_process">Ornstein-Uhlenbeck</a> process sped-up by a factor <img src="https://latex.codecogs.com/png.latex?1/%5Cvarepsilon"> that will very rapidly oscillate around <img src="https://latex.codecogs.com/png.latex?X%5E%7B%5Cvarepsilon%7D_t">, with Gaussian fluctuations with variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2%3E0">, ie:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Crho_x(dy)%20%5C;%20=%20%5C;%20%5Cfrac%7B%20e%5E%7B-(y-x)%5E2/2%7D%20%7D%7B%5Csqrt%7B2%5Cpi%20%5Csigma%5E2%7D%7D%5C,%20dy.%0A"></p>
<p>This averaging phenomenon is relatively straightforward and not extremely surprising. More interesting is the homogenization phenomenon described in the next Section.</p>
</section>
<section id="homogenization" class="level3">
<h3 class="anchored" data-anchor-id="homogenization">Homogenization</h3>
<p>Consider the presence of an additional intermediate time scale <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5E%7B-1/2%7D">, <img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%0A%5Cbegin%7Balign%7D%0AdX%5E%7B%5Cvarepsilon%7D/dt%20&amp;=%20%20%5Ctextcolor%7Bblue%7D%7B%5Cvarepsilon%5E%7B-1/2%7D%20H(X%5E%7B%5Cvarepsilon%7D,Y%5E%7B%5Cvarepsilon%7D)%7D%20%5C,%20+%20%5C,F(X%5E%7B%5Cvarepsilon%7D,Y%5E%7B%5Cvarepsilon%7D,%20W%5EX)%5C%5C%0AdY%5E%7B%5Cvarepsilon%7D/dt%20&amp;=%20%20%5Ctextcolor%7Bred%7D%7B%5Cvarepsilon%5E%7B-1%7D%7D%20%5C,%20G(X%5E%7B%5Cvarepsilon%7D,Y%5E%7B%5Cvarepsilon%7D,%20W%5EY)%5C%5C%0A%5Cend%7Balign%7D%0A%5Cright.%0A"> with the same assumption that for any fixed <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathcal%7BX%7D"> the process <img src="https://latex.codecogs.com/png.latex?dY%5E%7B%5Bx%5D%7D/dt%20=%20G(x,Y%5E%7B%5Bx%5D%7D,%20W%5EY)"> is ergodic with respect to the probability distribution <img src="https://latex.codecogs.com/png.latex?%5Crho_x(dy)">. The same reasoning as in the averaging case shows that averaging the term <img src="https://latex.codecogs.com/png.latex?F(X%5E%7B%5Cvarepsilon%7D,Y%5E%7B%5Cvarepsilon%7D,%20W%5EX)"> is relatively straightforward and has the exact same expression: it suffices to average under <img src="https://latex.codecogs.com/png.latex?%5Crho_x(dy)">. This means that one can study instead</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%0A%5Cbegin%7Balign%7D%0AdX%5E%7B%5Cvarepsilon%7D/dt%20&amp;=%20%20%5Ctextcolor%7Bblue%7D%7B%5Cvarepsilon%5E%7B-1/2%7D%20H(X%5E%7B%5Cvarepsilon%7D,Y%5E%7B%5Cvarepsilon%7D)%7D%20%5C,%20+%20%5C,%20%5Coverline%7BF%7D(X%5E%7B%5Cvarepsilon%7D,%20W%5EX)%5C%5C%0AdY%5E%7B%5Cvarepsilon%7D/dt%20&amp;=%20%20%5Ctextcolor%7Bred%7D%7B%5Cvarepsilon%5E%7B-1%7D%7D%20%5C,%20G(X%5E%7B%5Cvarepsilon%7D,Y%5E%7B%5Cvarepsilon%7D,%20W%5EY)%5C%5C%0A%5Cend%7Balign%7D%0A%5Cright.%0A"></p>
<p>with, informally, <img src="https://latex.codecogs.com/png.latex?%5Coverline%7BF%7D(x,w)%20=%20%5Cint%20F(x,y,w)%20%5C,%20%5Crho_x(dy)">. The new interesting phenomenon is coming from the intermediate time scale <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5E%7B-1/2%7D">. Contrarily to the averaging phenomenon of the previous section that was only relying on a Law of Large Numbers, dealing with the intermediate time-scale requires exploiting a CLT and quantifying the rate of mixing of the fast process <img src="https://latex.codecogs.com/png.latex?Y%5E%7B%5Bx%5D%7D"> Note that since <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5E%7B-1/2%7D%20%5Cgg%201">, for the dynamics to not explode one needs the <strong>centering condition</strong>:</p>
<p><span id="eq-centering"><img src="https://latex.codecogs.com/png.latex?%0A%5Cint_%7B%5Cmathcal%7BY%7D%7D%20H(x,y)%20%5C,%20%5Crho_x(dy)%20=%200%0A%5Cqquad%20%5Ctextrm%7Bfor%20all%20%7D%20x%20%5Cin%20%5Cmathcal%7BX%7D.%0A%5Ctag%7B3%7D"></span></p>
<p>Because of the centering condition*, the term <img src="https://latex.codecogs.com/png.latex?%20%5Ctextcolor%7Bblue%7D%7B%5Cvarepsilon%5E%7B-1/2%7D%20H(X%5E%7B%5Cvarepsilon%7D,Y%5E%7B%5Cvarepsilon%7D)%7D"> will contribute an additional noise term in the effective dynamics of the slow process. To describe this additional noise term, assume an ergodic central limit theorem (CLT) for the fast process <img src="https://latex.codecogs.com/png.latex?dY%5E%7B%5Bx%5D%7D/dt%20=%20G(x,Y%5E%7B%5Bx%5D%7D,%20W%5EY)">: for a test function <img src="https://latex.codecogs.com/png.latex?%5Cvarphi:%20%5Cmathcal%7BY%7D%5Cto%20%5Cmathbb%7BR%7D"> with zero expectation under <img src="https://latex.codecogs.com/png.latex?%5Crho_x(dy)"> we have:</p>
<p><span id="eq-CLT"><img src="https://latex.codecogs.com/png.latex?%0A%5Clim_%7Bt%20%5Cto%20%5Cinfty%7D%20%5C;%20T%5E%7B-1/2%7D%0A%5Cint_%7Bt=0%7D%5ET%20%5C,%20%5Cvarphi(Y%5E%7B%5Bx%5D%7D_t)%20%5C,%20dt%0A%5C;%20=%20%5C;%20%5Cmathcal%7BN%7D(0,%20V_x%5B%5Cvarphi%5D)%0A%5Ctag%7B4%7D"></span></p>
<p>for asymptotic variance <img src="https://latex.codecogs.com/png.latex?V_x%5B%5Cvarphi%5D%20%5Cgeq%200">. For a time increment <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%3E%200"> and assuming <img src="https://latex.codecogs.com/png.latex?X%5E%7B%5Cvarepsilon%7D_%7Bt%7D=x"> we have</p>
<p><span id="eq-split"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AX%5E%7B%5Cvarepsilon%7D_%7Bt+%5Cdelta%7D%20-%20x%0A&amp;%5Capprox%20%20%5Ctextcolor%7Bblue%7D%7B%20%5Cvarepsilon%5E%7B-1/2%7D%20%5C,%20%5Cint_%7Bu=t%7D%5E%7Bt+%5Cdelta%7D%20H(X%5E%7B%5Cvarepsilon%7D_u,Y%5E%7B%5Cvarepsilon%7D_u)%7D%20%5C,%20du%20%5C,%20+%20%5C,%20%5Cint_%7Bu=t%7D%5E%7Bt+%5Cdelta%7D%20%5Coverline%7BF%7D(x,%20W%5EX_u)%20%5C,%20du.%0A%5Cend%7Balign%7D%0A%5Ctag%7B5%7D"></span></p>
<p>The second integral term is an averaging term that can be treated easily. Approximating the process <img src="https://latex.codecogs.com/png.latex?t%20%5Cmapsto%20Y%5E%7B%5Cvarepsilon%7D_t"> by <img src="https://latex.codecogs.com/png.latex?t%20%5Cmapsto%20Y%5E%7B%5Bx%5D%7D_%7Bt%20%5Cvarepsilon%5E%7B-1%7D%7D">, the first integral on the RHS of Equation&nbsp;5 can be approximated as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cunderbrace%7B%5Cvarepsilon%5E%7B-1/2%7D%20%5Cint_%7Bu=t%7D%5E%7Bt+%5Cdelta%7D%20%20H(x,Y%5E%7B%5Bx%5D%7D_%7Bu%20%5C,%20%5Cvarepsilon%5E%7B-1%7D%7D)%20%5C,%20du%7D_%7B%5Ctextrm%7BCLT%7D%7D%20%5C,%0A+%0A%5Cunderbrace%7B%5Cint_%7Bu=t%7D%5E%7Bt+%5Cdelta%7D%20%5Cvarepsilon%5E%7B-1/2%7D%20%5Cpartial_x%20H(x,Y%5E%7B%5Bx%5D%7D_%7Bu%20%5C,%20%5Cvarepsilon%5E%7B-1%7D%7D)%20%5C,%20(X%5E%7B%5Cvarepsilon%7D_u%20-%20x)%20%5C,%20%5C,%20du%7D_%7B%5Ctextrm%7B(drift)%7D%7D.%0A%5Cend%7Balign%7D%0A"></p>
<p>After a time-rescaling, one can readily see that the first term is described by the CLT of Equation&nbsp;4,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cvarepsilon%5E%7B-1/2%7D%20%5Cint_%7Bu=t%7D%5E%7Bt+%5Cdelta%7D%20%20H(x,Y%5E%7B%5Bx%5D%7D_%7Bu%20%5C,%20%5Cvarepsilon%5E%7B-1%7D%7D)%20%5C,%20du%0A%5Capprox%20V_x%5BH(x,%20%5Ccdot)%5D%5E%7B1/2%7D%20%5Cmathcal%7BN%7D(0,%20%5Cdelta).%0A"></p>
<p>The second term is further approximated as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cvarepsilon%5E%7B-1%7D%20%5C,%20&amp;%5Cint_%7Bu=t%7D%5E%7Bt+%5Cdelta%7D%5Cint_%7Bv=t%7D%5E%7Bt+%5Cdelta%7D%0A%5Cpartial_x%20H(x,Y%5E%7B%5Bx%5D%7D_%7Bu%20%5C,%20%5Cvarepsilon%5E%7B-1%7D%7D)%20%5C,%20H(x,Y%5E%7B%5Bx%5D%7D_%7Bv%20%5C,%20%5Cvarepsilon%5E%7B-1%7D%7D)%20%5C,%201_%7Bv%3Cu%7D%20%5C,%20du%20%5C,%20dv%5C%5C%0A&amp;=%20%20%7B%5Cleft(%20%20%5Cfrac%7B1%7D%7B%5Cdelta%20%5Cvarepsilon%5E%7B-1%7D%7D%20%5Cint_%7Bu=t%7D%5E%7Bt+%5Cdelta%20%5Cvarepsilon%5E%7B-1%7D%7D%5Cint_%7Bv=t%7D%5E%7Bt+%5Cdelta%20%5Cvarepsilon%5E%7B-1%7D%7D%20%5Cpartial_x%20H(x,Y%5E%7B%5Bx%5D%7D_%7Bu%7D)%20%5C,%20H(x,Y%5E%7B%5Bx%5D%7D_%7Bv%7D)%20%5C,%201_%7Bv%3Cu%7D%20%20%5Cright)%7D%20%20%5C,%20%5Cdelta,%0A%5Cend%7Balign%7D%0A"></p>
<p>the second equality coming from the time-rescaling <img src="https://latex.codecogs.com/png.latex?t%20%5Cmapsto%20t%20%5Cvarepsilon">. The process <img src="https://latex.codecogs.com/png.latex?Y%5E%7B%5Bx%5D%7D"> mixes on scale <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(1)"> so that the term inside bracket <img src="https://latex.codecogs.com/png.latex?%20%7B%5Cleft(%20%5Cldots%20%5Cright)%7D%20"> converges to its expectation. Setting <img src="https://latex.codecogs.com/png.latex?T%20=%20%5Cdelta%20%5C,%20%5Cvarepsilon%5E%7B-1%7D%20%5Cto%20%5Cinfty">, one obtains</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AI(x)%20&amp;=%0A%5Cfrac%7B1%7D%7BT%7D%20%5Ciint_%7B%5B0,T%5D%5E2%7D%0A%5Cpartial_x%20H(x,Y%5E%7B%5Bx%5D%7D_%7Bu%7D)%20%5C,%20H(x,Y%5E%7B%5Bx%5D%7D_%7Bv%7D)%20%5C,%201_%7Bv%3Cu%7D%20%20%5C,%20du%20%5C,%20dv%20%5C%5C%0A&amp;%5Cto%0A%5Cint%20%5Crho_x(dy)%20%5C,%20H(x,y)%20%5C,%20%20%20%7B%5Cleft%5C%7B%20%20%5Cint_%7Bs=0%7D%5E%7B%5Cinfty%7D%20%5Cmathbb%7BE%7D%5B%5Cpartial_x%20H(%5Chat%7Bx%7D,%20Y%5E%7B%5Bx%5D%7D_s)%20%5C,%20%7C%20%20Y%5E%7B%5Bx%5D%7D_0=y%5D%20%5C,%20ds%20%20%5Cright%5C%7D%7D%20.%0A%5Cend%7Balign%7D%0A"></p>
<p>In conclusion, the fast-slow system</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%0A%5Cbegin%7Balign%7D%0AdX%5E%7B%5Cvarepsilon%7D%20&amp;=%20%20%5Ctextcolor%7Bblue%7D%7B%5Cvarepsilon%5E%7B-1/2%7D%20%5C,%20H(X%5E%7B%5Cvarepsilon%7D,%20Y%5E%7B%5Cvarepsilon%7D)%7D%20%5C,%20dt%20+%20%5Cmu(X%5E%7B%5Cvarepsilon%7D,%20Y%5E%7B%5Cvarepsilon%7D)%20%5C,%20dt%20+%20%5Csigma(X%5E%7B%5Cvarepsilon%7D,%20Y%5E%7B%5Cvarepsilon%7D)%20%5C,%20dW%5Ex%5C%5C%0AdY%5E%7B%5Cvarepsilon%7D%20&amp;=%20%20%5Ctextcolor%7Bred%7D%7B%5Cvarepsilon%5E%7B-1%7D%7D%20%5C,%20G(X%5E%7B%5Cvarepsilon%7D,Y%5E%7B%5Cvarepsilon%7D,%20W%5EY)%20%5C,%20dt%0A%5Cend%7Balign%7D%0A%5Cright.%0A"></p>
<p>can be described in the regime <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5Cto%200"> by the effective dynamics</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX%20=%20%20%5Ctextcolor%7Bblue%7D%7BI(X)%20%5C,%20dt%20+%20%5CGamma%5E%7B1/2%7D(X)%20%5C,%20dW%5E%7BH%7D%7D%0A+%0A%5Coverline%7B%5Cmu%7D(X)%20%5C,%20dt%20+%20%5Coverline%7B%5Csigma%7D(X)%20%5C,%20dW%5EX.%0A"></p>
<p>for two independent Brownian motions <img src="https://latex.codecogs.com/png.latex?W%5EX"> and <img src="https://latex.codecogs.com/png.latex?W%5EH">. The volatility terms <img src="https://latex.codecogs.com/png.latex?%20%5Ctextcolor%7Bblue%7D%7B%5CGamma(x)%7D"> comes from the CLT and the drift term <img src="https://latex.codecogs.com/png.latex?%20%5Ctextcolor%7Bblue%7D%7BI(x)%7D"> comes from the self-interaction term:</p>
<p><span id="eq-homogenized-terms"><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%0A%5Cbegin%7Balign%7D%0A%5CGamma(x)%0A&amp;=%20%5Clim_%7BT%20%5Cto%20%5Cinfty%7D%20%5C;%20%5Cmathop%7B%5Cmathrm%7BVar%7D%7D%20%7B%5Cleft%5C%7B%20T%5E%7B-1/2%7D%20%5Cint_%7B0%7D%5ET%20H(x,%20Y%5E%7B%5Bx%5D%7D_t)%20%5C,%20dt%20%5Cright%5C%7D%7D%20%5C%5C%0A%25%0AI(x)%0A&amp;=%20%5Clim_%7BT%20%5Cto%20%5Cinfty%7D%20%5C;%20%5Cfrac%7B1%7D%7BT%7D%20%5Ciint_%7B0%3Cu%3Cv%3CT%7D%20H(x,%20Y%5E%7B%5Bx%5D%7D_u)%20%5C,%20%5Cpartial_x%20H(x,%20Y%5E%7B%5Bx%5D%7D_v)%20%5C,%20du%20%5C,%20dv.%0A%5Cend%7Balign%7D%0A%5Cright.%0A%5Ctag%7B6%7D"></span></p>
<p>For the drift function, the scaling <img src="https://latex.codecogs.com/png.latex?T%5E%7B-1%7D%20%5Cint_%7B0%3Cu%3Cv%3CT%7D%20(%5Cldots)"> may look a bit surprising at first sight as one may expect <img src="https://latex.codecogs.com/png.latex?T%5E%7B-2%7D%20%5Cint_%7B0%3Cu%3Cv%3CT%7D%20(%5Cldots)"> instead. Note that since the process <img src="https://latex.codecogs.com/png.latex?Y%5E%7B%5Bx%5D%7D"> mixes on a time scale <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(1)"> and the centering condition <img src="https://latex.codecogs.com/png.latex?%5Cint%20H(x,%20y)%20%5Crho_x(dy)=0"> holds, the expectation <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BH(x,%20Y%5E%7B%5Bx%5D%7D_u)%20%5C,%20%5Cpartial_x%20H(x,%20Y%5E%7B%5Bx%5D%7D_v)%5D"> goes to zero as soon as <img src="https://latex.codecogs.com/png.latex?%7Cu-v%7C%20%5Cgg%201">. This means that only the subset <img src="https://latex.codecogs.com/png.latex?%7Cu-v%7C%20=%20%5Cmathcal%7BO%7D(1)"> of <img src="https://latex.codecogs.com/png.latex?%5B0,T%5D%5E2"> really matters in that double integral, hence the <img src="https://latex.codecogs.com/png.latex?(1/T)"> normalization factor.</p>
</section>
<section id="closed-form-solution-poisson-equation" class="level3">
<h3 class="anchored" data-anchor-id="closed-form-solution-poisson-equation">Closed form solution &amp; Poisson equation:</h3>
<p>The drift and volatility terms <img src="https://latex.codecogs.com/png.latex?%5CGamma(x)"> and <img src="https://latex.codecogs.com/png.latex?I(x)"> quantify the mixing properties of the fast process <img src="https://latex.codecogs.com/png.latex?Y%5E%7B%5Bx%5D%7D">. While formulas Equation&nbsp;6 are intuitive, they can be difficult to deal with if one needs the exact expressions of the drift and volatility functions. Instead, they can also be expressed in terms of the solution to an appropriate <a href="../../notes/Poisson_Eq_Asymp_Var/Poisson_Eq_Asymp_Var.html">Poisson equations</a>.</p>
<p><span id="eq-poisson"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AI(x)%20&amp;=%0A%5Cfrac%7B1%7D%7BT%7D%20%5Ciint_%7B%5B0,T%5D%5E2%7D%0AH(x,Y%5E%7B%5Bx%5D%7D_%7Bv%7D)%20%5C,%20%5Cpartial_x%20H(x,Y%5E%7B%5Bx%5D%7D_%7Bu%7D)%20%5C,%201_%7Bv%3Cu%7D%20%20%5C,%20du%20%5C,%20dv%20%5C%5C%0A&amp;%5Cto%0A%5Cint%20%5Crho_x(dy)%20%5C,%20H(x,y)%20%5C,%20%5Cpartial_%7B%5Chat%7Bx%7D%7D%20%20%7B%5Cleft%5C%7B%20%20%5Cint_%7Bs=0%7D%5E%7B%5Cinfty%7D%20%5Cmathbb%7BE%7D%5BH(%5Chat%7Bx%7D,%20Y%5E%7B%5Bx%5D%7D_s)%20%5C,%20%7CY%5E%7B%5Bx%5D%7D_0=y%5D%20%5C,%20ds%20%20%5Cright%5C%7D%7D%20%5C%5C%0A&amp;=%0A-%5Cint%20%5Crho_x(dy)%20%5C,%20H(x,y)%20%5C,%20%5Cpartial_%7Bx%7D%20%5CPhi(x,y)%5C%5C%0A&amp;=%20-%5Cleft%3C%20H(x,%20%5Ccdot),%20%5Cpartial_x%20%5CPhi(x,%20%5Ccdot)%20%5Cright%3E_%7B%5Crho_x%7D%0A%5Cend%7Balign%7D%0A%5Ctag%7B7%7D"></span></p>
<p>where the function <img src="https://latex.codecogs.com/png.latex?%5CPhi(x,y)"> is solution to the <a href="../../notes/Poisson_Eq_Asymp_Var/Poisson_Eq_Asymp_Var.html">Poisson equation</a></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D%5E%7BY%5E%7B%5Bx%5D%7D%7D%20%5CPhi(x,%20%5Ccdot)%20=%20H(x,%20%5Ccdot)%0A"></p>
<p>for all <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathcal%7BX%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D%5E%7BY%5E%7B%5Bx%5D%7D%7D"> is the generator of the fast process <img src="https://latex.codecogs.com/png.latex?dY%5E%7B%5Bx%5D%7D/dt%20=%20G(x,Y%5E%7B%5Bx%5D%7D,%20W%5EY)">. The last equality in Equation&nbsp;7 follows from the <a href="../../notes/Poisson_Eq_Asymp_Var/Poisson_Eq_Asymp_Var.html">integral representation</a> of the Poisson equation. Similarly, and also as explained <a href="../../notes/Poisson_Eq_Asymp_Var/Poisson_Eq_Asymp_Var.html">here</a>, the asymptotic variance term can also be expressed in terms of the function <img src="https://latex.codecogs.com/png.latex?%5CPhi">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5CGamma(x)%0A&amp;=%20%5Clim_%7BT%20%5Cto%20%5Cinfty%7D%20%5C;%20%5Cmathop%7B%5Cmathrm%7BVar%7D%7D%20%7B%5Cleft%5C%7B%20T%5E%7B-1/2%7D%20%5Cint_%7B0%7D%5ET%20H(x,%20Y%5E%7B%5Bx%5D%7D_t)%20%5C,%20dt%20%5Cright%5C%7D%7D%20%5C%5C%0A&amp;=%20-2%20%5Cint_%7B%5Cmathcal%7BY%7D%7D%20%5CPhi(x,%20y)%20%5C,%20H(x,%20y)%20%5C,%20%5Crho_x(dy)%5C%5C%0A&amp;=%20-2%20%5Cleft%3C%20%5CPhi,%20%5Cmathcal%7BL%7D%5E%7BY%5E%7B%5Bx%5D%7D%7D%20%5CPhi%20%5Cright%3E_%7B%5Crho_x%7D.%0A%5Cend%7Balign%7D%0A"></p>
</section>
<section id="example-integrated-ou-process" class="level3">
<h3 class="anchored" data-anchor-id="example-integrated-ou-process">Example: integrated OU process</h3>
<p>Consider a slow process obtained by integrating an OU process,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%0A%5Cbegin%7Balign%7D%0AdX%5E%7B%5Cvarepsilon%7D%20&amp;=%20%5Cvarepsilon%5E%7B-1/2%7D%20%5C,%20Y%5E%7B%5Cvarepsilon%7D%20%5C,%20dt%5C%5C%0AdY%5E%7B%5Cvarepsilon%7D%20&amp;=%20-%5Clambda%20%5Cvarepsilon%5E%7B-1%7D%5C,%20Y%5E%7B%5Cvarepsilon%7D%20%5C,%20dt%20+%20%5Csqrt%7B2%20%5Clambda/%5Cvarepsilon%7D%20%5C,%20dW%5EY,%0A%5Cend%7Balign%7D%0A%5Cright.%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Clambda%20%3E%200"> is just a fixed time-scaling parameter. The fast OU process mixes on time scales of order <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(%5Cvarepsilon)"> and has a standard Gaussian distribution as invariant distribution. Homogenization gives that in the regime <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5Cto%200">, the slow process can be approximated as</p>
<p><span id="eq-integ-OU"><img src="https://latex.codecogs.com/png.latex?%0AdX%20=%20%5Csqrt%7B2/%5Clambda%7D%20%5C,%20dW%0A%5Ctag%7B8%7D"></span></p>
<p>since the asymptotic variance is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathop%7B%5Cmathrm%7BVar%7D%7D%20%7B%5Cleft%5C%7B%20%20T%5E%7B-1/2%7D%20%5Cint_%7Bt=0%7D%5E%7BT%7D%20Y_t%20%5C,%20dt%20%5Cright%5C%7D%7D%0A%5Cto%0A2%20%5C,%20%5Cint_%7B0%7D%5E%7B%5Cinfty%7D%20C(r)%20%5C,%20dr%20=%202/%5Clambda%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?C(r)%20=%20%5Cmathbb%7BE%7D%5BY_t%20Y_%7Bt+r%7D%5D%20=%20%5Cexp%5B-%5Clambda%20r%5D"> is the autocorrelation function of the fast OU process, as explained <a href="../../notes/Poisson_Eq_Asymp_Var/Poisson_Eq_Asymp_Var.html">here</a>. The fact that the effective diffusion is (twice) the integrated autocorrelation of the fast process is an example of <a href="https://en.wikipedia.org/wiki/Green–Kubo_relations">Green-Kubo relations</a>.</p>
</section>
<section id="example-overdamped-langevin-dynamics" class="level3">
<h3 class="anchored" data-anchor-id="example-overdamped-langevin-dynamics">Example: Overdamped Langevin Dynamics</h3>
<p>This example does not exactly fall within the homogenization result described in the previous section, but almost. Consider a potential <img src="https://latex.codecogs.com/png.latex?U"> and the slow-fast dynamics:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%0A%5Cbegin%7Balign%7D%0AdX%5E%7B%5Cvarepsilon%7D%20&amp;=%20%5Cvarepsilon%5E%7B-1/2%7D%20%5C,%20Y%5E%7B%5Cvarepsilon%7D%20%5C,%20dt%5C%5C%0AdY%5E%7B%5Cvarepsilon%7D%20&amp;=%20-%20%5Cvarepsilon%5E%7B-1%7D%5C,%20%5BY%5E%7B%5Cvarepsilon%7D+%5Cvarepsilon%5E%7B1/2%7D%5Cnabla%20U(X%5E%7B%5Cvarepsilon%7D)%5D%20%5C,%20dt%20+%20%5Csqrt%7B2%20/%5Cvarepsilon%7D%20%5C,%20dW%5EY.%0A%5Cend%7Balign%7D%0A%5Cright.%0A"></p>
<p>For any fixed value of <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathcal%7BX%7D">, the fast OU-dynamics</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdY%20=%20-%5BY%5E%7B%5Cvarepsilon%7D+%5Cvarepsilon%5E%7B1/2%7D%5Cnabla%20U(x)%5D%20%5C,%20dt%20+%20%5Csqrt%7B2%7D%20%5C,%20dW%5EY%0A"></p>
<p>converges to a Gaussian distribution with mean <img src="https://latex.codecogs.com/png.latex?-%5Cnabla%20U(x)"> and unit variance. The same arguments as the previous section immediately give that, starting from <img src="https://latex.codecogs.com/png.latex?X%5E%7B%5Cvarepsilon%7D_0=x">, we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cvarepsilon%5E%7B-1/2%7D%20%5C,%20%5Cint_%7B0%7D%5E%7B%5Cdelta%7D%20Y%5E%7B%5Cvarepsilon%7D%20%5C,%20dt%0A%5C;%20%5Cto%20%5C;%0A-%5Cnabla%20U(x)%20%5C,%20%5Cdelta%20+%20%5Csqrt%7B2%20%5Cdelta%7D%20%5C,%20%5Cmathcal%7BN%7D(0,1).%0A"></p>
<p>The <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7B2%7D"> terms comes from the OU asymptotic variance. this shows that the slow process converges as <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5Cto%200"> to the <a href="https://en.wikipedia.org/wiki/Brownian_dynamics">overdamped Langevin dynamics</a></p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX%20=%20-%5Cnabla%20U(X)%20%5C;%20+%20%5C;%20%5Csqrt%7B2%7D%20%5C,%20dW.%0A"></p>
</section>
<section id="example-stratonovich-corrections" class="level3">
<h3 class="anchored" data-anchor-id="example-stratonovich-corrections">Example: Stratonovich Corrections</h3>
<p>Consider a function <img src="https://latex.codecogs.com/png.latex?f:%20%5Cmathbb%7BR%7D%5Cto%20%5Cmathbb%7BR%7D"> and the slow-fast system</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX%5E%7B%5Cvarepsilon%7D%20=%20%5Cvarepsilon%5E%7B-1/2%7D%20%5C,%20f(X%5E%7B%5Cvarepsilon%7D)%20%5C,%20Y%5E%7B%5Cvarepsilon%7D%20%5C,%20dt%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?dY%5E%7B%5Cvarepsilon%7D%20=%20-(%5Clambda/%5Cvarepsilon)%20Y%5E%7B%5Cvarepsilon%7D%20+%20%5Csqrt%7B2%20%5Clambda%20/%20%5Cvarepsilon%7D"> is a fast OU process mixing on scales of order <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(%5Cvarepsilon)"> and with standard centred Gaussian invariant distribution <img src="https://latex.codecogs.com/png.latex?%5Crho(dy)">.The discussion leading to Equation&nbsp;8 suggest that the term <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5E%7B-1/2%7D%20%5C,%20Y%5E%7B%5Cvarepsilon%7D%20%5C,%20dt"> can be heuristically be thought of as <img src="https://latex.codecogs.com/png.latex?(2/%5Clambda)%5E%7B1/2%7D%20%5C,%20dW">, which would imply that the effective dynamics for the slow-process is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX%20=%20%5Csqrt%7B2/%5Clambda%7D%20%5C,%20f(X)%20%5C,%20dW.%0A"></p>
<p>We will see that this heuristic is <strong>wrong</strong>! In order to obtain the effective dynamics of the slow process as <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5Cto%200">, since the generator of the fast-OU reads <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D%5Cvarphi=%20%5Clambda%20%5B%20-y%5C,%5Cvarphi_y%20+%20%5Cvarphi_%7Byy%7D%5D">, one can solve the Poisson equation <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D%5CPsi(x,y)%20=%20f(x)y"> to obtain that <img src="https://latex.codecogs.com/png.latex?%5CPhi(x,y)%20=%20-f(x)y/%5Clambda">. One already knows that <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BVar%7D%7D%5BT%5E%7B-1%7D%5Cint_%7B%5B0,T%5D%7D%20Y_t%20%5C,%20dt%5D%20=%202/%5Clambda">. The drift term is given by</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AI(x)%20&amp;=%20%5Cint%20f(x)%20%5Cpartial_x%20%5CPhi(x,y)%20%5C,%20%5Crho(dy)%5C%5C%0A&amp;=%20%5Clambda%5E%7B-1%7D%20%5Cint%20f(x)%20f'(x)%20y%5E2%20%5C,%20%5Crho(dy)%5C%5C%0A&amp;=%20%5Clambda%5E%7B-1%7D%20f(x)%20f'(x).%0A%5Cend%7Balign%7D%0A"></p>
<p>Putting everything together gives that the effective slow dynamics reads</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AdX%20&amp;=%20%20%5Ctextcolor%7Bblue%7D%7B%20%5Clambda%5E%7B-1%7D%20f'(X)%20f(X)%20%5C,%20dt%20%7D%20+%20%5Csqrt%7B2/%5Clambda%7D%20%5C,%20f(X)%20%5C,%20dW%5C%5C%0A&amp;=%20%5Csqrt%7B2/%5Clambda%7D%20%5C,%20f(X)%20%20%5Ctextcolor%7Bred%7D%7B%5Ccirc%7D%20dW%0A%5Cend%7Balign%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%20%5Ctextcolor%7Bred%7D%7B%5Ccirc%7D"> denotes <a href="https://en.wikipedia.org/wiki/Stratonovich_integral">Stratonovich integration</a>.</p>
</section>
<section id="readings" class="level3">
<h3 class="anchored" data-anchor-id="readings">Readings</h3>
<p>The book <span class="citation" data-cites="pavliotis2008multiscale">(Pavliotis and Stuart 2008)</span> is beautiful, and I quite like the section on multiscale expansion in <span class="citation" data-cites="weinan2011principles">(Weinan 2011)</span>. For proving this type of results with the “martingale problem” approach <span class="citation" data-cites="stroock1997multidimensional">(Stroock and Varadhan 1997)</span>, the lectures <span class="citation" data-cites="papanicolaou1977martingale">(Papanicolaou 1977)</span> are nicely done.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-hinch_1991" class="csl-entry">
Hinch, E. J. 1991. <em>Perturbation Methods</em>. Cambridge University Press.
</div>
<div id="ref-papanicolaou1977martingale" class="csl-entry">
Papanicolaou, George. 1977. <span>“Martingale Approach to Some Limit Theorems.”</span> In <em>Papers from the Duke Turbulence Conference, Duke Univ., Durham, NC, 1977</em>.
</div>
<div id="ref-pavliotis2008multiscale" class="csl-entry">
Pavliotis, Grigoris, and Andrew Stuart. 2008. <em>Multiscale Methods: Averaging and Homogenization</em>. Springer Science &amp; Business Media.
</div>
<div id="ref-stroock1997multidimensional" class="csl-entry">
Stroock, Daniel W, and SR Srinivasa Varadhan. 1997. <em>Multidimensional Diffusion Processes</em>. Vol. 233. Springer Science &amp; Business Media.
</div>
<div id="ref-weinan2011principles" class="csl-entry">
Weinan, E. 2011. <em>Principles of Multiscale Modeling</em>. Cambridge University Press.
</div>
</div></section></div> ]]></description>
  <category>diffusion</category>
  <guid>https://alexxthiery.github.io/notes/averaging_homogenization/averaging_homogenization.html</guid>
  <pubDate>Mon, 27 Nov 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Ensemble Kalman Smoother (EnKS)</title>
  <link>https://alexxthiery.github.io/notes/Gaussian_Assimilation/gaussian_assimilation_smoothing.html</link>
  <description><![CDATA[ 




<p>Consider a linear-Gaussian state space model with <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E%7BD_x%7D">-valued dynamics <img src="https://latex.codecogs.com/png.latex?X_%7Bt+1%7D%20%5Csim%20F%20%5C,%20X_t%20+%20%5Cmathcal%7BN%7D(0,Q)"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E%7BD_y%7D">-valued observations <img src="https://latex.codecogs.com/png.latex?Y_t%20%5Csim%20H%20X_t%20+%20%5Cmathcal%7BN%7D(0,R)">. Assuming a Gaussian initial distribution, the <strong>filtering distributions</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(X_t%20%5Cin%20dx%20%5C,%20%7C%20Y_%7B1:t%7D)%20%5Cequiv%20%5Cmathcal%7BN%7D(%5Cmu_%7Bt%7Ct%7D,%20P_%7Bt%7Ct%7D)"> are Gaussian and can be sequentially computed with the <a href="https://en.wikipedia.org/wiki/Kalman_filter">Kalman Filter</a>. Similarly, the <strong>predictive distributions</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(X_%7Bt+1%7D%20%5Cin%20dx%20%5C,%20%7C%20Y_%7B1:t%7D)%20%5Cequiv%20%5Cmathcal%7BN%7D(%5Cmu_%7Bt+1%7Ct%7D,%20P_%7Bt+1%7Ct%7D)"> are straightforward to obtain from the filtering distributions: <img src="https://latex.codecogs.com/png.latex?%5Cmu_%7Bt+1%7Ct%7D%20=%20F%20%5C,%20%5Cmu_%7Bt%7Ct%7D"> and <img src="https://latex.codecogs.com/png.latex?P_%7Bt+1%7Ct%7D%20=%20F%20%5C,%20P_%7Bt%7Ct%7D%20%5C,%20F%5E%5Ctop%20+%20Q">. Given observations <img src="https://latex.codecogs.com/png.latex?y_%7B1:T%7D%20%5Cequiv%20(y_1,%20%5Cldots,%20y_T)"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20t%20%5Cleq%20T">, the <strong>smoothing distributions</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(X_t%20%5Cin%20dx%20%5C,%20%7C%20Y_%7B1:T%7D)%20%5Cequiv%20%5Cmathcal%7BN%7D(%5Cmu_%7Bt%7CT%7D,%20P_%7Bt%7CT%7D)"> can computed by performing a “backward pass”. Since everything is linear and Gaussian, it is just an exercise in Linear Algebra &amp; Gaussian-conditioning, as described by the Rauch-Tung-Striebel <span class="citation" data-cites="rauch1965maximum">(Rauch, Tung, and Striebel 1965)</span> smoothing recursions. The backward recursion reads</p>
<p><span id="eq-RTS"><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%0A%5Cbegin%7Baligned%7D%0A%5Cmu_%7Bt%7CT%7D%0A&amp;=%20%5Cmu_%7Bt%7Ct%7D%20+%20B_t%20%5C,%20%20%7B%5Cleft(%20%5Cmu_%7Bt+1%7CT%7D%20-%20%5Cmu_%7Bt+1%7Ct%7D%20%5Cright)%7D%20%5C%5C%0AP_%7Bt%7CT%7D%0A&amp;=%0AP_%7Bt%7Ct%7D%20+%20B_t%20%20%7B%5Cleft(%20%20P_%7Bt+1%7CT%7D%20-%20P_%7Bt+1%7Ct%7D%20%20%5Cright)%7D%20%20B%5E%5Ctop_%7Bt%7D%0A%5Cend%7Baligned%7D%0A%5Cright.%0A%5Ctag%7B1%7D"></span></p>
<p>and allows one to compute the smoothing means and covariances matrices <img src="https://latex.codecogs.com/png.latex?(%5Cmu_%7Bt%7CT%7D,%20P_%7Bt%7CT%7D)"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20t%20%5Cleq%20T"> starting from the knowledge of <img src="https://latex.codecogs.com/png.latex?(%5Cmu_%7BT%7CT%7D,%20P_%7BT%7CT%7D)">. In Equation&nbsp;1, the <strong>smoothing gain matrix</strong> <img src="https://latex.codecogs.com/png.latex?B_t"> is given by</p>
<p><span id="eq-B-cond"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AB_t%20&amp;=%0A%5Cmathop%7B%5Cmathrm%7BCov%7D%7D(X_t,%20X_%7Bt+1%7D%20%5C,%20%7C%20y_%7B1:t%7D)%20%5C,%20%5Cmathop%7B%5Cmathrm%7BVar%7D%7D(X_%7Bt+1%7D%20%5C,%20%7C%20y_%7B1:t%7D)%5E%7B-1%7D%20%5C%5C%0A&amp;=%0AP_%7Bt%7Ct%7D%20F%5E%5Ctop%20%5C,%20%20%7B%5Cleft(%20F%20%5C,%20P_%7Bt%7Ct%7D%20%5C,%20F%5E%5Ctop%20+%20Q%20%5Cright)%7D%20%5E%7B-1%7D.%0A%5Cend%7Balign%7D%0A%5Ctag%7B2%7D"></span></p>
<p><a href="../../notes/Gaussian_Assimilation/gaussian_assimilation.html">The Ensemble Kalman Filter</a> (EnKF) is a non-linear equivalent of the Kalman filter, and the purpose of these notes is to derive the equivalent “ensemble version” of the backward recursion Equation&nbsp;1. For this purpose, it is important to understand slightly better the role of the smoothing gain matrix <img src="https://latex.codecogs.com/png.latex?B_t">. Consider the pair of random variable <img src="https://latex.codecogs.com/png.latex?(X%5Ef_t,%20X%5Ep_%7Bt+1%7D)"> distributed according to the joint distribution between the filtering distribution at time <img src="https://latex.codecogs.com/png.latex?t"> and the predictive distribution at time <img src="https://latex.codecogs.com/png.latex?t+1"> in the sense that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A(X%5Ef_t,%20X%5Ep_t)%20%5C;%20%5Cunderbrace%7B=%7D_%7B%5Ctext%7B(law)%7D%7D%5C;%20(X_t,%20X_%7Bt+1%7D%20%5C,%20%5Cmid%20%5C,%20y_%7B1:t%7D).%0A"></p>
<p>This means that <img src="https://latex.codecogs.com/png.latex?X%5Ef_t%20%5Csim%20%5Cmathcal%7BN%7D(%5Cmu_%7Bt%7Ct%7D,%20P_%7Bt%7Ct%7D)"> and <img src="https://latex.codecogs.com/png.latex?X%5Ep_%7Bt+1%7D%20%5Csim%20%5Cmathcal%7BN%7D(%5Cmu_%7Bt+1%7Ct%7D,%20P_%7Bt+1%7Ct%7D)"> and <img src="https://latex.codecogs.com/png.latex?X%5Ep_t%20=%20F%20%5C,%20X%5Ef_t%20+%20%5Cmathcal%7BN%7D(0,%20Q)">. Furthermore, Equation&nbsp;2 and the standard <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions">gaussian conditional probabilities</a> formulas give that the conditional means and covariances are given by</p>
<p><span id="eq-gauss-cond"><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%0A%5Cbegin%7Balign%7D%0A%5Ctextrm%7BMean%7D%20%7B%5Cleft(%20X%5Ef_t%20%7C%20(X%5Ep_%7Bt+1%7D=x_%7Bt+1%7D)%20%5Cright)%7D%20%20%0A%5C;%20&amp;=%20%5C;%0A%5Cmu_%7Bt%7Ct%7D%20+%20B_t%20(x_%7Bt+1%7D%20-%20%5Cmu_%7Bt+1%7Ct%7D)%20%5C%5C%0A%5Ctextrm%7BCov%7D%20%7B%5Cleft(%20X%5Ef_t%20%7C%20(X%5Ep_%7Bt+1%7D=x_%7Bt+1%7D)%20%5Cright)%7D%20%20%0A%5C;%20&amp;=%20%5C;%0AP_%7Bt%7Ct%7D%20-%20B_t%20%5C,%20P_%7Bt+1%7Ct%7D%20%5C,%20B_t%5E%5Ctop.%0A%5Cend%7Balign%7D%0A%5Cright.%0A%5Ctag%7B3%7D"></span></p>
<p>The above expression for the conditional mean also shows that the matrix <img src="https://latex.codecogs.com/png.latex?B_t"> is a minimizer of the loss</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AM%20%5C;%20%5Cmapsto%20%5C;%0A%5Cmathbb%7BE%7D%20%7B%5Cleft(%20%20%5Cleft%5C%7C%20(X%5Ef_t%20-%20%5Cmu_%7Bt%7Ct%7D)%20-%20B%20(X%5Ep_%7Bt+1%7D%20-%20%5Cmu_%7Bt+1%7Ct%7D)%20%5Cright%5C%7C%5E2%20%20%5Cright)%7D%0A"></p>
<p>over all matrices <img src="https://latex.codecogs.com/png.latex?M%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD_x,%20D_x%7D">. Heuristically, this shows that the smoothing gain matrix <img src="https://latex.codecogs.com/png.latex?B_t"> can easily be computed by <strong>regressing</strong> <img src="https://latex.codecogs.com/png.latex?X%5Ef_t"> against <img src="https://latex.codecogs.com/png.latex?X%5Ep_%7Bt+1%7D">. We can use this remark to build an ensemble version of the backward recursion Equation&nbsp;1. Recall that when running a EnKF for filtering the observations <img src="https://latex.codecogs.com/png.latex?y_%7B1:T%7D">, the final stage proceeds in two steps:</p>
<ol type="1">
<li>Obtain an ensemble of particles <img src="https://latex.codecogs.com/png.latex?X%5E%7Bi,p%7D_%7BT%7D%20=%20F%20%5C,%20X%5E%7Bi,f%7D_%7BT-1%7D%20+%20%5Cmathcal%7BN%7D(0,Q)"> that approximate the predictive distribution <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(X_T%20%7C%20y_%7B1:T-1%7D)">.<br>
</li>
<li><a href="../../notes/Gaussian_Assimilation/gaussian_assimilation.html">Assimilate</a> the last observation <img src="https://latex.codecogs.com/png.latex?y_T"> using the Kalman gain matrix <img src="https://latex.codecogs.com/png.latex?K_T"> and the correction <img src="https://latex.codecogs.com/png.latex?%5CDelta_T%5Ei%20=%20K_T%20%5C,%20(%5Ctilde%7By%7D_%7Bi,%5Cstar%7D%20-%20H%20%5C,%20X%5E%7Bi,p%7D_T)"> by setting <span id="eq-pred-perturb"><img src="https://latex.codecogs.com/png.latex?%0AX%5E%7Bi,s%7D_T%20=%20X%5E%7Bi,p%7D_T%20+%20%5CDelta_T%5Ei.%0A%5Ctag%7B4%7D"></span> The particles <img src="https://latex.codecogs.com/png.latex?X%5E%7Bi,s%7D_T"> approximate the smoothing distribution <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(X_T%20%7C%20y_%7B1:T%7D)">.</li>
</ol>
<p>Following our discussion of the smoothing gain matrix <img src="https://latex.codecogs.com/png.latex?B_%7Bt%7D"> and Equation&nbsp;4, it seems sensible to set</p>
<p><span id="eq-rec-ens"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AX%5E%7Bi,s%7D_%7BT-1%7D%0A&amp;=%20X%5E%7Bi,f%7D_%7BT-1%7D%20+%20B_%7BT-1%7D%20%5C,%20%5CDelta%5Ei_T%5C%5C%0A&amp;=%20X%5E%7Bi,f%7D_%7BT-1%7D%20+%20B_%7BT-1%7D%20%5C,%20(X%5E%7Bi,s%7D_%7BT%7D%20-%20X%5E%7Bi,p%7D_%7BT%7D)%0A%5Cend%7Balign%7D%0A%5Ctag%7B5%7D"></span></p>
<p>and hope that the ensemble of updated particles <img src="https://latex.codecogs.com/png.latex?X%5E%7Bi,s%7D_%7BT-1%7D"> approximate the smoothing distribution <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(X_%7BT-1%7D%20%7C%20y_%7B1:T%7D)">. In words, the particle <img src="https://latex.codecogs.com/png.latex?X%5E%7Bi,s%7D_%7BT-1%7D"> is obtained by “pulling” the correction term <img src="https://latex.codecogs.com/png.latex?%5CDelta%5Ei_%7BT%7D%20=%20X%5E%7Bi,s%7D_%7BT%7D%20-%20X%5E%7Bi,p%7D_%7BT%7D"> back to <img src="https://latex.codecogs.com/png.latex?X%5E%7Bi,f%7D_%7BT-1%7D"> through the “regression” smoothing gain matrix <img src="https://latex.codecogs.com/png.latex?B_%7BT-1%7D">. To check that the particles <img src="https://latex.codecogs.com/png.latex?X%5E%7Bi,s%7D_%7BT-1%7D"> indeed approximate the smoothing distribution <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(X_%7BT-1%7D%20%5C,%7Cy_%7B1:T%7D)">, it suffices to compute the mean/variance and verify that they are matching the one given by Equation&nbsp;1. Recall that Equation&nbsp;3 gives that the filtering/predictive distributions satisfy</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AX%5Ef_%7BT-1%7D%20=%20%5Cmu_%7BT-1%7CT-1%7D%20+%20B_%7BT-1%7D%20%5C,%20(X%5Ep_%7BT%7D%20-%20%5Cmu_%7BT%7CT-1%7D)%20+%20%5Cvarepsilon_t%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon_t%20%5Csim%20%5Cmathcal%7BN%7D(0,%20P_%7BT-1%7CT-1%7D%20-%20B_%7BT-1%7D%20%5C,%20P_%7BT%7CT-1%7D%20%5C,%20B_%7BT-1%7D%5E%5Ctop)"> is independent from all other sources of randomness. Plugging this into Equation&nbsp;5 gives that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AX%5E%7Bi,s%7D_%7BT-1%7D%0A=%0A%5Cmu_%7BT-1%7CT-1%7D%20+%20B_%7BT-1%7D%20%5C,%20(X%5E%7Bi,s%7D_%7BT%7D%20-%20%5Cmu_%7BT%7CT-1%7D)%20+%20%5Cvarepsilon_t.%0A"></p>
<p>Since the <img src="https://latex.codecogs.com/png.latex?X%5E%7Bi,s%7D_%7BT%7D"> are distributed according to the smoothing distribution, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?X%5E%7Bi,s%7D_%7BT%7D%20%5Csim%20%5Cmathcal%7BN%7D(%5Cmu_%7BT%7CT%7D,%20P_%7BT%7CT%7D)">, this immediately shows that <img src="https://latex.codecogs.com/png.latex?X%5E%7Bi,s%7D_%7BT-1%7D"> is Gaussian with</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%0A%5Cbegin%7Balign%7D%0A%5Ctextrm%7BMean%7D%20&amp;=%20%5Cmu_%7BT-1%7CT%7D%20=%20%5Cmu_%7BT-1%7CT-1%7D%20+%20B_%7BT-1%7D%20%5C,%20%20%7B%5Cleft(%20%5Cmu_%7BT%7CT%7D%20-%20%5Cmu_%7BT%7CT-1%7D%20%5Cright)%7D%20%5C%5C%0A%5Ctextrm%7BCovariance%7D%20&amp;=%20P_%7BT-1%7CT%7D%20=%20P_%7BT-1%7CT-1%7D%20+%20B_%7BT-1%7D%20%20%7B%5Cleft(%20%20P_%7BT%7CT%7D%20-%20P_%7BT%7CT-1%7D%20%20%5Cright)%7D%20%20B%5E%5Ctop_%7BT-1%7D,%0A%5Cend%7Balign%7D%0A%5Cright.%0A"></p>
<p>as it should. One can then iterate this construction to obtain particle approximations of the smoothing distributions <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(X_t%20%7C%20y_%7B1:T%7D)"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20t%20%5Cleq%20T"> by running a backward pass and recursively setting</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AX%5E%7Bi,s%7D_t%20%5C;%20=%20%5C;%20X%5E%7Bi,f%7D_t%20+%20B_t%20%5C,%20%20%7B%5Cleft(%20X%5E%7Bi,s%7D_%7Bt+1%7D%20-%20X%5E%7Bi,p%7D_%7Bt+1%7D%20%5Cright)%7D%20.%0A"></p>
<p>The ensemble of particles <img src="https://latex.codecogs.com/png.latex?X%5E%7Bi,s%7D_t"> approximates the smoothing distribution <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(X_t%20%7C%20y_%7B1:T%7D)">. In a nonlinear setting, it suffices to approximate the smoothing gain matrices with</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%7BB%7D_t%20=%20%5Cmathop%7B%5Cmathrm%7BCov%7D%7D%20%7B%5Cleft(%20%20x%5Ef_%7Bt,i%7D,%20x%5Ep_%7Bt+1,i%7D%20%20%5Cright)%7D%20%20%5C,%20%5Cmathop%7B%5Cmathrm%7BVar%7D%7D%20%7B%5Cleft(%20%20x%5Ep_%7Bt+1,i%7D%20%20%5Cright)%7D%20%5E%7B-1%7D.%0A"></p>
<p>[Experiments: TODO]</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-rauch1965maximum" class="csl-entry">
Rauch, Herbert E, F Tung, and Charlotte T Striebel. 1965. <span>“Maximum Likelihood Estimates of Linear Dynamic Systems.”</span> <em>AIAA Journal</em> 3 (8): 1445–50.
</div>
</div></section></div> ]]></description>
  <category>enkf</category>
  <category>data-assimilation</category>
  <guid>https://alexxthiery.github.io/notes/Gaussian_Assimilation/gaussian_assimilation_smoothing.html</guid>
  <pubDate>Fri, 17 Nov 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Asymptotic variance &amp; Poisson Equation</title>
  <link>https://alexxthiery.github.io/notes/Poisson_Eq_Asymp_Var/Poisson_Eq_Asymp_Var.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/Poisson_Eq_Asymp_Var/poisson_eq.jpg" class="img-fluid figure-img" style="width:80.0%"></p>
</figure>
</div>
</div>
<p>Consider a continuous time Markov process <img src="https://latex.codecogs.com/png.latex?X_t"> on <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5ED"> that is ergodic with respect to the probability distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi(dx)">. A Langevin diffusion is a typical example. Call <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D"> the generator of this process so that for a test function <img src="https://latex.codecogs.com/png.latex?%5Cvarphi:%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D"> we have</p>
<p><span id="eq-martingale"><img src="https://latex.codecogs.com/png.latex?%0A%5Cvarphi(X_t)%20=%20%5Cvarphi(X_0)%20+%20%5Cint_%7Bs=0%7D%5Et%20%5Cmathcal%7BL%7D%5Cvarphi(X_s)%20%5C,%20ds%20+%20%5Ctextrm%7B($M_t%20%5Cequiv$%20martingale)%7D.%0A%5Ctag%7B1%7D"></span></p>
<p>Now, assume further that <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7B%5Cpi%7D%5B%5Cvarphi(X)%5D%20=%200"> and that a Central Limit Theorem holds,</p>
<p><span id="eq-CLT"><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7B%5Csqrt%7BT%7D%7D%20%5Cint_%7Bs=0%7D%5ET%20%5Cvarphi(X_s)%20%5C,%20ds%20%5C;%20%5Cto%20%5C;%20%5Cmathcal%7BN%7D(0,%20%5Csigma%5E2).%0A%5Ctag%7B2%7D"></span></p>
<p>How can one estimate the asymptotic variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">?</p>
<section id="approach-i-integrated-autocovariance" class="level3">
<h3 class="anchored" data-anchor-id="approach-i-integrated-autocovariance">Approach I: Integrated autocovariance</h3>
<p>One can directly try to compute the second moment of Equation&nbsp;2 and obtain that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csigma%5E2%20%5C;%20=%20%5C;%20%5Clim_%7BT%20%5Cto%20%5Cinfty%7D%20%5C;%0A%5Cfrac%7B1%7D%7BT%7D%20%5C,%20%5Ciint_%7B0%20%5Cleq%20s,t%20%5Cleq%20T%7D%20%5Cmathbb%7BE%7D%5B%5Cvarphi(X_s)%20%5Cvarphi(X_t)%5D%20%5C,%20ds%20%5C,%20dt%0A"></p>
<p>Since <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5B%5Cvarphi(X_s)%20%5Cvarphi(X_t)%5D"> falls quickly to zero as <img src="https://latex.codecogs.com/png.latex?%7Cs-t%7C%20%5Cto%200"> and defining the auto-covariance at lag <img src="https://latex.codecogs.com/png.latex?r%20%3E%200"> as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AC(r)%20=%20%5Cmathbb%7BE%7D%5B%5Cvarphi(X_t)%20%5Cvarphi(X_%7Bt+r%7D)%5D,%0A"></p>
<p>one obtains that an expression of the asymptotic as the integrated autocovariance function,</p>
<p><span id="eq-autocov"><img src="https://latex.codecogs.com/png.latex?%0A%5Csigma%5E2%20%5C;%20=%20%5C;%202%20%5C,%20%5Cint_%7Br=0%7D%5E%5Cinfty%20C(r)%20%5C,%20dr.%0A%5Ctag%7B3%7D"></span></p>
<p>In the MCMC literature, this relation is often expressed as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csigma%5E2%20%5C;%20=%20%5C;%20%5Cmathop%7B%5Cmathrm%7BVar%7D%7D_%7B%5Cpi%7D%5B%5Cvarphi%5D%20%5C,%20%5Ctimes%20%5C,%20%5Ctextrm%7B(IACT)%7D%0A"></p>
<p>where the <strong>integrated autocorrelation</strong> function is defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctextrm%7B(IACT)%7D%20=%202%20%5C,%20%5Cint_%7Br=0%7D%5E%5Cinfty%20%5Crho(r)%20%5C,%20dr.%0A"></p>
<p>for autocorrelation at lag <img src="https://latex.codecogs.com/png.latex?r%5Cgeq%200"> defined as <img src="https://latex.codecogs.com/png.latex?%5Crho(r)%20%5Cequiv%20%5Cmathop%7B%5Cmathrm%7BCorr%7D%7D%5B%5Cvarphi(X_t),%20%5Cvarphi(X_%7Bt+r%7D)%5D">. The slower the autocorrelation function <img src="https://latex.codecogs.com/png.latex?%5Crho(r)"> falls to zero as <img src="https://latex.codecogs.com/png.latex?r%20%5Cto%20%5Cinfty">, the larger the asymptotic variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">. Although Equation&nbsp;3 is very intuitive, it can be difficult to estimate the autocorrelation function.</p>
</section>
<section id="approach-ii-poisson-equation" class="level3">
<h3 class="anchored" data-anchor-id="approach-ii-poisson-equation">Approach II: Poisson Equation</h3>
<p>Under relatively general and mild conditions, since the expectation of <img src="https://latex.codecogs.com/png.latex?%5Cvarphi"> under the invariant distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is zero and the Markov process is ergodic with respect to <img src="https://latex.codecogs.com/png.latex?%5Cpi">, there exists a function <img src="https://latex.codecogs.com/png.latex?%5CPhi:%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D"> such that</p>
<p><span id="eq-poisson"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D%5CPhi%20%5C;%20=%20%5C;%20%5Cvarphi.%0A%5Ctag%7B4%7D"></span></p>
<p>Equation&nbsp;4 is called a <a href="https://en.wikipedia.org/wiki/Poisson%27s_equation">Poisson Equation</a> since <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D"> is often a Laplacian-like operator (eg. diffusion-type processes). Equation&nbsp;1 gives that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7B%5Csqrt%7BT%7D%7D%20%5Cint_%7Bs=0%7D%5ET%20%5Cvarphi(X_s)%20%5C,%20ds%0A%5C;%20=%20%5C;%0A%5Cfrac%7BM_T%7D%7B%5Csqrt%7BT%7D%7D%20+%20%20%7B%5Cleft%5C%7B%20%20%5Cfrac%7B%5CPhi(X_T)%20-%20%5CPhi(X_0)%7D%7B%5Csqrt%7BT%7D%7D%20%20%5Cright%5C%7D%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?M_T"> is the martingale and <img src="https://latex.codecogs.com/png.latex?%5B%5CPhi(X_T)%20-%20%5CPhi(X_0)%5D/%5Csqrt%7BT%7D"> typically vanishes as <img src="https://latex.codecogs.com/png.latex?T%20%5Cto%20%5Cinfty"> and can be neglected. For computing the asymptotic variance, it suffices to estimate <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D(M_T%5E2)">. And using the martingale property, it equals <img src="https://latex.codecogs.com/png.latex?%5Cint_%7Bs=0%7D%5ET%20%5Cmathbb%7BE%7D(dM_t)%5E2">. Also, since <img src="https://latex.codecogs.com/png.latex?M_t%20=%20%5Cvarphi(X_t)%20-%20%5Cvarphi(X_0)%20-%20%5Cint_%7Bs=0%7D%5Et%20%5Cmathcal%7BL%7D%5Cvarphi(X_s)%20%5C,%20ds">, algebra gives that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7B%5Cvarepsilon%7D%20%5C,%20%5Cmathbb%7BE%7D%20%7B%5Cleft%5B%20%20(M_%7Bt+%5Cvarepsilon%7D%20-%20M_t)%5E2%20%20%5Cright%5D%7D%20%20%5Capprox%202%20%5Cmathbb%7BE%7D%20%7B%5Cleft%5B%20%20(%5CGamma%20%5CPhi)(X_t)%20%20%5Cright%5D%7D%0A"></p>
<p>where the so-called <strong>carré du champ</strong> <img src="https://latex.codecogs.com/png.latex?(%5CGamma%20%5CPhi)"> is defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A2%20%5C,%20(%5CGamma%20%5CPhi)(X_t)%0A%5C;%20=%20%5C;%20%20%7B%5Cleft(%20%20%5Cmathcal%7BL%7D(%5CPhi%5E2)%20-%202%20%5CPhi%20%5Cmathcal%7BL%7D%5CPhi%20%5Cright)%7D%20(X_t)%0A%5C;%20=%20%5C;%20%5Clim_%7B%5Cvarepsilon%5Cto%200%7D%20%5C;%20%5Cfrac%7B1%7D%7B%5Cvarepsilon%7D%20%5Cmathop%7B%5Cmathrm%7BVar%7D%7D(%5CPhi(X_%7Bt+%5Cvarepsilon%7D)%20%5C,%20%7C%20%5C,%20X_t).%0A"></p>
<p>This shows that the asymptotic variance satisfies</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csigma%5E2%0A%5C;%20=%20%5C;%0A%5Clim_%7BT%20%5Cto%20%5Cinfty%7D%20%5Cfrac%7B2%7D%7BT%7D%20%5Cint_%7Bs=0%7D%5ET%20%5CGamma%20%5CPhi(X_s)%20%5C,%20ds%0A%5C;%20=%20%5C;%0A2%20%5C,%20%5Cint_%7Bx%20%5Cin%20%5Cmathbb%7BR%7D%5ED%7D%20%5CGamma%20%5CPhi(x)%20%5C,%20%5Cpi(dx).%0A"></p>
<p>Finally, since <img src="https://latex.codecogs.com/png.latex?%5Cint%20(%5Cmathcal%7BL%7D%5CPhi%5E2)(x)%20%5C,%20%5Cpi(dx)%20=%200">, this can equivalently be written as</p>
<p><span id="eq-asymp-var-poisson"><img src="https://latex.codecogs.com/png.latex?%0A%5Csigma%5E2%0A%5C;%20=%20%5C;%0A-2%20%5C,%20%5Cint_%7Bx%20%5Cin%20%5Cmathbb%7BR%7D%5ED%7D%20%5CPhi(x)%20%5C,%20%5Cmathcal%7BL%7D%5CPhi(x)%20%5C,%20%5Cpi(dx)%0A%5C;%20=%20%5C;%202%20%5C,%20%5Cmathcal%7BD%7D(%5CPhi)%0A%5Ctag%7B5%7D"></span></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D(%5CPhi)"> is the so-called Dirichlet form. In summary, we have just shown that the asymptotic variance of the additive functional <img src="https://latex.codecogs.com/png.latex?T%5E%7B-1/2%7D%20%5C,%20%5Cint_0%5ET%20%5Cvarphi(X_s)%20%5C,%20ds"> is given by two times the Dirichlet form <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D(%5CPsi)"> where <img src="https://latex.codecogs.com/png.latex?%5CPhi"> is solution to the Poisson equation <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D%5CPhi%20=%20%5Cvarphi">. Note that this implies that the generator <img src="https://latex.codecogs.com/png.latex?%5CPhi"> is a negative operator in the sense that for a test function <img src="https://latex.codecogs.com/png.latex?%5CPhi"> we have that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%3C%20%5CPhi,%20%5Cmathcal%7BL%7D%5CPhi%20%5Cright%3E_%7B%5Cpi%7D%20%5C;%20%5Cleq%20%5C;%200%0A"></p>
<p>where we have used the dot-product notation <img src="https://latex.codecogs.com/png.latex?%5Cleft%3C%20f,g%20%5Cright%3E_%7B%5Cpi%7D%20=%20%5Cint%20f(x)%20g(x)%20%5Cpi(dx)">.</p>
</section>
<section id="poisson-equation-integral-representation" class="level3">
<h3 class="anchored" data-anchor-id="poisson-equation-integral-representation">Poisson equation: Integral representation</h3>
<p>It is often useful to think of the generator <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D"> as an infinite dimensional equivalent of a standard negative definite symmetric matrix/operator <img src="https://latex.codecogs.com/png.latex?M%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn,n%7D">. And since <img src="https://latex.codecogs.com/png.latex?M%5E%7B-1%7D%20=%20-%5Cint_%7Bt=0%7D%5E%7B%5Cinfty%7D%20%5Cexp(tM)%20%5C,%20dt">, as can be seen by diagonalizing <img src="https://latex.codecogs.com/png.latex?M">, one can expect the following equation to hold,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D%5E%7B-1%7D%20%5C;%20=%20%5C;%20-%5Cint_%7Bt=0%7D%5E%7B%5Cinfty%7D%20e%5E%7Bt%20%5C,%20%5Cmathcal%7BL%7D%7D%20%5C,%20dt.%0A"></p>
<p>That is just another way of writing that the solution <img src="https://latex.codecogs.com/png.latex?%5CPhi"> to the Poisson equation <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D%5CPhi%20=%20%5Cvarphi">, with the <strong>centering condition</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7B%5Cpi%7D%5B%5CPhi(X)%5D=0"> for picking one solution out of the many possible solutions to the Poisson equation differing from each other by an additive constant, can be expressed as</p>
<p><span id="eq-expansion"><img src="https://latex.codecogs.com/png.latex?%0A%5CPhi(x)%20%5C,%20=%20%5C,%20-%5Cint_%7Bt=0%7D%5E%7B%5Cinfty%7D%20%5Cmathbb%7BE%7D%5B%5Cvarphi(X_t)%7CX_0=x%5D%20%5C,%20dt.%0A%5Ctag%7B6%7D"></span></p>
<p>Equation&nbsp;6 is easily proved with Equation&nbsp;1 by writing</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5CPhi(x)-%5CPhi(X_T)%20=%20-%5Cint_%7Bt=0%7D%5E%5Cinfty%20%5Cvarphi(X_t)%20%5C,%20dt%20+%20%5Ctextrm%7B(martingale)%7D%0A"></p>
<p>and by taking expectation from both sides and noticing that <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5B%5CPhi(X_T)%5D%20%5Cto%200"> thanks to the assumed centering condition <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7B%5Cpi%7D%5B%5CPhi(X)%5D=0">. Note that this remarks allows to give another derivation of Equation&nbsp;5 starting from the integrated autocovariance formulation Equation&nbsp;3. Indeed, note that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Csigma%5E2%20&amp;=%202%20%5C,%20%5Cint_%7Br=0%7D%5E%7B%5Cinfty%7D%20C(r)%20%5C,%20dr%5C%5C%0A&amp;=%0A2%20%5C,%20%5Cint_%7Bx%20%5Cin%20%5Cmathbb%7BR%7D%5ED%7D%20%5Cvarphi(x)%20%20%7B%5Cleft%5C%7B%20%20%5Cint_%7Br=0%7D%5E%7B%5Cinfty%7D%20%20%5Cmathbb%7BE%7D%5B%5Cvarphi(X_t)%20%7C%20X_0=x%5D%20%5C,%20dr%20%20%5Cright%5C%7D%7D%20%20%5C,%20%5Cpi(dx)%5C%5C%0A&amp;=%0A-2%20%5C,%20%5Cint_%7Bx%20%5Cin%20%5Cmathbb%7BR%7D%5ED%7D%20%5Cvarphi(x)%20%5CPhi(x)%20%5C,%20%5Cpi(dx)%0A=%0A-2%20%5Cleft%3C%20%5CPhi,%20%5Cmathcal%7BL%7D%5CPhi%20%5Cright%3E_%7B%5Cpi%7D.%0A%5Cend%7Balign%7D%0A"></p>
</section>
<section id="example-ou-process" class="level3">
<h3 class="anchored" data-anchor-id="example-ou-process">Example: OU process</h3>
<p>Consider a OU process that is ergodic with respect to the standard Gaussian density <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)%20=%20e%5E%7B-x%5E2/2%7D%20/%20%5Csqrt%7B2%5Cpi%7D">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX%0A%5C;%20=%20%5C;%0A-%5Cvarepsilon%5E%7B-1%7DX%20%5C,%20dt%20+%20%5Csqrt%7B2%20%5C,%20%5Cvarepsilon%5E%7B-1%7D%7D%20%5C,%20dW.%0A"></p>
<p>That’s a standard OU process accelerated by a factor <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5E%7B-1%7D%20%3E%200">. Its generator reads</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D%5C,%20=%20%5C,%20%5Cvarepsilon%5E%7B-1%7D%20%5B-x%20%5C,%20%5Cpartial_x%20+%20%5Cpartial_%7Bxx%7D%5D.%0A"></p>
<p>The function <img src="https://latex.codecogs.com/png.latex?%5Cvarphi(x)=x"> is such that <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Cvarphi)=0"> and a solution to the Poisson equation <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D%5CPhi%20=%20%5Cvarphi"> is <img src="https://latex.codecogs.com/png.latex?%5CPhi(x)%20=%20-%5Cvarepsilon%5C,%20x">. This shows that the asymptotic variance is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csigma%5E2%0A%5C;%20=%20%5C;%0A2%20%5C,%20%5Cint_%7Bx%20%5Cin%20%5Cmathbb%7BR%7D%5ED%7D%20%5Cvarepsilon%20x%5E2%20%5C,%20%5Cpi(dx)%20%5C;%20=%20%5C;%202%20%5Cvarepsilon.%0A"></p>
<p>As expected, accelerating the OU process by a factor <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5E%7B-1%7D"> means reducing the variance by a factor <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon">.</p>


</section>

 ]]></description>
  <category>markov</category>
  <guid>https://alexxthiery.github.io/notes/Poisson_Eq_Asymp_Var/Poisson_Eq_Asymp_Var.html</guid>
  <pubDate>Fri, 10 Nov 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Gaussian Assimilation &amp; the EnKF</title>
  <link>https://alexxthiery.github.io/notes/Gaussian_Assimilation/gaussian_assimilation.html</link>
  <description><![CDATA[ 




<section id="ensemble-kalman-updates" class="level2">
<h2 class="anchored" data-anchor-id="ensemble-kalman-updates">Ensemble Kalman Updates</h2>
<p>Assume a prior Gaussian prior distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi_0%20%5Cequiv%20%5Cmathcal%7BN%7D(m_0,P_0)"> and a noisy observation <img src="https://latex.codecogs.com/png.latex?y_%5Cstar%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD_y%7D"> with</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay_%5Cstar%20=%20H%20x%20+%20%5Cxi%0A%5Cqquad%20%5Ctext%7Bwith%7D%20%5Cqquad%0A%5Cxi%20%5Csim%20%5Cmathcal%7BN%7D(0,R)%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD_x%7D"> is an unknown quantity of interest. The posterior distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi%20%5Cequiv%20%5Cmathcal%7BN%7D(m,P)"> is Gaussian and is given by</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cleft%5C%7B%0A%5Cbegin%7Baligned%7D%0Am%20&amp;=%20m_0%20+%20P_0%20H%5E%5Ctop%20%20%7B%5Cleft(%20H%20P_0%20H%5E%5Ctop%20+%20R%20%5Cright)%7D%20%5E%7B-1%7D(y_%5Cstar%20-%20H%20m)%5C%5C%0AP%20&amp;=%20P_0%20-%20P_0%20H%5E%5Ctop%20%20%7B%5Cleft(%20H%20P_0%20H%5E%5Ctop%20+%20R%20%5Cright)%7D%20%5E%7B-1%7D%20%5C,%20H%20P_0,%0A%5Cend%7Baligned%7D%0A%5Cright.%0A%5Cend%7Balign%7D%0A"></p>
<p>as standard <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">Gaussian conditioning</a> shows it. This can also be written as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cleft%5C%7B%0A%5Cbegin%7Baligned%7D%0Am%20&amp;=%20m_0%20+%20K%20%5C,%20(y_%5Cstar%20-%20H%20m_0)%5C%5C%0AP%20&amp;=%20(I%20-%20K%20%5C,%20H)%20P_0,%0A%5Cend%7Baligned%7D%0A%5Cright.%0A%5Cend%7Balign%7D%0A"></p>
<p>for <strong><a href="https://en.wikipedia.org/wiki/Kalman_filter">Kalman</a> Gain Matrix</strong> <img src="https://latex.codecogs.com/png.latex?K%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD_x,D_y%7D"> defined as</p>
<p><span id="eq-kalman"><img src="https://latex.codecogs.com/png.latex?%0AK%20%5C;%20=%20%5C;%20P_0%20H%5E%5Ctop%20%20%7B%5Cleft(%20H%20P_0%20H%5E%5Ctop%20+%20R%20%5Cright)%7D%20%5E%7B-1%7D.%0A%5Ctag%7B1%7D"></span></p>
<p>which can also be expressed as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AK%20%5C;%20=%20%5C;%20%5Cmathop%7B%5Cmathrm%7BCov%7D%7D(X,%20HX)%20%5C,%20%5Cmathop%7B%5Cmathrm%7BVar%7D%7D(Y)%5E%7B-1%7D.%0A"></p>
<p>for <img src="https://latex.codecogs.com/png.latex?X%20%5Csim%20%5Cpi_0"> and <img src="https://latex.codecogs.com/png.latex?Y%20=%20HX%20+%20%5Cmathcal%7BN%7D(0,R)">; this point of view can be a useful for establishing generalization to non-linear scenarios. The important remark is that the posterior covariance matrix <img src="https://latex.codecogs.com/png.latex?P%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD_x,D_x%7D"> and the posterior mean <img src="https://latex.codecogs.com/png.latex?m%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD_x%7D"> can also be expressed as</p>
<p><span id="eq-gain"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cleft%5C%7B%0A%5Cbegin%7Baligned%7D%0Am%20&amp;=%20%20%5Ctextcolor%7Bred%7D%7B(I%20-%20K%20%5C,%20H)%7D%20%5C,%20m_0%20+%20%20%5Ctextcolor%7Bblue%7D%7BK%7D%20%5C,%20y_%5Cstar%5C%5C%0AP%20&amp;=%20%5C;%20%20%5Ctextcolor%7Bred%7D%7B(I%20-%20K%20%5C,%20H)%7D%20%5C,%20P_0%20%5C,%20%20%5Ctextcolor%7Bred%7D%7B(I%20-%20K%20%5C,%20H)%5E%5Ctop%7D%20+%20%20%5Ctextcolor%7Bblue%7D%7BK%7D%20R%20%20%5Ctextcolor%7Bblue%7D%7BK%5E%5Ctop%7D.%0A%5Cend%7Baligned%7D%0A%5Cright.%0A%5Cend%7Balign%7D%0A%5Ctag%7B2%7D"></span></p>
<p>This shows that <img src="https://latex.codecogs.com/png.latex?P"> is indeed positive semi-definite. More importantly, this gives a mechanism for transforming samples from the prior distributions into samples from the posterior distributions. Indeed, consider <img src="https://latex.codecogs.com/png.latex?N"> iid samples from the prior distribution, <img src="https://latex.codecogs.com/png.latex?x_1,%20%5Cldots,%20x_N%20%5Csim%20%5Cpi_0(dx)">, and set</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidetilde%7Bx%7D_i%0A=%0A%5Ctextcolor%7Bred%7D%7B(I%20-%20K%20%5C,%20H)%7D%20%5C,%20x_i%20+%20%20%5Ctextcolor%7Bblue%7D%7BK%7D(y_%5Cstar%20+%20%5Cxi_i)%0A"></p>
<p>for iid noise terms <img src="https://latex.codecogs.com/png.latex?%5Cxi_i%20%5Csim%20%5Cmathcal%7BN%7D(0,R)">. From Equation&nbsp;2 it is clear that <img src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7Bx%7D_1,%20%5Cldots,%20%5Cwidetilde%7Bx%7D_N"> are iid samples from the Gaussian posterior distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi%20=%20%5Cmathcal%7BN%7D(m,P)">. It is more intuitive to write this as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidetilde%7Bx%7D_i%0A=%0Ax_i%20+%20K%20%5C,%20(%20%20%5Ctextcolor%7Bgreen%7D%7B%20%5Cwidetilde%7By%7D_%7Bi,%5Cstar%7D%20%7D%20-%20H%20%5C,%20x_i)%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7By%7D_%7Bi,%5Cstar%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD_y%7D"> are <strong>fake observations</strong> that are obtained by perturbing the actual observation <img src="https://latex.codecogs.com/png.latex?y_%5Cstar"> with additive Gaussian noise terms with covariance <img src="https://latex.codecogs.com/png.latex?R">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctextcolor%7Bgreen%7D%7B%5Cwidetilde%7By%7D_%7Bi,%5Cstar%7D%20%5C,%20=%20%5C,%20y_%5Cstar%20+%20%5Cxi_i%7D.%0A"></p>
<section id="empirical-version-non-linearity-and-non-gaussianity" class="level3">
<h3 class="anchored" data-anchor-id="empirical-version-non-linearity-and-non-gaussianity">Empirical version: non-linearity and non-Gaussianity</h3>
<p>Suppose that we would like to estimate <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD_x%7D"> from the noisy observation</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay_%5Cstar%20=%20%5Cmathcal%7BH%7D(x)%20+%20%5Cxi%0A%5Cqquad%20%5Ctext%7Bwith%7D%20%5Cqquad%0A%5Cxi%20%5Csim%20%5Cmathcal%7BN%7D(0,R)%0A"></p>
<p>and possibly-nonlinear observation operator <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BH%7D:%20%5Cmathbb%7BR%7D%5E%7BD_x%7D%20%5Cto%20%5Cmathbb%7BR%7D%5E%7BD_y%7D">. Assume that we also have <img src="https://latex.codecogs.com/png.latex?N"> samples <img src="https://latex.codecogs.com/png.latex?x_1,%20%5Cldots,%20x_N"> generated from some (unkown) prior distribution. For example, these samples could come from another numerical procedure. In order to obtain <img src="https://latex.codecogs.com/png.latex?N"> approximate samples from the posterior distribution, one can set</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidetilde%7Bx%7D_i%0A=%0Ax_i%20+%20%5Cwidehat%7BK%7D%20%5C,%20%5B%20%20%5Ctextcolor%7Bgreen%7D%7B%20%5Cwidetilde%7By%7D_%7Bi,%5Cstar%7D%20%7D%20-%20%5Cmathcal%7BH%7D(x_i)%5D%0A"></p>
<p>for fake observations <img src="https://latex.codecogs.com/png.latex?%20%5Ctextcolor%7Bgreen%7D%7B%5Cwidetilde%7By%7D_%7Bi,%5Cstar%7D%20%7D%20=%20y_%5Cstar%20+%20%5Cxi_i">. The approximate Kalman gain matrix <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BK%7D"> is obtained by noting that in Equation&nbsp;1 giving</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AK%20%5C;%20=%20%5C;%20P_0%20H%5E%5Ctop%20%20%7B%5Cleft(%20H%20P_0%20H%5E%5Ctop%20+%20R%20%5Cright)%7D%20%5E%7B-1%7D%0A"></p>
<p>we have <img src="https://latex.codecogs.com/png.latex?P_0%20H%5E%5Ctop%20=%20%5Cmathop%7B%5Cmathrm%7BCov%7D%7D(X,HX)"> and <img src="https://latex.codecogs.com/png.latex?H%20P_0%20H%5E%5Ctop%20=%20%5Cmathop%7B%5Cmathrm%7BVar%7D%7D(HX)"> for <img src="https://latex.codecogs.com/png.latex?X%20%5Csim%20%5Cpi_0">. This means that an approximate Kalman matrix can be obtained using empirical estimates of these covariance matrices:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%7BK%7D%20%5C;%20=%20%5C;%20%5Cwidehat%7B%5Cmathop%7B%5Cmathrm%7BCov%7D%7D%7D(%5Bx_i%5D_i,%20%5B%5Cmathcal%7BH%7D(x_i)%5D_i)%20%5C,%20%20%7B%5Cleft(%20%5Cwidehat%7B%5Cmathop%7B%5Cmathrm%7BVar%7D%7D%7D(%5B%5Cmathcal%7BH%7D(x_i)%5D_i)%20+%20R%20%5Cright)%7D%20%5E%7B-1%7D.%0A"></p>
<p>These updates form the basis of the <a href="https://en.wikipedia.org/wiki/Ensemble_Kalman_filter">Ensemble Kalman filter (EnKF)</a>, and very successful and scalable approach to data-assimilation of high-dimensional dynamical systems. This method is operationally employed across various weather forecasting centers across the globe.</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/Gaussian_Assimilation/evensen.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption">EnKF Bible by Geir Evensen</figcaption>
</figure>
</div>
</div>
</section>
<section id="derivative-free-optimization" class="level3">
<h3 class="anchored" data-anchor-id="derivative-free-optimization">Derivative-Free optimization</h3>
<p>Interestingly enough, the remarks above can be design in a relatively principled manner a derivative free optimizer <span class="citation" data-cites="huang2022efficient">(Huang et al. 2022)</span>. For example, assume one would like to minimize a functional of the type</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5CPsi(x)%20%5C;%20=%20%5C;%20%5C%7Cy_%5Cstar%20-%20%5Cmathcal%7BH%7D(x)%5C%7C%5E2_%7BR%5E%7B-1%7D%7D.%0A"></p>
<p>One can start with a cloud of particles <img src="https://latex.codecogs.com/png.latex?x_1,%20%5Cldots,%20x_N"> and keep updating them by assuming that one assimilates the noisy observation <img src="https://latex.codecogs.com/png.latex?y_%5Cstar"> generated from a postulated observation process</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay_%5Cstar%20=%20%5Cmathcal%7BH%7D(x)%20+%20%5Cvarepsilon%5E%7B-1%7D%20%5C,%20%5Cxi%0A"></p>
<p>for <img src="https://latex.codecogs.com/png.latex?%5Cxi%20%5Csim%20%5Cmathcal%7BN%7D(0,R)"> and <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5Cll%201"> a “step-size”. Each assimilation step steers the cloud of points towards the rights direction. Careful choice of the step-size <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon"> is often crucial, as in any optimization procedure. It is indeed related to Information-Geometric Optimization Algorithms (IGO): the article <span class="citation" data-cites="ollivier2017information">(Ollivier et al. 2017)</span> is beautiful!</p>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-huang2022efficient" class="csl-entry">
Huang, Daniel Zhengyu, Jiaoyang Huang, Sebastian Reich, and Andrew M Stuart. 2022. <span>“Efficient Derivative-Free Bayesian Inference for Large-Scale Inverse Problems.”</span> <em>Inverse Problems</em> 38 (12). IOP Publishing: 125006.
</div>
<div id="ref-ollivier2017information" class="csl-entry">
Ollivier, Yann, Ludovic Arnold, Anne Auger, and Nikolaus Hansen. 2017. <span>“Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles.”</span> <em>The Journal of Machine Learning Research</em> 18 (1). JMLR. org: 564–628.
</div>
</div></section></div> ]]></description>
  <category>enkf</category>
  <category>data-assimilation</category>
  <guid>https://alexxthiery.github.io/notes/Gaussian_Assimilation/gaussian_assimilation.html</guid>
  <pubDate>Sun, 22 Oct 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Deriving Langevin MCMC</title>
  <link>https://alexxthiery.github.io/notes/on_Langevin_MCMC/on_Langevin_MCMC.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/on_Langevin_MCMC/besag.jpg" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Julian Besag (1945 – 2010)</figcaption>
</figure>
</div>
</div>
<p>Consider a target density <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)"> in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5ED">. Since the Langevin diffusion</p>
<p><span id="eq-langevin"><img src="https://latex.codecogs.com/png.latex?%0AdX_t%20=%20%5Cnabla%20%5Clog%20%5Cpi(X_t)%20%5C,%20dt%20+%20%5Csqrt%7B2%7D%20%5C,%20dW%0A%5Ctag%7B1%7D"></span></p>
<p>is reversible with respect to <img src="https://latex.codecogs.com/png.latex?%5Cpi">, it is natural to use a <a href="https://en.wikipedia.org/wiki/Euler–Maruyama_method">Euler-Maruyama</a> discretization of Equation&nbsp;1 to build MCMC proposals: in a MCMC simulation and for a time discretization parameter <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%3E%200">, if the current position is <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathbb%7BR%7D%5ED">, a proposal <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cmathbb%7BR%7D%5ED"> can be generated as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay%20=%20x%20+%20%5Cvarepsilon%5C,%20%5Cnabla%20%5Clog%20%5Cpi(X_t)%20+%20%5Csqrt%7B2%20%5Cvarepsilon%7D%20%5C,%20%5Cxi%0A"></p>
<p>with <img src="https://latex.codecogs.com/png.latex?%5Cxi%20%5Csim%20%5Cmathcal%7BN%7D(0,I)"> before being accepted-or-reject according to the usual <a href="https://en.wikipedia.org/wiki/Metropolis–Hastings_algorithm">Metropolis-Hastings</a> ratio. This MCMC method, first proposed by <a href="https://en.wikipedia.org/wiki/Julian_Besag">Julian Besag</a> in 1994, is commonly referred to as the <a href="https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm">Metropolis-Adjusted-Langevin-Algorithm</a> (MALA). But how can one come-up with this proposal mechanism without knowing before hand the existence of this reversible Langevin diffusion Equation&nbsp;1? While it is intuitively clear that following the direction of <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20%5Clog%20%5Cpi"> is not such a bad idea, i.e.&nbsp;one would like to move towards areas of “high probability mass”, where does this <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7B2%7D"> comes from? Naturally, one could look at proposals of the type <img src="https://latex.codecogs.com/png.latex?y%20=%20x%20+%20%5Cnabla%20%5Clog%20%5Cpi(X_t)%20%5C,%20%5Cvarepsilon+%20%5Clambda%20%5C,%20%5Cxi"> for some free parameter <img src="https://latex.codecogs.com/png.latex?%5Clambda%20%3E%200"> and study the behavior of the Metropolis-Hastings ratio in the regime <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5Cto%200">: as simple as it sounds, it is not entirely straightforward and requires quite a bit of algebra (do it!). Instead, I very much like the type of approaches described in <span class="citation" data-cites="titsias2018auxiliary">(Titsias and Papaspiliopoulos 2018)</span>. To summarize, we would like to generate a MCMC proposal <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cmathbb%7BR%7D%5ED"> that stays in the vicinity of the current position <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathbb%7BR%7D%5ED"> while exploiting the knowledge of <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20%5Clog%20%5Cpi(x)">. One cannot simply approximate the target distribution as <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)%20%5Capprox%20%5Cpi(x_k)%20e%5E%7B%5Cleft%3C%20%5Cnabla%20%5Clog%20%5Cpi(x_k),%20x-x_k%20%5Cright%3E%7D"> and sample from this approximation since it is typically does not define a probability distribution. Instead, consider the following extended target distribution</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Coverline%7B%5Cpi%7D(x,z)%20%5C,%20%5Cpropto%20%5Cpi(x)%20%5C,%20%5Cexp%20%7B%5Cleft%5C%7B%20-%5Cfrac%7B1%7D%7B2%5Cvarepsilon%7D%5C%7Cz-x%5C%7C%5E2%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>In other words, the Gaussian auxiliary variable <img src="https://latex.codecogs.com/png.latex?z%20%5Cin%20%5Cmathbb%7BR%7D%5ED"> is centred at <img src="https://latex.codecogs.com/png.latex?x"> and at distance about <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7B%5Cvarepsilon%7D"> of it. Now, given the current position <img src="https://latex.codecogs.com/png.latex?x_k">, to generate a proposal <img src="https://latex.codecogs.com/png.latex?y_%5Cstar"> that stays in the vicinity of <img src="https://latex.codecogs.com/png.latex?x_k">, one can proceed in two steps, in the spirit of a Gibbs-sampling approach:</p>
<ol type="1">
<li><p>First, generate <img src="https://latex.codecogs.com/png.latex?z_%5Cstar%20%5Csim%20%5Coverline%7B%5Cpi%7D(dz%20%7C%20x_k)%20%5Csim%20%5Cmathcal%7BN%7D(x_k,%20%5Csqrt%7B%5Cvarepsilon%7DI)"></p></li>
<li><p>Second, sample from <img src="https://latex.codecogs.com/png.latex?y_%5Cstar%20%5Csim%20%5Coverline%7B%5Cpi%7D(dx%20%7C%20z_%5Cstar)">.</p></li>
</ol>
<p>Unfortunately, the second step is typically not tractable. Nevertheless, the conditional density <img src="https://latex.codecogs.com/png.latex?%5Coverline%7B%5Cpi%7D(dx%20%7C%20z_%5Cstar)"> is concentrated in a <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7B%5Cvarepsilon%7D">-neighborhood of <img src="https://latex.codecogs.com/png.latex?z_%5Cstar"> and a simple Gaussian approximation around <img src="https://latex.codecogs.com/png.latex?(x_k,%20z_%5Cstar)"> should be enough for our purpose. We have:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Clog%20%5Coverline%7B%5Cpi%7D(dx%20%7C%20z_%5Cstar)%0A&amp;=%0A%5Clog%20%5Cpi(x)%20-%20%5Cfrac%7B1%7D%7B2%20%5Cvarepsilon%7D%20%5C%7Cx%20-%20z_%5Cstar%5C%7C%5E2%20+%20%5Ctextrm%7B(Cst)%7D%5C%5C%0A&amp;%5Capprox%0A%5Cleft%3C%20%20%5Cnabla%20%5Clog%20%5Cpi(x_k),%20x-x_k%20%20%5Cright%3E%20-%20%5Cfrac%7B1%7D%7B2%20%5Cvarepsilon%7D%20%5C%7Cx%20-%20z_%5Cstar%5C%7C%5E2%20+%20%5Ctextrm%7B(Cst)%7D%5C%5C%0A&amp;=%0A-%20%5Cfrac%7B1%7D%7B2%20%5Cvarepsilon%7D%20%5C%7Cx%20-%20%5Bz_%5Cstar%20+%20%5Cvarepsilon%5C,%20%5Cnabla%20%5Clog%20%5Cpi(x_k)%5D%5C%7C%5E2%20+%20%5Ctextrm%7B(Cst)%7D.%0A%5Cend%7Balign%7D%0A"></p>
<p>This shows that the conditional <img src="https://latex.codecogs.com/png.latex?%5Coverline%7B%5Cpi%7D(dx%20%7C%20z_%5Cstar)"> can be approximated by a Gaussian distribution centred at <img src="https://latex.codecogs.com/png.latex?%5Bz_%5Cstar%20+%20%5Cnabla%20%5Clog%20%5Cpi(x_k)%5D"> and variance <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5C,%20I">. This means that the final proposal <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cmathbb%7BR%7D%5ED"> can be generated as <img src="https://latex.codecogs.com/png.latex?y%20%5Csim%20z_%5Cstar%20+%20%5Cnabla%20%5Clog%20%5Cpi(x_k)%20+%20%5Cxi"> where <img src="https://latex.codecogs.com/png.latex?%5Cxi%20%5Csim%20%5Cmathcal%7BN%7D(0,%5Cvarepsilon)">. But that is equivalent to setting</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay%20%5Csim%20x%20+%20%5Cvarepsilon%5C,%20%5Cnabla%20%5Clog%20%5Cpi(x_k)%20+%20%5Csqrt%7B2%20%5Cvarepsilon%7D%20%5C,%20%5Cxi%0A"></p>
<p>with <img src="https://latex.codecogs.com/png.latex?%5Cxi%20%5Csim%20%5Cmathcal%7BN%7D(0,I)"> since <img src="https://latex.codecogs.com/png.latex?z_%5Cstar%20%5Csim%20%5Cmathcal%7BN%7D(x,%20%5Csqrt%7B%5Cvarepsilon%7D%20I)">. It is exactly the MALA proposal. Naturally, one can also try to be slightly more clever and use an extended distribution</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Coverline%7B%5Cpi%7D(x,z)%20%5C,%20%5Cpropto%20%5C,%20%5Cpi(x)%20%5C,%20%5Cexp%20%7B%5Cleft%5C%7B%20%20-%5Cfrac%7B1%7D%7B2%5Cvarepsilon%7D%20%5Cleft%3C%20(z-x),%20M%5E%7B-1%7D%20%5C,%20(z-x)%20%5Cright%3E%20%20%5Cright%5C%7D%7D%0A"></p>
<p>for some appropriate positive-definite “mass” matrix <img src="https://latex.codecogs.com/png.latex?M%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BD,D%7D">. Indeed, this immediately leads to preconditioned MALA methods. I really like this approach since it can be adapted and generalized to quite a few other situations!</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-titsias2018auxiliary" class="csl-entry">
Titsias, Michalis K, and Omiros Papaspiliopoulos. 2018. <span>“Auxiliary Gradient-Based Sampling Algorithms.”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 80 (4). Oxford University Press: 749–67.
</div>
</div></section></div> ]]></description>
  <category>MCMC</category>
  <guid>https://alexxthiery.github.io/notes/on_Langevin_MCMC/on_Langevin_MCMC.html</guid>
  <pubDate>Wed, 18 Oct 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Wasserstein Gradients &amp; Langevin Diffusions</title>
  <link>https://alexxthiery.github.io/notes/wasserstein_langevin/wasserstein_langevin.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/wasserstein_langevin/langevin.gif" class="img-fluid figure-img" style="width:95.0%"></p>
<figcaption class="figure-caption">Overdamped Langevin Diffusion as a gradient flow on the space of probability distributions endowed with the <img src="https://latex.codecogs.com/png.latex?2">-Wasserstein metric</figcaption>
</figure>
</div>
</div>
<p>Consider a target probability density <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)%20=%20%5Cfrac%7B%5Coverline%7B%5Cpi%7D(x)%7D%7B%5Cmathcal%7BZ%7D%7D"> on <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5ED"> that is known up to a normalizing constant <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BZ%7D%3E%200">. We also have a different probability density <img src="https://latex.codecogs.com/png.latex?p_0(x)">. The goal is to gradually tweak <img src="https://latex.codecogs.com/png.latex?p_0(x)"> so that it eventually matches <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)">. More concretely, we aim to perform a gradient descent on the space of probability distributions to reduce the functional</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BF%7D(p)%20%5C;%20=%20%5C;%20D_%7B%5Ctext%7BKL%7D%7D%20%7B%5Cleft(%20p,%20%5Cpi%20%5Cright)%7D%20%20%5C;%20=%20%5C;%20%5Cint%20p(x)%20%5C,%20%5Clog%20%20%7B%5Cleft%5C%7B%20%5Cfrac%7Bp(x)%7D%7B%5Coverline%7B%5Cpi%7D(x)%7D%20%5Cright%5C%7D%7D%20%20%5C,%20dx%20%5C,%20+%20%5C,%20%5Ctextrm%7B(constant)%7D.%0A"></p>
<p>This approach can be discretized: assume <img src="https://latex.codecogs.com/png.latex?N%20%5Cgg%201"> particles <img src="https://latex.codecogs.com/png.latex?X_0%5E1,%20%5Cldots,%20X_0%5EN%20%5Cin%20%5Cmathbb%7BR%7D%5ED"> forming an empirical distribution that approximates <img src="https://latex.codecogs.com/png.latex?p_0(dx)">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap_0(dx)%20%5C;%20%5Capprox%20%5C;%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5EN%20%5C,%20%5Cdelta_%7BX_0%5Ei%7D(dx).%0A"></p>
<p>Define <img src="https://latex.codecogs.com/png.latex?X_%7B%5Cdelta%7D%5Ei%20=%20X_0%5Ei%20+%20%5Cdelta_t%20%5C,%20%5Cmu(X_0%5Ei)"> where <img src="https://latex.codecogs.com/png.latex?%5Cdelta_t%20%5Cll%201"> denotes a time discretization parameter and <img src="https://latex.codecogs.com/png.latex?%5Cmu:%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D%5ED"> is a “drift” function. Finding a suitable ‘drift function’ is the main problem. According to the <a href="https://en.wikipedia.org/wiki/Fokker–Planck_equation">Fokker-Planck</a> equation, the computed empirical distribution</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap_%7B%5Cdelta_t%7D(dx)%20%5C;%20%5Capprox%20%5C;%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5EN%20%5C,%20%5Cdelta_%7BX_%7B%5Cdelta_t%7D%5Ei%7D(dx)%0A"></p>
<p>approximates <img src="https://latex.codecogs.com/png.latex?p_%7B%5Cdelta_t%7D(x)"> given by</p>
<p><span id="eq-fokker"><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bp_%7B%5Cdelta_t%7D(x)-%20p_0(x)%7D%7B%5Cdelta_t%7D%20%5C;%20=%20%5C;%20-%5Cnabla%20%5Ccdot%20%20%7B%5Cleft%5B%20%5Cmu(x)%20%5C,%20p_0(x)%20%5Cright%5D%7D%20.%0A%5Ctag%7B1%7D"></span></p>
<p>What is the optimal drift function <img src="https://latex.codecogs.com/png.latex?%5Cmu:%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D%5ED"> that ensures that <img src="https://latex.codecogs.com/png.latex?p_%7B%5Cdelta_t%7D"> comes as close as possible to <img src="https://latex.codecogs.com/png.latex?%5Cpi">? Typically, we select <img src="https://latex.codecogs.com/png.latex?%5Cmu:%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D%5ED"> such that the quantity <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BF%7D(p_%7B%5Cdelta_t%7D)"> is minimized, provided that <img src="https://latex.codecogs.com/png.latex?p_%7B%5Cdelta_t%7D"> is not drastically different from <img src="https://latex.codecogs.com/png.latex?p_0">. One method is to use the <img src="https://latex.codecogs.com/png.latex?L%5E2"> <a href="https://en.wikipedia.org/wiki/Wasserstein_metric">Wasserstein distance</a> and assume the constraint</p>
<p><span id="eq-wass"><img src="https://latex.codecogs.com/png.latex?%0AD_%7B%5Ctext%7BWass%7D%7D(p_%7B0%7D,%20p_%7B%5Cdelta_t%7D)%20%5Capprox%20%5Cint%20p_0(x)%20%5C,%20%5C%7C%20%5Cdelta_t%20%5C,%20%5Cmu(x)%20%5C%7C%5E2%20%5C,%20dx%20%5Cleq%20%5Cvarepsilon%0A%5Ctag%7B2%7D"></span></p>
<p>for a parameter <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5Cll%201">. More pragmatically, it is generally easier (eg. proximal methods) to minimize the joint objective</p>
<p><span id="eq-joint-obj"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BF%7D(p_%7B%5Cdelta_t%7D)%20+%20%5Cfrac%7B1%7D%7B2%20%5Cvarepsilon%7D%20%5C,%20D_%7B%5Ctext%7BWass%7D%7D(p_%7B0%7D,%20p_%7B%5Cdelta_t%7D).%0A%5Ctag%7B3%7D"></span></p>
<p>Based on equations Equation&nbsp;1 and Equation&nbsp;2, a first-order expansion shows that the joint objective Equation&nbsp;3 can be approximated by</p>
<p><span id="eq-quad"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A-%5Cint%20&amp;%5Cnabla%20%5Ccdot%20%5CBig%5C%7B%20%20%5Ctextcolor%7Bred%7D%7B%5B%5Cdelta_t%20%5Cmu%5D%7D(x)%20%5C,%20p_0(x)%20%5CBig%5C%7D%20%5C,%20%5Clog%20%20%7B%5Cleft%5C%7B%20%20%5Cfrac%7Bp_0(x)%7D%7B%5Coverline%7B%5Cpi%7D(x)%7D%20%20%5Cright%5C%7D%7D%20%5C,%20dx%20%5C,%20%5C%5C%20&amp;%5Cqquad%20+%20%5Cqquad%20%5C,%20%5Cfrac%7B1%7D%7B2%20%5Cvarepsilon%7D%20%5C,%20%5Cint%20p_0(x)%20%5C,%20%5C%7C%20%20%5Ctextcolor%7Bred%7D%7B%5B%5Cdelta_t%20%5C,%20%5Cmu%5D%7D(x)%20%5C%7C%5E2%20%5C,%20dx,%0A%5Cend%7Balign%7D%0A%5Ctag%7B4%7D"></span></p>
<p>a relatively straightforward quadratic function of the drift function <img src="https://latex.codecogs.com/png.latex?%5Cmu:%20%5Cmathbb%7BR%7D%5ED%20%5Cto%20%5Cmathbb%7BR%7D%5ED">. The optimal drift function, ie. the minimizer of Equation&nbsp;4, is given by <img src="https://latex.codecogs.com/png.latex?%0A%5Cmu(x)%20%5C;%20=%20%5C;%20-%20%7B%5Cleft(%20%20%5Cfrac%7B%5Cvarepsilon%7D%7B%5Cdelta_t%7D%20%20%5Cright)%7D%20%20%5C,%20%5Cnabla%20%5Clog%20%20%7B%5Cleft%5C%7B%20%20%5Cfrac%7Bp_0(x)%7D%7B%5Coverline%7B%5Cpi%7D(x)%20%7D%20%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>Put simply, this suggests that we should select the drift function proportional to <img src="https://latex.codecogs.com/png.latex?-%5Cnabla%20%5Clog%5Bp_0(x)%20/%20%5Coverline%7B%5Cpi%7D(x)%5D">. To implement this scheme, we begin by sampling <img src="https://latex.codecogs.com/png.latex?N%20%5Cgg%201"> particles <img src="https://latex.codecogs.com/png.latex?X_0%5Ei%20%5Csim%20p_0(dx)"> and let evolve each particle according to the following differential equation</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%7D%7Bdt%7D%20X_t%5Ei%20%5C;%20=%20%5C;%20-%20%5Cnabla%20%5Clog%20%20%7B%5Cleft%5C%7B%20%20%5Cfrac%7Bp_t(X_t%5Ei)%20%7D%7B%20%5Coverline%7B%5Cpi%7D(X_t%5Ei)%20%7D%20%20%5Cright%5C%7D%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?p_t"> is the density of the set of particles at time <img src="https://latex.codecogs.com/png.latex?t">. It is the usual <a href="../../notes/DDPM_deterministic/DDPM_deterministic.html">diffusion-ODE trick</a> for describing the evolution of the density of an <a href="https://en.wikipedia.org/wiki/Brownian_dynamics">overdamped Langevin diffusion</a>,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AdX_t%20%5C;%20=%20%5C;%20-%5Cnabla%20%5Clog%20%5Coverline%7B%5Cpi%7D(X_t)%20%5C,%20dt%20%5C;%20+%20%5C;%20%5Csqrt%7B2%7D%20%5C,%20dW_t.%0A"></p>
<p>This can be shown by writing down the associated Fokker-Planck equation. This heuristic discussion shows that minimizing <img src="https://latex.codecogs.com/png.latex?D_%7B%5Ctext%7BKL%7D%7D(p,%20%5Cpi)"> by introducing a gradient flow in the space of probability distributions with the Wasserstein metric essentially produces a standard overdamped Langevin diffusion. Indeed, transforming this heuristic discussion into a formal statement is not trivial: the constructive proof in <span class="citation" data-cites="jordan1998variational">(Jordan, Kinderlehrer, and Otto 1998)</span> is now usually referred to as the JKO scheme.</p>
<p>The above derivation shows that the Wasserstein distance plays particularly well with minimizing functionals of the space of probability distributions. The same heuristic discussion shows that minimizing a functional of the type</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BF%7D(p)%20%5C;%20=%20%5C;%20%5Cint%20%5CPhi%5Bp(x)%5D%20%5C,%20%5Cnu(dx)%0A"></p>
<p>for some cost function <img src="https://latex.codecogs.com/png.latex?%5CPhi:%20(0,%20%5Cinfty)%20%5Cto%20%5Cmathbb%7BR%7D"> and distribution <img src="https://latex.codecogs.com/png.latex?%5Cnu(dx)"> leads to choosing a drift function <img src="https://latex.codecogs.com/png.latex?%5Cmu:%5Cmathbb%7BR%7D%5Cto%20%5Cmathbb%7BR%7D"> minimizing</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cint%20-%5Cnabla%20%5Ccdot%20%5CBig%5C%7B%20%20%5Ctextcolor%7Bred%7D%7B%5B%5Cdelta_t%20%5C,%20%5Cmu%5D%7D(x)%20%5C,%20p(x)%20%5CBig%5C%7D%20%5CPhi'%5Bp(x)%5D%20%5C,%20%5Cnu(dx)%0A%5C,%20+%20%5C,%20%5Cfrac%7B1%7D%7B2%20%5Cvarepsilon%7D%20%5C,%20%5Cint%20p_0(x)%20%5C,%20%5C%7C%20%20%5Ctextcolor%7Bred%7D%7B%5B%5Cdelta_t%20%5C,%20%5Cmu%5D%7D(x)%20%5C%7C%5E2%20%5C,%20dx.%0A"></p>
<p>This can be approached identically to what as been done in the case of minimizing <img src="https://latex.codecogs.com/png.latex?D_%7B%5Ctext%7BKL%7D%7D(p,%20%5Cpi)">.</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-jordan1998variational" class="csl-entry">
Jordan, Richard, David Kinderlehrer, and Felix Otto. 1998. <span>“The Variational Formulation of the Fokker–Planck Equation.”</span> <em>SIAM Journal on Mathematical Analysis</em> 29 (1). SIAM: 1–17.
</div>
</div></section></div> ]]></description>
  <category>diffusion</category>
  <guid>https://alexxthiery.github.io/notes/wasserstein_langevin/wasserstein_langevin.html</guid>
  <pubDate>Sun, 15 Oct 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Sanov’s Theorem</title>
  <link>https://alexxthiery.github.io/notes/sanov/sanov.html</link>
  <description><![CDATA[ 




<section id="sanovs-theorem" class="level3">
<h3 class="anchored" data-anchor-id="sanovs-theorem">Sanov’s Theorem</h3>
<p>Consider a random variable <img src="https://latex.codecogs.com/png.latex?X"> on the finite alphabet <img src="https://latex.codecogs.com/png.latex?%5C%7Ba_1,%20%5Cldots,%20a_K%5C%7D"> with <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(X=a_k)%20=%20p_k">. For <img src="https://latex.codecogs.com/png.latex?N%20%5Cgg%201">, consider a sequence <img src="https://latex.codecogs.com/png.latex?(x_1,%20%5Cldots,%20x_N)"> obtained by sampling <img src="https://latex.codecogs.com/png.latex?N"> times independently from <img src="https://latex.codecogs.com/png.latex?X"> and set</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%7Bp%7D_k%20=%20%5Cfrac%7B1%7D%7BN%7D%20%5C,%20%5Csum_%7Bi=1%7D%5EN%20%5C,%20%5Cmathbf%7B1%7D%20%7B%5Cleft(%20x_i%20=%20a_k%20%5Cright)%7D%0A"></p>
<p>the proportion of <img src="https://latex.codecogs.com/png.latex?a_k"> within this sequence. In other words, the empirical distribution obtained from the samples <img src="https://latex.codecogs.com/png.latex?(x_1,%20%5Cldots,%20x_N)"> reads</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%7Bp%7D%20=%20%5Csum_%7Bk=1%7D%5EK%20%5C,%20%5Cwidehat%7Bp%7D_k%20%5C,%20%5Cdelta_%7Ba_k%7D.%0A"></p>
<p>Indeed, the <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers">LLN</a> indicates that <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bp%7D_k%20%5Cto%20p_k"> as <img src="https://latex.codecogs.com/png.latex?N%20%5Cto%20%5Cinfty">, and it is important to estimate the probability that <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bp%7D_k"> significantly deviates from <img src="https://latex.codecogs.com/png.latex?p_k">. To this end, note that for another probability vector <img src="https://latex.codecogs.com/png.latex?q=(q_1,%20%5Cldots,%20q_K)"> the probability that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A(%5Cwidehat%7Bp%7D_1,%20%5Cldots,%20%5Cwidehat%7Bp%7D_K)%20%5C;%20=%20%5C;%20(q_1,%20%5Cldots,%20q_K)%0A"></p>
<p>is straightforward to compute and reads</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BP%7D(%5Cwidehat%7Bp%7D%20=%20q)%20%5C;%20=%20%5C;%20%5Cbinom%7BN%7D%7BN%20q_1,%20%5Cldots,%20N%20q_K%7D%20%5C,%20p_1%5E%7BN%20q_1%7D%20%5Cldots%20p_R%5E%7BN%20q_K%7D.%0A"></p>
<p><a href="https://en.wikipedia.org/wiki/Stirling%27s_approximation">Stirling’s approximation</a> <img src="https://latex.codecogs.com/png.latex?m!%20%5Casymp%20m%20%5C,%20%5Cln(m)"> then gives that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BP%7D(%5Cwidehat%7Bp%7D%20=%20q)%20%5C;%20%5Casymp%20%5C;%0A%5Cexp%20%7B%5Cleft(%20-N%20%5Ccdot%20D_%7B%5Ctext%7BKL%7D%7D(q,p)%20%5Cright)%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?D_%7B%5Ctext%7BKL%7D%7D(q,p)%20=%20%5Csum_%7Bk=1%7D%5EK%20q_k%20%5C,%20%5Clog%5Bq_k%20/%20p_k%5D"> is the <a href="https://en.wikipedia.org/wiki/Kullback–Leibler_divergence">Kullback–Leibler divergence</a> of <img src="https://latex.codecogs.com/png.latex?q"> from <img src="https://latex.codecogs.com/png.latex?p">. In other words, as soon as <img src="https://latex.codecogs.com/png.latex?q%20%5Cneq%20p">, the probability of observing <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bp%7D%20%5Capprox%20q"> falls exponentially quickly to zero. With the language of <a href="https://en.wikipedia.org/wiki/Large_deviations_theory">Large Deviations</a>, one can make this statement slightly more precise, rigorous and general, but it is essentially the content of <a href="https://en.wikipedia.org/wiki/Sanov%27s_theorem">Sanov’s Theorem</a>.</p>
</section>
<section id="rare-events-happen-in-the-least-unlikely-manner" class="level3">
<h3 class="anchored" data-anchor-id="rare-events-happen-in-the-least-unlikely-manner">Rare events happen in the least unlikely manner</h3>
<p>Given a list of mutually exclusive events <img src="https://latex.codecogs.com/png.latex?E_1,%20%5Cldots,%20E_R"> and the knowledge that at least one of these events has taken place, the probability that the event <img src="https://latex.codecogs.com/png.latex?E_k"> was the one that happened is <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(E_k)%20/%20%5B%5Cmathbb%7BP%7D(E_1)%20+%20%5Cldots%20+%20%5Cmathbb%7BP%7D(E_R)%5D">. The implication is that if all the events are rare, that is <img src="https://latex.codecogs.com/png.latex?p_k%20%5Capprox%20e%5E%7B-N%20%5C,%20I_k%7D%20%5Cll%201">, and it is known that one event has indeed occurred, there is a high probability that the event with the smallest <img src="https://latex.codecogs.com/png.latex?I_k"> value was the one that happened: the rare event took place in the least unlikely manner.</p>
<p>Consider an iid sequence <img src="https://latex.codecogs.com/png.latex?(X_1,%20%5Cldots,%20X_N)"> of <img src="https://latex.codecogs.com/png.latex?N%20%5Cgg%201"> discrete real-valued random variables with <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(X%20=%20a_k)%20=%20p_k"> and mean <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D(X)%20%5Cin%20%5Cmathbb%7BR%7D">. Suppose one observes the rare event</p>
<p><span id="eq-rare"><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5EN%20x_i%20%20%5Cgeq%20%5Cmu%0A%5Ctag%7B1%7D"></span></p>
<p>for some level <img src="https://latex.codecogs.com/png.latex?%5Cmu"> significantly above <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D(X)">. Naturally, the least unlikely way for this to happen is if <img src="https://latex.codecogs.com/png.latex?(x_1%20+%20%5Cldots%20+%20x_N)%20/%20N%20%5C,%20%5Capprox%20%5C,%20%5Cmu">. Furthermore, one may be interested in the empirical distribution <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bp%7D"> associated to the sequence <img src="https://latex.codecogs.com/png.latex?(x_1,%20%5Cldots,%20x_N)"> when the rare event Equation&nbsp;1 does happen. The least unlikely empirical distribution is the one that minimizes <img src="https://latex.codecogs.com/png.latex?D_%7B%5Ctext%7BKL%7D%7D(%5Cwidehat%7Bp%7D,%20p)"> under the constraint that</p>
<p><span id="eq-contraint"><img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bi=1%7D%5EK%20a_k%20%5C,%20%5Cwidehat%7Bp%7D_k%20=%20%5Cmu.%0A%5Ctag%7B2%7D"></span></p>
<p>The function <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7Bp%7D%20%5Cmapsto%20D_%7B%5Ctext%7BKL%7D%7D(%5Cwidehat%7Bp%7D,%20p)"> is convex and the introduction of Lagrange multipliers shows that the solution to this constraint minimization problem is given by the Boltzmann distribution <img src="https://latex.codecogs.com/png.latex?p_%7B%5Cbeta_%5Cmu%7D"> defined as</p>
<p><span id="eq-boltz"><img src="https://latex.codecogs.com/png.latex?%0Ap_%7B%5Cbeta_%5Cmu%7D(a_k)%20=%20%5Cfrac%7B%20p_k%20%5C,%20e%5E%7B-%5Cbeta_%7B%5Cmu%7D%20%5C,%20a_k%7D%20%7D%7BZ(%5Cbeta_%7B%5Cmu%7D)%7D.%0A%5Ctag%7B3%7D"></span></p>
<p>The parameter <img src="https://latex.codecogs.com/png.latex?%5Cbeta_%7B%5Cmu%7D%20%5Cin%20%5Cmathbb%7BR%7D"> is chosen so that the constraint Equation&nbsp;2 be satisfied and the minus sign is to follow the “physics” convention. Note in passing that, in fact, the joint function <img src="https://latex.codecogs.com/png.latex?(%5Cwidehat%7Bp%7D,p)%20%5Cmapsto%20D_%7B%5Ctext%7BKL%7D%7D(%5Cwidehat%7Bp%7D,%20p)"> is convex! As usual, if one defines the log-partition function as <img src="https://latex.codecogs.com/png.latex?%5CPhi(%5Cbeta)%20=%20-%5Clog%20Z(%5Cbeta)">, with</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AZ(%5Cbeta)%20%5C;%20=%20%5C;%20%5Csum_%7Bk=1%7D%5EK%20%5C,%20p_k%20%5C,%20e%5E%7B-%5Cbeta%20%5C,%20a_k%7D,%0A"></p>
<p>one obtains that the constraint is equivalent to requiring <img src="https://latex.codecogs.com/png.latex?%5Cmu%20=%20%5Cfrac%7Bd%7D%7Bd%20%5Cbeta%7D%20%5CPhi(%5Cbeta)%20%5Cmid_%7B%5Cbeta%20=%20%5Cbeta_%5Cmu%7D">. Furthermore, since <img src="https://latex.codecogs.com/png.latex?%5CPhi"> is smooth and strictly concave, the function <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20%5Cmapsto%20%5Cleft%3C%20%5Cmu,%20%5Cbeta%20%5Cright%3E%20-%20%5CPhi(%5Cbeta)"> is convex and the condition <img src="https://latex.codecogs.com/png.latex?%5Cmu%20=%20%5Cfrac%7Bd%7D%7Bd%20%5Cbeta%7D%20%5CPhi(%5Cbeta)%20%5Cmid_%7B%5Cbeta%20=%20%5Cbeta_%5Cmu%7D"> is equivalent to setting</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbeta_%7B%5Cmu%7D%20%5C;%20=%20%5C;%20%5Cmathop%7B%5Cmathrm%7Bargmin%7D%7D_%7B%5Cbeta%20%5Cin%20%5Cmathbb%7BR%7D%7D%20%5C;%20%5Cleft%3C%20%5Cmu,%20%5Cbeta%20%5Cright%3E%20-%20%5CPhi(%5Cbeta).%0A"></p>
<p>Naturally, one can now also estimate the probability of the event <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5B(X_1%20+%20%5Cldots%20+%20X_N)/N%20%5Capprox%20%5Cmu%5D"> happening since one now knows that it is equivalent (on a log scale) to <img src="https://latex.codecogs.com/png.latex?%5Cexp%5B-N%20%5C,%20D_%7B%5Ctext%7BKL%7D%7D(p_%7B%5Cbeta_%5Cmu%7D,%20p)%5D">. Algebra gives</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AD_%7B%5Ctext%7BKL%7D%7D(p_%7B%5Cbeta_%5Cmu%7D,%20p)%0A&amp;=%20%5CPhi(%5Cbeta_%5Cmu)%20-%20%5Cleft%3C%20%5Cmu,%20%5Cbeta_%5Cmu%20%5Cright%3E%5C%5C%0A&amp;=%20%5Cmax_%7B%5Cbeta%20%5Cin%20%5Cmathbb%7BR%7D%7D%20%5C;%20%5CPhi(%5Cbeta)%20-%20%5Cleft%3C%20%5Cmu,%20%5Cbeta%20%5Cright%3E.%0A%5Cend%7Balign%7D%0A"></p>
<p>As a sanity check, note that since <img src="https://latex.codecogs.com/png.latex?%5CPhi(0)=0">, we have that <img src="https://latex.codecogs.com/png.latex?D_%7B%5Ctext%7BKL%7D%7D(p_%7B%5Cbeta_%5Cmu%7D,%20p)%20%5Cgeq%200">, as required. The statement that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7BN%7D%20%5C,%20%5Clog%20%5Cmathbb%7BP%7D%20%7B%5Cleft%5C%7B%20%5Cfrac%7BX_1%20+%20%5Cldots%20+%20X_N%7D%7BN%7D%20%5C;%20%5Capprox%20%5C;%20%5Cmu%20%5Cright%5C%7D%7D%20%20%5C;%20=%20%5C;%20-%20I(%5Cmu)%0A"></p>
<p>with a (Large Deviation) rate function given by</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AI(%5Cmu)%20%5C;%20=%20%5C;%20%5Cmax_%7B%5Cbeta%20%5Cin%20%5Cmathbb%7BR%7D%7D%20%5C;%20%5CPhi(%5Cbeta)%20-%20%5Cleft%3C%20%5Cmu,%20%5Cbeta%20%5Cright%3E%0A"></p>
<p>is more or less the content of <a href="https://en.wikipedia.org/wiki/Cramér%27s_theorem_(large_deviations)">Cramer’s Theorem</a>. The rate function <img src="https://latex.codecogs.com/png.latex?I(%5Cmu)"> and the function <img src="https://latex.codecogs.com/png.latex?%5Clog%20Z(%20%5Ctextcolor%7Bred%7D%7B-%7D%5Cbeta)"> are related by a <a href="https://en.wikipedia.org/wiki/Legendre_transformation">Legendre transform</a>.</p>
</section>
<section id="example-averaging-uniforms" class="level3">
<h3 class="anchored" data-anchor-id="example-averaging-uniforms">Example: averaging uniforms…</h3>
<p>Now, to illustrate the above discussion, consider <img src="https://latex.codecogs.com/png.latex?N=10"> iid uniform random variables on the interval <img src="https://latex.codecogs.com/png.latex?%5B0,1%5D">. It is straightforward to simulate these <img src="https://latex.codecogs.com/png.latex?N=10"> uniforms conditioned on the event that their mean exceeds the level <img src="https://latex.codecogs.com/png.latex?%5Cmu%20=%200.7">, which is a relatively rare event. When this is done repetitively and the empirical distribution is plotted, the resulting distribution is as follows:</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/sanov/boltzman.png" class="img-fluid figure-img" style="width:95.0%"></p>
<figcaption class="figure-caption">Mean of <img src="https://latex.codecogs.com/png.latex?10"> uniforms conditioned on being larger than <img src="https://latex.codecogs.com/png.latex?%5Cmu%20=%200.7"></figcaption>
</figure>
</div>
</div>
<p>Indeed, the distribution in blue is (very close to) the Boltzmann distribution with density <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7B%5Cbeta%7D(x)%20=%20e%5E%7B-%5Cbeta%20%5C,%20x%7D%20/%20Z(%5Cbeta)"> with <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20%5Cin%20%5Cmathbb%7BR%7D"> chosen so that <img src="https://latex.codecogs.com/png.latex?%5Cint_%7B0%7D%5E%7B1%7D%20x%20%5C,%20%5Cmathcal%7BD%7D_%7B%5Cbeta%7D(dx)%20=%20%5Cmu">.</p>


</section>

 ]]></description>
  <category>LargeDeviation</category>
  <guid>https://alexxthiery.github.io/notes/sanov/sanov.html</guid>
  <pubDate>Sun, 08 Oct 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Auxiliary variable trick</title>
  <link>https://alexxthiery.github.io/notes/auxiliary_variable_trick/auxiliary_variable_trick.html</link>
  <description><![CDATA[ 




<p>Consider a complicated distribution on the state space <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathcal%7BX%7D"> given by</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(x)%20=%20%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D%7D%20%5C,%20e%5E%7BC(x)%20+%20B(x)%7D%0A"></p>
<p>for a “complicated” functions <img src="https://latex.codecogs.com/png.latex?C(x)"> and a simpler one <img src="https://latex.codecogs.com/png.latex?B(x)">. In some situations, it is possible to introduce an auxiliary random variable <img src="https://latex.codecogs.com/png.latex?a%20%5Cin%20%5Cmathcal%7BA%7D"> and an extended probability distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi(x,a)"> on the extended space <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D%5Ctimes%20%5Cmathcal%7BA%7D">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(x,a)%20=%20%5Cpi(x)%20%5C,%20%20%5Ctextcolor%7Bred%7D%7B%5Cpi(a%20%7C%20x)%7D%20=%20%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D%7D%20e%5E%7BC(x)%20+%20B(x)%7D%20%5C,%20%20%5Ctextcolor%7Bred%7D%7Be%5E%7B-C(x)%20+%20D(x,%20a)%7D%7D,%0A"></p>
<p>with a tractable conditional probability <img src="https://latex.codecogs.com/png.latex?%5Cpi(a%20%7C%20x)">. This extended target distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi(x,a)%20=%20(1/%5Cmathcal%7BZ%7D)%20%5C,%20%5Cexp%5BB(x)%20+%20D(x,a)%5D"> can be often be easier to explore, for example when <img src="https://latex.codecogs.com/png.latex?a"> is continuous while <img src="https://latex.codecogs.com/png.latex?x"> is discrete, or to analyze, since the “complicated” term <img src="https://latex.codecogs.com/png.latex?C(x)"> has disappeared. Furthermore, there are a number of scenarios when the variable <img src="https://latex.codecogs.com/png.latex?x"> can be averaged out of the extended distribution, i.e.&nbsp;the distribution</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(a)%20=%20%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D%7D%20%5C,%20%5Cint_%7Bx%20%5Cin%20%5Cmathcal%7BX%7D%7D%20e%5E%7BB(x)%20+%20D(x,a)%7D%0A"></p>
<p>can be evaluated exactly.</p>
<section id="swendsenwang-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="swendsenwang-algorithm">Swendsen–Wang algorithm</h3>
<p>Consider a set of edges <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BE%7D"> on a graph with vertices <img src="https://latex.codecogs.com/png.latex?%5C%7B1,%20%5Cldots,%20N%5C%7D">. The Ising model is defined as <img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(x)%20%5Cpropto%20%5Cexp%20%20%7B%5Cleft%5C%7B%20%20%5Csum_%7B(i,j)%20%5Cin%20%5Cmathcal%7BE%7D%7D%20%5Cbeta%20x_i%20x_j%20%20%5Cright%5C%7D%7D%0A"></p>
<p>for spin configurations <img src="https://latex.codecogs.com/png.latex?x=(x_1,%20%5Cldots,%20x_N)%20%5Cin%20%5C%7B-1,1%5C%7D%5EN">. The term <img src="https://latex.codecogs.com/png.latex?%5Cexp%5B%5Cbeta%20x_i%20x_j%5D"> couples the two spins <img src="https://latex.codecogs.com/png.latex?x_i"> and <img src="https://latex.codecogs.com/png.latex?x_j"> for each edge <img src="https://latex.codecogs.com/png.latex?(i,j)%20%5Cin%20%5Cmathcal%7BE%7D">. The idea of the <a href="https://en.wikipedia.org/wiki/Swendsen–Wang_algorithm">Swendsen–Wang_algorithm</a> is to introduce an auxiliary variable <img src="https://latex.codecogs.com/png.latex?u_%7Bi,j%7D"> for each edge <img src="https://latex.codecogs.com/png.latex?(i,j)%20%5Cin%20%5Cmathcal%7BE%7D"> that is uniformly distributed on the interval <img src="https://latex.codecogs.com/png.latex?%5B0,%20%5Cexp(%5Cbeta%20x_i%20x_j)%5D">, i.e.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(u_%7Bi,j%7D%20%7C%20x)%20%5C;%20=%20%5C;%20%5Cfrac%7B%20%5Cmathbf%7B1%7D%20%20%7B%5Cleft%5C%7B%20%200%20%3C%20u_%7Bi,j%7D%20%3C%20%5Cexp%5B%5Cbeta%20x_i%20x_j%5D%20%20%5Cright%5C%7D%7D%20%20%7D%7B%5Cexp%5B%5Cbeta%20x_i%20x_j%5D%20%7D%0A"></p>
<p>It follows that the extended distribution on <img src="https://latex.codecogs.com/png.latex?%5C%7B-1,1%5C%7D%5EN%20%5Ctimes%20(0,%5Cinfty)%5E%7B%7C%5Cmathcal%7BE%7D%7C%7D"> reads</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(x,u)%20=%20%5Cfrac%7B1%7D%7BZ%7D%20%5Cprod_%7B(i,j)%20%5Cin%20%5Cmathcal%7BE%7D%7D%20%5C;%20%5Cmathbf%7B1%7D%20%20%7B%5Cleft%5C%7B%20%200%20%3C%20u_%7Bi,j%7D%20%3C%20%5Cexp%5B%5Cbeta%20x_i%20x_j%5D%20%20%5Cright%5C%7D%7D%0A"></p>
<p>for <img src="https://latex.codecogs.com/png.latex?x=(x_1,%20%5Cldots,%20x_N)"> and <img src="https://latex.codecogs.com/png.latex?u%20=%20(u_%7Bi,j%7D)_%7B(i,j)%20%5Cin%20%5Cmathcal%7BE%7D%7D">: the coupling term <img src="https://latex.codecogs.com/png.latex?%5Cexp%5B%5Cbeta%20x_i%20x_j%5D"> has disappeared. Furthermore, it is straightforward to sample from the conditional distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi(u%20%7C%20x)"> and, perhaps surprisingly, it is also relatively straightforward to sample from the other conditional distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi(x%20%7C%20u)"> – this boils down to finding the connect components of the graph on <img src="https://latex.codecogs.com/png.latex?%5C%7B1,%20%5Cldots,%20N%5C%7D"> with an edge <img src="https://latex.codecogs.com/png.latex?i%20%5Csim%20j"> present if <img src="https://latex.codecogs.com/png.latex?u_%7Bi,j%7D%20%3E%20e%5E%7B-%5Cbeta%7D"> and flipping a fair coin for setting each connected component to <img src="https://latex.codecogs.com/png.latex?%5Cpm%201">. This leads to an efficient Gibbs sampling scheme to sample from the extended distribution.</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/auxiliary_variable_trick/ising.gif" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Swendsen-Wang MCMC algorithm at critical temperature</figcaption>
</figure>
</div>
</div>
</section>
<section id="gaussian-integral-trick-curie-weiss-model" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-integral-trick-curie-weiss-model">Gaussian Integral trick: Curie-Weiss model</h3>
<p>For an inverse temperature <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20%3E%200">, consider the distribution on <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5C%7B-1,1%5C%7D%5EN"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(x)%20=%20%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D(%5Cbeta)%7D%20%5C,%20e%5E%7B%5Cbeta%20%5C,%20N%20%5C,%20m%5E2%7D%0A"></p>
<p>where the magnetization of the system of spins <img src="https://latex.codecogs.com/png.latex?x=(x_1,%20%5Cldots,%20x_N)"> is defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Am%20=%20%5Cfrac%7Bx_1%20+%20%5Cldots%20+%20x_N%7D%7BN%7D.%0A"></p>
<p>The distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)"> for <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20%5Cgg%201"> favours configurations with a magnetization close to <img src="https://latex.codecogs.com/png.latex?+1"> or <img src="https://latex.codecogs.com/png.latex?-1">. The normalization constant (i.e.&nbsp;partition function) <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BZ%7D(%5Cbeta)"> is a sum of <img src="https://latex.codecogs.com/png.latex?2%5EN"> terms,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BZ%7D(%5Cbeta)%20=%20%5Csum_%7Bs_1%20%5Cin%20%5C%7B%20%5Cpm%201%5C%7D%20%7D%20%5Cldots%20%5Csum_%7Bs_N%20%5Cin%20%5C%7B%20%5Cpm%201%5C%7D%20%7D%20%5Cexp%20%20%7B%5Cleft%5C%7B%20%20%5Cfrac%7B%5Cbeta%7D%7BN%7D%20%20%7B%5Cleft(%20%20%5Csum_%7Bi=1%7D%5EN%20x_i%20%20%5Cright)%7D%20%5E2%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>It is not difficult to estimate <img src="https://latex.codecogs.com/png.latex?%5Clog%20%5Cmathcal%7BZ%7D(%5Cbeta)"> as <img src="https://latex.codecogs.com/png.latex?N%20%5Cto%20%5Cinfty"> with combinatorial arguments. Nevertheless, another way to proceed is as follows. One can introduce a Gaussian auxiliary random variable <img src="https://latex.codecogs.com/png.latex?%5Cpi(a%20%7C%20x)%20=%20%5Cmathcal%7BN%7D(%5Calpha%20%5C,%20%20%7B%5Cleft(%20%5Csum_i%20x_i%20%5Cright)%7D%20%20,%20%5Csigma%5E2)"> with mean <img src="https://latex.codecogs.com/png.latex?%5Cmu%20=%20%5Calpha%20%5C,%20%20%7B%5Cleft(%20%5Csum_i%20x_i%20%5Cright)%7D%20"> and variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">: the parameters <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2%20%3E%200"> can then be judiciously chosen to cancel the bothering term <img src="https://latex.codecogs.com/png.latex?%5Cexp%5B%5Cfrac%7B%5Cbeta%7D%7BN%7D%20%5C,%20m%5E2%5D">. This approach is often called the a <a href="https://en.wikipedia.org/wiki/Hubbard–Stratonovich_transformation">Hubbard-Stratonovich transformation</a>. The bothering “coupling” term disappears when when choosing <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Calpha%5E2%7D%7B2%20%5Csigma%5E2%7D%20=%20%5Cfrac%7B%5Cbeta%7D%7BN%7D">. With such a choice, it follows that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(x,%20a)%20=%20%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D(%5Cbeta)%7D%20%5C,%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%20%5Cpi%20%5Csigma%5E2%7D%7D%20%5Cexp%20%7B%5Cleft%5C%7B%20%20-%20%5Cfrac%7Ba%5E2%7D%7B2%20%5Csigma%5E2%7D%20+%20%5Cfrac%7B%5Calpha%7D%7B%5Csigma%5E2%7D%20a%20%5C,%20%20%7B%5Cleft(%20%5Csum_i%20x_i%20%5Cright)%7D%20%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>Averaging out the <img src="https://latex.codecogs.com/png.latex?x_i%20%5Cin%20%5C%7B-1,%20+1%5C%7D"> gives that the partition function reads</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BZ%7D(%5Cbeta)%20%5C;%20=%20%5C;%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B2%20%5Cpi%20%5Csigma%5E2%7D%7D%20%5C,%0A%5Cint_%7Ba%20=%20-%5Cinfty%7D%5E%7Ba=%5Cinfty%7D%20%5C,%0A%5Cexp%20%7B%5Cleft%5C%7B%20-%5Cfrac%7Ba%5E2%7D%7B2%20%5Csigma%5E2%7D%20+%20%20%5Ctextcolor%7Bred%7D%7BN%7D%20%5C,%20%5Clog%5B%202%20%5C,%20%5Ccosh(%5Calpha%20a%20/%20%5Csigma%5E2)%5D%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>In order to use the <a href="https://en.wikipedia.org/wiki/Method_of_steepest_descent">method of steepest descent</a>, it would be useful to have an integrand of the type <img src="https://latex.codecogs.com/png.latex?%5Cexp%5BN%20%5Ctimes%20(%5Cldots)%5D">. One can choose <img src="https://latex.codecogs.com/png.latex?1/(2%20%5C,%20%5Csigma%5E2)%20=%20%5Cbeta%20%5C,%20N"> and <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%201/N">. This gives</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BZ%7D(%5Cbeta)%20%5C;%20=%20%5C;%0A%5Cfrac%7B1%7D%7B%5Csqrt%7B2%20%5Cpi%20%5Csigma%5E2%7D%7D%20%5C,%0A%5Cint_%7Ba%20=%20-%5Cinfty%7D%5E%7Ba=%5Cinfty%7D%20%5C,%0A%5Cexp%20%20%7B%5Cleft%5C%7B%20%20%20%5Ctextcolor%7Bred%7D%7BN%7D%20%20%7B%5Cleft%5B%20%20-%5Cbeta%20%5C,%20a%5E2%20+%20%5Clog%5B%202%20%5C,%20%5Ccosh(2%20%5Cbeta%20a%20)%5D%20%5Cright%5D%7D%20%20%5Cright%5C%7D%7D%20%20%5C,%20da%0A"></p>
<p>from which one directly obtains that:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Clim_%7BN%20%5Cto%20%5Cinfty%7D%20%5C;%20-%5Cfrac%7B%5Clog%20%5Cmathcal%7BZ%7D(%5Cbeta)%7D%7BN%7D%0A%5C;%20=%20%5C;%0A%5Cmin_%7Ba%20%5Cin%20%5Cmathbb%7BR%7D%7D%20%5C;%20%5CBig%5C%7B%20%5Cbeta%20a%5E2%20-%20%5Clog%5B2%20%5C,%20%5Ccosh(2%20%5Cbeta%20a)%5D%20%5CBig%5C%7D.%0A"></p>
</section>
<section id="sherringtonkirkpatrick-model" class="level3">
<h3 class="anchored" data-anchor-id="sherringtonkirkpatrick-model">Sherrington–Kirkpatrick model</h3>
<p>Consider the distribution on <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5C%7B-1,1%5C%7D%5EN"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(x)%20=%20%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D(%5Cbeta)%7D%20%5C,%0A%5Cexp%20%7B%5Cleft%5C%7B%20%5Cfrac%2012%20%5C,%20%5Csum_%7Bi,j%7D%20W_%7Bij%7D%20x_i%20x_j%20%5Cright%5C%7D%7D%0A=%0A%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D(%5Cbeta)%7D%20%5C,%0A%5Cexp%20%7B%5Cleft%5C%7B%20%5Cfrac%2012%20%5C,%20%5Cleft%3C%20x,%20W%20x%20%5Cright%3E%20%5Cright%5C%7D%7D%0A"></p>
<p>where the <img src="https://latex.codecogs.com/png.latex?w_%7Bij%7D"> are some fixed weights with <img src="https://latex.codecogs.com/png.latex?w_%7Bij%7D%20=%20w_%7Bji%7D">. We assume that the matrix <img src="https://latex.codecogs.com/png.latex?W%20=%20%5BW_%7Bij%7D%5D_%7Bij%7D"> is positive definite: this can be achieved by adding <img src="https://latex.codecogs.com/png.latex?%5Clambda%20%5C,%20I_N"> to it if necessary, which does not change the distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi">. As described in <span class="citation" data-cites="zhang2012continuous">(Zhang et al. 2012)</span>, although I would not be surprised if similar ideas appeared in the physics literature much earlier, one can introduce a Gaussian auxiliary random variable <img src="https://latex.codecogs.com/png.latex?a%20=%20(a_1,%20%5Cldots,%20a_N)"> so that <img src="https://latex.codecogs.com/png.latex?%5Cpi(a%20%7C%20x)"> has mean <img src="https://latex.codecogs.com/png.latex?Fx"> and covariance <img src="https://latex.codecogs.com/png.latex?%5CGamma">. In other words,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(a%20%7C%20x)%20=%20%5Cfrac%7B1%7D%7B(2%20%5Cpi)%5E%7Bd/2%7D%20%5C,%20%7C%5CGamma%7C%5E%7B1/2%7D%7D%0A%5C,%20%5Cexp%20%7B%5Cleft%5C%7B%20-%5Cfrac%2012%20%5Cleft%3C%20(a%20-%20Fx),%20%5CGamma%5E%7B-1%7D%20(a%20-%20Fx)%20%5Cright%3E%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>In order to cancel-out the <img src="https://latex.codecogs.com/png.latex?%5Cleft%3C%20x,%20W,%20x%20%5Cright%3E"> it suffices to make sure that <img src="https://latex.codecogs.com/png.latex?F%5E%5Ctop%20%5C,%20%5CGamma%5E%7B-1%7D%20%5C,%20F%20=%20W">. There are a number of possibilities, the simplest approaches being perhaps</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A(F,%5CGamma)%20=%20(W%5E%7B1/2%7D,%20I_N)%0A%5Cquad%20%5Ctext%7Bor%7D%20%5Cquad%0A(F,%5CGamma)%20=%20(I,%20W%5E%7B-1%7D)%0A%5Cquad%20%5Ctext%7Bor%7D%20%5Cquad%0A(F,%5CGamma)%20=%20(W,%20W).%0A"></p>
<p>In any case, the joint distribution reads</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(x,a)%20%5C;%20=%20%5C;%20%5Cfrac%7B1%7D%7B%5Cmathcal%7BZ%7D%7D%20%5C,%20%5Cfrac%7B1%7D%7B(2%20%5Cpi)%5E%7Bd/2%7D%20%5C,%20%7C%5CGamma%7C%5E%7B1/2%7D%7D%20%5C,%0A%5Cexp%20%7B%5Cleft%5C%7B%20-%5Cfrac%7B1%7D%7B2%7D%20%5Cleft%3C%20a,%20%5CGamma%5E%7B-1%7D%20a%20%5Cright%3E%20+%20%5Cleft%3C%20x,%20F%5E%5Ctop%20%5CGamma%5E%7B-1%7D%20%5C,%20a%20%5Cright%3E%20%5Cright%5C%7D%7D%20.%0A"></p>
<p>Indeed, one can try to implement some Gibbs-style update in order to explain this joint distribution since both <img src="https://latex.codecogs.com/png.latex?%5Cpi(x%7Ca)"> and <img src="https://latex.codecogs.com/png.latex?%5Cpi(a%7Cx)"> are straightforward to sample from: it is indeed related <a href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine">Restricted Boltzmann Machine</a> models. One can also average-out the spins <img src="https://latex.codecogs.com/png.latex?x_i%20%5Cin%20%5C%7B-1,1%5C%7D"> and obtain that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BZ%7D=%20%5Cint_%7B%5Cmathbb%7BR%7D%5EN%7D%0A%5Cexp%20%7B%5Cleft%5C%7B%20%20%5Csum_%7Bi=1%7D%5EN%20%5Clog%20%7B%5Cleft(%202%20%5C,%20%5Ccosh(%5BF%5E%5Ctop%20%5CGamma%5E%7B-1%7D%20%5C,%20a%5D_i)%20%5Cright)%7D%20%20%5Cright%5C%7D%7D%20%20%5C,%20%5Cmathcal%7BD%7D_%7B%5CGamma%7D(da)%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7B%5CGamma%7D"> is the density of a centred Gaussian distribution with covariance <img src="https://latex.codecogs.com/png.latex?%5CGamma">. [<strong>TODO:</strong> add SMC experiments to estimate <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BZ%7D">].</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-zhang2012continuous" class="csl-entry">
Zhang, Yichuan, Zoubin Ghahramani, Amos J Storkey, and Charles Sutton. 2012. <span>“Continuous Relaxations for Discrete Hamiltonian Monte Carlo.”</span> <em>Advances in Neural Information Processing Systems</em> 25.
</div>
</div></section></div> ]]></description>
  <category>auxiliary-variable</category>
  <guid>https://alexxthiery.github.io/notes/auxiliary_variable_trick/auxiliary_variable_trick.html</guid>
  <pubDate>Mon, 02 Oct 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Shearer’s lemma</title>
  <link>https://alexxthiery.github.io/notes/information_theory_shearer_lemma/shearer_lemma.html</link>
  <description><![CDATA[ 




<p>The <a href="https://en.wikipedia.org/wiki/Shearer%27s_inequality">Shearer’s lemma</a> <span class="citation" data-cites="chung1986some">(Chung et al. 1986)</span> is concerned with a generalization of the sub-additivity of the Shannon Entropy,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AH(X_1,%20%5Cldots,%20X_N)%20%5C;%20%5Cleq%20%5C;%20H(X_1)%20+%20%5Cldots%20+%20H(X_N).%0A"></p>
<p>Instead, consider an integer <img src="https://latex.codecogs.com/png.latex?t%20%5Cgeq%201"> and a family <img src="https://latex.codecogs.com/png.latex?S_1,%20%5Cldots,%20S_K"> of subsets of <img src="https://latex.codecogs.com/png.latex?%5C%7B1,%20%5Cldots,%20N%5C%7D"> such that any index <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20n%20%5Cleq%20N"> appears in at least <img src="https://latex.codecogs.com/png.latex?t"> of these subsets. Note that for a subset <img src="https://latex.codecogs.com/png.latex?S_i%20=%20%5C%7B%20%5Calpha_1,%20%5Cldots,%20%5Calpha_%7Br_i%7D%5C%7D"> with <img src="https://latex.codecogs.com/png.latex?%5Calpha_1%20%3C%20%5Cldots%20%3C%20%5Calpha_%7Br_i%7D"> we have</p>
<p><span id="eq-add"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AH(X_%7BS_i%7D)%20&amp;%5Cequiv%20H(X_%7B%5Calpha_1%7D,%20%5Cldots,%20X_%7B%5Calpha_%7Br_i%7D%7D)%5C%5C%0A&amp;=%20H(X_%7B%5Calpha_1%7D)%20+%20H(X_%7B%5Calpha_2%7D%20%7C%20X_%7B%5Calpha_1%7D)%20+%20%5Cldots%20+%20H(X_%7B%5Calpha_%7Br_i%7D%7D%20%7C%20X_%7B%5Calpha_%7Br_i-1%7D%7D,%20%5Cldots,%20X_%7B%5Calpha_1%7D%20)%20%5C%5C%0A&amp;%5Cgeq%20H(X_%7B%5Calpha_1%7D)%20+%20H(X_%7B%5Calpha_2%7D%20%7C%20X_%7B1:(%5Calpha_2-1)%7D)%20+%20%5Cldots%20+%20H(X_%7B%5Calpha_%7Br_i%7D%7D%20%7C%20X_%7B1:%5Calpha_%7Br_i%7D-1%7D).%0A%5Cend%7Balign%7D%0A%5Ctag%7B1%7D"></span></p>
<p>Since each index appears in at least <img src="https://latex.codecogs.com/png.latex?t"> of the subsets, summing Equation&nbsp;1 over all the subset <img src="https://latex.codecogs.com/png.latex?S_i"> yields</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bi=1%7D%5EK%20H(X_%7BS_k%7D)%20%5Cgeq%20t%20%5C,%20%5Csum_%7Bi=1%7D%5EN%20H(X_i%20%5C,%20%7C%20X_%7B1:(i-1)%7D)%20=%20t%20%5C,%20H(X).%0A"></p>
<p>This means that the following inequality holds,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AH(X)%20%5Cleq%20%5Cfrac%7B1%7D%7Bt%7D%20%5C,%20%5Csum_%7Bk=1%7D%5EK%20H(X_%7BS_k%7D)%0A"></p>
<p>Indeed, the standard sub-additivity property of the entropy corresponds to the set <img src="https://latex.codecogs.com/png.latex?S_k%20=%20%5Bk%5D"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20k%20%5Cleq%20N"> and <img src="https://latex.codecogs.com/png.latex?t=1">.</p>
<section id="application-projection-on-hyperplanes" class="level3">
<h3 class="anchored" data-anchor-id="application-projection-on-hyperplanes">Application: projection on hyperplanes</h3>
<p>Consider a measurable set <img src="https://latex.codecogs.com/png.latex?A%20%5Csubset%20%5Cmathbb%7BR%7D%5En"> and call <img src="https://latex.codecogs.com/png.latex?A_k"> the projection of <img src="https://latex.codecogs.com/png.latex?A"> on the hyperplane <img src="https://latex.codecogs.com/png.latex?%5C%7Bx=(x_1,%20%5Cldots,%20x_n)%20%5Cin%20%5Cmathbb%7BR%7D%5En%20%5C,%20:%20%5C,%20x_k=0%5C%7D">. A Theorem of Loomis and Whitney <span class="citation" data-cites="loomis1949inequality">(Loomis and Whitney 1949)</span> states that the lebesgue measure <img src="https://latex.codecogs.com/png.latex?%7CA%7C"> of the set <img src="https://latex.codecogs.com/png.latex?A"> satisfies</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7CA%7C%20%5C;%20%5Cleq%20%5C;%20%5Cprod_%7Bk=1%7D%5En%20%7CA_k%7C%5E%7B1/(n-1)%7D.%0A"></p>
<p>In other words, if all the projections <img src="https://latex.codecogs.com/png.latex?A_k"> of the set <img src="https://latex.codecogs.com/png.latex?A"> are small then, necessarily, the set <img src="https://latex.codecogs.com/png.latex?A"> itself is small. To proceed, one can approximate this set <img src="https://latex.codecogs.com/png.latex?A"> with the union <img src="https://latex.codecogs.com/png.latex?A_%7B%5Cvarepsilon%7D"> of small cubes of side <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon"> centred on <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5C,%20%5Cmathbb%7BZ%7D%5En">. If one can prove the statement for <img src="https://latex.codecogs.com/png.latex?A%5E%7B%5B%5Cvarepsilon%5D%7D">, the results follows from a standard approximation argument (ie. outer measure). Now, each cube can be indexed with a <img src="https://latex.codecogs.com/png.latex?n">-uple of integers <img src="https://latex.codecogs.com/png.latex?(x_1,%20%5Cldots,%20x_n)%20%5Cin%20%5Cmathbb%7BZ%7D%5En">, and one can consider the random variable <img src="https://latex.codecogs.com/png.latex?X=(X_1,%20%5Cldots,%20X_n)"> that is uniformly distributed on the set of cubes coordinates. Because <img src="https://latex.codecogs.com/png.latex?2%5E%7BH(X)%7D%20=%20%7CA%5E%7B%5B%5Cvarepsilon%5D%7D%7C%20/%20%5Cvarepsilon%5En"> and <img src="https://latex.codecogs.com/png.latex?2%5E%7BH(X_2,%20%5Cldots,%20X_n)%7D%20=%20%7CA_1%5E%7B%5B%5Cvarepsilon%5D%7D%7C%20/%20%5Cvarepsilon%5En"> etc…, choosing the subsets <img src="https://latex.codecogs.com/png.latex?S_i=%5B1:n%5D%20%5Csetminus%20%5C%7Bi%5C%7D"> and <img src="https://latex.codecogs.com/png.latex?t%20=%20(n-1)"> in Shearer’s Lemma immediately gives the conclusion.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-chung1986some" class="csl-entry">
Chung, Fan RK, Ronald L Graham, Peter Frankl, and James B Shearer. 1986. <span>“Some Intersection Theorems for Ordered Sets and Graphs.”</span> <em>Journal of Combinatorial Theory, Series A</em> 43 (1). Academic Press: 23–37.
</div>
<div id="ref-loomis1949inequality" class="csl-entry">
Loomis, Lynn H, and Hassler Whitney. 1949. <span>“An Inequality Related to the Isoperimetric Inequality.”</span>
</div>
</div></section></div> ]]></description>
  <category>infoTheory</category>
  <guid>https://alexxthiery.github.io/notes/information_theory_shearer_lemma/shearer_lemma.html</guid>
  <pubDate>Sun, 01 Oct 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Information Theory: References and Readings</title>
  <link>https://alexxthiery.github.io/notes/information_theory_references/information_theory_references.html</link>
  <description><![CDATA[ 




<section id="books" class="level3">
<h3 class="anchored" data-anchor-id="books">Books</h3>
<ul>
<li>“Elements of information theory” by T. M. Cover and J. A. Thomas – <em>perfect intro book to the topic.</em></li>
<li>“<a href="https://www.inference.org.uk/mackay/itila/">Information Theory, Inference, and Learning Algorithms</a>” by David J.C. MacKay</li>
<li>“<a href="http://www.stat.yale.edu/~yw562/ln.html">Information Theory From Coding to Learning</a>” by Yury Polyanskiy and Yihong Wu</li>
</ul>
</section>
<section id="lecture-notes-articles" class="level3">
<h3 class="anchored" data-anchor-id="lecture-notes-articles">Lecture Notes &amp; Articles</h3>
<ul>
<li>“<a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">A Mathematical Theory of Communication</a>” by C. Shannon (2948) – <em>entertaining and readable, even 70+ years later!</em></li>
<li>“<a href="https://web.stanford.edu/class/stats311/book.html">Lecture Notes on Statistics and Information Theory</a>” by John Duchi</li>
<li>“<a href="http://www.stat.yale.edu/~yw562/teaching/598/handouts.html">Information-theoretic methods for high-dimensional statistics</a>” by Yihong Wu</li>
</ul>


</section>

 ]]></description>
  <category>infoTheory</category>
  <guid>https://alexxthiery.github.io/notes/information_theory_references/information_theory_references.html</guid>
  <pubDate>Fri, 29 Sep 2023 16:00:00 GMT</pubDate>
</item>
<item>
  <title>Shannon Source Coding Theorem</title>
  <link>https://alexxthiery.github.io/notes/information_theory_shannon_coding/information_theory_shannon_coding.html</link>
  <description><![CDATA[ 




<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/information_theory_shannon_coding/shannon.jpg" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Claude Shannon (1916 – 2001)</figcaption>
</figure>
</div>
</div>
<section id="transmission-through-a-noisy-channel" class="level3">
<h3 class="anchored" data-anchor-id="transmission-through-a-noisy-channel">Transmission through a noisy channel</h3>
<p>Consider a scenario involving a “noisy channel,” where a message <img src="https://latex.codecogs.com/png.latex?(x_1,x_2,%20%5Cldots)"> expressed in an alphabet <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D"> is transmitted before being received as a potentially different and corrupted message <img src="https://latex.codecogs.com/png.latex?(y_1,%20y_2,%5Cldots)"> expressed using a potentially different alphabet <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BY%7D">. One can assume that letter <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathcal%7BX%7D"> is transformed into <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cmathcal%7BY%7D"> with probability <img src="https://latex.codecogs.com/png.latex?p(x%20%5Cto%20y)"> so that the matrix <img src="https://latex.codecogs.com/png.latex?M_%7Bx,y%7D%20=%20%5Bp(x%20%5Cto%20y)%5D_%7B(x,y)%20%5Cin%20%5Cmathcal%7BX%7D%5Ctimes%20%5Cmathcal%7BY%7D%7D"> has rows summing-up to one, and that the “letters” of the message <img src="https://latex.codecogs.com/png.latex?(x_1%20x_2%20%5Cldots)"> are transmitted one by one and independently from each other (ie. there is no memory effect in the channel).</p>
<p>Now, imagine I have a text that needs to be transmitted through this channel. Assume that the text is represented using <img src="https://latex.codecogs.com/png.latex?N"> bits. My goal is to encode it in a way that introduces redundancy, utilizing the alphabet <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D">, so that it can be transmitted through the noisy channel. Eventually, I want it to be successfully decoded, allowing the original message to be recovered with minimal errors.</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/information_theory_shannon_coding/transmission.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">A Mathematical Theory of Communication</figcaption>
</figure>
</div>
</div>
<p>If transmitting each letter from the alphabet <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D"> takes <img src="https://latex.codecogs.com/png.latex?1"> unit of time, I need to estimate the overall time it will take to transmit the entire text of <img src="https://latex.codecogs.com/png.latex?N"> bits. In cases where the channel does not completely destroy the information, one can use about any non-idiotic encoding and transmit the same text multiple times to increase the chances of accurately recovering the original message. This can be achieved by employing techniques such as majority voting to decode the received messages, which are all distorted versions of the original text.</p>
<p>The transmission rate represents the inverse of the time required to transfer a single bit of information:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctextrm%7BR%20=%20(Transmission%20Rate)%7D%20=%20%5Cfrac%7B1%7D%7B%5Ctextrm%7B(average%20time%20it%20takes%20to%20transfer%20one%20bit)%7D%7D.%0A"></p>
<p>In other words, it takes about <img src="https://latex.codecogs.com/png.latex?N%20%5Ctimes%20R"> unit of times to transfer a text of <img src="https://latex.codecogs.com/png.latex?N"> bits. Moreover, the error rate refers to the percentage of errors in the decoded message, indicating the fraction of erroneous bits among the <img src="https://latex.codecogs.com/png.latex?N"> decoded bits. Naturally, there exists a tradeoff, and it is evident that one can reach a vanishing error rate if one is willing to allow an arbitrarily slow transmission rate (eg. majority voting after transmitting a very large number of times the same text). For example, if <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D=%20%5Cmathcal%7BY%7D=%20%5C%7B0,1%5C%7D"> and bits are flipped with probability <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctext%7Bflip%7D%7D%20%5Cll%201">, transmitting the text <img src="https://latex.codecogs.com/png.latex?(2K+1)"> times would lead to a transmission rate of <img src="https://latex.codecogs.com/png.latex?R%20=%201/(2K+1)"> and an error rate approximately equal to <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctext%7Bflip%7D%7D%5E%7BK+1%7D">.</p>
<p>The groundbreaking discovery made by Shannon is that it is possible to achieve a vanishing error rate even when transmitting at a finite transmission rate. He also managed to identify this optimal transmission rate. Shannon’s paper <span class="citation" data-cites="shannon1948mathematical">(Shannon 1948)</span> is beautifully written and surprisingly readable for a text written more than 50 years ago.</p>
</section>
<section id="vanishing-error-rate-shannon-codebooks" class="level3">
<h3 class="anchored" data-anchor-id="vanishing-error-rate-shannon-codebooks">Vanishing error rate: Shannon Codebooks</h3>
<p>Let’s imagine that we have a piece of information encoded in a variable, <img src="https://latex.codecogs.com/png.latex?X">. We send <img src="https://latex.codecogs.com/png.latex?X"> through a noisy channel, and at the other end we receive a somewhat distorted message, <img src="https://latex.codecogs.com/png.latex?Y">. So, how much of our original information actually was transmitted? To reconstruct our original message, <img src="https://latex.codecogs.com/png.latex?X">, using our received message, <img src="https://latex.codecogs.com/png.latex?Y">, we require an average of <img src="https://latex.codecogs.com/png.latex?H(X%7CY)"> additional bits of information. On average, <img src="https://latex.codecogs.com/png.latex?X"> contains <img src="https://latex.codecogs.com/png.latex?H(X)"> bits of information. So, if we encode <img src="https://latex.codecogs.com/png.latex?H(X)"> bits of useful information in <img src="https://latex.codecogs.com/png.latex?X">, the variable <img src="https://latex.codecogs.com/png.latex?Y"> that is correlated with <img src="https://latex.codecogs.com/png.latex?X"> still holds <img src="https://latex.codecogs.com/png.latex?I(X;Y)%20=%20H(X)%20-%20H(X%20%5C,%20%7C%20Y)"> bits of that original information. The quantity <img src="https://latex.codecogs.com/png.latex?I(X;Y)"> is the <a href="../../notes/information_theory_basics/information_theory_entropy.html">mutual information</a> between the random variables <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y">. In a noisy channel that transmits one “letter” at a time, the conditional probabilities <img src="https://latex.codecogs.com/png.latex?p(x%20%5Crightarrow%20y)"> are fixed. However, we can optimize the distribution of incoming messages. For instance, we can choose to transmit letters that are less likely to be corrupted. This discussion suggests that on average, transmitting <img src="https://latex.codecogs.com/png.latex?N"> symbols through the channel can provide up to <img src="https://latex.codecogs.com/png.latex?N%20%5Ctimes%20C"> bits of information, where <img src="https://latex.codecogs.com/png.latex?C%20=%20%5Cmax%20I(X;Y)">, the maximization being over the distribution of <img src="https://latex.codecogs.com/png.latex?X"> while keeping the conditional probabilities <img src="https://latex.codecogs.com/png.latex?p(x%20%5Crightarrow%20y)"> fixed. It may seem that this implies a noisy channel cannot transmit information at a rate higher than <img src="https://latex.codecogs.com/png.latex?C">. This hypothesis was precisely proven by Claude Shannon, who further established that this transmission rate can indeed be reached.</p>
<p>To prove that this transmission rate is achievable, Shannon’s idea was to simultaneously encode blocks of letters. To put it simply, consider the <img src="https://latex.codecogs.com/png.latex?2%5EN"> feasible blocks <img src="https://latex.codecogs.com/png.latex?%5C%7B%20t%5E%7B%5B1%5D%7D,%20%5Cldots,%20t%5E%7B%5B2%5EN%5D%7D%20%5C%7D"> of <img src="https://latex.codecogs.com/png.latex?N"> binary letters. Each block <img src="https://latex.codecogs.com/png.latex?t%5E%7B%5Bi%5D%7D%20%5Cin%20%5C%7B0,1%5C%7D%5EN"> has <img src="https://latex.codecogs.com/png.latex?N"> binary letters, <img src="https://latex.codecogs.com/png.latex?t%5E%7B%5Bi%5D%7D%20=%20(t_1%5E%7B%5Bi%5D%7D,%20%5Cldots,%20t_N%5E%7B%5Bi%5D%7D)">. Associate to each of block <img src="https://latex.codecogs.com/png.latex?t%5E%7B%5Bi%5D%7D%20%5Cin%20%5C%7B0,1%5C%7D%5EN"> a <strong>codeword</strong> <img src="https://latex.codecogs.com/png.latex?x%5E%7B%5Bi%5D%7D%20%5Cin%20%5Cmathcal%7BX%7D%5EK"> of size <img src="https://latex.codecogs.com/png.latex?K"> in the alphabet <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D">. The set of these <img src="https://latex.codecogs.com/png.latex?2%5EN"> codewords is usually called the <strong>codebook</strong>,</p>
<p><span id="eq-encode"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BC%7D=%20%5Cleft%5C%7B%20x%5E%7B%5B1%5D%7D,%20x%5E%7B%5B2%5D%7D,%20%5Cldots,%20x%5E%7B%5B2%5EN%5D%7D%20%5Cright%5C%7D%20%5C;%20%5Csubset%20%5Cmathcal%7BX%7D%5E%7BK%7D%0A%5Ctag%7B1%7D"></span></p>
<p>To transmit a block of <img src="https://latex.codecogs.com/png.latex?N"> letters from the original text, this block is first transformed into its associated codeword <img src="https://latex.codecogs.com/png.latex?x=(x_1,%20%5Cldots,%20x_K)%20%5Cin%20%5Cmathcal%7BX%7D%5EK">. This codeword is then sent through the noisy channel, resulting in a received message <img src="https://latex.codecogs.com/png.latex?(y_1,%20%5Cldots,%20y_K)%20%5Cin%20%5Cmathcal%7BY%7D%5EK">. The objective is to design a codebook with enough redundancy so that one can reconstruct the original codeword from the received message <img src="https://latex.codecogs.com/png.latex?(y_1,%20%5Cldots,%20y_K)">: the higher the ratio <img src="https://latex.codecogs.com/png.latex?K/N">, the larger the redundancy and the easier it should be to achieve this goal. The transmission rate is defined as <img src="https://latex.codecogs.com/png.latex?R%20=%20%5Cfrac%7BN%7D%7BK%7D"> since transmitting a binary text of length <img src="https://latex.codecogs.com/png.latex?N"> with vanishing errors takes <img src="https://latex.codecogs.com/png.latex?K"> units of time.</p>
<p>For generating the codebook in Equation&nbsp;1, Shannon adopted a simple approach consisting in generating each <img src="https://latex.codecogs.com/png.latex?x%5E%7B%5Bi%5D%7D_k"> for <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%5Cleq%202%5EN"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20k%20%5Cleq%20K"> independently at random from some (encoding) distribution <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctext%7Bcode%7D%7D(dx)">. The choice of this encoding distribution can be optimized at a later stage.</p>
<p>Consider the codeword <img src="https://latex.codecogs.com/png.latex?x%5E%7B%5B0%5D%7D%20=%20(x%5E%7B%5B0%5D%7D_1,%20%5Cldots,%20x%5E%7B%5B0%5D%7D_K)">. After being transmitted through the noisy channel, this gives rise to a message <img src="https://latex.codecogs.com/png.latex?y_%7B%5Cstar%7D">. The codeword <img src="https://latex.codecogs.com/png.latex?x%5E%7B%5B0%5D%7D"> can be easily recovered if <img src="https://latex.codecogs.com/png.latex?(x%5E%7B%5B0%5D%7D,%20y_%5Cstar)"> is typical while all the other pairs <img src="https://latex.codecogs.com/png.latex?(x%5E%7B%5Bi%5D%7D,%20y_%5Cstar)"> for <img src="https://latex.codecogs.com/png.latex?2%20%5Cleq%20i%20%5Cleq%202%5EN"> are atypical. Since there are about <img src="https://latex.codecogs.com/png.latex?2%5E%7BK%20%5C,%20H(X%20%7C%20Y)%7D"> elements <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cmathcal%7BX%7D%5EK"> such that <img src="https://latex.codecogs.com/png.latex?(x,%20y_%5Cstar)"> is typical, and each codeword was chosen approximately uniformly at random within its typical set of size <img src="https://latex.codecogs.com/png.latex?2%5E%7BK%20%5C,%20H(X)%7D">, the probability for a random codeword to be atypical is about</p>
<p><img src="https://latex.codecogs.com/png.latex?1-2%5E%7B-K%20%5C,%20%5BH(X)%20-%20H(X%7CY)%5D%7D%20=%201%20-%202%5E%7B-K%20%5C,%20I(X;Y)%7D"></p>
<p>Consequently, the probability that all the other pairs <img src="https://latex.codecogs.com/png.latex?(x%5E%7B%5Bi%5D%7D,%20y_%5Cstar)"> for <img src="https://latex.codecogs.com/png.latex?2%20%5Cleq%20i%20%5Cleq%202%5EN"> are atypical is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap_%7B%5Ctext%7Bsuccess%7D%7D%20=%20(1%20-%202%5E%7B-K%20%5C,%20I(X;Y)%7D)%5E%7B2%5EN-1%7D%20%5Capprox%20(1%20-%202%5E%7B-K%20%5C,%20I(X;Y)%7D)%5E%7B2%5E%7BKR%7D%7D.%0A"></p>
<p>The probability <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctext%7Bsuccess%7D%7D%20%5Cto%201"> as soon as <img src="https://latex.codecogs.com/png.latex?R%20%3C%20I(X;Y)"> as <img src="https://latex.codecogs.com/png.latex?N%20%5Cto%20%5Cinfty">. Furthermore, remembering that one were free to optimize the encoding distribution <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctext%7Bcode%7D%7D(dx)">, a vanishing error rate is possible as soon as the transmission <img src="https://latex.codecogs.com/png.latex?R"> rate is lower than</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7B(Channel%20Capacity)%7D%20=%20C%20%5Cequiv%20%5Cmax_%7Bp_%7B%5Ctext%7Bcode%7D%7D%7D%20%5C;%20I(X;Y).%0A"></p>
<p>To sum-up, consider <img src="https://latex.codecogs.com/png.latex?p_%7B%5Cmathcal%7BC%7D,%20%5Ctext%7Bsuccess%7D%7D"> the success rate of the codebook <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D">, ie. the probability that a random codeword of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D"> is successfully decoded when passing through the noisy channel. The reasoning above shows that the averaged success rate <img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctext%7Bsuccess%7D%7D%20=%20%5Cleft%3C%20p_%7B%5Cmathcal%7BC%7D,%20%5Ctext%7Bsuccess%7D%7D%20%5Cright%3E">, i.e.&nbsp;averaging <img src="https://latex.codecogs.com/png.latex?p_%7B%5Cmathcal%7BC%7D,%20%5Ctext%7Bsuccess%7D%7D"> over all possible codebooks <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D">, converges to one as long as the transmission rate is below the channel capacity <img src="https://latex.codecogs.com/png.latex?C">. This means that one can find at least one codebook that works well! This reasoning is an example of the “probabilistic method”… Indeed, one also expect <strong>most</strong> random codebook to work well!</p>
</section>
<section id="no-vanishing-error-below-the-channel-capacity" class="level3">
<h3 class="anchored" data-anchor-id="no-vanishing-error-below-the-channel-capacity">No vanishing error below the channel capacity</h3>
<p>To demonstrate that transmission at vanishing error-rate is impossible when the transmission rate exceeds the channel capacity, <img src="https://latex.codecogs.com/png.latex?C">, we can utilize <a href="../../notes/information_theory_fano/information_theory_fano.html">Fano’s inequality</a>.</p>
<p>Imagine selecting a message <img src="https://latex.codecogs.com/png.latex?M"> uniformly at random within <img src="https://latex.codecogs.com/png.latex?%5C%7B0,1%5C%7D%5EN"> and encode this message into the sequence <img src="https://latex.codecogs.com/png.latex?X=(X_1,%20...,%20X_K)%20%5Cin%20%5Cmathcal%7BX%7D%5EK">. We send <img src="https://latex.codecogs.com/png.latex?X"> through a channel with capacity <img src="https://latex.codecogs.com/png.latex?C"> and receive a corresponding, though somewhat distorted, signal <img src="https://latex.codecogs.com/png.latex?Y=(Y_1,%20...,%20Y_K)">. Finally, we decode this received message into <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BM%7D">, an estimate of our original message:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AM%20%5Crightarrow%20X%20%5Crightarrow%20Y%20%5Crightarrow%20%5Cwidehat%7BM%7D.%0A"></p>
<p>Fano’s inequality points out that the error probability, <img src="https://latex.codecogs.com/png.latex?p_E%20=%20%5Cmathbb%7BP%7D(%5Cwidehat%7BM%7D%20%5Cneq%20M)"> is such that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AH(M%20%7C%20%5Cwidehat%7BM%7D)%0A&amp;%5Cleq%201%20+%20p_E%20%5C,%20%5Clog_2(%5C#%20%5Ctextrm%7Bpossible%20values%20of%20%7D%20M)%5C%5C%0A&amp;=%201%20+%20p_E%20%5C,%20N%0A%5Cend%7Balign%7D%0A"></p>
<p>Applying the data-processing inequality to <img src="https://latex.codecogs.com/png.latex?M%20%5Crightarrow%20X%20%5Crightarrow%20Y%20%5Crightarrow%20%5Cwidehat%7BM%7D"> proves:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AN%20&amp;=%20H(M)%20=%20H(M%20%7C%20%5Cwidehat%7BM%7D)%20+%20I(M;%20%5Cwidehat%7BM%7D)%20%5C%5C%0A&amp;%20%5Cleq%20H(M%20%7C%20%5Cwidehat%7BM%7D)%20+%20I(X;%20Y)%5C%5C%0A&amp;%20%5Cleq%201%20+%20N%20%5C,%20p_E%20+%20I(X;%20Y).%0A%5Cend%7Balign%7D%0A"></p>
<p>To wrap up, recall that each received letter <img src="https://latex.codecogs.com/png.latex?Y_i"> in the message (Y_1, , Y_K)$ depends solely on the corresponding letter <img src="https://latex.codecogs.com/png.latex?X_i"> in the message sent through the channel. This <a href="../../notes/information_theory_basics/information_theory_entropy.html">implies</a> that <img src="https://latex.codecogs.com/png.latex?I(X;%20Y)%20%5Cleq%20%5Csum_%7Bi=1%7D%5EK%20I(X_i;%20Y_i)%20%5Cleq%20K%20%5C,%20C">.This yields:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AN%20%5Cleq%201%20+%20N%20%5C,%20p_E%20+%20K%20%5C,%20C.%0A"></p>
<p>This reveals that for the probability of error to go to zero, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?p_E%20%5Crightarrow%200"> as <img src="https://latex.codecogs.com/png.latex?N%20%5Crightarrow%20%5Cinfty">, the transmission rate <img src="https://latex.codecogs.com/png.latex?N/K"> must be lower than <img src="https://latex.codecogs.com/png.latex?C">.</p>
</section>
<section id="experiment" class="level3">
<h3 class="anchored" data-anchor-id="experiment">Experiment</h3>
<p>Consider the <a href="https://en.wikipedia.org/wiki/Binary_symmetric_channel">Binary Symmetric Channel (BSC)</a> that randomly flips <img src="https://latex.codecogs.com/png.latex?0%20%5Cmapsto%201"> and <img src="https://latex.codecogs.com/png.latex?1%20%5Cmapsto%200"> with equal probability <img src="https://latex.codecogs.com/png.latex?0%3Cq%3C1">. The capacity of this channel is easily computed and equals <img src="https://latex.codecogs.com/png.latex?C%20=%201%20-%20h_2(q)"> where <img src="https://latex.codecogs.com/png.latex?h_2(q)%20=%20-%5Bq%20%5C,%20%5Clog_2(q)%20+%20(1-q)%20%5C,%20%5Clog_2(1-q)%5D"> is the binary entropy function: the optimal encoding distribution is</p>
<p><img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctext%7Bcode%7D%7D(0)%20=%20p_%7B%5Ctext%7Bcode%7D%7D(2)%20=%201/2."></p>
<p>For a flipping rate of <img src="https://latex.codecogs.com/png.latex?q=0.1"> the channel capacity equals <img src="https://latex.codecogs.com/png.latex?C=0.53">. To estimate the performance of the random Shannon codebook strategy, I chose <img src="https://latex.codecogs.com/png.latex?N=13"> and several values of <img src="https://latex.codecogs.com/png.latex?K%20%5Cgeq%20N">. This means generating a random codebook <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D=%20%5C%7Bx%5E%7B%5B1%5D%7D,%20%5Cldots,%20x%5E%7B%5B2%5EN%5D%7D%5C%7D"> of size <img src="https://latex.codecogs.com/png.latex?2%5E%7B13%7D%20=%208192"> consisting of random binary vectors of size <img src="https://latex.codecogs.com/png.latex?K">. For a randomly chosen codeword <img src="https://latex.codecogs.com/png.latex?x%5E%7B%5Bi%5D%7D">, a received message <img src="https://latex.codecogs.com/png.latex?y_%5Cstar"> is generated by flipping each of the <img src="https://latex.codecogs.com/png.latex?K"> coordinates of <img src="https://latex.codecogs.com/png.latex?x%5E%7B%5Bi%5D%7D"> independently with probability <img src="https://latex.codecogs.com/png.latex?q">. In the BSC setting, it is easily seen that the codeword of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D"> that was the most likely to have originated <img src="https://latex.codecogs.com/png.latex?y_%7B%5Cstar%7D"> is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax_%5Cstar%20%5C;%20=%20%5C;%20%5Cmathop%7B%5Cmathrm%7Bargmin%7D%7D_%7Bx%20%5Cin%20%5Cmathcal%7BC%7D%7D%20%5C;%20%5C%7Cx%20-%20y_%5Cstar%5C%7C_%7BL%5E2%7D.%0A"></p>
<p>The nearest neighbor <img src="https://latex.codecogs.com/png.latex?x_%5Cstar"> can be relatively efficiently computed with a nearest-neighbor routine (eg. <a href="https://github.com/facebookresearch/faiss/wiki">FAISS</a>). The figure below reports the probability of error (i.e.&nbsp;“Block Error Rate”),</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7B(Block%20Error%20Rate)%7D%20%5C;%20=%20%5C;%20%5Cmathbb%7BP%7D(x_%5Cstar%20%5Cneq%20x%5E%7B%5Bi%5D%7D)%0A"></p>
<p>when the codeword <img src="https://latex.codecogs.com/png.latex?x%5E%7B%5Bi%5D%7D"> is chosen uniformly at random within the codebook.</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://alexxthiery.github.io/notes/information_theory_shannon_coding/shannon_code.png" class="img-fluid figure-img" style="width:80.0%"></p>
</figure>
</div>
</div>
<p>It can be seen that, although the error rate does go to zero for low transmission rate, the choice of <img src="https://latex.codecogs.com/png.latex?K%20=%20N%20/%20C"> where <img src="https://latex.codecogs.com/png.latex?C"> is the channel capacity still yields a relatively large block error rate. This indicates that the block size <img src="https://latex.codecogs.com/png.latex?N=13"> is still far too low for the “law of large number” arguments presented in the previous section to kick-in. I did try for <img src="https://latex.codecogs.com/png.latex?N=20"> and a codebook of <img src="https://latex.codecogs.com/png.latex?2%5E%7B20%7D%20%5Capprox%2010%5E6"> and the performace was still not impressive. This shows that even though the Shannon codebook approaches is an elegant construction, it is far from being practically useful. It requires a very large codebook of size <img src="https://latex.codecogs.com/png.latex?2%5EN"> and decoding requires doing a nearest-neighbors search that can become slow as <img src="https://latex.codecogs.com/png.latex?N"> increases.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-shannon1948mathematical" class="csl-entry">
Shannon, Claude Elwood. 1948. <span>“A Mathematical Theory of Communication.”</span> <em>The Bell System Technical Journal</em> 27 (3). Nokia Bell Labs: 379–423.
</div>
</div></section></div> ]]></description>
  <category>infoTheory</category>
  <guid>https://alexxthiery.github.io/notes/information_theory_shannon_coding/information_theory_shannon_coding.html</guid>
  <pubDate>Mon, 25 Sep 2023 16:00:00 GMT</pubDate>
</item>
</channel>
</rss>
