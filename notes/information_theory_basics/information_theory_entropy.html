<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Alexandre Thiéry - Information Theory: Entropy and Basic Definitions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<link href="../../data/dice.png" rel="icon" type="image/png">
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-HJ0JLEF802"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-HJ0JLEF802', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Alexandre Thiéry - Information Theory: Entropy and Basic Definitions">
<meta property="og:description" content="">
<meta property="og:image" content="https://alexxthiery.github.io/notes/information_theory_basics/shannon_destruction.gif">
<meta property="og:site-name" content="Alexandre Thiéry">
<meta name="twitter:title" content="Alexandre Thiéry - Information Theory: Entropy and Basic Definitions">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://alexxthiery.github.io/notes/information_theory_basics/shannon_destruction.gif">
<meta name="twitter:creator" content="@alexxthiery">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Alexandre Thiéry</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../people/index_people.html" rel="" target="">
 <span class="menu-text">Research Team</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../publications/index_pubs.html" rel="" target="">
 <span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching/teaching.html" rel="" target="">
 <span class="menu-text">Teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes/index_notes.html" rel="" target="">
 <span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about/about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../../notes/index_notes_as_list.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Information Theory: Entropy and Basic Definitions</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">infoTheory</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">23 09 2023</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">27 09 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#shannon-entropy-compression" id="toc-shannon-entropy-compression" class="nav-link active" data-scroll-target="#shannon-entropy-compression">Shannon Entropy &amp; Compression</a></li>
  <li><a href="#sequence-of-random-variables" id="toc-sequence-of-random-variables" class="nav-link" data-scroll-target="#sequence-of-random-variables">Sequence of random variables</a></li>
  <li><a href="#asymptotic-equipartition-property-aep" id="toc-asymptotic-equipartition-property-aep" class="nav-link" data-scroll-target="#asymptotic-equipartition-property-aep">Asymptotic Equipartition Property (AEP)</a></li>
  <li><a href="#mutual-information" id="toc-mutual-information" class="nav-link" data-scroll-target="#mutual-information">Mutual information</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shannon_destruction.gif" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption class="figure-caption">Claude Shannon (1916 – 2001)</figcaption>
</figure>
</div>
</div>
<section id="shannon-entropy-compression" class="level3">
<h3 class="anchored" data-anchor-id="shannon-entropy-compression">Shannon Entropy &amp; Compression</h3>
<p>If Alice chooses a number <span class="math inline">\(X\)</span> uniformly at random from the set <span class="math inline">\(\{1,2, \ldots, N\}\)</span>, Bob can use a simple “dichotomy” strategy to ask Alice <span class="math inline">\(\log_2(N)\)</span> binary Yes/No questions and correctly identify the number. This result can be generalized to non-uniform distributions (cf.&nbsp;<a href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman codes</a>, and also <a href="https://en.wikipedia.org/wiki/Kraft–McMillan_inequality">Kraft-McMillan inequality</a>). If Alice chooses a number <span class="math inline">\(X\)</span> from <span class="math inline">\(\{1,2, \ldots, N\}\)</span> with probabilities <span class="math inline">\(\mathbb{P}(X=k) = p_k\)</span>, Bob can design a deterministic strategy to find the answer using, on average, about</p>
<p><span id="eq-OU"><span class="math display">\[
H(X) = - \sum_{k=1}^N p_k \, \log_2(p_k)
\tag{1}\]</span></span></p>
<p>binary questions, ie. bits. To be more precise, there are strategies that require at most <span class="math inline">\(H(X) + 1\)</span> questions on average, and none that can require less than <span class="math inline">\(H(X)\)</span>. Note that applying this remark to an iid sequence <span class="math inline">\(X_{1:T} = (X_1, \ldots, X_T)\)</span> and using the the fact that <span class="math inline">\(H(X_1, \ldots, X_T) = T \, H(X)\)</span>, this shows that one can exactly determining the sequence <span class="math inline">\(X_{1:T}\)</span> with at most <span class="math inline">\(T \, H(X) + 1\)</span> binary questions on average. The quantity <span class="math inline">\(H(X)\)</span> defined in <a href="#eq-OU">Equation&nbsp;1</a>, known as the <strong>Shannon Entropy</strong> of the distribution <span class="math inline">\((p_1, \ldots, p_N)\)</span>, also implies that there are strategies that can encode each integer <span class="math inline">\(1 \leq x \leq N\)</span> as a binary string of length <span class="math inline">\(L(x)\)</span> (i.e.&nbsp;with <span class="math inline">\(L(x)\)</span> bits), with the expected length <span class="math inline">\(\mathbb{E}[L(X)]\)</span> approximately equal to <span class="math inline">\(H(X)\)</span>. It is because a sequence of binary questions can be thought of as a binary tree, etc…</p>
<p>This remark can be used for compression. Imagine a very long sequence <span class="math inline">\((X_1, \ldots, X_T)\)</span> of iid samples from <span class="math inline">\(X\)</span>. Encoding each <span class="math inline">\(X_i\)</span> with <span class="math inline">\(L(X_i)\)</span> bits, one should be able to encode the resulting sequence with</p>
<p><span class="math display">\[
L(X_1) + \ldots + L(X_T) \approx T \, \mathbb{E}[L(X)] \approx T \cdot H(X)
\]</span></p>
<p>bits. Can the usual <strong>zip compression</strong> algorithm do this? To test this, choose a probability distribution on <span class="math inline">\(\{1, \ldots, N\}\)</span>, generate an iid sequence of length <span class="math inline">\(T \gg 1\)</span>, compress this using the <span class="math inline">\(\texttt{gzip}(\ldots)\)</span> command, and finally look at the size of the resulting file (in bits). I have done that a few times, with a few different values of <span class="math inline">\(16 \leq N \leq 256\)</span> and a few random distributions on <span class="math inline">\(\{1, \ldots, N\}\)</span>, and with <span class="math inline">\(T = 10^6\)</span>. The plot of size of the compressed files versus the Shannon entropy <span class="math inline">\(H\)</span> looks as below:</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shannon_vs_zip.png" class="img-fluid figure-img" style="width:90.0%"></p>
</figure>
</div>
</div>
<p>Seems like the zip-algorithm works almost optimally for compressing iid sequences.</p>
</section>
<section id="sequence-of-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="sequence-of-random-variables">Sequence of random variables</h3>
<p>Now consider a pair of discrete random variables <span class="math inline">\((X,Y)\)</span>. If Alice draws samples from this pair of rvs, one can ask <span class="math inline">\(H(X,Y)\)</span> binary questions on average to exactly find out these values. To do that, one can ask <span class="math inline">\(H(X)\)</span> questions to estimate <span class="math inline">\(X\)</span>, and once <span class="math inline">\(X=x\)</span> is estimated, one can then ask about <span class="math inline">\(H(Y|X=x) = -\sum_y \mathbb{P}(Y=y|X=x) \, \log_2(\mathbb{P}(Y=y|X=x))\)</span> to estimate <span class="math inline">\(Y\)</span>. This strategy requires on average <span class="math inline">\(H(X) + \sum_x \mathbb{P}(X=x) \, H(Y|X=x)\)</span> binary questions and is actually optimal, showing that</p>
<p><span id="eq-chain-rule"><span class="math display">\[
H(X,Y) = H(X) + H(Y | X)
\tag{2}\]</span></span></p>
<p>where we have defined <span class="math inline">\(H(Y | X) = \sum_x \mathbb{P}(X=x) \, H(Y|X=x)\)</span>.</p>
<p>Indeed, one can generalize these concepts to more than two random variables. Iterating <a href="#eq-chain-rule">Equation&nbsp;2</a> shows that the trajectory <span class="math inline">\(X_{1:T} \equiv (X_1, \ldots, X_N)\)</span> of a stationary ergodic Markov chain can be estimated on average with <span class="math inline">\(H(X_{1:T})\)</span> binary questions where</p>
<p><span class="math display">\[
\begin{align}
H(X_{1:T})
&amp;= H(X_1) + H(X_2|X_1) + \ldots + H(X_{T} | X_{t-1})\\
&amp;\approx T \, H(X_{k+1} | X_k)\\
&amp;= - T \, \sum_x \pi(x) \, \sum_{y} p(x \to y) \, \log_2[p(x \to y)].
\end{align}
\]</span></p>
<p>Here, <span class="math inline">\(\pi(dx)\)</span> is the equilibrium distribution of the Markov chain and <span class="math inline">\(p(x \to y)\)</span> are the transition probabilities.</p>
<p>Can <span class="math inline">\(\texttt{gzip}(\ldots)\)</span> compress Markovian trajectories and roughly achieve that level of compression? Indeed, one can test this by generating random Markov transition matrices (that are, with very high probability, ergodic with respect to an equilibrium distribution <span class="math inline">\(\pi\)</span>). Doing this with trajectories of length <span class="math inline">\(10^4\)</span> (ie. quite short because it is quite slow to) on <span class="math inline">\(\{1, \ldots, N\}\)</span> with <span class="math inline">\(2 \leq N \leq 64\)</span>, one get the following results:</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shannon_vs_zip_markov.png" class="img-fluid figure-img" style="width:90.0%"></p>
</figure>
</div>
</div>
<p>In red is the entropy estimated without using the Markovian structure and assuming that the <span class="math inline">\(X_i\)</span> are iid samples. One can clearly see that the dependencies between successive samples are exploited for better compression. Nevertheless, the optimal compression rate given by the Shannon entropy is not reached. Indeed, <span class="math inline">\(\texttt{gzip}(\ldots)\)</span> is not an optimal algorithm – it cannot even compress well enough the sequence <span class="math inline">\((1,2,3,1,2,3,1,2,3,\ldots)\)</span>!</p>
</section>
<section id="asymptotic-equipartition-property-aep" class="level3">
<h3 class="anchored" data-anchor-id="asymptotic-equipartition-property-aep">Asymptotic Equipartition Property (AEP)</h3>
<p>The AEP is simple remark that gives a convenient way of reasoning about long sequences random variables <span class="math inline">\(X_{1:T} = (X_1, \ldots, X_T)\)</span> with <span class="math inline">\(T \gg 1\)</span>. For example, assuming that the random variables <span class="math inline">\(X_i\)</span> are independent and identically distributed as the random variable <span class="math inline">\(X\)</span>, the law of large numbers (LLN) gives that</p>
<p><span class="math display">\[
-\frac{1}{T} \, \log_2 p(X_{1:T}) = -\frac{1}{T} \, \sum \log_2 p(X_i) \approx H(X).
\]</span></p>
<p>This means that <strong>any</strong> “typical” sequence has a probability about <span class="math inline">\(2^{-T \, H(X)}\)</span> of occurring, which also means that there are about <span class="math inline">\(2^{T \, H(X)}\)</span> such “typical” sequences. Indeed, one could use the LLN for Markov chains and obtain a similar statement for Markovian sequences. All this can be made rigorous with large deviation theory, for example. This also establishes the link with the “statistical physics” definition of entropy as the logarithm of the number of configurations… Set of the type</p>
<p><span class="math display">\[
A_{\varepsilon} = \left\{ x_{1:T} \; : \; \left| -\frac{1}{T} \, \log_2 p(x_{1:T}) - H(X)\right| &lt; \varepsilon\right\}
\]</span></p>
<p>are usually called <strong>typical set</strong>: for any <span class="math inline">\(\varepsilon&gt; 0\)</span>, the probability of <span class="math inline">\(X_{1:T}\)</span> to belongs to <span class="math inline">\(A_{\varepsilon}\)</span> goes to one as <span class="math inline">\(T \to \infty\)</span>. For these reasons, it is often a good heuristic to think of a draw of <span class="math inline">\((X_1, \ldots, X_T)\)</span> as a uniformly distributed on the associated typical set. For example, if <span class="math inline">\((X_1, \ldots, X_N)\)</span> are <span class="math inline">\(N\)</span> iid draws from a Bernoulli distribution with <span class="math inline">\(\mathbb{P}(X=1) = 1-\mathbb{P}(X=0) =p\)</span>, the set <span class="math inline">\(A \subset \{0,1\}^N\)</span> of sequences such that <span class="math inline">\(x_1 + \ldots + x_N = Np\)</span> has <span class="math inline">\(\binom{N}{Np} \approx 2^{N \, h_2(q)}\)</span> elements where <span class="math inline">\(h_2(q) = -[q \, \log_2(q) + (1-q) \, \log_2(q)]\)</span> is the entropy of a <span class="math inline">\(\text{Bern}(q)\)</span> random variable.</p>
</section>
<section id="mutual-information" class="level3">
<h3 class="anchored" data-anchor-id="mutual-information">Mutual information</h3>
<p>Consider a pair of random variables <span class="math inline">\((X,Y)\)</span>. Assuming that <span class="math inline">\(X\)</span> stores (on average) <span class="math inline">\(H(X)\)</span> bits of useful information, how much of this information can be extracted from <span class="math inline">\(Y\)</span>? Let us call this quantity <span class="math inline">\(I(X;Y)\)</span> since we will see in a second that this quantity is symmetric. If <span class="math inline">\(Y\)</span> is independent from <span class="math inline">\(X\)</span>, no useful information about <span class="math inline">\(X\)</span> is contained in <span class="math inline">\(Y\)</span> and <span class="math inline">\(I(X;Y) = 0\)</span>. On the contrary, if <span class="math inline">\(X=Y\)</span>, the knowledge of <span class="math inline">\(Y\)</span> already contains all the information about <span class="math inline">\(Y\)</span> and <span class="math inline">\(I(X;Y) = H(X) = H(Y)\)</span>. If one knows <span class="math inline">\(Y\)</span>, one needs on average <span class="math inline">\(H(X|Y)\)</span> binary questions (ie. bits of additional information) in order to determine <span class="math inline">\(X\)</span> certainly and recover all the information contained in <span class="math inline">\(X\)</span>. This means that the knowledge of <span class="math inline">\(Y\)</span> already contains <span class="math inline">\(I(X;Y) = H(X) - H(X|Y)\)</span> useful bits of information about <span class="math inline">\(X\)</span>! This quantity is called the <strong>mutual information</strong> of the two random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and it has the good taste of being symmetric:</p>
<p><span class="math display">\[
\begin{align}
I(X;Y) &amp;= H(X) - H(X|Y) \\
&amp;= H(X) + H(Y) - H(X,Y).
\end{align}
\]</span></p>
<p>Naturally, one can define conditional version of it by setting <span class="math inline">\(I(X;Y \, |Z) = \sum_{z} \mathbb{P}(Z=z) \, I(X_z | Y_z)\)</span> where <span class="math inline">\((X_z, Y_z)\)</span> has the law of <span class="math inline">\((X,Z)\)</span> conditioned on <span class="math inline">\(Z=z\)</span>. Since <span class="math inline">\(I(X;Y \,|Z)\)</span> is the reduction in uncertainty of <span class="math inline">\(X\)</span> due to <span class="math inline">\(Y\)</span> when <span class="math inline">\(Z\)</span> is given, there are indeed situations when <span class="math inline">\(I(X;Y \, | Z)\)</span> is larger than <span class="math inline">\(I(X;Y)\)</span> – it is to be contrasted to the intuitive inequality <span class="math inline">\(H(X|Z) \leq H(X)\)</span>, which is indeed true. A standard such examples is when <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent <span class="math inline">\(\text{Bern}(1/2)\)</span> random variables and <span class="math inline">\(Z = X+Y\)</span>: a short computation gives that <span class="math inline">\(I(X;Y \, | Z) = 1/2\)</span> while, indeed, <span class="math inline">\(I(X;Y) = 0\)</span>. This definition of conditional mutual information leads to a chain-rule property,</p>
<p><span class="math display">\[
I(X; (Y_1,Y_2)) = I(X;Y_1) + I(X;Y_2 | Y_1),
\]</span></p>
<p>which can indeed be generalized to any number of variables. Furthermore, if the <span class="math inline">\(Y_i\)</span> are conditionally independent given <span class="math inline">\(X\)</span> (eg. if <span class="math inline">\(X=(X_1, \ldots, X_T)\)</span> and <span class="math inline">\(Y_i\)</span> only depend on <span class="math inline">\(X_i\)</span>), then the sub-additivity of the entropy readily gives that</p>
<p><span class="math display">\[
I(X; (Y_1, \ldots, Y_N)) \leq \sum_{i=1}^N I(X; Y_i).
\]</span></p>
<p>Importantly, algebra shows that <span class="math inline">\(I(X;Y)\)</span> can also be expressed as the <a href="https://en.wikipedia.org/wiki/Kullback–Leibler_divergence">Kullback-Leibler divergence</a> between the joint distribution <span class="math inline">\(\mathbb{P}_{(X,Y)}\)</span> and the product of the marginals <span class="math inline">\(\mathbb{P}_X \otimes \mathbb{P}_Y\)</span>,</p>
<p><span class="math display">\[
I(X;Y) \; = \;
D_{\text{KL}} {\left(  (X,Y) \, \| \, X \otimes Y \right)} .
\]</span></p>
<p>This diagram from <span class="citation" data-cites="mackay2003information">(<a href="#ref-mackay2003information" role="doc-biblioref">MacKay 2003</a>)</span> nicely illustrate the different fundamental quantities <span class="math inline">\(H(X)\)</span> and <span class="math inline">\(H(X,Y)\)</span> and <span class="math inline">\(H(Y|X)\)</span> and <span class="math inline">\(I(X;Y)\)</span> and <span class="math inline">\(H(X,Y)\)</span>:</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./entropies.png" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption class="figure-caption">From: <a href="http://www.inference.org.uk/mackay/itila/book.html">Information Theory, Inference, and Learning Algorithms</a></figcaption>
</figure>
</div>
</div>
<p>Naturally, if one considers three random variables <span class="math inline">\(X \mapsto Y \mapsto Z\)</span> forming a “Markov chain”, we have the so-called <strong>data-processing</strong> inequality,</p>
<p><span class="math display">\[
I(X;Z) \leq I(X;Y)
\qquad \text{and} \qquad
I(X;Z) \leq I(Y;Z).
\]</span></p>
<p>The first inequality is clear since all the useful information contained in <span class="math inline">\(Z\)</span> must be coming from <span class="math inline">\(Y\)</span>, and <span class="math inline">\(Y\)</span> only contains <span class="math inline">\(I(X;Y)\)</span> bits about <span class="math inline">\(X\)</span>. For the second inequality, note that if <span class="math inline">\(Z\)</span> contains <span class="math inline">\(I(Y;Z)\)</span> bits about <span class="math inline">\(Y\)</span>, and <span class="math inline">\(Y\)</span> contains <span class="math inline">\(H(Y;X)\)</span> bits about <span class="math inline">\(X\)</span>, then <span class="math inline">\(Z\)</span> cannot contain more than <span class="math inline">\(I(Y;Z)\)</span> bits of <span class="math inline">\(X\)</span>:</p>
<div style="text-align:center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./mutual.jpg" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption class="figure-caption">Data Processing Inequality for Markov <span class="math inline">\(X \to Y \to Z\)</span></figcaption>
</figure>
</div>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-mackay2003information" class="csl-entry" role="listitem">
MacKay, David JC. 2003. <em>Information Theory, Inference and Learning Algorithms</em>. Cambridge university press.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="alexxthiery/blogComments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>