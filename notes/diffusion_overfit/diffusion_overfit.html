<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>TODO TODO – Alexandre Thiéry</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<link href="../../data/dice.png" rel="icon" type="image/png">
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-2c49a34178fbd84a6f1fb51273f28ac8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-HJ0JLEF802"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-HJ0JLEF802', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="TODO TODO – Alexandre Thiéry">
<meta property="og:description" content="Alex Thiery Notes">
<meta property="og:site_name" content="Alexandre Thiéry">
<meta name="twitter:title" content="TODO TODO – Alexandre Thiéry">
<meta name="twitter:description" content="Alex Thiery Notes">
<meta name="twitter:creator" content="@alexxthiery">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Alexandre Thiéry</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../people/index_people.html"> 
<span class="menu-text">Research Team</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../publications/index_pubs.html"> 
<span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching/teaching.html"> 
<span class="menu-text">Teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes/index_notes.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about/about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../../notes/index_notes_as_list.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">TODO TODO</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">diffusion</div>
                <div class="quarto-category">replica</div>
                <div class="quarto-category">generative</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">05 12 2025</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">05 12 2025</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#why-diffusion-models-generalize-before-they-memorize" id="toc-why-diffusion-models-generalize-before-they-memorize" class="nav-link active" data-scroll-target="#why-diffusion-models-generalize-before-they-memorize">Why Diffusion Models Generalize Before They Memorize</a>
  <ul class="collapse">
  <li><a href="#score-matching-at-fixed-diffusion-time" id="toc-score-matching-at-fixed-diffusion-time" class="nav-link" data-scroll-target="#score-matching-at-fixed-diffusion-time">1. Score matching at fixed diffusion time</a>
  <ul class="collapse">
  <li><a href="#data-and-diffusion" id="toc-data-and-diffusion" class="nav-link" data-scroll-target="#data-and-diffusion">1.1 Data and diffusion</a></li>
  </ul></li>
  <li><a href="#random-features-score-model" id="toc-random-features-score-model" class="nav-link" data-scroll-target="#random-features-score-model">2. Random-features score model</a>
  <ul class="collapse">
  <li><a href="#pre-activations-and-features" id="toc-pre-activations-and-features" class="nav-link" data-scroll-target="#pre-activations-and-features">2.1 Pre-activations and features</a></li>
  <li><a href="#random-features-score-network" id="toc-random-features-score-network" class="nav-link" data-scroll-target="#random-features-score-network">2.2 Random-features score network</a></li>
  </ul></li>
  <li><a href="#from-the-loss-to-a-linear-ode-in-a" id="toc-from-the-loss-to-a-linear-ode-in-a" class="nav-link" data-scroll-target="#from-the-loss-to-a-linear-ode-in-a">3. From the loss to a linear ODE in <span class="math inline">\(A\)</span></a></li>
  <li><a href="#training-dynamics-in-the-eigenbasis-of-u" id="toc-training-dynamics-in-the-eigenbasis-of-u" class="nav-link" data-scroll-target="#training-dynamics-in-the-eigenbasis-of-u">4. Training dynamics in the eigenbasis of <span class="math inline">\(U\)</span></a></li>
  <li><a href="#the-spectral-problem-what-do-we-actually-need" id="toc-the-spectral-problem-what-do-we-actually-need" class="nav-link" data-scroll-target="#the-spectral-problem-what-do-we-actually-need">5. The spectral problem: what do we actually need?</a></li>
  <li><a href="#the-resolvent-stieltjes-transform-of-u" id="toc-the-resolvent-stieltjes-transform-of-u" class="nav-link" data-scroll-target="#the-resolvent-stieltjes-transform-of-u">6. The resolvent (Stieltjes transform) of <span class="math inline">\(U\)</span></a></li>
  <li><a href="#gaussian-equivalence-and-hermite-expansion" id="toc-gaussian-equivalence-and-hermite-expansion" class="nav-link" data-scroll-target="#gaussian-equivalence-and-hermite-expansion">7. Gaussian equivalence and Hermite expansion</a>
  <ul class="collapse">
  <li><a href="#gaussian-pre-activations" id="toc-gaussian-pre-activations" class="nav-link" data-scroll-target="#gaussian-pre-activations">7.1 Gaussian pre-activations</a></li>
  <li><a href="#hermite-expansion-of-the-nonlinearity" id="toc-hermite-expansion-of-the-nonlinearity" class="nav-link" data-scroll-target="#hermite-expansion-of-the-nonlinearity">7.2 Hermite expansion of the nonlinearity</a></li>
  </ul></li>
  <li><a href="#why-we-need-a-log-determinant-representation" id="toc-why-we-need-a-log-determinant-representation" class="nav-link" data-scroll-target="#why-we-need-a-log-determinant-representation">8. Why we need a log-determinant representation</a>
  <ul class="collapse">
  <li><a href="#from-the-resolvent-to-logdet" id="toc-from-the-resolvent-to-logdet" class="nav-link" data-scroll-target="#from-the-resolvent-to-logdet">8.1 From the resolvent to <span class="math inline">\(\log\det\)</span></a></li>
  <li><a href="#from-logdet-to-a-gaussian-partition-function" id="toc-from-logdet-to-a-gaussian-partition-function" class="nav-link" data-scroll-target="#from-logdet-to-a-gaussian-partition-function">8.2 From <span class="math inline">\(\log\det\)</span> to a Gaussian partition function</a></li>
  </ul></li>
  <li><a href="#quenched-vs-annealed-averages-and-the-replica-idea" id="toc-quenched-vs-annealed-averages-and-the-replica-idea" class="nav-link" data-scroll-target="#quenched-vs-annealed-averages-and-the-replica-idea">9. Quenched vs annealed averages and the replica idea</a></li>
  <li><a href="#disorder-average-and-overlap-order-parameters" id="toc-disorder-average-and-overlap-order-parameters" class="nav-link" data-scroll-target="#disorder-average-and-overlap-order-parameters">10. Disorder average and overlap order parameters</a></li>
  <li><a href="#asymptotic-spectrum-and-separation-of-scales" id="toc-asymptotic-spectrum-and-separation-of-scales" class="nav-link" data-scroll-target="#asymptotic-spectrum-and-separation-of-scales">11. Asymptotic spectrum and separation of scales</a></li>
  <li><a href="#from-spectral-edges-to-training-time-scales" id="toc-from-spectral-edges-to-training-time-scales" class="nav-link" data-scroll-target="#from-spectral-edges-to-training-time-scales">12. From spectral edges to training time scales</a></li>
  <li><a href="#conceptual-summary" id="toc-conceptual-summary" class="nav-link" data-scroll-target="#conceptual-summary">13. Conceptual summary</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<!-- ::: {style="text-align:center;"}
![Overdamped Langevin Diffusion as a gradient flow on the space of probability distributions endowed with the $2$-Wasserstein metric](./langevin.gif){fig-align="center" width=95%}
::: -->
<section id="why-diffusion-models-generalize-before-they-memorize" class="level1">
<h1>Why Diffusion Models Generalize Before They Memorize</h1>
<p><em>A Spectral Story via Random Features and Random Matrix Theory</em></p>
<p>Diffusion models trained by score matching behave in a way that is empirically very robust but theoretically nontrivial: they reach good generation early, and only much later start to memorize individual training samples. The goal of this note is to explain this phenomenon <strong>purely from training dynamics</strong>, within a random-features model where all relevant quantities can be analyzed analytically.</p>
<p>We will see that the key mechanism is <strong>spectral</strong>: the training operator has two distinct eigenvalue scales, corresponding to “population” and “sample-specific” directions. Gradient descent learns the population modes quickly, and the sample-specific modes only on a time scale that grows with the dataset size <span class="math inline">\(n\)</span>. Memorization is therefore dynamically delayed.</p>
<hr>
<section id="score-matching-at-fixed-diffusion-time" class="level2">
<h2 class="anchored" data-anchor-id="score-matching-at-fixed-diffusion-time">1. Score matching at fixed diffusion time</h2>
<p>We begin by specifying the data and the score-matching objective.</p>
<section id="data-and-diffusion" class="level3">
<h3 class="anchored" data-anchor-id="data-and-diffusion">1.1 Data and diffusion</h3>
<p>Let <span class="math inline">\(x \in \mathbb{R}^d\)</span> be a data vector drawn from a distribution <span class="math display">\[
x \sim P_x
\quad\text{with}\quad
\mathbb{E}[x] = 0,
\qquad
\mathbb{E}[x x^\top] = \Sigma.
\]</span></p>
<p>The forward diffusion process is defined by <span id="eq-forward-diffusion"><span class="math display">\[
x(t,\xi)
=
e^{-t} x
+
\sqrt{\Delta_t}\,\xi,
\qquad
\Delta_t = 1-e^{-2t},
\qquad
\xi \sim \mathcal{N}(0,I_d).
\tag{1}\]</span></span></p>
<p>The marginal distribution of <span class="math inline">\(x(t,\xi)\)</span> is a smoothed version of <span class="math inline">\(P_x\)</span>; its density is denoted <span class="math inline">\(P_t\)</span>. The (Stein) score at time <span class="math inline">\(t\)</span> is <span class="math display">\[
s^\star_t(x) = \nabla_x \log P_t(x).
\]</span></p>
<p>A parametric model <span class="math inline">\(s_\theta(x,t)\)</span> is trained to approximate <span class="math inline">\(s^\star_t(x)\)</span> via denoising score matching. At fixed <span class="math inline">\(t\)</span>, the population objective is <span id="eq-population-loss"><span class="math display">\[
\mathcal{L}_t(\theta)
=
\frac{1}{d}
\mathbb{E}_{x\sim P_x}
\mathbb{E}_{\xi\sim\mathcal{N}(0,I_d)}
\Big\|
\sqrt{\Delta_t}\, s_\theta(x(t,\xi),t) + \xi
\Big\|^2.
\tag{2}\]</span></span></p>
<p>Given a training set <span class="math inline">\(\{x_\nu\}_{\nu=1}^n\)</span>, the empirical loss is <span id="eq-empirical-loss"><span class="math display">\[
L_{\mathrm{train}}(\theta)
=
\frac{1}{dn}
\sum_{\nu=1}^n
\mathbb{E}_\xi
\Big\|
\sqrt{\Delta_t}\, s_\theta(x_\nu(t,\xi),t) + \xi
\Big\|^2.
\tag{3}\]</span></span></p>
<p>We are interested in the <strong>training dynamics</strong> of <span class="math inline">\(L_{\mathrm{train}}(\theta)\)</span> for a particular model class.</p>
<hr>
</section>
</section>
<section id="random-features-score-model" class="level2">
<h2 class="anchored" data-anchor-id="random-features-score-model">2. Random-features score model</h2>
<p>Directly analyzing a deep U-Net is out of reach. We therefore simplify the architecture while preserving three key ingredients:</p>
<ol type="1">
<li>Score matching at a fixed diffusion time.</li>
<li>Overparameterization.</li>
<li>Nonlinearity in the data.</li>
</ol>
<p>This leads to the <strong>random-features score model</strong>.</p>
<section id="pre-activations-and-features" class="level3">
<h3 class="anchored" data-anchor-id="pre-activations-and-features">2.1 Pre-activations and features</h3>
<p>Let <span class="math display">\[
W \in \mathbb{R}^{p\times d}
\]</span> have i.i.d. entries <span class="math inline">\(W_{ij} \sim \mathcal{N}(0,1)\)</span>. For an input <span class="math inline">\(x\in\mathbb{R}^d\)</span> we define:</p>
<ul>
<li>The pre-activation vector <span id="eq-preactivation"><span class="math display">\[
h(x) = \frac{W x}{\sqrt{d}} \in \mathbb{R}^p,
\tag{4}\]</span></span></li>
<li>The feature vector <span id="eq-feature-map"><span class="math display">\[
\varphi(x) = \sigma\big(h(x)\big) \in \mathbb{R}^p,
\tag{5}\]</span></span> where <span class="math inline">\(\sigma:\mathbb{R}\to\mathbb{R}\)</span> is a fixed nonlinearity applied coordinate-wise.</li>
</ul>
<p>For a noisy training sample <span class="math inline">\(x_\nu(t,\xi)\)</span>, we will write <span id="eq-feature-noisy"><span class="math display">\[
\varphi_\nu(\xi)
=
\varphi\big(x_\nu(t,\xi)\big)
=
\sigma\!\left(\frac{W x_\nu(t,\xi)}{\sqrt{d}}\right).
\tag{6}\]</span></span></p>
</section>
<section id="random-features-score-network" class="level3">
<h3 class="anchored" data-anchor-id="random-features-score-network">2.2 Random-features score network</h3>
<p>The score model is linear in the feature space: <span id="eq-score-model"><span class="math display">\[
s_A(x)
=
\frac{A}{\sqrt{p}} \,\varphi(x),
\qquad
A \in \mathbb{R}^{d\times p}.
\tag{7}\]</span></span></p>
<p>Here:</p>
<ul>
<li><span class="math inline">\(W\)</span> is random and fixed (first layer),</li>
<li><span class="math inline">\(A\)</span> is trainable (second layer),</li>
<li>The overall model remains nonlinear in <span class="math inline">\(x\)</span> through <span class="math inline">\(\sigma\)</span>.</li>
</ul>
<p>Plugging <a href="#eq-score-model" class="quarto-xref">Equation&nbsp;7</a> into the empirical loss <a href="#eq-empirical-loss" class="quarto-xref">Equation&nbsp;3</a> at fixed time <span class="math inline">\(t\)</span> yields <span id="eq-loss-A"><span class="math display">\[
L_{\mathrm{train}}(A)
=
\frac{1}{dn}
\sum_{\nu=1}^n
\mathbb{E}_\xi
\Big\|
\sqrt{\Delta_t}\, \frac{A}{\sqrt{p}} \,\varphi_\nu(\xi)
+ \xi
\Big\|^2.
\tag{8}\]</span></span></p>
<p>At this point, the loss is <strong>quadratic in <span class="math inline">\(A\)</span></strong>, which is the main structural simplification: gradient descent dynamics will be linear in <span class="math inline">\(A\)</span>.</p>
<hr>
</section>
</section>
<section id="from-the-loss-to-a-linear-ode-in-a" class="level2">
<h2 class="anchored" data-anchor-id="from-the-loss-to-a-linear-ode-in-a">3. From the loss to a linear ODE in <span class="math inline">\(A\)</span></h2>
<p>We briefly compute the gradient of <a href="#eq-loss-A" class="quarto-xref">Equation&nbsp;8</a> with respect to <span class="math inline">\(A\)</span>.</p>
<p>Write, for each <span class="math inline">\((\nu,\xi)\)</span>, <span class="math display">\[
\ell_{\nu,\xi}(A)
=
\frac{1}{d}
\Big\|
\sqrt{\Delta_t}\, \frac{A}{\sqrt{p}} \,\varphi_\nu(\xi)
+ \xi
\Big\|^2.
\]</span></p>
<p>Expanding the square, <span class="math display">\[
\ell_{\nu,\xi}(A)
=
\frac{\Delta_t}{dp}\, \varphi_\nu(\xi)^\top A^\top A \,\varphi_\nu(\xi)
+
\frac{2\sqrt{\Delta_t}}{d\sqrt{p}}\, \xi^\top A \,\varphi_\nu(\xi)
+
\frac{1}{d}\|\xi\|^2.
\]</span></p>
<p>The gradient with respect to <span class="math inline">\(A\)</span> is <span class="math display">\[
\nabla_A \ell_{\nu,\xi}(A)
=
\frac{2\Delta_t}{dp}\, A \,\varphi_\nu(\xi)\varphi_\nu(\xi)^\top
+
\frac{2\sqrt{\Delta_t}}{d\sqrt{p}}\, \xi \,\varphi_\nu(\xi)^\top.
\]</span></p>
<p>Averaging over <span class="math inline">\(\nu\)</span> and <span class="math inline">\(\xi\)</span> yields <span id="eq-gradient-A"><span class="math display">\[
\nabla_A L_{\mathrm{train}}(A)
=
\frac{2\Delta_t}{d}\, A U
+
\frac{2}{\sqrt{\Delta_t p}}\, V^\top,
\tag{9}\]</span></span> where we have defined</p>
<ul>
<li>the <strong>Gram matrix</strong> <span id="eq-U-def"><span class="math display">\[
U
=
\frac{1}{n}
\sum_{\nu=1}^n
\mathbb{E}_\xi
\big[
    \varphi_\nu(\xi)\varphi_\nu(\xi)^\top
\big]
\in \mathbb{R}^{p\times p},
\tag{10}\]</span></span></li>
<li>and the “noise-feature” coupling <span id="eq-V-def"><span class="math display">\[
V
=
\frac{1}{n}
\sum_{\nu=1}^n
\mathbb{E}_\xi
\big[
    \varphi_\nu(\xi)\xi^\top
\big]
\in \mathbb{R}^{p\times d}.
\tag{11}\]</span></span></li>
</ul>
<p>The stochastic gradient descent update is then <span class="math display">\[
A^{(k+1)}
=
A^{(k)}
-
\eta \,\nabla_A L_{\mathrm{train}}(A^{(k)}).
\]</span></p>
<p>In the small learning-rate regime, taking a continuum limit with rescaled time <span class="math display">\[
\tau = \frac{k\eta}{d^2},
\]</span> gives the ODE <span id="eq-A-ODE"><span class="math display">\[
\frac{d}{d\tau} A(\tau)
=
- \frac{2\Delta_t}{d}\,A(\tau)\,U
-
\frac{2}{\sqrt{\Delta_t p}}\,V^\top.
\tag{12}\]</span></span></p>
<p>This is a linear ODE in <span class="math inline">\(A\)</span>, driven by the random matrix <span class="math inline">\(U\)</span>.</p>
<hr>
</section>
<section id="training-dynamics-in-the-eigenbasis-of-u" class="level2">
<h2 class="anchored" data-anchor-id="training-dynamics-in-the-eigenbasis-of-u">4. Training dynamics in the eigenbasis of <span class="math inline">\(U\)</span></h2>
<p>To understand the time scales of learning, we diagonalize the Gram matrix: <span class="math display">\[
U = Q \Lambda Q^\top,
\]</span> where <span class="math display">\[
\Lambda = \operatorname{diag}(\lambda_1,\dots,\lambda_p).
\]</span></p>
<p>Let <span class="math display">\[
B(\tau) = A(\tau) Q \in \mathbb{R}^{d\times p},
\qquad
\tilde V^\top = V^\top Q.
\]</span></p>
<p>Then <a href="#eq-A-ODE" class="quarto-xref">Equation&nbsp;12</a> becomes <span id="eq-B-ODE"><span class="math display">\[
\frac{d}{d\tau} B(\tau)
=
- \frac{2\Delta_t}{d}\,B(\tau)\,\Lambda
-
\frac{2}{\sqrt{\Delta_t p}}\, \tilde V^\top.
\tag{13}\]</span></span></p>
<p>For each column <span class="math inline">\(b_i(\tau) \in \mathbb{R}^d\)</span> of <span class="math inline">\(B(\tau)\)</span>, we have a scalar-matrix ODE: <span class="math display">\[
\frac{d}{d\tau} b_i(\tau)
=
- \frac{2\Delta_t}{d}\,\lambda_i\, b_i(\tau)
-
\frac{2}{\sqrt{\Delta_t p}}\, \tilde v_i,
\]</span> where <span class="math inline">\(\tilde v_i\)</span> is the <span class="math inline">\(i\)</span>-th column of <span class="math inline">\(\tilde V^\top\)</span>. This is solved explicitly as <span class="math display">\[
b_i(\tau)
=
e^{-(2\Delta_t\lambda_i/d)\,\tau}\, b_i(0)
-
\frac{2}{\sqrt{\Delta_t p}} \int_0^\tau
e^{-(2\Delta_t\lambda_i/d)\,(\tau-s)}
\tilde v_i\, ds.
\]</span></p>
<p>Ignoring the forcing term for the moment (or thinking about relaxation to equilibrium), the homogeneous solution decays as <span class="math display">\[
\|b_i(\tau)\|
\approx
\|b_i(0)\| \exp\!\Big(-\frac{2\Delta_t\lambda_i}{d}\,\tau\Big).
\]</span></p>
<p>This suggests defining the <strong>relaxation time</strong> associated with eigenvalue <span class="math inline">\(\lambda\)</span>: <span id="eq-relax-time"><span class="math display">\[
\tau(\lambda)
=
\frac{d}{2\Delta_t\lambda}
=
\frac{\psi_p}{\Delta_t \lambda}.
\tag{14}\]</span></span></p>
<p>Modes with larger <span class="math inline">\(\lambda\)</span> are learned faster; modes with small <span class="math inline">\(\lambda\)</span> take longer to relax. Therefore:</p>
<blockquote class="blockquote">
<p>The time scale at which different directions in parameter space (and thus in function space) are learned is determined by the eigenvalues of the Gram matrix <span class="math inline">\(U\)</span>.</p>
</blockquote>
<p>Our task reduces to understanding the <strong>spectrum</strong> of <span class="math inline">\(U\)</span> in the high-dimensional limit.</p>
<hr>
</section>
<section id="the-spectral-problem-what-do-we-actually-need" class="level2">
<h2 class="anchored" data-anchor-id="the-spectral-problem-what-do-we-actually-need">5. The spectral problem: what do we actually need?</h2>
<p>Let <span class="math display">\[
\lambda_1,\dots,\lambda_p
\]</span> be the eigenvalues of <span class="math inline">\(U\)</span>. The empirical spectral density is <span class="math display">\[
\rho_U^{(p)}(\lambda)
=
\frac{1}{p} \sum_{i=1}^p \delta(\lambda - \lambda_i).
\]</span></p>
<p>We are interested in its limit <span class="math inline">\(\rho_U(\lambda)\)</span> as <span class="math inline">\(d,p,n\to\infty\)</span> with <span class="math inline">\(\psi_p,\psi_n\)</span> fixed.</p>
<p>More specifically, we need:</p>
<ol type="1">
<li>The <strong>location and scale</strong> of the bulk (or bulks) of <span class="math inline">\(\rho_U(\lambda)\)</span>.</li>
<li>The <strong>behavior of the smallest nonzero eigenvalues</strong> <span class="math inline">\(\lambda_{\min}\)</span>, since <a href="#eq-relax-time" class="quarto-xref">Equation&nbsp;14</a> shows that <span class="math display">\[
\tau_{\mathrm{slow}} \sim \frac{\psi_p}{\Delta_t \lambda_{\min}}.
\]</span></li>
</ol>
<p>The central question thus becomes:</p>
<blockquote class="blockquote">
<p><strong>What is the limiting spectral distribution of <span class="math inline">\(U\)</span>, and how do its bulk edges scale with <span class="math inline">\(\psi_p = p/d\)</span> and <span class="math inline">\(\psi_n = n/d\)</span>?</strong></p>
</blockquote>
<hr>
</section>
<section id="the-resolvent-stieltjes-transform-of-u" class="level2">
<h2 class="anchored" data-anchor-id="the-resolvent-stieltjes-transform-of-u">6. The resolvent (Stieltjes transform) of <span class="math inline">\(U\)</span></h2>
<p>Random matrix theory typically approaches such questions via the <strong>resolvent</strong> or <strong>Stieltjes transform</strong>: <span id="eq-resolvent-def"><span class="math display">\[
q(z)
=
\frac{1}{p}\operatorname{Tr}(U - zI)^{-1}
=
\frac{1}{p}\sum_{i=1}^p \frac{1}{\lambda_i - z},
\qquad
z \in \mathbb{C}\setminus \mathbb{R}.
\tag{15}\]</span></span></p>
<p>The spectral density is recovered from <span class="math inline">\(q(z)\)</span> via <span id="eq-density-resolvent"><span class="math display">\[
\rho_U(\lambda)
=
\frac{1}{\pi}
\lim_{\varepsilon\to 0^+}
\operatorname{Im}\, q(\lambda + i\varepsilon).
\tag{16}\]</span></span></p>
<p>In classical settings, for instance when <span class="math display">\[
U = \frac{1}{n} X X^\top
\]</span> with i.i.d. entries, the resolvent <span class="math inline">\(q(z)\)</span> satisfies an explicit deterministic equation (the Marchenko–Pastur equation), from which one obtains the limiting spectral density. More generally, with a population covariance <span class="math inline">\(\Sigma\)</span>, one obtains the Silverstein–Pastur equations.</p>
<p>In our case, however, <span class="math inline">\(U\)</span> is <strong>not</strong> a linear sample covariance: its entries involve the nonlinear map <span class="math inline">\(\sigma\)</span>, the random weights <span class="math inline">\(W\)</span>, and the diffusion noise. No classical closed-form resolvent equation is available.</p>
<p>We will now outline how to obtain such an equation using two key tools:</p>
<ol type="1">
<li><strong>Gaussian equivalence</strong> for the pre-activations.</li>
<li><strong>Replica methods</strong> applied to a Gaussian integral representation of the resolvent.</li>
</ol>
<hr>
</section>
<section id="gaussian-equivalence-and-hermite-expansion" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-equivalence-and-hermite-expansion">7. Gaussian equivalence and Hermite expansion</h2>
<p>We first simplify the distribution of the pre-activations <span class="math inline">\(h(x) = W x / \sqrt{d}\)</span>.</p>
<section id="gaussian-pre-activations" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-pre-activations">7.1 Gaussian pre-activations</h3>
<p>For fixed <span class="math inline">\(x\)</span>, each coordinate <span class="math inline">\(h_i(x) = W_i x / \sqrt{d}\)</span> is Gaussian with <span class="math display">\[
\mathbb{E}[h_i(x)] = 0,
\qquad
\mathbb{E}[h_i(x)^2] = \frac{1}{d}\|x\|^2.
\]</span></p>
<p>For two inputs <span class="math inline">\(x,x'\)</span>, the pair <span class="math inline">\((h_i(x), h_i(x'))\)</span> is Gaussian with covariance <span class="math display">\[
\mathbb{E}[h_i(x) h_i(x')]
=
\frac{1}{d} x^\top x'.
\]</span></p>
<p>For our noisy data <span class="math inline">\(x_\nu(t,\xi)\)</span>, this gives a Gaussian field <span class="math inline">\(H\)</span> of pre-activations: <span class="math display">\[
H_{\nu i}(\xi) = h_i(x_\nu(t,\xi))
\]</span> with a covariance that can be expressed in terms of <span class="math inline">\(\Sigma\)</span> and <span class="math inline">\(t\)</span>.</p>
<p><strong>Gaussian equivalence</strong> (in this context) means that for many observables of interest (including spectral densities and generalization errors), we may replace the exact distribution of the feature matrix by a Gaussian process with the same covariance structure. Intuitively, the pre-activations are sums of many independent contributions, and high-dimensional central limit/type universality effects apply.</p>
</section>
<section id="hermite-expansion-of-the-nonlinearity" class="level3">
<h3 class="anchored" data-anchor-id="hermite-expansion-of-the-nonlinearity">7.2 Hermite expansion of the nonlinearity</h3>
<p>Once the inputs to <span class="math inline">\(\sigma\)</span> are Gaussian, the nonlinearity can be analyzed via its Hermite expansion: <span id="eq-hermite-expansion"><span class="math display">\[
\sigma(z) = \sum_{k=0}^\infty \frac{\alpha_k}{k!} \mathrm{He}_k(z),
\tag{17}\]</span></span> where <span class="math inline">\(\{\mathrm{He}_k\}\)</span> are the (probabilists’) Hermite polynomials orthogonal with respect to <span class="math inline">\(\mathcal{N}(0,1)\)</span>.</p>
<p>For a pair <span class="math inline">\((Z_1,Z_2)\)</span> of jointly Gaussian variables with correlation <span class="math inline">\(\rho\)</span>, <span id="eq-hermite-covariance"><span class="math display">\[
\mathbb{E}[\sigma(Z_1)\sigma(Z_2)]
=
\sum_{k=0}^\infty \alpha_k^2 \rho^k.
\tag{18}\]</span></span></p>
<p>In our setting, <span class="math inline">\(\rho\)</span> will be a function of inner products like <span class="math inline">\(x_\nu^\top x_{\nu'}/d\)</span> and of the diffusion time <span class="math inline">\(t\)</span>.</p>
<p>The key point is that the entries of <span class="math inline">\(U\)</span> in <a href="#eq-U-def" class="quarto-xref">Equation&nbsp;10</a> can be expressed in terms of a <strong>finite number of Gaussian expectations</strong> such as <a href="#eq-hermite-covariance" class="quarto-xref">Equation&nbsp;18</a>. These are summarized in four scalar parameters <span class="math display">\[
a_t,\; b_t,\; v_t,\; s_t,
\]</span> which are explicit integrals involving <span class="math inline">\(\sigma\)</span>, the noise level <span class="math inline">\(t\)</span>, and the data covariance <span class="math inline">\(\Sigma\)</span>.</p>
<p>As a result, the Gram matrix can be decomposed schematically as <span id="eq-U-structure"><span class="math display">\[
U
\approx
s_t^2 I_p
+
\underbrace{\frac{v_t^2}{n}\sum_{\nu=1}^n g_\nu g_\nu^\top}_{\text{sample-specific part}}
+
\underbrace{\tilde U}_{\text{population part}},
\tag{19}\]</span></span> where:</p>
<ul>
<li><span class="math inline">\(g_\nu \in \mathbb{R}^p\)</span> are Gaussian vectors (the high-dimensional limit of the features),</li>
<li><span class="math inline">\(\tilde U\)</span> is a deterministic “population covariance” term depending on <span class="math inline">\(\Sigma\)</span> and <span class="math inline">\((a_t,b_t,v_t,s_t)\)</span>.</li>
</ul>
<p>Equation <a href="#eq-U-structure" class="quarto-xref">Equation&nbsp;19</a> is schematic but captures the structure: <span class="math inline">\(U\)</span> is a sum of a diagonal term, a sample-specific random covariance, and a population-level covariance. All randomness is now <strong>Gaussian</strong>, and the nonlinearity is encoded in a small number of scalars.</p>
<hr>
</section>
</section>
<section id="why-we-need-a-log-determinant-representation" class="level2">
<h2 class="anchored" data-anchor-id="why-we-need-a-log-determinant-representation">8. Why we need a log-determinant representation</h2>
<p>We want to compute the <strong>quenched resolvent</strong> <span class="math display">\[
\bar q(z)
=
\mathbb{E}_{W,X} q(z)
=
\frac{1}{p}\,\mathbb{E}_{W,X}
\operatorname{Tr}(U - zI)^{-1}.
\]</span></p>
<p>Directly averaging <span class="math inline">\((U-zI)^{-1}\)</span> over the randomness of <span class="math inline">\(U\)</span> is intractable:</p>
<ul>
<li>The inverse is a highly nonlinear function of all matrix entries.</li>
<li><span class="math inline">\(U\)</span> has complicated correlations due to its nonlinear construction.</li>
<li>Classical results (e.g.&nbsp;for <span class="math inline">\(XX^\top\)</span>-type matrices) do not apply.</li>
</ul>
<p>A standard tool in random matrix theory and statistical physics is to transform the <strong>inverse</strong> problem into a <strong>log-determinant</strong> problem, which can then be represented as a Gaussian integral.</p>
<section id="from-the-resolvent-to-logdet" class="level3">
<h3 class="anchored" data-anchor-id="from-the-resolvent-to-logdet">8.1 From the resolvent to <span class="math inline">\(\log\det\)</span></h3>
<p>For any invertible matrix <span class="math inline">\(M\)</span>, we have <span class="math display">\[
\frac{\partial}{\partial z} \log \det(M - zI)
=
- \operatorname{Tr}(M - zI)^{-1}.
\]</span></p>
<p>Applying this with <span class="math inline">\(M = U\)</span>, we obtain <span id="eq-q-logdet"><span class="math display">\[
q(z)
=
\frac{1}{p} \operatorname{Tr}(U - zI)^{-1}
=
- \frac{1}{p} \frac{\partial}{\partial z} \log \det(U - zI).
\tag{20}\]</span></span></p>
<p>Therefore, understanding <span class="math inline">\(\bar q(z)\)</span> is equivalent to understanding the average of <span class="math inline">\(\log\det(U - zI)\)</span>.</p>
</section>
<section id="from-logdet-to-a-gaussian-partition-function" class="level3">
<h3 class="anchored" data-anchor-id="from-logdet-to-a-gaussian-partition-function">8.2 From <span class="math inline">\(\log\det\)</span> to a Gaussian partition function</h3>
<p>A second standard identity is the Gaussian integral <span class="math display">\[
\int_{\mathbb{R}^p}
\exp\!\left( -\tfrac12 x^\top M x \right) dx
\propto
(\det M)^{-1/2},
\quad
M \succ 0.
\]</span></p>
<p>Define the <strong>partition function</strong> <span id="eq-Z-def"><span class="math display">\[
Z(z;U)
=
\int_{\mathbb{R}^p}
\exp\!\left(
- \tfrac12 \varphi^\top (U - zI)\varphi
\right) d\varphi.
\tag{21}\]</span></span></p>
<p>Then <span class="math display">\[
\log Z(z;U)
=
-\tfrac12 \log \det(U - zI) + \text{const},
\]</span> and using <a href="#eq-q-logdet" class="quarto-xref">Equation&nbsp;20</a>: <span id="eq-q-logZ"><span class="math display">\[
q(z)
=
\frac{1}{p} \frac{\partial}{\partial z} \log Z(z;U).
\tag{22}\]</span></span></p>
<p>We have therefore transformed the resolvent into the derivative of the log of a <strong>Gaussian integral</strong>.</p>
<hr>
</section>
</section>
<section id="quenched-vs-annealed-averages-and-the-replica-idea" class="level2">
<h2 class="anchored" data-anchor-id="quenched-vs-annealed-averages-and-the-replica-idea">9. Quenched vs annealed averages and the replica idea</h2>
<p>We are interested in the <strong>typical</strong> behavior of the spectrum, corresponding to the <strong>quenched</strong> average of <span class="math inline">\(\log Z\)</span>: <span class="math display">\[
\mathbb{E}\log Z(z;U).
\]</span></p>
<p>This differs from the <strong>annealed</strong> quantity <span class="math inline">\(\log \mathbb{E}Z(z;U)\)</span>. In large systems, <span class="math inline">\(\log Z\)</span> behaves like a sum of many weakly dependent contributions and concentrates around its mean (self-averaging). Therefore, the typical spectral density corresponds to <span class="math display">\[
\frac{1}{p} \mathbb{E}\log Z(z;U),
\]</span> not to <span class="math inline">\(-\frac{1}{p}\log \mathbb{E}Z\)</span>.</p>
<p>The <strong>replica trick</strong> provides a formal way to compute <span class="math inline">\(\mathbb{E}\log Z\)</span>: <span id="eq-replica-identity"><span class="math display">\[
\mathbb{E}\log Z
=
\lim_{s\to0}
\frac{1}{s}
\log \mathbb{E}Z^s.
\tag{23}\]</span></span></p>
<p>For integer <span class="math inline">\(s\)</span>, <span id="eq-Zs-integral"><span class="math display">\[
Z^s
=
\int \prod_{a=1}^s d\varphi^a
\exp\!\left(
- \frac12 \sum_{a=1}^s \varphi^{a\top}(U - zI)\varphi^a
\right).
\tag{24}\]</span></span></p>
<p>The integrand now depends on <span class="math inline">\(U\)</span> only through quadratic forms in the replicated fields <span class="math inline">\(\varphi^a\)</span>.</p>
<hr>
</section>
<section id="disorder-average-and-overlap-order-parameters" class="level2">
<h2 class="anchored" data-anchor-id="disorder-average-and-overlap-order-parameters">10. Disorder average and overlap order parameters</h2>
<p>We now average <span class="math inline">\(Z^s\)</span> over the randomness of <span class="math inline">\(U\)</span>, which itself comes from the randomness of <span class="math inline">\(W\)</span> and the data <span class="math inline">\(\{x_\nu\}\)</span>. Using the Gaussian structure <a href="#eq-U-structure" class="quarto-xref">Equation&nbsp;19</a> and the fact that everything is quadratic in <span class="math inline">\(\varphi^a\)</span>, the disorder average can be performed explicitly.</p>
<p>A standard step is to introduce <strong>overlap matrices</strong>: <span class="math display">\[
Q_{ab}
=
\frac{1}{p} \varphi^{a\top}\varphi^b,
\qquad
a,b=1,\dots,s,
\]</span> and related order parameters coupling to the population covariance <span class="math inline">\(\Sigma\)</span>. After integrating out the Gaussian fields and applying Gaussian equivalence, one arrives at an effective representation <span id="eq-effective-action"><span class="math display">\[
\mathbb{E}Z^s
=
\int dQ \, dR \, dS \;
\exp\big(
p\,\mathcal{S}(Q,R,S; z)
\big),
\tag{25}\]</span></span> where:</p>
<ul>
<li><span class="math inline">\(Q,R,S\)</span> are finite-dimensional order parameters summarizing the overlaps,</li>
<li><span class="math inline">\(\mathcal{S}\)</span> is an explicit “effective action” depending on <span class="math inline">\(\psi_p,\psi_n,\rho_\Sigma,a_t,b_t,v_t,s_t\)</span>.</li>
</ul>
<p>In the limit <span class="math inline">\(p\to\infty\)</span>, the integral <a href="#eq-effective-action" class="quarto-xref">Equation&nbsp;25</a> is dominated by <strong>saddle points</strong> of <span class="math inline">\(\mathcal{S}\)</span>, that is by solutions of <span class="math display">\[
\frac{\partial \mathcal{S}}{\partial Q} = 0,
\quad
\frac{\partial \mathcal{S}}{\partial R} = 0,
\quad
\frac{\partial \mathcal{S}}{\partial S} = 0.
\]</span></p>
<p>Assuming replica symmetry (no reason for symmetry breaking in this setting), the saddle point can be parametrized by <strong>three scalar functions</strong> of <span class="math inline">\(z\)</span>:</p>
<ul>
<li>the resolvent <span class="math inline">\(q(z)\)</span>,</li>
<li>a projected resolvent <span class="math inline">\(r(z)\)</span> aligned with <span class="math inline">\(\Sigma\)</span>,</li>
<li>an auxiliary scalar <span class="math inline">\(s(z)\)</span>.</li>
</ul>
<p>These satisfy a closed system of <strong>self-consistent equations</strong> of the form <span id="eq-fixed-point"><span class="math display">\[
q(z) = F_q(q,r,s;z),
\quad
r(z) = F_r(q,r,s;z),
\quad
s(z) = F_s(q,r,s;z).
\tag{26}\]</span></span></p>
<p>One can think of <a href="#eq-fixed-point" class="quarto-xref">Equation&nbsp;26</a> as a generalized Silverstein–Pastur equation for this nonlinear random-feature ensemble.</p>
<p>The paper’s main theorem can be read as giving an explicit expression for <span class="math inline">\(F_q,F_r,F_s\)</span> in terms of <span class="math inline">\(\rho_\Sigma\)</span> and the scalars <span class="math inline">\(a_t,b_t,v_t,s_t\)</span>. The resolvent <span class="math inline">\(q(z)\)</span> is then defined implicitly as the solution of <a href="#eq-fixed-point" class="quarto-xref">Equation&nbsp;26</a>.</p>
<hr>
</section>
<section id="asymptotic-spectrum-and-separation-of-scales" class="level2">
<h2 class="anchored" data-anchor-id="asymptotic-spectrum-and-separation-of-scales">11. Asymptotic spectrum and separation of scales</h2>
<p>The self-consistent equations <a href="#eq-fixed-point" class="quarto-xref">Equation&nbsp;26</a> are complicated in general but become tractable in the regime <span class="math display">\[
\psi_p = \frac{p}{d} \gg 1,
\qquad
\psi_n = \frac{n}{d} \gg 1.
\]</span></p>
<p>By expanding the equations in this limit, one finds:</p>
<ol type="1">
<li><p>A <strong>population bulk</strong> <span class="math inline">\(\rho_2(\lambda)\)</span> at scale <span class="math display">\[
\lambda = O(\psi_p),
\]</span> essentially matching the spectrum of the population covariance <span class="math inline">\(\tilde U = \mathbb{E}[U]\)</span>.</p></li>
<li><p>A <strong>sample-specific bulk</strong> <span class="math inline">\(\rho_1(\lambda)\)</span> at the smaller scale <span class="math display">\[
\lambda = O(\psi_p/\psi_n).
\]</span></p></li>
<li><p>A residual delta mass at <span class="math inline">\(\lambda = s_t^2\)</span>, which does not affect training/test losses.</p></li>
</ol>
<p>More concretely, the support of <span class="math inline">\(\rho_1(\lambda)\)</span> behaves (schematically) like <span class="math display">\[
\text{supp}(\rho_1)
\approx
\Big[
s_t^2 + c_1 v_t^2 (1 - \sqrt{\psi_p/\psi_n})^2,\;
s_t^2 + c_1 v_t^2 (1 + \sqrt{\psi_p/\psi_n})^2
\Big],
\]</span> for some constant <span class="math inline">\(c_1\)</span> depending on the details of the model. The important point is the <strong>scaling</strong>: <span class="math display">\[
\lambda_{\min}^{(1)}
\sim O\!\left(\frac{\psi_p}{\psi_n}\right),
\qquad
\lambda_{\max}^{(2)}
\sim O(\psi_p).
\]</span></p>
<p>Thus, in the large-(_p,_n$ limit, the spectrum splits into two well-separated bulks.</p>
<hr>
</section>
<section id="from-spectral-edges-to-training-time-scales" class="level2">
<h2 class="anchored" data-anchor-id="from-spectral-edges-to-training-time-scales">12. From spectral edges to training time scales</h2>
<p>Recall from <a href="#eq-relax-time" class="quarto-xref">Equation&nbsp;14</a> that the relaxation time associated with an eigenvalue <span class="math inline">\(\lambda\)</span> is <span class="math display">\[
\tau(\lambda)
=
\frac{\psi_p}{\Delta_t \lambda}.
\]</span></p>
<p>Using the scaling of the two bulks:</p>
<ul>
<li><p>For population modes ((_2$, <span class="math inline">\(\lambda \sim O(\psi_p)\)</span>): <span class="math display">\[
\tau_{\mathrm{gen}}
\sim
\frac{\psi_p}{\Delta_t\,O(\psi_p)}
= O(1).
\]</span></p></li>
<li><p>For sample-specific modes ((_1$, <span class="math inline">\(\lambda_{\min} \sim O(\psi_p/\psi_n)\)</span>): <span class="math display">\[
\tau_{\mathrm{mem}}
\sim
\frac{\psi_p}{\Delta_t \lambda_{\min}}
\sim
\frac{\psi_p}{\Delta_t (\psi_p/\psi_n)}
=
\frac{\psi_n}{\Delta_t}
\propto n.
\]</span></p></li>
</ul>
<p>We obtain two distinct time scales:</p>
<ul>
<li>A <strong>fast</strong>, dataset-size–independent time <span class="math inline">\(\tau_{\mathrm{gen}}\)</span> at which population-level structure is learned.</li>
<li>A <strong>slow</strong>, dataset-size–proportional time <span class="math inline">\(\tau_{\mathrm{mem}}\)</span> at which sample-specific details (and thus memorization) are learned.</li>
</ul>
<p>This matches the empirical observation: diffusion models generalize early and memorize only later, with the onset of memorization pushed out as <span class="math inline">\(n\)</span> increases.</p>
<hr>
</section>
<section id="conceptual-summary" class="level2">
<h2 class="anchored" data-anchor-id="conceptual-summary">13. Conceptual summary</h2>
<p>Starting from diffusion score matching and a random-features score model, we:</p>
<ol type="1">
<li>Expressed the training dynamics as a linear ODE in the readout weights <span class="math inline">\(A\)</span>.</li>
<li>Identified the Gram matrix <span class="math inline">\(U\)</span> as the operator controlling time scales.</li>
<li>Used Gaussian equivalence and Hermite expansions to reduce <span class="math inline">\(U\)</span> to a Gaussian random matrix parameterized by a few scalars.</li>
<li>Introduced the resolvent and expressed it via a log-determinant and a Gaussian partition function.</li>
<li>Used replica and saddle-point techniques to derive self-consistent equations for the resolvent.</li>
<li>Analyzed these equations in the overparameterized regime and found a <strong>two-bulk spectrum</strong>, with eigenvalues at scales <span class="math inline">\(O(\psi_p)\)</span> and <span class="math inline">\(O(\psi_p/\psi_n)\)</span>.</li>
<li>Translated the spectral edges into relaxation times, obtaining: <span class="math display">\[
\tau_{\mathrm{gen}} = O(1),
\qquad
\tau_{\mathrm{mem}} = O(n).
\]</span></li>
</ol>
<p>The key insight is that <strong>dynamical regularization</strong> arises from a <strong>spectral separation between population modes and sample-specific modes</strong>:</p>
<ul>
<li>Large eigenvalues correspond to directions that capture the underlying data distribution; they are learned quickly.</li>
<li>Small eigenvalues correspond to directions that encode high-frequency, dataset-specific variations; they are learned slowly, on a time scale that grows with <span class="math inline">\(n\)</span>.</li>
</ul>
<p>Even when the model is expressive enough to memorize the data, and even in the absence of explicit regularization, the spectral geometry of the training operator creates a <strong>large window of training time where the model has generalized but not yet memorized</strong>.</p>
<p>This is the spectral mechanism behind “generalization before memorization” in (this random-feature approximation of) diffusion models.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/alexxthiery\.github\.io");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="alexxthiery/blogComments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->




</body></html>